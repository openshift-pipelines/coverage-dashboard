<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Coverage Report</title>
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/go.min.js"></script>
    <style>
      :root {
  --sidebar-width: 280px;
  --topbar-height: 48px;
  --line-height: 20px;
  --font-mono: ui-monospace, SFMono-Regular, "SF Mono", Menlo, Consolas, "Liberation Mono", monospace;
}

[data-theme="dark"] {
  --bg: #1e1e1e;
  --bg-secondary: #252526;
  --bg-tertiary: #2d2d2d;
  --text: #d4d4d4;
  --text-muted: #808080;
  --border: #3c3c3c;
  --covered: rgba(35, 134, 54, 0.25);
  --covered-gutter: #238636;
  --uncovered: rgba(218, 54, 51, 0.25);
  --uncovered-gutter: #da3633;
  --highlight: #264f78;
  --highlight-match: #613214;
  --accent: #569cd6;
  --hover: #2a2d2e;
}

[data-theme="light"] {
  --bg: #ffffff;
  --bg-secondary: #f3f3f3;
  --bg-tertiary: #e8e8e8;
  --text: #24292f;
  --text-muted: #656d76;
  --border: #d0d7de;
  --covered: rgba(35, 134, 54, 0.15);
  --covered-gutter: #1a7f37;
  --uncovered: rgba(218, 54, 51, 0.15);
  --uncovered-gutter: #cf222e;
  --highlight: #ddf4ff;
  --highlight-match: #fff8c5;
  --accent: #0969da;
  --hover: #f6f8fa;
}

* {
  margin: 0;
  padding: 0;
  box-sizing: border-box;
}

html, body {
  height: 100%;
  overflow: hidden;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif;
  font-size: 14px;
  background: var(--bg);
  color: var(--text);
}

#app {
  display: grid;
  grid-template-columns: var(--sidebar-width) 1fr;
  height: 100%;
}

/* Sidebar */
#sidebar {
  display: flex;
  flex-direction: column;
  background: var(--bg-secondary);
  border-right: 1px solid var(--border);
  overflow: hidden;
}

#sidebar-header {
  padding: 16px;
  border-bottom: 1px solid var(--border);
}

/* Logo link */
#logo-link {
  text-decoration: none;
  color: inherit;
  display: block;
}

#logo-link:hover #logo-container {
  opacity: 0.8;
}

#logo-container {
  display: flex;
  align-items: center;
  gap: 10px;
  margin-bottom: 12px;
  transition: opacity 0.15s ease;
}

#logo-container .github-icon {
  flex-shrink: 0;
  color: var(--text-muted);
  transition: color 0.15s ease;
}

#logo-link:hover .github-icon {
  color: var(--accent);
}

#logo {
  flex-shrink: 0;
  width: 32px;
  height: 32px;
}

#logo-text {
  flex: 1;
  min-width: 0;
}

#sidebar-header h1 {
  font-size: 16px;
  font-weight: 600;
  margin: 0;
  line-height: 1.2;
}

#tagline {
  font-size: 12px;
  font-weight: 500;
  color: var(--text-muted);
  margin: 2px 0 0 0;
  line-height: 1.3;
}

#summary {
  font-size: 13px;
  color: var(--text-muted);
}

#summary .percent {
  font-weight: 600;
  color: var(--text);
}

#search-box {
  padding: 8px 16px;
  border-bottom: 1px solid var(--border);
}

#search-input {
  width: 100%;
  padding: 6px 10px;
  border: 1px solid var(--border);
  border-radius: 4px;
  background: var(--bg);
  color: var(--text);
  font-size: 13px;
}

#search-input:focus {
  outline: none;
  border-color: var(--accent);
}

/* Sort controls */
#sort-controls {
  display: flex;
  gap: 0;
  margin: 8px 12px;
  border: 1px solid var(--border);
  border-radius: 4px;
  overflow: hidden;
}

.sort-btn {
  flex: 1;
  display: flex;
  align-items: center;
  justify-content: center;
  gap: 4px;
  padding: 6px 8px;
  background: var(--bg-secondary);
  color: var(--text);
  border: none;
  cursor: pointer;
  font-size: 12px;
  transition: background-color 0.2s, color 0.2s;
}

.sort-btn:hover {
  background: var(--hover);
}

.sort-btn.active {
  background: var(--accent);
  color: #fff;
}

.sort-btn .icon {
  font-weight: 600;
}

.sort-btn .label {
  font-size: 11px;
}

/* Coverage badges for directories */
.coverage-badge {
  margin-left: auto;
  padding-left: 8px;
  font-size: 11px;
  color: var(--text-muted);
  font-weight: 500;
  font-family: var(--font-mono);
}

#file-tree {
  flex: 1;
  overflow-y: auto;
  padding: 8px 0;
}

/* Sidebar footer */
#sidebar-footer {
  padding: 12px 16px;
  border-top: 1px solid var(--border);
  font-size: 12px;
  text-align: center;
}

#sidebar-footer a {
  color: var(--text-muted);
  text-decoration: none;
  display: inline-flex;
  align-items: center;
  gap: 6px;
}

#sidebar-footer a:hover {
  color: var(--accent);
  text-decoration: underline;
}

#sidebar-footer .github-icon {
  flex-shrink: 0;
}

.tree-node {
  cursor: pointer;
  user-select: none;
}

.tree-item {
  display: flex;
  align-items: center;
  padding: 4px 16px;
  gap: 6px;
  white-space: nowrap;
}

.tree-item:hover {
  background: var(--hover);
}

.tree-item.selected {
  background: var(--highlight);
}

.tree-item .icon {
  width: 16px;
  text-align: center;
  font-size: 12px;
  color: var(--text-muted);
}

.tree-item .name {
  font-size: 13px;
  overflow: hidden;
  text-overflow: ellipsis;
  flex: 1;
  min-width: 0;
}

.tree-children {
  display: none;
}

.tree-node.expanded > .tree-children {
  display: block;
}

.tree-children .tree-item {
  padding-left: calc(16px + var(--depth, 0) * 16px);
}

.tree-node.hidden {
  display: none;
}

/* Canvas */
#canvas {
  display: flex;
  flex-direction: column;
  overflow: hidden;
}

#topbar {
  display: flex;
  align-items: center;
  justify-content: space-between;
  height: var(--topbar-height);
  padding: 0 16px;
  background: var(--bg-secondary);
  border-bottom: 1px solid var(--border);
  gap: 16px;
}

#file-path {
  font-size: 13px;
  font-family: var(--font-mono);
  color: var(--text-muted);
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

#topbar-actions {
  display: flex;
  align-items: center;
  gap: 12px;
}

#in-file-search {
  display: flex;
  align-items: center;
  gap: 8px;
}

#content-search {
  width: 180px;
  padding: 4px 8px;
  border: 1px solid var(--border);
  border-radius: 4px;
  background: var(--bg);
  color: var(--text);
  font-size: 12px;
}

#content-search:focus {
  outline: none;
  border-color: var(--accent);
}

#match-info {
  font-size: 12px;
  color: var(--text-muted);
  min-width: 60px;
}

#prev-match, #next-match {
  padding: 2px 8px;
  border: 1px solid var(--border);
  border-radius: 4px;
  background: var(--bg);
  color: var(--text);
  cursor: pointer;
  font-size: 10px;
}

#prev-match:hover, #next-match:hover {
  background: var(--hover);
}

#theme-toggle {
  padding: 6px 10px;
  border: 1px solid var(--border);
  border-radius: 4px;
  background: var(--bg);
  color: var(--text);
  cursor: pointer;
  font-size: 16px;
}

#theme-toggle:hover {
  background: var(--hover);
}

#syntax-toggle {
  padding: 6px 10px;
  border: 1px solid var(--border);
  border-radius: 4px;
  background: var(--bg);
  color: var(--text);
  cursor: pointer;
  font-size: 14px;
  font-family: var(--font-mono);
}

#syntax-toggle:hover {
  background: var(--hover);
}

#syntax-toggle.active {
  background: var(--accent);
  color: #fff;
  border-color: var(--accent);
}

#help-toggle {
  padding: 6px 10px;
  border: 1px solid var(--border);
  border-radius: 4px;
  background: var(--bg);
  color: var(--text);
  cursor: pointer;
  font-size: 14px;
  font-weight: 600;
}

#help-toggle:hover {
  background: var(--hover);
}

/* Override highlight.js to use theme-aware colors */
.hljs { background: transparent !important; }

[data-theme="dark"] .hljs-keyword { color: #569cd6; }
[data-theme="dark"] .hljs-type { color: #4ec9b0; }
[data-theme="dark"] .hljs-string { color: #ce9178; }
[data-theme="dark"] .hljs-number { color: #b5cea8; }
[data-theme="dark"] .hljs-comment { color: #6a9955; }
[data-theme="dark"] .hljs-built_in { color: #dcdcaa; }
[data-theme="dark"] .hljs-literal { color: #569cd6; }
[data-theme="dark"] .hljs-function { color: #dcdcaa; }

[data-theme="light"] .hljs-keyword { color: #0000ff; }
[data-theme="light"] .hljs-type { color: #267f99; }
[data-theme="light"] .hljs-string { color: #a31515; }
[data-theme="light"] .hljs-number { color: #098658; }
[data-theme="light"] .hljs-comment { color: #008000; }
[data-theme="light"] .hljs-built_in { color: #795e26; }
[data-theme="light"] .hljs-literal { color: #0000ff; }
[data-theme="light"] .hljs-function { color: #795e26; }

/* Viewport */
#viewport {
  flex: 1;
  overflow: auto;
  background: var(--bg);
  outline: none;
}

#viewport::-webkit-scrollbar {
  width: 14px;
  height: 14px;
}

#viewport::-webkit-scrollbar-track {
  background: var(--bg);
}

#viewport::-webkit-scrollbar-thumb {
  background: var(--border);
  border: 3px solid var(--bg);
  border-radius: 7px;
}

#viewport::-webkit-scrollbar-thumb:hover {
  background: var(--text-muted);
}

.code-container {
  display: table;
  min-width: 100%;
  font-family: var(--font-mono);
  font-size: 13px;
  line-height: var(--line-height);
}

.code-line {
  display: table-row;
}

.code-line:hover {
  background: var(--hover);
}

.code-line.covered {
  background: var(--covered);
}

.code-line.uncovered {
  background: var(--uncovered);
}

.code-line.covered:hover {
  background: var(--covered);
}

.code-line.uncovered:hover {
  background: var(--uncovered);
}

.gutter {
  display: table-cell;
  width: 4px;
  min-width: 4px;
}

.code-line.covered .gutter {
  background: var(--covered-gutter);
}

.code-line.uncovered .gutter {
  background: var(--uncovered-gutter);
}

.line-number {
  display: table-cell;
  width: 50px;
  min-width: 50px;
  padding: 0 12px 0 8px;
  text-align: right;
  color: var(--text-muted);
  user-select: none;
  vertical-align: top;
}

.line-content {
  display: table-cell;
  padding-right: 16px;
  white-space: pre;
  tab-size: 4;
}

.match-highlight {
  background: var(--highlight-match);
  border-radius: 2px;
}

.current-match {
  background: var(--accent);
  color: #fff;
}

/* Empty state */
.empty-state {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  height: 100%;
  color: var(--text-muted);
  gap: 8px;
}

.empty-state .icon {
  font-size: 48px;
  opacity: 0.5;
}

/* Scrollbar for file tree */
#file-tree::-webkit-scrollbar {
  width: 8px;
}

#file-tree::-webkit-scrollbar-track {
  background: transparent;
}

#file-tree::-webkit-scrollbar-thumb {
  background: var(--border);
  border-radius: 4px;
}

#file-tree::-webkit-scrollbar-thumb:hover {
  background: var(--text-muted);
}

/* Help modal */
.modal {
  position: fixed;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  background: rgba(0, 0, 0, 0.5);
  display: flex;
  align-items: center;
  justify-content: center;
  z-index: 1000;
}

.modal.hidden {
  display: none;
}

.modal-content {
  background: var(--bg-secondary);
  border: 1px solid var(--border);
  border-radius: 8px;
  padding: 24px;
  max-width: 400px;
  width: 90%;
}

.modal-content h2 {
  margin-bottom: 16px;
  font-size: 18px;
}

.modal-content dl {
  display: grid;
  grid-template-columns: auto 1fr;
  gap: 8px 16px;
}

.modal-content dt {
  font-family: var(--font-mono);
  background: var(--bg-tertiary);
  padding: 2px 6px;
  border-radius: 4px;
  font-size: 13px;
}

.modal-content dd {
  color: var(--text-muted);
}

.modal-content button {
  margin-top: 20px;
  padding: 8px 16px;
  border: 1px solid var(--border);
  border-radius: 4px;
  background: var(--accent);
  color: #fff;
  cursor: pointer;
  width: 100%;
}

/* Selected line range (multi-line selection) */
.code-line.selected-line {
  background-color: var(--highlight);
}

.code-line.selected-line.covered {
  background-color: var(--highlight);
}

.code-line.selected-line.uncovered {
  background-color: var(--highlight);
}

/* Line number click indicator */
.line-number {
  cursor: pointer;
}

.line-number:hover {
  color: var(--accent);
}

    </style>
  </head>
  <body data-theme="dark">
    <div id="app">
      <aside id="sidebar">
        <div id="sidebar-header">
          <a
            href="https://github.com/chmouel/go-better-html-coverage"
            id="logo-link"
            target="_blank"
            rel="noopener"
          >
            <div id="logo-container">
              <svg id="logo" viewBox="0 0 32 32" width="32" height="32">
                <defs>
                  <linearGradient
                    id="logoGradient"
                    x1="0%"
                    y1="0%"
                    x2="100%"
                    y2="100%"
                  >
                    <stop
                      offset="0%"
                      style="stop-color: var(--accent); stop-opacity: 1"
                    />
                    <stop
                      offset="100%"
                      style="stop-color: var(--accent); stop-opacity: 0.7"
                    />
                  </linearGradient>
                </defs>
                
                <circle
                  cx="16"
                  cy="16"
                  r="14"
                  fill="none"
                  stroke="url(#logoGradient)"
                  stroke-width="2"
                  opacity="0.3"
                />
                
                <path
                  d="M 10 17 L 14 21 L 22 11"
                  fill="none"
                  stroke="var(--accent)"
                  stroke-width="2.5"
                  stroke-linecap="round"
                  stroke-linejoin="round"
                />
                
                <circle
                  cx="24"
                  cy="10"
                  r="1.5"
                  fill="var(--accent)"
                  opacity="0.6"
                />
                <circle
                  cx="26"
                  cy="12"
                  r="1.5"
                  fill="var(--accent)"
                  opacity="0.6"
                />
              </svg>
              <div id="logo-text">
                <h1>GO Coverage</h1>
                <div id="tagline">A better HTML Go Coverage</div>
              </div>
            </div>
          </a>
          <div id="summary"></div>
        </div>
        <div id="search-box">
          <input type="text" id="search-input" placeholder="Search files..." />
        </div>
        <div id="sort-controls">
          <button
            class="sort-btn active"
            data-sort="name"
            title="Sort alphabetically"
          >
            <span class="icon">Aâ†’Z</span>
            <span class="label">Name</span>
          </button>
          <button
            class="sort-btn"
            data-sort="coverage"
            title="Sort by coverage percentage"
          >
            <span class="icon">%</span>
            <span class="label">Coverage</span>
          </button>
        </div>
        <div id="file-tree"></div>
        <footer id="sidebar-footer">
          <a
            href="https://github.com/chmouel/go-better-html-coverage"
            target="_blank"
            rel="noopener"
          >
            <svg
              class="github-icon"
              viewBox="0 0 16 16"
              width="14"
              height="14"
              fill="currentColor"
            >
              <path
                d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"
              />
            </svg>
            chmouel/go-better-html-coverage
          </a>
        </footer>
      </aside>
      <main id="canvas">
        <header id="topbar">
          <div id="file-path"></div>
          <div id="topbar-actions">
            <div id="in-file-search">
              <input
                type="text"
                id="content-search"
                placeholder="Search in file..."
              />
              <span id="match-info"></span>
              <button id="prev-match" title="Previous match">&#9650;</button>
              <button id="next-match" title="Next match">&#9660;</button>
            </div>
            <button id="syntax-toggle" title="Toggle syntax highlighting">
              &lt;/&gt;
            </button>
            <button id="theme-toggle" title="Toggle theme">&#9788;</button>
            <button id="help-toggle" title="Keyboard shortcuts">?</button>
          </div>
        </header>
        <div id="viewport" tabindex="-1"></div>
      </main>
      <div id="help-modal" class="modal hidden">
        <div class="modal-content">
          <h2>Keyboard Shortcuts</h2>
          <dl>
            <dt>Ctrl+P</dt>
            <dd>Focus file search</dd>
            <dt>Ctrl+F</dt>
            <dd>Search in file</dd>
            <dt>Enter</dt>
            <dd>Next match</dd>
            <dt>Shift+Enter</dt>
            <dd>Previous match</dd>
            <dt>?</dt>
            <dd>Show this help</dd>
            <dt>Esc</dt>
            <dd>Close modal</dd>
          </dl>
          <h2>Permalinks</h2>
          <dl>
            <dt>Click line</dt>
            <dd>Select line, update URL</dd>
            <dt>Shift+Click</dt>
            <dd>Select line range</dd>
          </dl>
          <button id="close-help">Close</button>
        </div>
      </div>
    </div>
    <script>
      window.COVERAGE_DATA = {"files":[{"id":0,"path":"internal/fieldmask/fieldmask.go","lines":["// Package fieldmask provides field mask filtering for protobuf messages.","package fieldmask","","import (","\t\"context\"","\t\"log\"","\t\"net/http\"","\t\"os\"","\t\"strings\"","\t\"sync/atomic\"","","\tjsoniter \"github.com/json-iterator/go\"","\t\"github.com/tidwall/gjson\"","\t\"google.golang.org/grpc\"","\t\"google.golang.org/grpc/metadata\"","\t\"google.golang.org/protobuf/proto\"","\t\"google.golang.org/protobuf/reflect/protoreflect\"","\t\"google.golang.org/protobuf/types/known/fieldmaskpb\"",")","","const (","\tkey = \"fields\"","","\terrStr = \"\\033[35m\" + \"%s\\n\" + \"\\033[0m\" + \"\\033[31m\" + \"[error] \" + \"\\033[0m\"",")","","var (","\tlogger = newLogger()",")","","// Logger logger interface","type Logger interface {","\tError(context.Context, string, ...interface{})","}","","func newLogger() Logger {","\treturn \u0026stdLogger{","\t\tlog: log.New(os.Stdout, \"\\r\\n\", log.LstdFlags),","\t}","}","","type stdLogger struct {","\tlog *log.Logger","}","","func (l *stdLogger) Error(_ context.Context, msg string, data ...interface{}) {","\tl.log.Printf(errStr+msg, data...)","}","","// SetLogger sets the logger for the fieldmask package.","func SetLogger(l Logger) {","\tlogger = l","}","","// FieldMask is recursive structure to define a path mask","//","// Reference: https://protobuf.dev/reference/protobuf/google.protobuf/#json-encoding-field-masks","//","// Reference: https://github.com/protocolbuffers/protobuf/blob/main/src/google/protobuf/field_mask.proto","//","// For example, given the message:","//","//\tf {","//\t  b {","//\t    d: 1","//\t    x: 2","//\t  }","//\t  c: [1]","//\t}","//","// then if the path is:","//","// paths: [\"f.b.d\"]","//","// then the result will be:","//","//\tf {","//\t  b {","//\t    d: 10","//\t  }","//\t}","type FieldMask map[string]FieldMask","","// Build populates a FieldMask from the input array of paths recursively.","// The array should contain JSON paths with dot \".\" notation.","func (fm FieldMask) Build(paths []string) {","\tif len(paths) == 0 {","\t\treturn","\t}","","\tfields := strings.Split(paths[0], \".\")","\tm := fm","\tfor _, field := range fields {","\t\tif _, ok := m[field]; !ok {","\t\t\tm[field] = FieldMask{}","\t\t}","\t\tm = m[field]","\t}","","\tfm.Build(paths[1:]) //nolint:gosec // disable G602","}","","// Filter takes a Proto message as input and updates the message according to the FieldMask.","func (fm FieldMask) Filter(message proto.Message) {","\tif len(fm) == 0 {","\t\treturn","\t}","","\treflect := message.ProtoReflect()","\treflect.Range(func(fd protoreflect.FieldDescriptor, v protoreflect.Value) bool {","\t\tmask, ok := fm[string(fd.Name())]","\t\tif !ok {","\t\t\treflect.Clear(fd)","\t\t}","","\t\tif len(mask) == 0 {","\t\t\treturn true","\t\t}","","\t\tswitch {","\t\tcase fd.IsMap():","\t\t\tm := reflect.Get(fd).Map()","\t\t\tm.Range(func(k protoreflect.MapKey, v protoreflect.Value) bool {","\t\t\t\tif fm, ok := mask[k.String()]; ok {","\t\t\t\t\tif i, ok := v.Interface().(protoreflect.Message); ok \u0026\u0026 len(fm) \u003e 0 {","\t\t\t\t\t\tfm.Filter(i.Interface())","\t\t\t\t\t}","\t\t\t\t} else {","\t\t\t\t\tm.Clear(k)","\t\t\t\t}","\t\t\t\treturn true","\t\t\t})","\t\tcase fd.IsList():","\t\t\tlist := reflect.Get(fd).List()","\t\t\tfor i := 0; i \u003c list.Len(); i++ {","\t\t\t\tmask.Filter(list.Get(i).Message().Interface())","\t\t\t}","\t\tcase fd.Kind() == protoreflect.MessageKind:","\t\t\tmask.Filter(reflect.Get(fd).Message().Interface())","\t\tcase fd.Kind() == protoreflect.BytesKind:","\t\t\tif b := v.Bytes(); gjson.ValidBytes(b) {","\t\t\t\tb, err := jsoniter.Marshal(mask.FilterJSON(b, []string{}))","\t\t\t\tif err == nil {","\t\t\t\t\treflect.Set(fd, protoreflect.ValueOfBytes(b))","\t\t\t\t}","\t\t\t}","\t\tdefault:","\t\t\t// TODO: We can configure zap logger from main instead of using default logger.","\t\t\tlogger.Error(context.Background(), \"unsupported field type: %s\", fd.Kind())","\t\t}","\t\treturn true","\t})","}","","// Paths return the dot \".\" JSON notation os all the paths in the FieldMask.","// Parameter root []string is used internally for recursion, but it can also be used for setting an initial root path.","func (fm FieldMask) Paths(path []string) (paths []string) {","\tfor k, v := range fm {","\t\tpath = append(path, k)","\t\tif len(v) == 0 {","\t\t\tpaths = append(paths, strings.Join(path, \".\"))","\t\t}","\t\tpaths = append(paths, v.Paths(path)...)","\t\tpath = path[:len(path)-1]","\t}","\treturn","}","","// FilterJSON takes a JSON as input and return a map of the filtered JSON according to the FieldMask.","func (fm FieldMask) FilterJSON(json []byte, path []string) (out map[string]any) {","\tfor k, v := range fm {","\t\tif out == nil {","\t\t\tout = make(map[string]interface{})","\t\t}","\t\tpath = append(path, k)","\t\tif len(v) == 0 {","\t\t\tout[k] = gjson.GetBytes(json, strings.Join(path, \".\")).Value()","\t\t} else {","\t\t\tout[k] = v.FilterJSON(json, path)","\t\t}","\t\tpath = path[:len(path)-1]","\t}","\treturn","}","","// FromMetadata gets all the filter definitions from gRPC metadata.","func FromMetadata(md metadata.MD) FieldMask {","\tfm := \u0026fieldmaskpb.FieldMask{}","\tmasks := md.Get(key)","\tfor _, mask := range masks {","\t\tpaths := strings.Split(mask, \",\")","\t\tfor _, path := range paths {","\t\t\tfm.Paths = append(fm.Paths, strings.TrimSpace(path))","\t\t}","\t}","\tfm.Normalize()","\tm := FieldMask{}","\tm.Build(fm.Paths)","\treturn m","}","","// MetadataAnnotator injects key from query parameter to gRPC metadata (for REST client).","func MetadataAnnotator(_ context.Context, req *http.Request) metadata.MD {","\tif err := req.ParseForm(); err == nil \u0026\u0026 req.Form.Has(key) {","\t\treturn metadata.Pairs(key, req.Form.Get(key))","\t}","\treturn nil","}","","// UnaryServerInterceptor updates the response message according to the FieldMask.","func UnaryServerInterceptor(enabled *atomic.Bool) grpc.UnaryServerInterceptor {","\treturn func(ctx context.Context, req any, _ *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (any, error) {","\t\tresp, err := handler(ctx, req)","\t\tif err != nil || !enabled.Load() {","\t\t\treturn resp, err","\t\t}","","\t\tmessage, ok := resp.(proto.Message)","\t\tif !ok {","\t\t\treturn resp, err","\t\t}","","\t\tmd, ok := metadata.FromIncomingContext(ctx)","\t\tif !ok {","\t\t\treturn resp, err","\t\t}","","\t\tfm := FromMetadata(md)","\t\tfm.Filter(message)","\t\treturn resp, err","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,0,0,0,0,1,1,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,2,2,2,2,2,2,2,0,0,2,0,0,0,2,2,1,1,0,2,2,2,2,2,2,0,2,2,2,0,2,1,1,1,1,1,1,1,1,1,1,1,0,2,2,2,2,2,1,1,2,2,2,2,2,2,0,1,1,1,0,2,0,0,0,0,0,2,2,2,2,2,2,2,2,0,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,0,2,0,0,0,2,2,2,2,2,2,2,2,0,2,2,2,2,0,0,0,2,2,2,2,1,0,0,0,1,1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,0,1,1,1,0,0]},{"id":1,"path":"pkg/api/server/cel/cel.go","lines":["// Copyright 2020 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","// Package cel provides definitions for defining the Results CEL environment.","package cel","","import (","\t\"context\"","\t\"log\"","","\t\"github.com/google/cel-go/cel\"","\t\"github.com/google/cel-go/common/decls\"","\t\"github.com/google/cel-go/common/types\"","\t\"github.com/google/cel-go/common/types/ref\"","\tppb \"github.com/tektoncd/results/proto/pipeline/v1/pipeline_go_proto\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/status\"",")","","// NewEnv returns the CEL environment for Results, loading in definitions for","// known types.","func NewEnv() (*cel.Env, error) {","\treturn cel.NewEnv(","\t\tcel.Types(\u0026pb.Result{}, \u0026pb.Record{}, \u0026ppb.PipelineRun{}, \u0026ppb.TaskRun{}),","\t\tcel.VariableDecls(decls.NewVariable(\"result\", cel.ObjectType(\"tekton.results.v1alpha2.Result\"))),","\t\tcel.VariableDecls(decls.NewVariable(\"record\", cel.ObjectType(\"tekton.results.v1alpha2.Record\"))),","\t)","}","","// ParseFilter creates a CEL program based on the given filter string.","func ParseFilter(env *cel.Env, filter string) (cel.Program, error) {","\tif filter == \"\" {","\t\treturn allowAll{}, nil","\t}","","\tast, issues := env.Compile(filter)","\tif issues != nil \u0026\u0026 issues.Err() != nil {","\t\treturn nil, status.Errorf(codes.InvalidArgument, \"error parsing filter: %v\", issues.Err())","\t}","","\tprg, err := env.Program(ast)","\tif err != nil {","\t\treturn nil, status.Errorf(codes.InvalidArgument, \"error creating filter query evaluator: %v\", err)","\t}","\treturn prg, nil","}","","// allowAll is a CEL program implementation that always returns true.","type allowAll struct{}","","func (allowAll) ContextEval(context.Context, any) (ref.Val, *cel.EvalDetails, error) {","\treturn types.Bool(true), nil, nil","}","","func (allowAll) Eval(any) (ref.Val, *cel.EvalDetails, error) {","\treturn types.Bool(true), nil, nil","}","","// Match determines whether the given CEL filter matches the result.","func Match(prg cel.Program, data map[string]any) (bool, error) {","\tif prg == nil {","\t\treturn true, nil","\t}","\tif data == nil {","\t\treturn false, nil","\t}","","\tout, details, err := prg.Eval(data)","\tif err != nil {","\t\tlog.Printf(\"failed to evaluate the expression: %v\", err)","\t\treturn false, status.Errorf(codes.InvalidArgument, \"failed to evaluate filter: %v. Details: %+v\", err, details)","\t}","\tb, ok := out.Value().(bool)","\tif !ok {","\t\treturn false, status.Errorf(codes.InvalidArgument, \"expected boolean result, got %s\", out.Type().TypeName())","\t}","\treturn b, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,0,0,2,2,2,2,0,2,2,2,2,0,2,2,1,1,2,0,0,0,0,0,1,1,1,0,1,1,1,0,0,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0]},{"id":2,"path":"pkg/api/server/cel/env.go","lines":["package cel","","import (","\t\"github.com/google/cel-go/cel\"","\t\"github.com/google/cel-go/common/types\"","\tresultspb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"google.golang.org/protobuf/types/known/timestamppb\"",")","","const (","\ttypePipelineRun = \"tekton.dev/v1.PipelineRun\"","\ttypeTaskRun     = \"tekton.dev/v1.TaskRun\"",")","","// NewResultsEnv creates a CEL program to build SQL filters for Result objects.","func NewResultsEnv() (*cel.Env, error) {","\treturn cel.NewEnv(","\t\tcel.Constant(\"PIPELINE_RUN\", cel.StringType, types.String(typePipelineRun)),","\t\tcel.Constant(\"TASK_RUN\", cel.StringType, types.String(typeTaskRun)),","\t\tcel.Constant(\"UNKNOWN\", cel.IntType, types.Int(resultspb.RecordSummary_UNKNOWN)),","\t\tcel.Constant(\"SUCCESS\", cel.IntType, types.Int(resultspb.RecordSummary_SUCCESS)),","\t\tcel.Constant(\"FAILURE\", cel.IntType, types.Int(resultspb.RecordSummary_FAILURE)),","\t\tcel.Constant(\"TIMEOUT\", cel.IntType, types.Int(resultspb.RecordSummary_TIMEOUT)),","\t\tcel.Constant(\"CANCELLED\", cel.IntType, types.Int(resultspb.RecordSummary_CANCELLED)),","\t\tcel.Types(\u0026resultspb.RecordSummary{},","\t\t\t\u0026timestamppb.Timestamp{}),","\t\tcel.Variable(\"parent\", cel.StringType),","\t\tcel.Variable(\"uid\", cel.StringType),","\t\tcel.Variable(\"annotations\", cel.MapType(cel.StringType, cel.StringType)),","\t\tcel.Variable(\"summary\",","\t\t\tcel.ObjectType(\"tekton.results.v1alpha2.RecordSummary\")),","\t\tcel.Variable(\"create_time\",","\t\t\tcel.ObjectType(\"google.protobuf.Timestamp\")),","\t\tcel.Variable(\"update_time\",","\t\t\tcel.ObjectType(\"google.protobuf.Timestamp\")),","\t)","}","","// NewRecordsEnv creates a CEL program to build SQL filters for Record objects.","func NewRecordsEnv() (*cel.Env, error) {","\treturn cel.NewEnv(","\t\tcel.Constant(\"PIPELINE_RUN\", cel.StringType, types.String(typePipelineRun)),","\t\tcel.Constant(\"TASK_RUN\", cel.StringType, types.String(typeTaskRun)),","\t\tcel.Variable(\"parent\", cel.StringType),","\t\tcel.Variable(\"result_name\", cel.StringType),","\t\tcel.Variable(\"name\", cel.StringType),","\t\tcel.Variable(\"data_type\", cel.StringType),","\t\tcel.Variable(\"data\", cel.AnyType),","\t)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1]},{"id":3,"path":"pkg/api/server/cel2sql/concat.go","lines":["// Copyright 2023 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","// Package cel2sql provides utilities for converting CEL expressions to SQL queries.","package cel2sql","","import (","\t\"fmt\"","","\texprpb \"google.golang.org/genproto/googleapis/api/expr/v1alpha1\"",")","","// mayBeTranslatedToStringConcatExpression returns a boolean whether the","// expression is a string concatenation. If it is, returns both arguments.","func (i *interpreter) mayBeTranslatedToStringConcatExpression(expr *exprpb.Expr_Call) bool {","\tif function := expr.GetFunction(); !isAddOperator(function) {","\t\treturn false","\t}","\targ1 := expr.Args[0]","\targ2 := expr.Args[1]","\treturn i.isString(arg1) || i.isString(arg2)","}","","func (i interpreter) allStringConcatArgs(expr *exprpb.Expr) []*exprpb.Expr {","\targs := []*exprpb.Expr{}","\tswitch node := expr.ExprKind.(type) {","\tcase *exprpb.Expr_CallExpr:","\t\tif isAddOperator(node.CallExpr.Function) {","\t\t\targ1 := node.CallExpr.Args[0]","\t\t\targ2 := node.CallExpr.Args[1]","\t\t\targs = append(args, i.allStringConcatArgs(arg1)...)","\t\t\targs = append(args, i.allStringConcatArgs(arg2)...)","\t\t}","\tdefault:","\t\targs = append(args, expr)","\t}","\treturn args","}","","func (i *interpreter) translateToStringConcatExpression(expr *exprpb.Expr) error {","\targs := i.allStringConcatArgs(expr)","\tfmt.Fprintf(\u0026i.query, \"CONCAT(\")","\tfor j, arg := range args {","\t\terr := i.interpretExpr(arg)","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\tif j != len(args)-1 {","\t\t\tfmt.Fprintf(\u0026i.query, \", \")","\t\t}","\t}","\tfmt.Fprintf(\u0026i.query, \")\")","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,0,2,0,0,2,2,2,2,2,2,1,1,2,2,2,0,2,2,0]},{"id":4,"path":"pkg/api/server/cel2sql/convert.go","lines":["// Copyright 2023 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package cel2sql","","import (","\t\"fmt\"","","\t\"github.com/google/cel-go/cel\"",")","","// Convert takes CEL expressions and attempt to convert them into Postgres SQL","// filters.","func Convert(env *cel.Env, filters string) (string, error) {","\tast, issues := env.Compile(filters)","\tif issues != nil \u0026\u0026 issues.Err() != nil {","\t\treturn \"\", fmt.Errorf(\"error compiling CEL filters: %w\", issues.Err())","\t}","","\tif outputType := ast.OutputType(); !outputType.IsAssignableType(cel.BoolType) {","\t\treturn \"\", fmt.Errorf(\"expected boolean expression, but got %s\", outputType.String())","\t}","","\tinterpreter, err := newInterpreter(ast)","\tif err != nil {","\t\treturn \"\", fmt.Errorf(\"error creating cel2sql interpreter: %w\", err)","\t}","","\treturn interpreter.interpret()","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,1,1,0,2,2,2,0,2,2,1,1,0,2,0]},{"id":5,"path":"pkg/api/server/cel2sql/functions.go","lines":["// Copyright 2023 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package cel2sql","","import (","\t\"fmt\"","","\t\"github.com/google/cel-go/common/overloads\"","\texprpb \"google.golang.org/genproto/googleapis/api/expr/v1alpha1\"",")","","func (i *interpreter) interpretFunctionCallExpr(id int64, expr *exprpb.Expr_Call) error {","\tfunction := expr.GetFunction()","\tswitch function {","\tcase overloads.Contains:","\t\treturn i.interpretContainsFunction(expr)","","\tcase overloads.EndsWith:","\t\treturn i.translateToBinaryCall(expr, \"LIKE '%' ||\")","","\tcase overloads.TimeGetDate:","\t\treturn i.translateToExtractFunctionCall(expr, \"DAY\", false)","","\tcase overloads.TimeGetDayOfMonth:","\t\treturn i.translateToExtractFunctionCall(expr, \"DAY\", true)","","\tcase overloads.TimeGetDayOfWeek:","\t\treturn i.translateToExtractFunctionCall(expr, \"DOW\", false)","","\tcase overloads.TimeGetDayOfYear:","\t\treturn i.translateToExtractFunctionCall(expr, \"DOY\", true)","","\tcase overloads.TimeGetFullYear:","\t\treturn i.translateToExtractFunctionCall(expr, \"YEAR\", false)","","\tcase overloads.StartsWith:","\t\treturn i.interpretStartsWithFunction(expr)","","\tcase overloads.Matches:","\t\treturn i.translateToBinaryCall(expr, \"~\")","","\tcase overloads.TypeConvertTimestamp:","\t\treturn i.interpretTimestampFunction(expr)","","\t}","","\treturn i.unsupportedExprError(id, fmt.Sprintf(\"`%s` function\", function))","}","","func (i *interpreter) interpretContainsFunction(expr *exprpb.Expr_Call) error {","\tfmt.Fprintf(\u0026i.query, \"POSITION(\")","\tif err := i.interpretExpr(expr.Args[0]); err != nil {","\t\treturn err","\t}","\tfmt.Fprintf(\u0026i.query, \" IN \")","\tif err := i.interpretExpr(expr.GetTarget()); err != nil {","\t\treturn err","\t}","\ti.query.WriteString(\") \u003c\u003e 0\")","\treturn nil","}","","func (i *interpreter) interpretStartsWithFunction(expr *exprpb.Expr_Call) error {","\tif err := i.translateToBinaryCall(expr, \"LIKE\"); err != nil {","\t\treturn err","\t}","\ti.query.WriteString(\" || '%'\")","\treturn nil","}","","func (i *interpreter) translateToBinaryCall(expr *exprpb.Expr_Call, infixTerm string) error {","\tif err := i.interpretExpr(expr.GetTarget()); err != nil {","\t\treturn err","\t}","\tfmt.Fprintf(\u0026i.query, \" %s \", infixTerm)","\treturn i.interpretExpr(expr.Args[0])","}","","func (i *interpreter) translateToExtractFunctionCall(expr *exprpb.Expr_Call, field string, decrementReturnValue bool) error {","\tif decrementReturnValue {","\t\ti.query.WriteString(\"(\")","\t}","\tfmt.Fprintf(\u0026i.query, \"EXTRACT(%s FROM \", field)","\tif err := i.interpretExpr(expr.GetTarget()); err != nil {","\t\treturn err","\t}","\tif i.isDyn(expr.Target) {","\t\ti.coerceWellKnownType(exprpb.Type_TIMESTAMP)","\t}","\ti.query.WriteString(\")\")","\tif decrementReturnValue {","\t\ti.query.WriteString(\" - 1)\")","\t}","\treturn nil","}","","func (i *interpreter) interpretTimestampFunction(expr *exprpb.Expr_Call) error {","\tif err := i.interpretExpr(expr.Args[0]); err != nil {","\t\treturn err","\t}","\ti.query.WriteString(\"::TIMESTAMP WITH TIME ZONE\")","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,2,2,0,2,2,0,2,2,0,2,2,0,2,2,0,2,2,0,2,2,0,2,2,0,2,2,0,0,0,1,0,0,2,2,2,1,1,2,2,1,1,2,2,0,0,2,2,1,1,2,2,0,0,2,2,1,1,2,2,0,0,2,2,2,2,2,2,1,1,2,2,2,2,2,2,2,2,0,0,2,2,1,1,2,2,0]},{"id":6,"path":"pkg/api/server/cel2sql/index.go","lines":["// Copyright 2023 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package cel2sql","","import (","\t\"fmt\"","","\t\"github.com/google/cel-go/common/operators\"","\texprpb \"google.golang.org/genproto/googleapis/api/expr/v1alpha1\"",")","","func (i *interpreter) mayBeTranslatedToJSONPathContainsExpression(arg1 *exprpb.Expr, function string, arg2 *exprpb.Expr) bool {","\tconstExpr := arg2.GetConstExpr()","\tif constExpr == nil {","\t\treturn false","\t}","\tif _, ok := constExpr.GetConstantKind().(*exprpb.Constant_StringValue); !ok {","\t\treturn false","\t}","\treturn isIndexExpr(arg1) \u0026\u0026","\t\tfunction == operators.Equals \u0026\u0026","\t\t!i.isDyn(arg1.GetCallExpr().Args[0])","}","","func isIndexExpr(expr *exprpb.Expr) bool {","\tif callExpr := expr.GetCallExpr(); callExpr != nil \u0026\u0026 isIndexOperator(callExpr.GetFunction()) {","\t\treturn true","\t}","\treturn false","}","","func isIndexOperator(symbol string) bool {","\treturn symbol == operators.Index","}","","func (i *interpreter) translateToJSONPathContainsExpression(arg1 *exprpb.Expr, arg2 *exprpb.Expr) error {","\tcallExprArgs := arg1.GetCallExpr().GetArgs()","\tkey := callExprArgs[len(callExprArgs)-1]","\tfor _, expr := range callExprArgs[0 : len(callExprArgs)-1] {","\t\tif err := i.interpretExpr(expr); err != nil {","\t\t\treturn err","\t\t}","\t}","","\tfmt.Fprintf(\u0026i.query, ` @\u003e '{\"%s\":\"%s\"}'::jsonb`,","\t\tkey.GetConstExpr().GetStringValue(),","\t\targ2.GetConstExpr().GetStringValue())","","\treturn nil","}","","func (i *interpreter) interpretIndexExpr(id int64, expr *exprpb.Expr_Call) error {","\targs := expr.GetArgs()","\tif args[0].GetSelectExpr() != nil {","\t\treturn i.interpretSelectExpr(id, args[0].ExprKind.(*exprpb.Expr_SelectExpr), args[1])","\t}","\tif args[0].GetIdentExpr() != nil {","\t\tif err := i.interpretExpr(args[0]); err != nil {","\t\t\treturn err","\t\t}","","\t\tfmt.Fprintf(\u0026i.query, \"-\u003e\u003e'%s'\", args[1].GetConstExpr().GetStringValue())","","\t\treturn nil","\t}","\treturn i.unsupportedExprError(args[1].Id, \"index\")","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,0,0,2,2,2,0,2,2,2,2,2,1,1,0,0,2,2,2,2,2,0,0,2,2,2,2,2,2,2,1,1,0,2,2,2,0,1,0]},{"id":7,"path":"pkg/api/server/cel2sql/interpreter.go","lines":["// Copyright 2023 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package cel2sql","","import (","\t\"errors\"","\t\"fmt\"","\t\"strings\"","\t\"time\"","","\t\"github.com/google/cel-go/cel\"","\texprpb \"google.golang.org/genproto/googleapis/api/expr/v1alpha1\"",")","","const (","\tspace = \" \"",")","","// ErrUnsupportedExpression is a sentinel error returned when the CEL expression","// cannot be converted to a set of compatible SQL filters.","var ErrUnsupportedExpression = errors.New(\"unsupported CEL\")","","// interpreter is a statefull converter of CEL expressions to equivalent SQL","// filters in the Postgres dialect.","type interpreter struct {","\tcheckedExpr *exprpb.CheckedExpr","","\tquery strings.Builder","}","","// newInterpreter takes an abstract syntax tree and returns an Interpreter object capable","// of converting it to a set of SQL filters.","func newInterpreter(ast *cel.Ast) (*interpreter, error) {","\tcheckedExpr, err := cel.AstToCheckedExpr(ast)","\tif err != nil {","\t\treturn nil, err","\t}","\treturn \u0026interpreter{","\t\tcheckedExpr: checkedExpr,","\t}, nil","}","","// interpret attempts to convert the CEL AST into a set of valid SQL filters. It","// returns an error if the conversion cannot be done.","func (i *interpreter) interpret() (string, error) {","\tif err := i.interpretExpr(i.checkedExpr.Expr); err != nil {","\t\treturn \"\", err","\t}","\treturn strings.TrimSpace(i.query.String()), nil","}","","func (i *interpreter) interpretExpr(expr *exprpb.Expr) error {","\tid := expr.Id","\tswitch node := expr.ExprKind.(type) {","\tcase *exprpb.Expr_ConstExpr:","\t\treturn i.interpretConstExpr(id, node.ConstExpr)","","\tcase *exprpb.Expr_IdentExpr:","\t\treturn i.interpretIdentExpr(id, node)","","\tcase *exprpb.Expr_SelectExpr:","\t\treturn i.interpretSelectExpr(id, node)","","\tcase *exprpb.Expr_CallExpr:","\t\treturn i.interpretCallExpr(id, expr)","","\tcase *exprpb.Expr_ListExpr:","\t\treturn i.interpretListExpr(node)","","\tdefault:","\t\treturn i.unsupportedExprError(id, \"\")","\t}","}","","// unsupportedExprError attempts to return a descriptive error on why the","// provided CEL expression could not be converted.","func (i *interpreter) unsupportedExprError(id int64, name string) error {","\tsourceInfo := i.checkedExpr.SourceInfo","\tcolumn := sourceInfo.Positions[id]","\tvar line int","\tfor i, offset := range sourceInfo.LineOffsets {","\t\tline = i + 1","\t\tif offset \u003e column {","\t\t\tbreak","\t\t}","\t}","","\tif name != \"\" {","\t\tname += \" \"","\t}","","\treturn fmt.Errorf(\"%w %sstatement at line %d, column %d\", ErrUnsupportedExpression, name, line, column)","}","","func (i *interpreter) interpretConstExpr(id int64, expr *exprpb.Constant) error {","\tswitch expr.ConstantKind.(type) {","","\tcase *exprpb.Constant_NullValue:","\t\ti.query.WriteString(\"NULL\")","","\tcase *exprpb.Constant_BoolValue:","\t\tif expr.GetBoolValue() {","\t\t\ti.query.WriteString(\"TRUE\")","\t\t} else {","\t\t\ti.query.WriteString(\"FALSE\")","\t\t}","","\tcase *exprpb.Constant_Int64Value:","\t\tfmt.Fprintf(\u0026i.query, \"%d\", expr.GetInt64Value())","","\tcase *exprpb.Constant_Uint64Value:","\t\tfmt.Fprintf(\u0026i.query, \"%d\", expr.GetInt64Value())","","\tcase *exprpb.Constant_DoubleValue:","\t\tfmt.Fprintf(\u0026i.query, \"%f\", expr.GetDoubleValue())","","\tcase *exprpb.Constant_StringValue:","\t\tfmt.Fprintf(\u0026i.query, \"'%s'\", expr.GetStringValue())","","\tcase *exprpb.Constant_DurationValue:","\t\tfmt.Fprintf(\u0026i.query, \"'%d SECONDS'\", expr.GetDurationValue().Seconds)","","\tcase *exprpb.Constant_TimestampValue:","\t\ttimestamp := expr.GetTimestampValue()","\t\tfmt.Fprintf(\u0026i.query, \"TIMESTAMP WITH TIME ZONE '%s'\", timestamp.AsTime().Format(time.RFC3339))","","\tdefault:","\t\treturn i.unsupportedExprError(id, \"constant\")","\t}","\treturn nil","}","","var identToColumn = map[string]string{","\t\"uid\":         \"id\",","\t\"create_time\": \"created_time\",","\t\"update_time\": \"updated_time\",","\t\"data_type\":   \"type\",","}","","func (i *interpreter) interpretIdentExpr(id int64, expr *exprpb.Expr_IdentExpr) error {","\tif reference, found := i.checkedExpr.ReferenceMap[id]; found \u0026\u0026 reference.GetValue() != nil {","\t\treturn i.interpretConstExpr(id, reference.GetValue())","\t}","\tname := expr.IdentExpr.GetName()","\tif column, found := identToColumn[name]; found {","\t\tname = column","\t}","\ti.query.WriteString(name)","\treturn nil","}","","type Unquoted struct {","\ts string","}","","func (u *Unquoted) String() string {","\treturn u.s","}","","type SingleQuoted struct {","\ts string","}","","func (s *SingleQuoted) String() string {","\treturn fmt.Sprintf(\"'%s'\", s.s)","}","","func (i *interpreter) getIndexKey(expr *exprpb.Expr) (fmt.Stringer, error) {","\tcallExprArgs := expr.GetCallExpr().GetArgs()","\tlastArg := callExprArgs[len(callExprArgs)-1]","\tkey := lastArg.GetConstExpr()","","\tswitch key.ConstantKind.(type) {","\tcase *exprpb.Constant_Int64Value:","\t\treturn \u0026Unquoted{fmt.Sprintf(\"%d\", key.GetInt64Value())}, nil","","\tcase *exprpb.Constant_StringValue:","\t\treturn \u0026SingleQuoted{key.GetStringValue()}, nil","","\tdefault:","\t\treturn nil, i.unsupportedExprError(lastArg.Id, \"constant\")","\t}","}","","func (i *interpreter) getSelectFields(expr *exprpb.Expr) ([]fmt.Stringer, error) {","\tvar target *exprpb.Expr","\tfields := []fmt.Stringer{}","\tswitch node := expr.ExprKind.(type) {","\tcase *exprpb.Expr_SelectExpr:","\t\tfields = append(fields, \u0026SingleQuoted{node.SelectExpr.GetField()})","\t\ttarget = node.SelectExpr.GetOperand()","","\tcase *exprpb.Expr_CallExpr:","\t\tif !isIndexExpr(expr) {","\t\t\t// TODO: return which function is not supported","\t\t\treturn nil, i.unsupportedExprError(expr.Id, \"function\")","\t\t}","\t\t// Sanity check, index function should always have two arguments","\t\tif len(node.CallExpr.Args) != 2 {","\t\t\treturn nil, ErrUnsupportedExpression","\t\t}","\t\ttarget = node.CallExpr.Args[0]","\t\tindex, err := i.getIndexKey(expr)","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\tfields = append(fields, index)","\tcase *exprpb.Expr_IdentExpr:","\t\tfields = append(fields, \u0026Unquoted{node.IdentExpr.GetName()})","\t\ttarget = nil","\tdefault:","\t\treturn nil, ErrUnsupportedExpression","\t}","","\tif target != nil {","\t\tnewFields, err := i.getSelectFields(target)","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\tfields = append(fields, newFields...)","\t}","","\treturn fields, nil","}","","func (i *interpreter) interpretSelectExpr(id int64, expr *exprpb.Expr_SelectExpr, additionalExprs ...*exprpb.Expr) error {","\tfields, err := i.getSelectFields(\u0026exprpb.Expr{Id: id, ExprKind: expr})","\tif err != nil {","\t\treturn err","\t}","","\treversedFields := make([]fmt.Stringer, len(fields))","\tfor j, k := 0, len(fields)-1; j \u003c len(reversedFields); j, k = j+1, k-1 {","\t\treversedFields[j] = fields[k]","\t}","","\tfor _, node := range additionalExprs {","\t\tswitch node.ExprKind.(type) {","\t\tcase *exprpb.Expr_ConstExpr:","\t\t\treversedFields = append(reversedFields, \u0026SingleQuoted{node.GetConstExpr().GetStringValue()})","","\t\tdefault:","\t\t\treturn ErrUnsupportedExpression","\t\t}","\t}","","\tif i.isDyn(expr.SelectExpr.GetOperand()) {","\t\ti.translateToJSONAccessors(reversedFields)","\t\treturn nil","\t}","","\tif i.isRecordSummary(expr.SelectExpr.GetOperand()) {","\t\ti.translateToRecordSummaryColumn(reversedFields)","\t\treturn nil","\t}","","\treturn fmt.Errorf(\"%w. %s: not recognized field\", i.unsupportedExprError(id, \"select\"), reversedFields[0])","}","","func (i *interpreter) interpretCallExpr(id int64, expr *exprpb.Expr) error {","\tcallExpr := expr.GetCallExpr()","\tfunction := callExpr.GetFunction()","\tif isUnaryOperator(function) {","\t\treturn i.interpretUnaryCallExpr(callExpr)","\t}","\tif isBinaryOperator(function) {","\t\treturn i.interpretBinaryCallExpr(expr)","\t}","","\tif isIndexOperator(function) {","\t\treturn i.interpretIndexExpr(id, callExpr)","\t}","","\treturn i.interpretFunctionCallExpr(id, callExpr)","}","","func (i *interpreter) interpretUnaryCallExpr(expr *exprpb.Expr_Call) error {","\tsqlOperator := unaryOperators[expr.GetFunction()]","\ti.query.WriteString(sqlOperator)","\ti.query.WriteString(space)","\ti.query.WriteString(\"(\")","\tif err := i.interpretExpr(expr.Args[0]); err != nil {","\t\treturn err","\t}","\ti.query.WriteString(\")\")","\treturn nil","}","","func (i *interpreter) interpretBinaryCallExpr(expr *exprpb.Expr) error {","\tcallExpr := expr.GetCallExpr()","\tif isConcat := i.mayBeTranslatedToStringConcatExpression(callExpr); isConcat {","\t\treturn i.translateToStringConcatExpression(expr)","\t}","","\tfunction := callExpr.GetFunction()","\targ1 := callExpr.Args[0]","\targ2 := callExpr.Args[1]","","\tif i.mayBeTranslatedToJSONPathContainsExpression(arg1, function, arg2) {","\t\treturn i.translateToJSONPathContainsExpression(arg1, arg2)","\t}","","\tif i.mayBeTranslatedToJSONPathContainsExpression(arg2, function, arg1) {","\t\treturn i.translateToJSONPathContainsExpression(arg2, arg1)","\t}","","\tsqlOperator := binaryOperators[function]","\tif (i.isString(arg1) || i.isString(arg2)) \u0026\u0026 isAddOperator(function) {","\t\tsqlOperator = postgresqlConcatOperator","\t}","","\ti.query.WriteString(\"(\")","","\tif err := i.interpretExpr(arg1); err != nil {","\t\treturn err","\t}","","\t// Implicit coercion","\tif i.isDyn(arg1) {","\t\tif err := i.coerceToTypeOf(arg2); err != nil {","\t\t\treturn err","\t\t}","\t}","","\ti.query.WriteString(space)","\ti.query.WriteString(sqlOperator)","\ti.query.WriteString(space)","","\tif err := i.interpretExpr(arg2); err != nil {","\t\treturn err","\t}","","\t// Implicit coercion","\tif i.isDyn(arg2) {","\t\tif err := i.coerceToTypeOf(arg1); err != nil {","\t\t\treturn err","\t\t}","\t}","\ti.query.WriteString(\")\")","","\treturn nil","}","","func (i *interpreter) interpretListExpr(expr *exprpb.Expr_ListExpr) error {","\telements := expr.ListExpr.GetElements()","\ti.query.WriteString(\"(\")","\tfor index, elem := range elements {","\t\tif err := i.interpretExpr(elem); err != nil {","\t\t\treturn err","\t\t}","\t\tif index \u003c len(elements)-1 {","\t\t\ti.query.WriteString(\", \")","\t\t}","\t}","\ti.query.WriteString(\")\")","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,1,1,2,2,2,0,0,0,0,2,2,1,1,2,0,0,2,2,2,2,2,0,2,2,0,2,2,0,2,2,0,2,2,0,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,1,1,1,0,1,0,0,2,2,0,1,1,0,1,1,1,1,1,1,0,2,2,0,1,1,0,1,1,0,2,2,0,1,1,0,1,1,1,0,1,1,0,2,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,0,2,2,2,0,0,0,0,0,2,2,2,0,2,2,2,2,2,2,2,2,0,2,2,0,1,1,0,0,0,2,2,2,2,2,2,2,0,2,2,1,1,1,0,2,1,1,2,2,2,1,1,2,2,2,2,1,1,0,0,2,2,2,1,1,2,0,0,2,0,0,2,2,2,1,1,0,2,2,2,2,0,2,2,2,2,0,1,1,0,0,0,2,2,2,2,0,2,2,2,2,0,1,0,0,2,2,2,2,2,2,2,2,2,0,2,2,2,0,2,0,0,2,2,2,2,2,2,1,1,2,2,0,0,2,2,2,2,2,0,2,2,2,2,2,2,2,0,2,2,2,0,2,2,1,1,0,2,2,2,1,1,0,0,2,2,1,1,0,0,2,2,2,2,2,1,1,0,0,2,2,1,1,0,2,2,2,0,0,2,2,2,2,2,1,1,2,2,2,0,2,2,0]},{"id":8,"path":"pkg/api/server/cel2sql/operators.go","lines":["// Copyright 2023 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package cel2sql","","import (","\t\"github.com/google/cel-go/common/operators\"",")","","var (","\tunaryOperators = map[string]string{","\t\toperators.LogicalNot: \"NOT\",","\t}","","\tbinaryOperators = map[string]string{","\t\toperators.LogicalAnd:    \"AND\",","\t\toperators.LogicalOr:     \"OR\",","\t\toperators.Equals:        \"=\",","\t\toperators.NotEquals:     \"\u003c\u003e\",","\t\toperators.Less:          \"\u003c\",","\t\toperators.LessEquals:    \"\u003c=\",","\t\toperators.Greater:       \"\u003e\",","\t\toperators.GreaterEquals: \"\u003e=\",","\t\toperators.Add:           \"+\",","\t\toperators.Subtract:      \"-\",","\t\toperators.Multiply:      \"*\",","\t\toperators.Divide:        \"/\",","\t\toperators.Modulo:        \"%\",","\t\toperators.In:            \"IN\",","\t}","\tpostgresqlConcatOperator = \"||\"",")","","// isUnaryOperator returns true if the symbol in question is a CEL unary","// operator.","func isUnaryOperator(symbol string) bool {","\t_, found := unaryOperators[symbol]","\treturn found","}","","// isBinaryOperator returns true if the symbol in question is a CEL binary","// operator.","func isBinaryOperator(symbol string) bool {","\t_, found := binaryOperators[symbol]","\treturn found","}","","func isAddOperator(symbol string) bool {","\treturn symbol == operators.Add","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,0,0,2,2,2,2,0,2,2,2]},{"id":9,"path":"pkg/api/server/cel2sql/select.go","lines":["// Copyright 2023 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package cel2sql","","import (","\t\"fmt\"","","\t\"gorm.io/gorm/schema\"",")","","// translateToJSONAccessors converts the provided field path to a Postgres JSON","// property selection directive. This allows us to yield appropriate SQL","// expressions to navigate through the record.data field, for instance.","func (i *interpreter) translateToJSONAccessors(fieldPath []fmt.Stringer) {","\tlastField := fieldPath[len(fieldPath)-1]","\tfmt.Fprintf(\u0026i.query, \"(\")","\tif len(fieldPath) \u003e 1 {","\t\tfor _, field := range fieldPath[0 : len(fieldPath)-1] {","\t\t\tfmt.Fprintf(\u0026i.query, \"%s-\u003e\", field)","\t\t}","\t}","\tfmt.Fprintf(\u0026i.query, \"\u003e%s\", lastField)","\tfmt.Fprintf(\u0026i.query, \")\")","}","","// translateToRecordSummaryColumn","func (i *interpreter) translateToRecordSummaryColumn(fieldPath []fmt.Stringer) {","\tnamer := \u0026schema.NamingStrategy{}","\tswitch f := fieldPath[1].(type) {","\tcase *Unquoted:","\t\tfmt.Fprintf(\u0026i.query, \"recordsummary_%s\", namer.ColumnName(\"\", f.s))","\tcase *SingleQuoted:","\t\tfmt.Fprintf(\u0026i.query, \"recordsummary_%s\", namer.ColumnName(\"\", f.s))","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,0,2,2,0,0,0,2,2,2,1,1,2,2,0,0]},{"id":10,"path":"pkg/api/server/cel2sql/type_coercion.go","lines":["// Copyright 2023 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package cel2sql","","import (","\texprpb \"google.golang.org/genproto/googleapis/api/expr/v1alpha1\"",")","","// isDyn returns true if the provided expression is a CEL dyn type or false","// otherwise.","func (i *interpreter) isDyn(expr *exprpb.Expr) bool {","\tif theType, found := i.checkedExpr.TypeMap[expr.GetId()]; found {","\t\tif _, ok := theType.GetTypeKind().(*exprpb.Type_Dyn); ok {","\t\t\treturn true","\t\t}","\t}","\treturn false","}","","// isString returns true if the provided expression is a CEL string type or","// false otherwise.","func (i *interpreter) isString(expr *exprpb.Expr) bool {","\tif theType, found := i.checkedExpr.TypeMap[expr.GetId()]; found {","\t\tif p, ok := theType.GetTypeKind().(*exprpb.Type_Primitive); ok {","\t\t\treturn p.Primitive == exprpb.Type_STRING","\t\t}","\t}","\treturn false","}","","func (i *interpreter) isRecordSummary(expr *exprpb.Expr) bool {","\tif theType, found := i.checkedExpr.TypeMap[expr.GetId()]; found {","\t\tif messageType := theType.GetMessageType(); messageType == \"tekton.results.v1alpha2.RecordSummary\" {","\t\t\treturn true","\t\t}","\t}","\treturn false","}","","// coerceToTypeOf writes a Postgres cast directive to the current position of","// the SQL statement in the buffer, in order to cast the current SQL expression","// to the SQL type of the provided CEL expression. This feature provides","// implicit coercion to the supported expressions, by allowing users to compare","// dyn types to more specific types in a transparent manner.","//","// For instance, in the following expression:","// ```go","// data.status.completionTime \u003e timestamp(\"2022/10/30T21:45:00.000Z\")","// ```","// the data field is a dyn type which maps to a jsonb in the Postgres","// database. The implicit coercion casts the completionTime to a SQL timestamp","// in the returned SQL filter.","func (i *interpreter) coerceToTypeOf(expr *exprpb.Expr) error {","\tif theType, found := i.checkedExpr.TypeMap[expr.GetId()]; found {","\t\tswitch theType.GetTypeKind().(type) {","","\t\tcase *exprpb.Type_WellKnown:","\t\t\ti.coerceWellKnownType(theType.GetWellKnown())","\t\t}","\t\treturn nil","\t}","\treturn ErrUnsupportedExpression","}","","func (i *interpreter) coerceWellKnownType(wellKnown exprpb.Type_WellKnownType) {","\tswitch wellKnown {","","\tcase exprpb.Type_TIMESTAMP:","\t\ti.query.WriteString(\"::TIMESTAMP WITH TIME ZONE\")","","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,2,0,0,0,0,2,2,2,2,2,0,2,0,0,2,2,2,2,2,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,2,2,0,2,0,1,0,0,2,2,0,2,2,0,0,0]},{"id":11,"path":"pkg/api/server/config/config.go","lines":["package config","","import (","\t\"log\"","","\t\"github.com/spf13/viper\"",")","","type Config struct {","\tDB_USER                  string `mapstructure:\"DB_USER\"`","\tDB_PASSWORD              string `mapstructure:\"DB_PASSWORD\"`","\tDB_HOST                  string `mapstructure:\"DB_HOST\"`","\tDB_PORT                  string `mapstructure:\"DB_PORT\"`","\tDB_NAME                  string `mapstructure:\"DB_NAME\"`","\tDB_SSLMODE               string `mapstructure:\"DB_SSLMODE\"`","\tDB_SSLROOTCERT           string `mapstructure:\"DB_SSLROOTCERT\"`","\tDB_ENABLE_AUTO_MIGRATION bool   `mapstructure:\"DB_ENABLE_AUTO_MIGRATION\"`","\tDB_MAX_IDLE_CONNECTIONS  int    `mapstructure:\"DB_MAX_IDLE_CONNECTIONS\"`","\tDB_MAX_OPEN_CONNECTIONS  int    `mapstructure:\"DB_MAX_OPEN_CONNECTIONS\"`","\tSERVER_PORT              string `mapstructure:\"SERVER_PORT\"`","\tPROMETHEUS_PORT          string `mapstructure:\"PROMETHEUS_PORT\"`","\tPROMETHEUS_HISTOGRAM     bool   `mapstructure:\"PROMETHEUS_HISTOGRAM\"`","\tLOG_LEVEL                string `mapstructure:\"LOG_LEVEL\"`","\tSQL_LOG_LEVEL            string `mapstructure:\"SQL_LOG_LEVEL\"`","\tTLS_PATH                 string `mapstructure:\"TLS_PATH\"`","\tFEATURE_GATES            string `mapstructure:\"FEATURE_GATES\"`","","\tGRPC_WORKER_POOL int `mapstructure:\"GRPC_WORKER_POOL\"`","\tK8S_QPS          int `mapstructure:\"K8S_QPS\"`","\tK8S_BURST        int `mapstructure:\"K8S_BURST\"`","","\tAUTH_DISABLE     bool `mapstructure:\"AUTH_DISABLE\"`","\tAUTH_IMPERSONATE bool `mapstructure:\"AUTH_IMPERSONATE\"`","","\tLOGS_API         bool   `mapstructure:\"LOGS_API\"`","\tLOGS_TYPE        string `mapstructure:\"LOGS_TYPE\"`","\tLOGS_BUFFER_SIZE int    `mapstructure:\"LOGS_BUFFER_SIZE\"`","\tLOGS_PATH        string `mapstructure:\"LOGS_PATH\"`","\tLOGS_TIMESTAMPS  bool   `mapstructure:\"LOGS_TIMESTAMPS\"`","","\tPROFILING      bool   `mapstructure:\"PROFILING\"`","\tPROFILING_PORT string `mapstructure:\"PROFILING_PORT\"`","","\tGCS_BUCKET_NAME       string `mapstructure:\"GCS_BUCKET_NAME\"`","\tSTORAGE_EMULATOR_HOST string `mapstructure:\"STORAGE_EMULATOR_HOST\"`","","\tS3_BUCKET_NAME        string `mapstructure:\"S3_BUCKET_NAME\"`","\tS3_ENDPOINT           string `mapstructure:\"S3_ENDPOINT\"`","\tS3_HOSTNAME_IMMUTABLE bool   `mapstructure:\"S3_HOSTNAME_IMMUTABLE\"`","\tS3_REGION             string `mapstructure:\"S3_REGION\"`","\tS3_ACCESS_KEY_ID      string `mapstructure:\"S3_ACCESS_KEY_ID\"`","\tS3_SECRET_ACCESS_KEY  string `mapstructure:\"S3_SECRET_ACCESS_KEY\"`","\tS3_MULTI_PART_SIZE    int64  `mapstructure:\"S3_MULTI_PART_SIZE\"`","","\tCONVERTER_ENABLE   bool `mapstructure:\"CONVERTER_ENABLE\"`","\tCONVERTER_DB_LIMIT int  `mapstructure:\"CONVERTER_DB_LIMIT\"`","","\tLOGGING_PLUGIN_API_URL                  string `mapstructure:\"LOGGING_PLUGIN_API_URL\"`","\tLOGGING_PLUGIN_NAMESPACE_KEY            string `mapstructure:\"LOGGING_PLUGIN_NAMESPACE_KEY\"`","\tLOGGING_PLUGIN_CONTAINER_KEY            string `mapstructure:\"LOGGING_PLUGIN_CONTAINER_KEY\"`","\tLOGGING_PLUGIN_STATIC_LABELS            string `mapstructure:\"LOGGING_PLUGIN_STATIC_LABELS\"`","\tLOGGING_PLUGIN_TOKEN_PATH               string `mapstructure:\"LOGGING_PLUGIN_TOKEN_PATH\"`","\tLOGGING_PLUGIN_PROXY_PATH               string `mapstructure:\"LOGGING_PLUGIN_PROXY_PATH\"`","\tLOGGING_PLUGIN_CA_CERT                  string `mapstructure:\"LOGGING_PLUGIN_CA_CERT\"`","\tLOGGING_PLUGIN_QUERY_LIMIT              uint   `mapstructure:\"LOGGING_PLUGIN_QUERY_LIMIT\"`","\tLOGGING_PLUGIN_TLS_VERIFICATION_DISABLE bool   `mapstructure:\"LOGGING_PLUGIN_TLS_VERIFICATION_DISABLE\"`","\tLOGGING_PLUGIN_FORWARDER_DELAY_DURATION int64  `mapstructure:\"LOGGING_PLUGIN_FORWARDER_DELAY_DURATION\"`","\tLOGGING_PLUGIN_QUERY_PARAMS             string `mapstructure:\"LOGGING_PLUGIN_QUERY_PARAMS\"`","\tLOGGING_PLUGIN_MULTIPART_REGEX          string `mapstructure:\"LOGGING_PLUGIN_MULTIPART_REGEX\"`","}","","func Get() *Config {","\tviper.SetConfigName(\"config\")","\tviper.SetConfigType(\"env\")","\tviper.AddConfigPath(\"/etc/tekton/results\")","\tviper.AddConfigPath(\"config/base/env\")","\tviper.AddConfigPath(\"config\")","\tviper.AddConfigPath(\".\")","","\tviper.AutomaticEnv()","","\terr := viper.ReadInConfig()","\tif err != nil {","\t\tlog.Fatalf(\"Error reading config: %v\", err)","\t}","","\tconfig := Config{}","\tif err := viper.Unmarshal(\u0026config); err != nil {","\t\tlog.Fatal(\"Cannot load config:\", err)","\t}","","\treturn \u0026config","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1,0]},{"id":12,"path":"pkg/api/server/db/errors/errors.go","lines":["// Copyright 2020 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package errors //nolint:revive // Package provides db-specific error handling","","import (","\t\"errors\"","","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/status\"","\t\"gorm.io/gorm\"",")","","var (","\terrorSpace ErrorSpace",")","","// ErrorSpace allows implementations to inject database specific error checking","// to the application.","type ErrorSpace func(error) codes.Code","","// RegisterErrorSpace registers the ErrorSpace - last one wins.","func RegisterErrorSpace(f ErrorSpace) {","\terrorSpace = f","}","","// Wrap converts database error codes into their corresponding gRPC status","// codes.","func Wrap(err error) error {","\tif err == nil {","\t\treturn err","\t}","","\t// Check for gorm provided errors first - these are more likely to be","\t// supported across drivers.","\tif code, ok := gormCode(err); ok {","\t\treturn status.Error(code, err.Error())","\t}","","\t// Fallback to implementation specific codes.","\tif errorSpace != nil {","\t\treturn status.Error(errorSpace(err), err.Error())","\t}","","\treturn err","}","","// gormCode returns gRPC status codes corresponding to gorm errors. This is not","// an exhaustive list.","// See https://pkg.go.dev/gorm.io/gorm@v1.20.7#pkg-variables for list of","// errors.","func gormCode(err error) (codes.Code, bool) {","\tif errors.Is(err, gorm.ErrRecordNotFound) {","\t\treturn codes.NotFound, true","\t}","\treturn codes.Unknown, false","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,1,1,1,1,0,0,0,1,1,1,0,0,1,1,1,0,1,0,0,0,0,0,0,1,1,1,1,1,0]},{"id":13,"path":"pkg/api/server/db/errors/postgres/postgres.go","lines":["// Copyright 2024 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","// Package postgres provides postgres-specific error checking.","package postgres","","import (","\t\"errors\"","","\tdberrors \"github.com/tektoncd/results/pkg/api/server/db/errors\"","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/status\"",")","","const (","\tsqlStateUniqueViolation = \"23505\"","\tsqlStateForeignKey      = \"23503\"",")","","// sqlStateError captures the subset of PgError behavior we rely on.","type sqlStateError interface {","\tSQLState() string","}","","// translate converts postgres error codes to gRPC status codes.","func translate(err error) codes.Code {","\tvar sqlErr sqlStateError","\tif errors.As(err, \u0026sqlErr) {","\t\tswitch sqlErr.SQLState() {","\t\tcase sqlStateUniqueViolation:","\t\t\treturn codes.AlreadyExists","\t\tcase sqlStateForeignKey:","\t\t\treturn codes.FailedPrecondition","\t\t}","\t}","\treturn status.Code(err)","}","","func init() {","\tdberrors.RegisterErrorSpace(translate)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,0,0,2,0,0,2,2,2]},{"id":14,"path":"pkg/api/server/db/errors/sqlite/sqlite.go","lines":["// Copyright 2020 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","// Package sqlite provides sqlite-specific error checking. This is","// purposefully broken out from the rest of the errors package so that we can","// isolate go-sqlite3's cgo dependency away from the main MySQL based library","// to simplify our testing + deployment.","package sqlite","","import (","\t\"github.com/mattn/go-sqlite3\"","\t\"github.com/tektoncd/results/pkg/api/server/db/errors\"","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/status\"",")","","// sqlite converts sqlite3 error codes to gRPC status codes. This is not an","// exhaustive list.","// See https://pkg.go.dev/github.com/mattn/go-sqlite3#pkg-variables for list of","// error codes.","func sqlite(err error) codes.Code {","\tserr, ok := err.(sqlite3.Error)","\tif !ok {","\t\treturn status.Code(err)","\t}","","\tswitch serr.Code {","\tcase sqlite3.ErrConstraint:","\t\tswitch serr.ExtendedCode {","\t\tcase sqlite3.ErrConstraintUnique:","\t\t\treturn codes.AlreadyExists","\t\tcase sqlite3.ErrConstraintForeignKey:","\t\t\treturn codes.FailedPrecondition","\t\t}","\t\treturn codes.InvalidArgument","\tcase sqlite3.ErrNotFound:","\t\treturn codes.NotFound","\t}","\treturn status.Code(err)","}","","func init() {","\terrors.RegisterErrorSpace(sqlite)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,1,1,1,1,1,1,1,0,1,1,1,0,1,0,0,1,1,1]},{"id":15,"path":"pkg/api/server/db/log_level.go","lines":["// Copyright 2025 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","// Package db defines database models for Result data.","package db","","import (","\t\"errors\"","","\tgormlogger \"gorm.io/gorm/logger\"",")","","var (","\tlogLevel = map[string]gormlogger.LogLevel{","\t\t\"silent\": gormlogger.Silent,","\t\t\"error\":  gormlogger.Error,","\t\t\"warn\":   gormlogger.Warn,","\t\t\"info\":   gormlogger.Info,","\t}",")","","// SetLogLevel for the Default GormLogger","func SetLogLevel(level string) error {","\tif _, ok := logLevel[level]; !ok {","\t\treturn errors.New(\"undefined log level for sql\")","\t}","\tgormlogger.Default.LogMode(logLevel[level])","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0]},{"id":16,"path":"pkg/api/server/db/model.go","lines":["// Copyright 2020 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","// Package db defines database models for Result data.","package db","","import (","\t\"database/sql/driver\"","\t\"encoding/json\"","\t\"errors\"","\t\"fmt\"","\t\"time\"",")","","// Result is the database model of a Result.","type Result struct {","\tParent      string      `gorm:\"primaryKey;uniqueIndex:results_by_name,priority:1;size:64;\"`","\tID          string      `gorm:\"primaryKey;size:64;\"`","\tName        string      `gorm:\"uniqueIndex:results_by_name,priority:2;size:64;\"`","\tAnnotations Annotations `gorm:\"type:jsonb;\"`","","\tCreatedTime time.Time `gorm:\"default:current_timestamp;\"`","\tUpdatedTime time.Time `gorm:\"default:current_timestamp;\"`","","\tSummary RecordSummary `gorm:\"embedded;embeddedPrefix:recordsummary_;\"`","","\tEtag string `gorm:\"size:128;\"`","}","","// RecordSummary is the database model of a Result.RecordSummary.","type RecordSummary struct {","\tRecord string `gorm:\"size:256;\"`","\t// Napkin Math (with a bit of buffer): 256 (DNS Subdomain) * 3 (Group +","\t// Version + Kind).","\tType        string `gorm:\"size:768;\"`","\tStartTime   *time.Time","\tEndTime     *time.Time","\tStatus      int32","\tAnnotations Annotations `gorm:\"type:jsonb;\"`","}","","func (r Result) String() string {","\treturn fmt.Sprintf(\"(%s, %s)\", r.Parent, r.ID)","}","","// Record is the database model of a Record","type Record struct {","\t// Result is used to create the relationship between the Result and Records","\t// table. Data will not be returned here during reads. Use the foreign key","\t// fields instead.","\tResult     Result `gorm:\"foreignKey:Parent,ResultID;references:Parent,ID;constraint:OnUpdate:CASCADE,OnDelete:CASCADE;\"`","\tParent     string `gorm:\"primaryKey;uniqueIndex:records_by_name,priority:1;size:64;\"`","\tResultID   string `gorm:\"primaryKey;size:64;\"`","\tResultName string `gorm:\"uniqueIndex:records_by_name,priority:2;size:64;\"`","","\tID   string `gorm:\"primaryKey;size:64;\"`","\tName string `gorm:\"index:records_by_name,priority:3;size:64;\"`","","\t// Napkin Math (with a bit of buffer): 256 (DNS Subdomain) * 3 (Group +","\t// Version + Kind).","\tType string `gorm:\"size:768;\"`","\tData []byte `gorm:\"type:jsonb;\"`","","\tCreatedTime time.Time `gorm:\"default:current_timestamp;\"`","\tUpdatedTime time.Time `gorm:\"default:current_timestamp;\"`","","\tEtag string `gorm:\"size:128;\"`","}","","// Annotations is a custom-defined type of a gorm model field.","type Annotations map[string]string","","// Scan resolves serialized data read from database into an Annotation.","// This implements the sql.Scanner interface.","func (ann *Annotations) Scan(value any) error {","\tif ann == nil {","\t\treturn errors.New(\"the annotation pointer mustn't be nil\")","\t}","\tbytes, ok := value.([]byte)","\tif !ok {","\t\treturn fmt.Errorf(\"wanted []byte, got %T: %+v\", value, value)","\t}","\treturn json.Unmarshal(bytes, ann)","}","","// Value returns the value of Annotations for database driver. This implements driver.Valuer.","// gorm uses this function to convert a database model's Annotation field into a type that gorm","// driver can write into the database.","func (ann Annotations) Value() (driver.Value, error) {","\tbytes, err := json.Marshal(ann)","\tif err != nil {","\t\treturn nil, err","\t}","\treturn bytes, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,1,1,2,0,0,0,0,0,2,2,2,1,1,2,0]},{"id":17,"path":"pkg/api/server/db/pagination/pagination.go","lines":["// Copyright 2020 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","// Package pagination provides utilities for paginating database query results.","package pagination","","import (","\t\"encoding/base64\"","\t\"math\"","","\tpb \"github.com/tektoncd/results/pkg/api/server/db/pagination/proto/internal_go_proto\"","\t\"google.golang.org/protobuf/proto\"",")","","// EncodeToken encodes a name + filter to an opaque page token","func EncodeToken(name, filter string) (token string, err error) {","\tpi := \u0026pb.ListPageIdentifier{","\t\tName:   name,","\t\tFilter: filter,","\t}","\tvar tokenByte []byte","\tif tokenByte, err = proto.Marshal(pi); err != nil {","\t\treturn \"\", err","\t}","\tencodedResult := make([]byte, base64.RawURLEncoding.EncodedLen(len(tokenByte)))","\tbase64.RawURLEncoding.Encode(encodedResult, tokenByte)","\treturn base64.RawURLEncoding.EncodeToString(encodedResult), nil","}","","// DecodeToken decodes an opaque page token into its name and filter parts.","func DecodeToken(token string) (name, filter string, err error) {","\tvar encodedToken []byte","\tif encodedToken, err = base64.RawURLEncoding.DecodeString(token); err != nil {","\t\treturn \"\", \"\", err","\t}","\ttokenByte := make([]byte, base64.RawURLEncoding.DecodedLen(len(encodedToken)))","\tif _, err = base64.RawURLEncoding.Decode(tokenByte, encodedToken); err != nil {","\t\treturn \"\", \"\", err","\t}","\tpi := \u0026pb.ListPageIdentifier{}","\tif err = proto.Unmarshal(tokenByte, pi); err != nil {","\t\treturn \"\", \"\", err","\t}","\treturn pi.GetName(), pi.GetFilter(), err","}","","// Batcher suggests dynamic batch sizes for list queries.","type Batcher struct {","\t// Total number of items we want to fetch","\twant int","","\t// Min/Max thresholds of number of items to fetch for a given batch.","\tmin, max int","","\t// ratio is used to detemine batch sizes relative to the wanted number of","\t// results. This value changes each iteration based the number of items","\t// successfully fetches and the total number of items fetched in the","\t// previous batch. The less the previous ratio is, the bigger the upcoming","\t// batch_size is.","\tratio float64","}","","// NewBatcher creates a new batcher for the given requested page size.","func NewBatcher(want, mini, maxi int) *Batcher {","\treturn \u0026Batcher{","\t\twant:  want,","\t\tmin:   mini,","\t\tmax:   maxi,","\t\tratio: 1,","\t}","}","","// Update updates the Batcher based on the results of the last batch.","// `matched` is the number of items successfully matched by filters.","// `total` is the total number of rows last fetched.","// This calculates a new value to be used for calls to Next.","func (b *Batcher) Update(matched, total int) {","\tb.ratio = float64(matched) / float64(total)","}","","// Next returns the recommended next batch size to query.","func (b *Batcher) Next() int {","\tn := int(math.Ceil(float64(b.want) / b.ratio))","\tif n \u003e b.max {","\t\treturn b.max","\t}","\treturn n","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,1,1,2,2,2,0,0,0,2,2,2,1,1,2,2,1,1,2,2,1,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,0,0,0,0,0,2,2,2,0,0,2,2,2,2,2,2,0]},{"id":18,"path":"pkg/api/server/features/features.go","lines":["// Package features provides feature flag management for the API server.","package features","","import (","\t\"fmt\"","\t\"sort\"","\t\"strconv\"","\t\"strings\"","\t\"sync/atomic\"",")","","// Feature is a string representation of a set of enabled/disabled features.","type Feature string","","const (","\t// PartialResponse feature enables response filtering","\tPartialResponse Feature = \"PartialResponse\"",")","","var defaultFeatures = map[Feature]bool{","\tPartialResponse: false,","}","","// FeatureGate provides a interface to manipulate features.","type FeatureGate interface {","\t// Get returns true if the key is enabled.","\tGet(f Feature) *atomic.Bool","\t// Set parses and stores flag gates for known features","\t// from a string like feature1=true,feature2=false,...","\tSet(value string) error","\t// String returns a string representation of the known features","\t// in the following form feature1=true,feature2=false,...","\tString() string","\t// Enable will enable a feature","\tEnable(f Feature)","\t// Disable will disable a feature","\tDisable(f Feature)","}","","type featureGate map[Feature]*atomic.Bool","","// NewFeatureGate creates a new FeatureGate","func NewFeatureGate() FeatureGate {","\tfg := featureGate{}","\tfor k, v := range defaultFeatures {","\t\ta := new(atomic.Bool)","\t\ta.Store(v)","\t\tfg[k] = a","\t}","\treturn fg","}","","// Get returns if a feature is enabled","func (fg featureGate) Get(f Feature) *atomic.Bool {","\tif v, ok := fg[f]; ok {","\t\treturn v","\t}","\treturn nil","}","","// Set parses an input string and sets the FeatureGate","func (fg featureGate) Set(value string) error {","\tvalue = strings.TrimSpace(value)","\tfor _, s := range strings.Split(value, \",\") {","\t\tif len(s) == 0 {","\t\t\tcontinue","\t\t}","\t\tpair := strings.SplitN(s, \"=\", 2)","\t\tf := Feature(pair[0])","\t\tv := pair[1]","\t\tif len(pair) != 2 {","\t\t\treturn fmt.Errorf(\"missing value for feature %s\", f)","\t\t}","\t\tb, err := strconv.ParseBool(v)","\t\tif err != nil {","\t\t\treturn fmt.Errorf(\"invalid value of %s=%s, err: %v\", f, v, err)","\t\t}","\t\tif v, ok := fg[f]; ok {","\t\t\tv.Swap(b)","\t\t} else {","\t\t\treturn fmt.Errorf(\"feature '%s' is not supported\", f)","\t\t}","\t}","\treturn nil","}","","// Disable disables a feature","func (fg featureGate) Disable(f Feature) {","\tif v, ok := fg[f]; ok {","\t\tv.Swap(false)","\t}","}","","// Enable enables a feature","func (fg featureGate) Enable(f Feature) {","\tif v, ok := fg[f]; ok {","\t\tv.Swap(true)","\t}","}","","// String returns the value of the FeatureGate as string representation","func (fg featureGate) String() string {","\tvar pairs []string","\tfor k, v := range fg {","\t\tpairs = append(pairs, fmt.Sprintf(\"%s=%t\", k, v.Load()))","\t}","\tsort.Strings(pairs)","\treturn strings.Join(pairs, \",\")","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,0,0,0,2,2,2,2,1,0,0,0,2,2,2,2,1,0,2,2,2,2,1,1,2,2,1,1,2,2,2,1,1,0,2,0,0,0,2,2,2,2,0,0,0,2,2,2,2,0,0,0,2,2,2,2,2,2,2,0]},{"id":19,"path":"pkg/api/server/logger/logger.go","lines":["// Package logger provides logging utilities for the API server.","package logger","","import (","\t\"log\"","","\t\"go.uber.org/zap\"",")","","// Get returns instance sugared zap logger with production configuration","func Get(level string) *zap.SugaredLogger {","\tconfig := zap.NewProductionConfig()","\tif len(level) \u003e 0 {","\t\tvar err error","\t\tif config.Level, err = zap.ParseAtomicLevel(level); err != nil {","\t\t\tlog.Fatalf(\"Failed to parse log level from config: %v\", err)","\t\t}","\t}","","\tlogger, err := config.Build()","\tif err != nil {","\t\tlog.Fatalf(\"Failed to initialize zap logger: %v\", err)","\t}","","\treturn logger.Sugar()","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,1,1,1,1,0,1,0]},{"id":20,"path":"pkg/api/server/v1alpha2/auth/impersonation/impersonation.go","lines":["// Package impersonation provides user impersonation support for API requests.","package impersonation","","import (","\t\"context\"","\t\"errors\"","\t\"fmt\"","","\t\"github.com/grpc-ecosystem/grpc-gateway/v2/runtime\"","\t\"google.golang.org/grpc/metadata\"","\tauthenticationv1 \"k8s.io/api/authentication/v1\"","\tauthorizationv1 \"k8s.io/api/authorization/v1\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/apiserver/pkg/authentication/user\"","\tauthorizationclient \"k8s.io/client-go/kubernetes/typed/authorization/v1\"","","\t\"net/url\"","\t\"strings\"","","\t\"k8s.io/apiserver/pkg/authentication/serviceaccount\"",")","","// Impersonation represents component to add Kubernetes impersonation header processing,","// make impersonation access check and RBAC for Tekton results resources with impersonated user data","type Impersonation struct {","\tresourceAttributes []authorizationv1.ResourceAttributes","\tuserInfo           *user.DefaultInfo","}","","var (","\t// ErrNoImpersonationData is an error message for no impersonation data case","\tErrNoImpersonationData = errors.New(\"no impersonation data found\")","\t// ErrImpersonateUserRequired is an error message about required impersonate user to impersonate another info.","\tErrImpersonateUserRequired = errors.New(\"impersonate user is required to impersonate groups, UID, extra\")",")","","// NewImpersonation returns an impersonation request if any impersonation data is found, returns error otherwise.","func NewImpersonation(md metadata.MD) (*Impersonation, error) {","\ti := \u0026Impersonation{}","\tif err := i.parseMetadata(md); err != nil {","\t\treturn nil, err","\t}","\treturn i, nil","}","","// parseMetadata parses the gRPC metadata and returns a lst of object references that represents the Impersonation data.","func (i *Impersonation) parseMetadata(md metadata.MD) error {","\ti.userInfo = \u0026user.DefaultInfo{}","\tusers := md.Get(authenticationv1.ImpersonateUserHeader)","\thasUser := len(users) \u003e 0","\tif hasUser {","\t\tif namespace, name, err := serviceaccount.SplitUsername(users[0]); err == nil {","\t\t\ti.userInfo.Name = serviceaccount.MakeUsername(namespace, name)","\t\t\ti.resourceAttributes = append(i.resourceAttributes, authorizationv1.ResourceAttributes{","\t\t\t\tName:      name,","\t\t\t\tNamespace: namespace,","\t\t\t\tResource:  \"serviceaccounts\",","\t\t\t\tVerb:      \"impersonate\",","\t\t\t})","","\t\t\t// If groups aren't specified for a service account, we know the groups because it's a fixed mapping.","\t\t\tif len(md.Get(authenticationv1.ImpersonateGroupHeader)) == 0 {","\t\t\t\ti.userInfo.Groups = serviceaccount.MakeGroupNames(namespace)","\t\t\t}","\t\t} else {","\t\t\ti.userInfo.Name = users[0]","\t\t\ti.resourceAttributes = append(i.resourceAttributes, authorizationv1.ResourceAttributes{","\t\t\t\tName:     users[0],","\t\t\t\tResource: \"users\",","\t\t\t\tVerb:     \"impersonate\",","\t\t\t})","\t\t}","\t}","","\tgroups := md.Get(authenticationv1.ImpersonateGroupHeader)","\thasGroups := len(groups) \u003e 0","\tif hasGroups {","\t\tfor _, group := range groups {","\t\t\ti.userInfo.Groups = append(i.userInfo.Groups, group)","\t\t\ti.resourceAttributes = append(i.resourceAttributes, authorizationv1.ResourceAttributes{","\t\t\t\tName:     group,","\t\t\t\tResource: \"groups\",","\t\t\t\tVerb:     \"impersonate\",","\t\t\t})","\t\t}","\t}","","\tUIDs := md.Get(authenticationv1.ImpersonateUIDHeader)","\thasUID := len(UIDs) \u003e 0","\tif hasUID {","\t\ti.userInfo.UID = UIDs[0]","\t\ti.resourceAttributes = append(i.resourceAttributes, authorizationv1.ResourceAttributes{","\t\t\tGroup:    authenticationv1.SchemeGroupVersion.Group,","\t\t\tName:     UIDs[0],","\t\t\tResource: \"uids\",","\t\t\tVerb:     \"impersonate\",","\t\t})","\t}","","\thasExtra := false","\tfor name, values := range md {","\t\tif !strings.HasPrefix(name, strings.ToLower(authenticationv1.ImpersonateUserExtraHeaderPrefix)) {","\t\t\tcontinue","\t\t}","","\t\thasExtra = true","\t\textraKey := unescapeExtraKey(strings.ToLower(name[len(authenticationv1.ImpersonateUserExtraHeaderPrefix):]))","\t\ti.userInfo.Extra = make(map[string][]string)","\t\t// Each extra value is a separate resource to check.","\t\tfor _, value := range values {","\t\t\ti.userInfo.Extra[extraKey] = append(i.userInfo.Extra[extraKey], value)","\t\t\ti.resourceAttributes = append(i.resourceAttributes, authorizationv1.ResourceAttributes{","\t\t\t\tGroup:       authenticationv1.SchemeGroupVersion.Group,","\t\t\t\tName:        value,","\t\t\t\tResource:    \"userextras\",","\t\t\t\tSubresource: extraKey,","\t\t\t\tVerb:        \"impersonate\",","\t\t\t})","\t\t}","\t}","","\t// When impersonating a non-anonymous user, include the 'system:authenticated' group in the impersonated user info:","\t// - if no groups were specified","\t// - if a group has been specified other than 'system:authenticated'","\t// If 'system:unauthenticated' group has been specified we should not include the 'system:authenticated' group.","\tif hasUser {","\t\tif i.userInfo.Name != user.Anonymous {","\t\t\taddAuthenticated := true","\t\t\tfor _, group := range i.userInfo.Groups {","\t\t\t\tif group == user.AllAuthenticated || group == user.AllUnauthenticated {","\t\t\t\t\taddAuthenticated = false","\t\t\t\t\tbreak","\t\t\t\t}","\t\t\t}","","\t\t\tif addAuthenticated {","\t\t\t\ti.userInfo.Groups = append(i.userInfo.Groups, user.AllAuthenticated)","\t\t\t}","\t\t} else {","\t\t\taddUnauthenticated := true","\t\t\tfor _, group := range i.userInfo.Groups {","\t\t\t\tif group == user.AllUnauthenticated {","\t\t\t\t\taddUnauthenticated = false","\t\t\t\t\tbreak","\t\t\t\t}","\t\t\t}","","\t\t\tif addUnauthenticated {","\t\t\t\ti.userInfo.Groups = append(i.userInfo.Groups, user.AllUnauthenticated)","\t\t\t}","\t\t}","\t} else {","\t\ti.userInfo = nil","","\t\t// Impersonate-User header is mandatory.","\t\t// https://kubernetes.io/docs/reference/access-authn-authz/authentication/#user-impersonation","\t\tif hasGroups || hasExtra || hasUID {","\t\t\treturn ErrImpersonateUserRequired","\t\t}","\t\treturn ErrNoImpersonationData","\t}","","\treturn nil","}","","func unescapeExtraKey(encodedKey string) string {","\tkey, err := url.PathUnescape(encodedKey) // Decode %-encoded bytes.","\tif err != nil {","\t\treturn encodedKey // Always record extra strings, even if malformed/unencoded.","\t}","\treturn key","}","","// Check checks if the requester has permission to impersonate every resource.","func (i *Impersonation) Check(ctx context.Context, authorizer authorizationclient.AuthorizationV1Interface, requester string) error {","\tif i.resourceAttributes == nil {","\t\treturn ErrNoImpersonationData","\t}","\tfor _, resourceAttribute := range i.resourceAttributes {","\t\tsar, err := authorizer.SubjectAccessReviews().Create(ctx, \u0026authorizationv1.SubjectAccessReview{","\t\t\tSpec: authorizationv1.SubjectAccessReviewSpec{","\t\t\t\tUser: requester,","\t\t\t\tResourceAttributes: \u0026authorizationv1.ResourceAttributes{","\t\t\t\t\tGroup:       resourceAttribute.Group,","\t\t\t\t\tName:        resourceAttribute.Name,","\t\t\t\t\tNamespace:   resourceAttribute.Namespace,","\t\t\t\t\tResource:    resourceAttribute.Resource,","\t\t\t\t\tSubresource: resourceAttribute.Subresource,","\t\t\t\t\tVerb:        resourceAttribute.Verb,","\t\t\t\t},","\t\t\t},","\t\t}, metav1.CreateOptions{})","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\tif !sar.Status.Allowed {","\t\t\treturn fmt.Errorf(\"forbidden: '%s' doesn't have permission to impersonate '%s'\", requester, resourceAttribute.Resource)","\t\t}","\t}","","\treturn nil","}","","// GetUserInfo returns the impersonated user information.","func (i *Impersonation) GetUserInfo() *user.DefaultInfo {","\treturn i.userInfo","}","","// HeaderMatcher matches the impersonation header for adding to GRPC metadata.","func HeaderMatcher(key string) (string, bool) {","\tif strings.HasPrefix(key, authenticationv1.ImpersonateUserExtraHeaderPrefix) {","\t\treturn key, true","\t}","","\tswitch key {","\tcase authenticationv1.ImpersonateUserHeader:","\t\treturn key, true","\tcase authenticationv1.ImpersonateGroupHeader:","\t\treturn key, true","\tcase authenticationv1.ImpersonateUIDHeader:","\t\treturn key, true","\tdefault:","\t\treturn runtime.DefaultHeaderMatcher(key)","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,0,2,2,2,2,2,1,1,0,0,0,2,2,2,1,1,1,1,1,1,0,0,0,1,1,1,0,2,2,2,2,2,2,2,2,2,0,0,2,0,0,2,2,2,1,1,2,0,0,0,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,0,0,2,0,0,0,1,1,1,0,0,2,2,2,2,0,2,2,2,1,1,1,1,2,2,0,0]},{"id":21,"path":"pkg/api/server/v1alpha2/auth/nop.go","lines":["// Copyright 2021 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package auth","","import \"context\"","","// AllowAll is an auth check that allows every request, regardless of the","// params. Useful for testing or cases where you want to disable auth checks.","type AllowAll struct{}","","// Check does nothing.","func (AllowAll) Check(context.Context, string, string, string) error {","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1]},{"id":22,"path":"pkg/api/server/v1alpha2/auth/rbac.go","lines":["// Copyright 2021 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package auth","","import (","\t\"context\"","\t\"errors\"","\t\"fmt\"","\t\"log\"","\t\"strings\"","","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/auth/impersonation\"","","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/metadata\"","\t\"google.golang.org/grpc/status\"","\tauthnv1 \"k8s.io/api/authentication/v1\"","\tauthzv1 \"k8s.io/api/authorization/v1\"","\tcorev1 \"k8s.io/api/core/v1\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/client-go/kubernetes\"","\tauthnclient \"k8s.io/client-go/kubernetes/typed/authentication/v1\"","\tauthzclient \"k8s.io/client-go/kubernetes/typed/authorization/v1\"",")","","// RBAC is a Kubernetes RBAC based auth checker. This uses the Kubernetes","// TokenReview and SubjectAccessReview APIs to defer auth decisions to the","// cluster.","// Users should pass in `token` metadata through the gRPC context.","// This checks RBAC permissions in the `results.tekton.dev` group, and assumes","// checks are done at the namespace","type RBAC struct {","\tallowImpersonation bool","\tauthn              authnclient.AuthenticationV1Interface","\tauthz              authzclient.AuthorizationV1Interface","}","","// Option is configuration option for RBAC checker.","type Option func(*RBAC)","","// NewRBAC returns new instance of the Kubernetes RBAC based auth checker.","func NewRBAC(client kubernetes.Interface, options ...Option) *RBAC {","\trbac := \u0026RBAC{","\t\tauthn: client.AuthenticationV1(),","\t\tauthz: client.AuthorizationV1(),","\t}","\tfor _, option := range options {","\t\toption(rbac)","\t}","\treturn rbac","}","","// Check determines if resource can be accessed with impersonation metadata stored in the context.","func (r *RBAC) Check(ctx context.Context, namespace, resource, verb string) error {","\tmd, ok := metadata.FromIncomingContext(ctx)","\tif !ok {","\t\treturn status.Error(codes.Unauthenticated, \"unable to get context metadata\")","\t}","","\t// Parse Impersonation header if the feature is enabled","\tvar impersonator *impersonation.Impersonation","\tvar err error","\tif r.allowImpersonation {","\t\timpersonator, err = impersonation.NewImpersonation(md)","\t\t// Ignore ErrorNoImpersonationData errors. This means that the request does not have any","\t\t// impersonation headers and should be processed normally.","\t\tif err != nil \u0026\u0026 !errors.Is(err, impersonation.ErrNoImpersonationData) {","\t\t\tlog.Println(err)","\t\t\treturn status.Error(codes.Unauthenticated, \"invalid impersonation data\")","\t\t}","\t}","","\tv := md.Get(\"authorization\")","\tif len(v) == 0 {","\t\treturn status.Error(codes.Unauthenticated, \"unable to find token\")","\t}","","\tif verb == PermissionList \u0026\u0026 namespace == \"-\" {","\t\t// In list operations `-` means that the caller wants to list","\t\t// resources across all parents. Thus, let's assume all","\t\t// namespaces here.","\t\tnamespace = corev1.NamespaceAll","\t}","","\tretMsg := \"permission denied\"","\tfor _, raw := range v {","\t\t// We expect tokens to be in the form \"Bearer \u003ctoken\u003e\". Parse the token out.","\t\ts := strings.SplitN(raw, \" \", 2)","\t\tif len(s) \u003c 2 {","\t\t\tlog.Println(\"unknown auth token format\")","\t\t\tcontinue","\t\t}","\t\tt := s[1]","","\t\t// Authenticate the token by sending it to the API Server for review.","\t\ttr, err := r.authn.TokenReviews().Create(ctx, \u0026authnv1.TokenReview{","\t\t\tSpec: authnv1.TokenReviewSpec{","\t\t\t\tToken: t,","\t\t\t},","\t\t}, metav1.CreateOptions{})","\t\tif err != nil {","\t\t\tlog.Println(err)","\t\t\tcontinue","\t\t}","\t\tif !tr.Status.Authenticated {","\t\t\tcontinue","\t\t}","","\t\tuser := tr.Status.User.Username","\t\tUID := tr.Status.User.UID","\t\tgroups := tr.Status.User.Groups","\t\textra := convertExtra[authnv1.ExtraValue](tr.Status.User.Extra)","","\t\t// Check whether the authenticated user has permission to impersonate","\t\tif impersonator != nil {","\t\t\tif err := impersonator.Check(ctx, r.authz, user); err != nil {","\t\t\t\tlog.Println(err)","\t\t\t\tretMsg = fmt.Sprintf(\"%s: %s\", retMsg, err.Error())","\t\t\t\treturn status.Error(codes.Unauthenticated, retMsg)","\t\t\t}","\t\t\t// Change user data to impersonated user","\t\t\tuserInfo := impersonator.GetUserInfo()","\t\t\tuser = userInfo.GetName()","\t\t\tUID = userInfo.GetUID()","\t\t\tgroups = userInfo.GetGroups()","\t\t\textra = convertExtra[[]string](userInfo.GetExtra())","\t\t}","","\t\t// Authorize the request by checking the RBAC permissions for the resource.","\t\tsar, err := r.authz.SubjectAccessReviews().Create(ctx, \u0026authzv1.SubjectAccessReview{","\t\t\tSpec: authzv1.SubjectAccessReviewSpec{","\t\t\t\tUser:   user,","\t\t\t\tUID:    UID,","\t\t\t\tGroups: groups,","\t\t\t\tExtra:  extra,","\t\t\t\tResourceAttributes: \u0026authzv1.ResourceAttributes{","\t\t\t\t\tNamespace: namespace,","\t\t\t\t\tGroup:     \"results.tekton.dev\",","\t\t\t\t\tResource:  resource,","\t\t\t\t\tVerb:      verb,","\t\t\t\t},","\t\t\t},","\t\t}, metav1.CreateOptions{})","\t\tif err != nil {","\t\t\tretMsg = fmt.Sprintf(\"%s user %q in groups %q employing %q against %q: %s\", retMsg, user, groups, verb, resource, err.Error())","\t\t\tlog.Println(err)","\t\t\tcontinue","\t\t}","\t\tif sar.Status.Allowed {","\t\t\treturn nil","\t\t}","\t}","\t// Return Unauthenticated - we don't know if we failed because of invalid","\t// token or unauthorized user, so this is safer to not leak any state.","\treturn status.Error(codes.Unauthenticated, retMsg)","}","","// convertExtra converts the map[string][]string or authnv1.ExtraValue to map[string]ExtraValue for Subject Access Review.","func convertExtra[T []string | authnv1.ExtraValue](extra map[string]T) map[string]authzv1.ExtraValue {","\tvar newExtra = make(map[string]authzv1.ExtraValue)","\tfor key, value := range extra {","\t\tnewExtra[key] = authzv1.ExtraValue(value)","\t}","\treturn newExtra","}","","// WithImpersonation is an option function to enable Impersonation","func WithImpersonation(enabled bool) Option {","\treturn func(r *RBAC) {","\t\tr.allowImpersonation = enabled","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,1,1,0,0,2,2,2,2,2,2,2,1,1,1,0,0,2,2,1,1,0,2,1,1,1,1,1,0,2,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,1,1,0,2,2,0,0,2,2,2,2,2,2,2,2,1,1,1,1,0,2,2,2,2,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,0,2,2,2,0,0,0,2,0,0,0,2,2,2,2,2,2,0,0,0,2,2,2,2,0]},{"id":23,"path":"pkg/api/server/v1alpha2/handlers.go","lines":["// Package server provides the v1alpha2 API server implementation.","package server","","import (","\t\"net/http\"","","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/plugin\"",")","","// Handler returns a http.Handler that serves the gRPC server and the log plugin server","func Handler(grpcMux http.Handler, pluginServer *plugin.LogServer) http.Handler {","\tmux := http.NewServeMux()","\tmux.Handle(\"/\", grpcMux)","\tif pluginServer != nil \u0026\u0026 pluginServer.IsLogPluginEnabled {","\t\tmux.Handle(\"/apis/results.tekton.dev/v1alpha2/parents/{parent}/results/{resultID}/logs/{recordID}\", pluginServer.LogMux())","\t\tmux.Handle(\"/apis/results.tekton.dev/v1alpha3/parents/{parent}/results/{resultID}/logs/{recordID}\", pluginServer.LogMux())","\t}","\treturn mux","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0]},{"id":24,"path":"pkg/api/server/v1alpha2/lister/aggregator.go","lines":["// Copyright 2021 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","// Package lister provides list and aggregation operations for Results resources.","package lister","","import (","\t\"context\"","\t\"fmt\"","\t\"regexp\"","\t\"strings\"","","\t\"github.com/google/cel-go/cel\"","\ttdb \"github.com/tektoncd/results/pkg/api/server/db\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/status\"","\t\"google.golang.org/protobuf/types/known/structpb\"","\t\"gorm.io/gorm\"",")","","const (","\tdurationQuery          = \"(data-\u003e'status'-\u003e\u003e'completionTime')::TIMESTAMP WITH TIME ZONE - (data-\u003e'status'-\u003e\u003e'startTime')::TIMESTAMP WITH TIME ZONE\"","\tstatusQuery            = \"(data-\u003e'status'-\u003e'conditions'-\u003e0-\u003e\u003e'reason')\"","\tgroupByTimeQuery       = \"(data-\u003e'metadata'-\u003e\u003e'creationTimestamp')::TIMESTAMP WITH TIME ZONE\"","\tgroupByParentQuery     = \"data-\u003e'metadata'-\u003e\u003e'namespace'\"","\tgroupByPipelineQuery   = \"data-\u003e'metadata'-\u003e'labels'-\u003e\u003e'tekton.dev/pipeline'\"","\tgroupByRepositoryQuery = \"data-\u003e'metadata'-\u003e'annotations'-\u003e\u003e'pipelinesascode.tekton.dev/repository'\"","\tstartTimeQuery         = \"(data-\u003e'status'-\u003e\u003e'startTime')::TIMESTAMP WITH TIME ZONE\"",")","","type summaryRequest interface {","\tGetParent() string","\tGetFilter() string","\tGetGroupBy() string","\tGetSummary() string","\tGetOrderBy() string","}","","// Aggregator contains the query builders for filters and aggregate functions for summary","type Aggregator struct {","\tqueryBuilders []queryBuilder","\taggregators   []aggregateFunc","}","","func newAggregator(env *cel.Env, aggregateObjectRequest summaryRequest, clauses ...equalityClause) (*Aggregator, error) {","\tfilters := \u0026filter{","\t\tenv:             env,","\t\texpr:            strings.TrimSpace(aggregateObjectRequest.GetFilter()),","\t\tequalityClauses: clauses,","\t}","","\t// Summary is required","\tsummary := strings.Split(strings.TrimSpace(aggregateObjectRequest.GetSummary()), \",\")","\tif len(summary) == 1 \u0026\u0026 summary[0] == \"\" {","\t\t// include 'total' by default","\t\tsummary = append(summary, \"total\")","\t}","","\taggregators, err := getAggregateFunc(summary)","\tif err != nil {","\t\treturn nil, err","\t}","","\t// Group by is optional","\tgroup := strings.TrimSpace(aggregateObjectRequest.GetGroupBy())","\tif group != \"\" {","\t\tgroupQuery, err := checkAndBuildGroupQuery(group)","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\taggregators = append(aggregators, groupBy(groupQuery))","\t}","","\torderQuery := strings.TrimSpace(aggregateObjectRequest.GetOrderBy())","\t// Order by is only allowed when group by is present","\tif orderQuery != \"\" \u0026\u0026 group != \"\" {","\t\torderSelect, err := checkAndBuildOrderBy(orderQuery, summary)","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\taggregators = append(aggregators, orderBy(orderSelect))","\t}","","\treturn \u0026Aggregator{","\t\taggregators: aggregators,","\t\tqueryBuilders: []queryBuilder{","\t\t\tfilters,","\t\t},","\t}, nil","}","","// Aggregate function runs the aggregation tasks and returns Summary","func (a *Aggregator) Aggregate(ctx context.Context, db *gorm.DB) (*pb.RecordListSummary, error) {","\tvar err error","\tsummary := make([]map[string]interface{}, 0)","\tdb = db.Model(\u0026tdb.Record{})","\tdb, err = a.buildQuery(ctx, db)","\tif err != nil {","\t\treturn nil, err","\t}","","\tdb = a.applyAggregateFunc(ctx, db)","\tdb.Scan(\u0026summary)","","\tsm, err := toSummary(summary)","\tif err != nil {","\t\treturn nil, err","\t}","\treturn sm, nil","}","","// buildQuery applies filters","func (a *Aggregator) buildQuery(ctx context.Context, db *gorm.DB) (*gorm.DB, error) {","\tvar err error","\tdb = db.WithContext(ctx)","","\tfor _, builder := range a.queryBuilders {","\t\tdb, err = builder.build(db)","\t\tif err != nil {","\t\t\treturn nil, status.Error(codes.InvalidArgument, err.Error())","\t\t}","\t}","\treturn db, err","}","","// ToSummary converts the array of summary map to Summary proto","func toSummary(summary []map[string]interface{}) (*pb.RecordListSummary, error) {","\tvar data []*structpb.Struct","\tfor _, s := range summary {","\t\tm := make(map[string]*structpb.Value)","\t\tfor sk, sv := range s {","\t\t\tpbValue, err := structpb.NewValue(sv)","\t\t\tif err != nil {","\t\t\t\treturn nil, err","\t\t\t}","\t\t\tm[sk] = pbValue","\t\t}","\t\tdata = append(data, \u0026structpb.Struct{Fields: m})","\t}","","\treturn \u0026pb.RecordListSummary{","\t\tSummary: data,","\t}, nil","}","","// aggregateFunc is a function that applies aggregate functions to the query","type aggregateFunc func(db *gorm.DB) *gorm.DB","","var summaryFuncs = map[string]aggregateFunc{","\t\"total\":          getCount(\"*\", \"total\"),","\t\"avg_duration\":   getDuration(\"AVG\", durationQuery, \"avg_duration\"),","\t\"max_duration\":   getDuration(\"MAX\", durationQuery, \"max_duration\"),","\t\"total_duration\": getDuration(\"SUM\", durationQuery, \"total_duration\"),","\t\"min_duration\":   getDuration(\"MIN\", durationQuery, \"min_duration\"),","\t\"last_runtime\":   getTime(\"MAX\", startTimeQuery, \"last_runtime\"),","\t\"succeeded\":      getStatus(statusQuery, \"Succeeded\"),","\t\"failed\":         getStatus(statusQuery, \"Failed\"),","\t\"cancelled\":      getStatus(statusQuery, \"Cancelled\"),","\t\"running\":        getStatus(statusQuery, \"Running\"),","\t\"others\":         getStatus(statusQuery, \"Others\"),","}","","func getAggregateFunc(queries []string) ([]aggregateFunc, error) {","\tfns := make([]aggregateFunc, 0, len(queries))","\tfor _, q := range queries {","\t\tfn, ok := summaryFuncs[q]","\t\tif !ok {","\t\t\treturn nil, status.Errorf(codes.InvalidArgument, \"invalid aggregate query: %s\", q)","\t\t}","\t\tfns = append(fns, fn)","\t}","\treturn fns, nil","}","","func (a *Aggregator) applyAggregateFunc(ctx context.Context, db *gorm.DB) *gorm.DB {","\tdb = db.WithContext(ctx)","\tfor _, fn := range a.aggregators {","\t\tdb = fn(db)","\t}","\treturn db","}","","func getStatus(query, reason string) aggregateFunc {","\treturn func(db *gorm.DB) *gorm.DB {","\t\tstatusSelect := \"\"","\t\tswitch reason {","\t\tcase \"Succeeded\":","\t\t\tstatusSelect = fmt.Sprintf(\"COUNT(CASE WHEN %s IN ('Succeeded', 'Completed') THEN 1 END) AS %s\",","\t\t\t\tquery, strings.ToLower(reason))","\t\tcase \"Others\":","\t\t\tstatusSelect = fmt.Sprintf(\"COUNT(CASE WHEN %s NOT IN ('Failed', 'Succeeded', 'Cancelled', 'Running', 'Completed') THEN 1 END) AS %s\",","\t\t\t\tquery, strings.ToLower(reason))","\t\tdefault:","\t\t\tstatusSelect = fmt.Sprintf(\"COUNT(CASE WHEN %s = '%s' THEN 1 END) AS %s\",","\t\t\t\tquery, reason, strings.ToLower(reason))","\t\t}","\t\treturn db.Select(db.Statement.Selects, statusSelect)","\t}","}","","func getCount(query, countName string) aggregateFunc {","\treturn func(db *gorm.DB) *gorm.DB {","\t\treturn db.Select(db.Statement.Selects, fmt.Sprintf(\"COUNT(%s) AS %s\", query, countName))","\t}","}","","func getDuration(fn, query, value string) aggregateFunc {","\treturn func(db *gorm.DB) *gorm.DB {","\t\treturn db.Select(db.Statement.Selects, fmt.Sprintf(\"%s(%s)::INTERVAL AS %s\", fn, query, value))","\t}","}","","func getTime(fn, query, as string) aggregateFunc {","\treturn func(db *gorm.DB) *gorm.DB {","\t\treturn db.Select(db.Statement.Selects, fmt.Sprintf(\"%s(EXTRACT(EPOCH FROM %s)) AS %s\", fn, query, as))","\t}","}","","var validGroups = map[string]bool{","\t\"year\":       true,","\t\"month\":      true,","\t\"week\":       true,","\t\"day\":        true,","\t\"hour\":       true,","\t\"minute\":     true,","\t\"pipeline\":   false,","\t\"namespace\":  false,","\t\"repository\": false,","}","","func groupBy(groupSelect string) aggregateFunc {","\treturn func(db *gorm.DB) *gorm.DB {","\t\treturn db.Select(db.Statement.Selects, groupSelect).Group(\"group_value\")","\t}","}","","// checkAndBuildGroupQuery checks if the group by query is valid and returns the group by select query","func checkAndBuildGroupQuery(query string) (string, error) {","\tparts := strings.Split(query, \" \")","\tisTime, ok := validGroups[parts[0]]","\tif !ok {","\t\treturn \"\", status.Errorf(codes.InvalidArgument, \"group_by does not recognize %s\", query)","\t}","\tswitch {","\tcase isTime \u0026\u0026 len(parts) == 1:","\t\treturn fmt.Sprintf(\"EXTRACT(EPOCH FROM DATE_TRUNC('%s', %s)) AS group_value\", parts[0], groupByTimeQuery), nil","\tcase isTime \u0026\u0026 len(parts) == 2:","\t\tif parts[1] != \"completionTime\" \u0026\u0026 parts[1] != \"startTime\" {","\t\t\treturn \"\", status.Errorf(codes.InvalidArgument, \"group_by does not recognize %s\", parts[1])","\t\t}","\t\treturn fmt.Sprintf(\"EXTRACT(EPOCH FROM DATE_TRUNC('%s', (data-\u003e'status'-\u003e\u003e'%s')::TIMESTAMP WITH TIME ZONE)) AS group_value\", parts[0], parts[1]), nil","\tcase !isTime \u0026\u0026 len(parts) == 1:","\t\tswitch parts[0] {","\t\tcase \"namespace\":","\t\t\treturn fmt.Sprintf(\"%s AS group_value\", groupByParentQuery), nil","\t\tcase \"pipeline\":","\t\t\t// use 'namespace/pipeline' as group value because different namespaces may have pipelines with same name","\t\t\treturn fmt.Sprintf(\"CONCAT(%s, '/', %s) AS group_value\", groupByParentQuery, groupByPipelineQuery), nil","\t\tcase \"repository\":","\t\t\treturn fmt.Sprintf(\"CONCAT(%s, '/', %s) AS group_value\", groupByParentQuery, groupByRepositoryQuery), nil","\t\t}","\tdefault:","\t\treturn \"\", status.Errorf(codes.InvalidArgument, \"group_by does not recognize %s\", query)","\t}","\treturn \"\", nil","}","","func orderBy(orderSelect string) aggregateFunc {","\treturn func(db *gorm.DB) *gorm.DB {","\t\treturn db.Select(db.Statement.Selects).Order(orderSelect)","\t}","}","","// checkAndBuildOrderBy checks if the order by query is valid and returns the order by select query","func checkAndBuildOrderBy(query string, allowedFields []string) (string, error) {","\tparts := strings.Split(query, \" \")","\tvar orderByPattern = regexp.MustCompile(`^([\\w\\.]+)\\s*(ASC|asc|DESC|desc)?$`)","","\tif len(parts) != 2 || !orderByPattern.MatchString(query) {","\t\treturn \"\", status.Errorf(codes.InvalidArgument, \"order_by does not recognize %s\", query)","\t}","","\tfieldName := parts[0]","\torderDirection := strings.ToUpper(parts[1])","","\t// Check if the field name is in the list of allowed fields, must be one of the summary query","\tisAllowedField := false","\tfor _, field := range allowedFields {","\t\tif field == fieldName {","\t\t\tisAllowedField = true","\t\t\tbreak","\t\t}","\t}","","\tif !isAllowedField {","\t\treturn \"\", status.Errorf(codes.InvalidArgument, \"field name %s is not allowed, must be one of summary\", fieldName)","\t}","","\treturn fmt.Sprintf(\"%s %s\", fieldName, orderDirection), nil","}","","// OfRecordList returns a new Aggregator for Record List Summary Request","func OfRecordList(env *cel.Env, resultParent, resultName string, request *pb.RecordListSummaryRequest) (*Aggregator, error) {","\treturn newAggregator(env, request, equalityClause{","\t\tcolumnName: \"parent\",","\t\tvalue:      resultParent,","\t}, equalityClause{","\t\tcolumnName: \"result_name\",","\t\tvalue:      resultName,","\t})","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,0,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,0,1,0,0,0,1,1,1,1,1,1,1,1,1,1,0,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,1,0,0,1,1,1,1,1,1,0,0,2,2,1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,0,2,2,1,1,0,0,2,2,1,1,0,0,2,2,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,2,2,2,2,2,2,2,0,1,1,0,1,0,0,1,1,1,1,0,0,0,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,0,2,0,0,0,1,1,1,1,1,1,1,1,1]},{"id":25,"path":"pkg/api/server/v1alpha2/lister/filter.go","lines":["// Copyright 2023 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package lister","","import (","\t\"fmt\"","\t\"strings\"","","\tpagetokenpb \"github.com/tektoncd/results/pkg/api/server/v1alpha2/lister/proto/pagetoken_go_proto\"","","\t\"github.com/google/cel-go/cel\"","\t\"github.com/tektoncd/results/pkg/api/server/cel2sql\"","\t\"gorm.io/gorm\"",")","","type filter struct {","\tenv             *cel.Env","\texpr            string","\tequalityClauses []equalityClause","}","","type equalityClause struct {","\tcolumnName string","\tvalue      any","}","","// validateToken implements the queryBuilder interface.","func (f *filter) validateToken(token *pagetokenpb.PageToken) error {","\tif strings.TrimSpace(f.expr) != strings.TrimSpace(token.Filter) {","\t\treturn fmt.Errorf(\"the provided filter differs from the filter used in the previous query\\nexpected: %s\\ngot: %s\", token.Filter, f.expr)","\t}","\treturn nil","}","","// build implements the queryBuilder interface.","func (f *filter) build(db *gorm.DB) (*gorm.DB, error) {","\tfor _, clause := range f.equalityClauses {","\t\t// Specifying `-` allows users to read Results/Records","\t\t// without passing the parent.","\t\t// See https://google.aip.dev/159 for more details.","\t\tif clause.value == \"-\" {","\t\t\tcontinue","\t\t}","\t\tdb = db.Where(clause.columnName+\" = ?\", clause.value)","\t}","","\tif expr := strings.TrimSpace(f.expr); expr != \"\" {","\t\tsql, err := cel2sql.Convert(f.env, expr)","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\tdb = db.Where(sql)","\t}","\treturn db, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,0,0,2,2,2,2,2,2,2,0,2,0,0,2,2,2,1,1,2,0,2,0]},{"id":26,"path":"pkg/api/server/v1alpha2/lister/limit.go","lines":["// Copyright 2023 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package lister","","import (","\t\"fmt\"","","\tpagetokenpb \"github.com/tektoncd/results/pkg/api/server/v1alpha2/lister/proto/pagetoken_go_proto\"","\t\"gorm.io/gorm\"",")","","const (","\tdefaultPageSize = 50","\tminPageSize     = 5","\tmaxPageSize     = 10000",")","","type limit struct {","\tpageSize int","}","","// validateToken implements the queryBuilder interface.","func (l *limit) validateToken(token *pagetokenpb.PageToken) error { //nolint:revive","\treturn nil","}","","// build implements the queryBuilder interface.","func (l *limit) build(db *gorm.DB) (*gorm.DB, error) {","\tif l.pageSize \u003c minPageSize || l.pageSize \u003e maxPageSize {","\t\treturn nil, fmt.Errorf(\"invalid page size (%d): value must be greater than %d and less than %d\", l.pageSize, minPageSize, maxPageSize)","\t}","\t// Fetch n + 1 items to determine whether there are more pages and","\t// therefore, whether a page token should be included in the response.","\tdb = db.Limit(l.pageSize + 1)","\treturn db, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,2,2,2,2,0,0,2,2,0]},{"id":27,"path":"pkg/api/server/v1alpha2/lister/lister.go","lines":["// Copyright 2023 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package lister","","import (","\t\"context\"","\t\"strings\"","","\t\"google.golang.org/protobuf/types/known/timestamppb\"","","\t\"github.com/google/cel-go/cel\"","\t\"github.com/tektoncd/results/pkg/api/server/db\"","\t\"github.com/tektoncd/results/pkg/api/server/db/errors\"","\tpagetokenpb \"github.com/tektoncd/results/pkg/api/server/v1alpha2/lister/proto/pagetoken_go_proto\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/record\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/result\"","\tresultspb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/status\"","\t\"gorm.io/gorm\"",")","","// wireObject represents commonalities of Results and Records for the purposes of","// this package.","type wireObject interface {","\tGetUid() string","\tGetCreateTime() *timestamppb.Timestamp","\tGetUpdateTime() *timestamppb.Timestamp","}","","// request represents commonalities of ListResultsRequest and ListRecordsRequest","// objects.","type request interface {","\tGetParent() string","\tGetFilter() string","\tGetOrderBy() string","\tGetPageSize() int32","\tGetPageToken() string","}","","type queryBuilder interface {","\tvalidateToken(token *pagetokenpb.PageToken) error","\tbuild(db *gorm.DB) (*gorm.DB, error)","}","","// Converter is a generic function which converts a database model to its wire","// form.","type Converter[M any, W wireObject] func(M) W","","// PageTokenGenerator takes a wire object and returns a page token for","// retrieving more resources from thee API.","type PageTokenGenerator func(wireObject) (string, error)","","// Lister is a generic utility to list, filter, sort and paginate Results and","// Records in a uniform and consistent manner.","type Lister[M any, W wireObject] struct {","\tqueryBuilders    []queryBuilder","\tpageSize         int","\tpageToken        *pagetokenpb.PageToken","\tconvert          Converter[M, W]","\tgenNextPageToken PageTokenGenerator","}","","func (l *Lister[M, W]) buildQuery(ctx context.Context, db *gorm.DB) (*gorm.DB, error) {","\tvar err error","\tdb = db.WithContext(ctx)","\tfor _, builder := range l.queryBuilders {","\t\t// First, let queryBuilders validate the incoming token if","\t\t// applicable to make sure that the query parameters match those","\t\t// passed in the previous request or that the token in question","\t\t// wasn't improperly modified by the caller.","\t\tif l.pageToken != nil {","\t\t\tif err := builder.validateToken(l.pageToken); err != nil {","\t\t\t\treturn nil, status.Errorf(codes.InvalidArgument, \"invalid page token: %v\", err)","\t\t\t}","\t\t}","","\t\t// Add clauses for filtering, sorting and paginating resources.","\t\tdb, err = builder.build(db)","\t\tif err != nil {","\t\t\treturn nil, status.Error(codes.InvalidArgument, err.Error())","\t\t}","\t}","\treturn db, nil","}","","// List lists resources applying filters, sorting elements and handling","// pagination. It returns resources in their wire form and a token to be used","// later for retrieving more pages if applicable.","func (l *Lister[M, W]) List(ctx context.Context, db *gorm.DB) ([]W, string, error) {","\tvar err error","\tdb, err = l.buildQuery(ctx, db)","\tif err != nil {","\t\treturn nil, \"\", err","\t}","","\tvar models = make([]M, 0)","\tdb.Find(\u0026models)","","\tif err := errors.Wrap(err); err != nil {","\t\treturn nil, \"\", err","\t}","","\twire := make([]W, 0, len(models))","\tfor _, model := range models {","\t\twire = append(wire, l.convert(model))","\t}","","\tvar nextPageToken string","\tif len(wire) \u003e l.pageSize {","\t\t// Generate the page token using the last resource in thee","\t\t// returned collection, so it will be used as the starting point","\t\t// for next queries.","\t\twire = wire[:l.pageSize]","\t\tnextPageToken, err = l.genNextPageToken(wire[len(wire)-1])","\t}","","\treturn wire, nextPageToken, err","}","","// OfResults creates a Lister for Result objects.","func OfResults(env *cel.Env, request *resultspb.ListResultsRequest) (*Lister[*db.Result, *resultspb.Result], error) {","\treturn newLister(env, resultFieldsToColumns, request, result.ToAPI, equalityClause{","\t\tcolumnName: \"parent\",","\t\tvalue:      strings.TrimSpace(request.GetParent()),","\t})","}","","func newLister[M any, W wireObject](env *cel.Env, fieldsToColumns map[string]string, listObjectsRequest request, convert Converter[M, W], clauses ...equalityClause) (*Lister[M, W], error) {","\tpageToken, err := decodePageToken(strings.TrimSpace(listObjectsRequest.GetPageToken()))","\tif err != nil {","\t\treturn nil, err","\t}","","\tparent := strings.TrimSpace(listObjectsRequest.GetParent())","\tif pageToken != nil \u0026\u0026 pageToken.Parent != parent {","\t\treturn nil, status.Errorf(codes.InvalidArgument, \"invalid page token: provided parent (%s) differs from the parent used in the previous query (%s)\", parent, pageToken.Parent)","\t}","","\torder, err := newOrder(strings.TrimSpace(listObjectsRequest.GetOrderBy()), fieldsToColumns)","\tif err != nil {","\t\treturn nil, err","\t}","","\tfilter := \u0026filter{","\t\tenv:             env,","\t\texpr:            strings.TrimSpace(listObjectsRequest.GetFilter()),","\t\tequalityClauses: clauses,","\t}","","\tpageSize := listObjectsRequest.GetPageSize()","\tif pageSize == 0 {","\t\tpageSize = defaultPageSize","\t}","","\treturn \u0026Lister[M, W]{","\t\tqueryBuilders: []queryBuilder{","\t\t\t\u0026offset{order: order, pageToken: pageToken},","\t\t\tfilter,","\t\t\torder,","\t\t\t\u0026limit{pageSize: int(pageSize)},","\t\t},","\t\tpageSize:         int(pageSize),","\t\tpageToken:        pageToken,","\t\tconvert:          convert,","\t\tgenNextPageToken: makePageTokenGenerator(parent, filter.expr, order),","\t}, nil","}","","func makePageTokenGenerator(parent, filter string, order *order) PageTokenGenerator {","\treturn func(obj wireObject) (string, error) {","\t\tpageToken := \u0026pagetokenpb.PageToken{","\t\t\tParent: parent,","\t\t\tFilter: filter,","\t\t\tLastItem: \u0026pagetokenpb.Item{","\t\t\t\tUid: obj.GetUid(),","\t\t\t},","\t\t}","","\t\tif fieldName := order.fieldName; fieldName != \"\" {","\t\t\tpageToken.LastItem.OrderBy = \u0026pagetokenpb.Order{","\t\t\t\tFieldName: fieldName,","\t\t\t\tValue:     getTimestamp(obj, fieldName),","\t\t\t\tDirection: pagetokenpb.Order_Direction(pagetokenpb.Order_Direction_value[order.direction]),","\t\t\t}","\t\t}","","\t\treturn encodePageToken(pageToken)","\t}","}","","func getTimestamp(in wireObject, fieldName string) (timestamp *timestamppb.Timestamp) {","\tswitch fieldName {","\tcase \"create_time\":","\t\ttimestamp = in.GetCreateTime()","","\tcase \"update_time\":","\t\ttimestamp = in.GetUpdateTime()","","\tcase \"summary.start_time\":","\t\tif result, ok := in.(*resultspb.Result); ok {","\t\t\tif summary := result.Summary; summary != nil {","\t\t\t\ttimestamp = summary.GetStartTime()","\t\t\t}","\t\t}","","\tcase \"summary.end_time\":","\t\tif result, ok := in.(*resultspb.Result); ok {","\t\t\tif summary := result.Summary; summary != nil {","\t\t\t\ttimestamp = summary.GetEndTime()","\t\t\t}","\t\t}","\t}","\treturn","}","","// OfRecords creates a Lister for Record objects.","func OfRecords(env *cel.Env, resultParent, resultName string, request *resultspb.ListRecordsRequest) (*Lister[*db.Record, *resultspb.Record], error) {","\treturn newLister(env, recordFieldsToColumns, request, record.ToAPI, equalityClause{","\t\tcolumnName: \"parent\",","\t\tvalue:      resultParent,","\t},","\t\tequalityClause{","\t\t\tcolumnName: \"result_name\",","\t\t\tvalue:      resultName,","\t\t})","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,1,1,0,2,0,0,0,0,0,1,1,1,1,1,1,0,1,1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1,1,1,1,0,1,0,0,0,1,1,1,1,1,1,0,1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,0,1,1,1,1,0,1,1,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,0,0,0,1,1,1,1,1,1,1,1,1,1]},{"id":28,"path":"pkg/api/server/v1alpha2/lister/offset.go","lines":["// Copyright 2023 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package lister","","import (","\t\"errors\"","\t\"fmt\"","","\tpagetokenpb \"github.com/tektoncd/results/pkg/api/server/v1alpha2/lister/proto/pagetoken_go_proto\"","\t\"gorm.io/gorm\"",")","","type offset struct {","\torder     *order","\tpageToken *pagetokenpb.PageToken","}","","// validateToken implements the queryBuilder interface.","func (o *offset) validateToken(token *pagetokenpb.PageToken) error {","\tlastItem := token.LastItem","\tif lastItem == nil {","\t\treturn errors.New(\"last_item: missing required field\")","\t}","\tif lastItem.Uid == \"\" {","\t\treturn errors.New(\"last_item.uid: missing required field\")","\t}","\treturn nil","}","","// build implements the queryBuilder interface.","func (o *offset) build(db *gorm.DB) (*gorm.DB, error) {","\tif o.pageToken != nil {","\t\tif lastItem := o.pageToken.LastItem; lastItem != nil {","\t\t\tvar leftHandSideExpression, rightHandSideExpression string","\t\t\tcomparisonOperator := \"\u003e\"","\t\t\tvalues := []any{}","\t\t\tif orderBy := lastItem.OrderBy; orderBy != nil {","\t\t\t\tleftHandSideExpression = fmt.Sprintf(\"(%s, %s)\", o.order.columnName, defaultOrderByColumn)","\t\t\t\trightHandSideExpression = \"(?, ?)\"","\t\t\t\tvalues = append(values, orderBy.Value.AsTime())","\t\t\t\tif orderBy.Direction == pagetokenpb.Order_DESC {","\t\t\t\t\tcomparisonOperator = \"\u003c\"","\t\t\t\t}","\t\t\t} else {","\t\t\t\tleftHandSideExpression = defaultOrderByColumn","\t\t\t\trightHandSideExpression = \"?\"","\t\t\t}","\t\t\tvalues = append(values, lastItem.Uid)","\t\t\tdb = db.Where(fmt.Sprintf(\"%s %s %s\", leftHandSideExpression, comparisonOperator, rightHandSideExpression), values...)","\t\t}","\t}","\treturn db, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,0]},{"id":29,"path":"pkg/api/server/v1alpha2/lister/order.go","lines":["// Copyright 2023 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package lister","","import (","\t\"errors\"","\t\"fmt\"","\t\"regexp\"","\t\"sort\"","\t\"strings\"","","\tpagetokenpb \"github.com/tektoncd/results/pkg/api/server/v1alpha2/lister/proto/pagetoken_go_proto\"","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/status\"","\t\"gorm.io/gorm\"",")","","const (","\tdefaultOrderByColumn    = \"id\"","\tdefaultOrderByDirection = \"ASC\"",")","","var (","\tresultFieldsToColumns = map[string]string{","\t\t\"create_time\": \"created_time\",","\t\t\"update_time\": \"updated_time\",","","\t\t// Fields of RecordSummary type.","\t\t\"summary.start_time\": \"recordsummary_start_time\",","\t\t\"summary.end_time\":   \"recordsummary_end_time\",","\t}","","\trecordFieldsToColumns = map[string]string{","\t\t\"create_time\": \"created_time\",","\t\t\"update_time\": \"updated_time\",","\t}","","\torderByPattern = regexp.MustCompile(`^([\\w\\.]+)\\s*(ASC|asc|DESC|desc)?$`)",")","","type order struct {","\tcolumnName string","\tfieldName  string","\tdirection  string","}","","// validateToken implements the queryBuilder interface.","func (o *order) validateToken(token *pagetokenpb.PageToken) error {","\t// Validate the token only if the caller wants to sort the collection by","\t// a custom field.","\tif fieldName := o.fieldName; fieldName != \"\" {","\t\torderBy := token.LastItem.OrderBy","\t\tif orderBy == nil {","\t\t\treturn errors.New(\"last_item.order_by: missing required field\")","\t\t}","\t\ttokenDirection := pagetokenpb.Order_Direction_name[int32(orderBy.Direction)]","\t\tif orderBy.FieldName != fieldName || tokenDirection != o.direction {","\t\t\treturn fmt.Errorf(\"the provided order by clause differs from the value passed in the previous query\\nexpected: %s %s\\ngot: %s %s\",","\t\t\t\torderBy.FieldName, tokenDirection,","\t\t\t\tfieldName, o.direction)","\t\t}","\t}","\treturn nil","}","","// build implements the queryBuilder interface.","func (o *order) build(db *gorm.DB) (*gorm.DB, error) {","\tdirection := defaultOrderByDirection","\tif o.direction != \"\" {","\t\tdirection = o.direction","\t}","\tif o.columnName != \"\" {","\t\tdb = db.Order(o.columnName + \" \" + direction)","\t}","\treturn db.Order(defaultOrderByColumn + \" \" + direction), nil","}","","// parseOrderBy attempts to parse the input into a suitable tuple of column and","// direction to be used in the sql order by clause.","func parseOrderBy(in string, fieldsToColumns map[string]string) (columnName string, fieldName string, direction string, err error) {","\tin = strings.TrimSpace(in)","\tif in == \"\" {","\t\treturn \"\", \"\", \"\", nil","\t}","","\tmatches := orderByPattern.FindStringSubmatch(in)","\tif matches == nil {","\t\treturn \"\", \"\", \"\", status.Errorf(codes.InvalidArgument, \"invalid order by statement:\\n%s\", explainOrderByFormat(fieldsToColumns))","\t}","","\tfieldName = matches[1]","\tcolumnName = fieldsToColumns[fieldName]","\tif columnName == \"\" {","\t\treturn \"\", \"\", \"\", status.Errorf(codes.InvalidArgument, \"%s: field is unknown or cannot be used in the order by clause\\n%s\", fieldName, explainOrderByFormat(fieldsToColumns))","\t}","","\tif desiredDirection := matches[2]; desiredDirection == \"\" {","\t\tdirection = defaultOrderByDirection","\t} else {","\t\tdirection = strings.ToUpper(strings.TrimSpace(matches[2]))","\t}","","\treturn","}","","// explainOrderByFormat returns a descriptive message to inform callers on the","// OrderBy field's correct format.","func explainOrderByFormat(fieldsToColumns map[string]string) string {","\tvalidFields := make([]string, 0, len(fieldsToColumns))","\tfor field := range fieldsToColumns {","\t\tvalidFields = append(validFields, field)","\t}","\tsort.Strings(validFields)","\treturn fmt.Sprintf(\"the value must obey the format \u003c%s\u003e [asc|desc]\", strings.Join(validFields, \"|\"))","}","","// newOrder creates a new order object from the provided orderBy value.","func newOrder(orderBy string, fieldsToColumns map[string]string) (*order, error) {","\tcolumn, field, direction, err := parseOrderBy(orderBy, fieldsToColumns)","\tif err != nil {","\t\treturn nil, err","\t}","\treturn \u0026order{","\t\tcolumnName: column,","\t\tfieldName:  field,","\t\tdirection:  direction,","\t}, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,0,0,0,2,2,2,2,2,2,2,2,2,0,0,0,0,2,2,2,1,1,0,2,2,2,2,0,2,2,2,2,2,0,2,2,2,2,2,0,2,0,0,0,0,2,2,2,2,2,2,2,0,0,0,1,1,1,1,1,1,1,1,1,1,0]},{"id":30,"path":"pkg/api/server/v1alpha2/lister/page_token.go","lines":["// Copyright 2023 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package lister","","import (","\t\"encoding/base64\"","","\tpagetokenpb \"github.com/tektoncd/results/pkg/api/server/v1alpha2/lister/proto/pagetoken_go_proto\"","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/status\"","\t\"google.golang.org/protobuf/proto\"",")","","// decodePageToken attempts to convert the provided token into a PageToken","// object.","func decodePageToken(in string) (*pagetokenpb.PageToken, error) {","\tif in == \"\" {","\t\treturn nil, nil","\t}","\tdecodedData, err := base64.RawURLEncoding.DecodeString(in)","\tif err != nil {","\t\treturn nil, status.Error(codes.InvalidArgument, err.Error())","\t}","\tpageToken := new(pagetokenpb.PageToken)","\tif err := proto.Unmarshal(decodedData, pageToken); err != nil {","\t\treturn nil, status.Error(codes.Internal, err.Error())","\t}","\treturn pageToken, nil","}","","// encodePageToken turns the PageToken object into a string suitable to be","// delivered by the API.","func encodePageToken(in *pagetokenpb.PageToken) (string, error) {","\twire, err := proto.Marshal(in)","\tif err != nil {","\t\treturn \"\", status.Error(codes.Internal, err.Error())","\t}","\treturn base64.RawURLEncoding.EncodeToString(wire), nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,1,1,2,2,1,1,2,2,1,1,2,0,0,0,0,2,2,2,1,1,2,0]},{"id":31,"path":"pkg/api/server/v1alpha2/log/file.go","lines":["package log","","import (","\t\"bufio\"","\t\"context\"","\t\"fmt\"","\t\"io\"","\t\"os\"","\t\"path/filepath\"","","\t\"github.com/tektoncd/results/pkg/api/server/config\"","","\t\"github.com/tektoncd/results/pkg/apis/v1alpha3\"",")","","type fileStream struct {","\tpath string","\tsize int","\tctx  context.Context","}","","// NewFileStream returns a LogStreamer that streams directly from a log file on local disk.","func NewFileStream(ctx context.Context, log *v1alpha3.Log, config *config.Config) (Stream, error) {","\tif log.Status.Path == \"\" {","\t\tfilePath, err := FilePath(log)","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\tlog.Status.Path = filePath","\t}","","\tsize := config.LOGS_BUFFER_SIZE","\tif size \u003c 1 {","\t\tsize = DefaultBufferSize","\t}","","\treturn \u0026fileStream{","\t\tpath: filepath.Join(config.LOGS_PATH, log.Status.Path),","\t\tsize: size,","\t\tctx:  ctx,","\t}, nil","}","","func (*fileStream) Type() string {","\treturn string(v1alpha3.FileLogType)","}","","// WriteTo reads the contents of the TaskRun log file and writes them to the provided writer, such","// as os.Stdout.","func (fs *fileStream) WriteTo(w io.Writer) (n int64, err error) {","\t_, err = os.Stat(fs.path)","\tif err != nil {","\t\treturn 0, fmt.Errorf(\"failed to stat %s: %w\", fs.path, err)","\t}","\tfile, err := os.Open(fs.path)","\tif err != nil {","\t\treturn 0, fmt.Errorf(\"failed to open file %s: %w\", fs.path, err)","\t}","\tdefer func() {","\t\tcloseErr := file.Close()","\t\tif err == nil \u0026\u0026 closeErr != nil {","\t\t\terr = closeErr","\t\t}","\t}()","\t// Use the buffered reader to ensure file contents are not read entirely into memory","\treader := bufio.NewReaderSize(file, fs.size)","\tn, err = reader.WriteTo(w)","\treturn","}","","// ReadFrom reads the log contents from the provided io.Reader, and writes them to the TaskRun log","// file on disk.","func (fs *fileStream) ReadFrom(r io.Reader) (n int64, err error) {","\t// Ensure that the directories in the path already exist","\tdir := filepath.Dir(fs.path)","\terr = os.MkdirAll(dir, 0750)","\tif err != nil {","\t\treturn 0, fmt.Errorf(\"failed to create directory %s, %w\", dir, err)","\t}","\t// Open the file with Append + Create + WriteOnly modes.","\t// This ensures the file is created if it does not exist.","\t// If the file does exist, data is appended instead of overwritten/truncated","\t// 0600 is safe since the same process reads and writes these log files","\tfile, err := os.OpenFile(fs.path, os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0600)","\tif err != nil {","\t\treturn 0, fmt.Errorf(\"failed to open file %s: %w\", fs.path, err)","\t}","\tdefer func() {","\t\tcloseErr := file.Close()","\t\tif err == nil \u0026\u0026 closeErr != nil {","\t\t\terr = closeErr","\t\t}","\t}()","\twriter := bufio.NewWriterSize(file, fs.size)","\tn, err = writer.ReadFrom(r)","\tif err != nil {","\t\treturn","\t}","\terr = writer.Flush()","\treturn","}","","func (fs *fileStream) Delete() error {","\treturn os.RemoveAll(fs.path)","}","","func (fs *fileStream) Flush() error {","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,1,1,2,0,0,2,2,2,2,0,2,2,2,2,2,0,0,2,2,2,0,0,0,2,2,2,1,1,2,2,1,1,2,2,2,1,1,0,0,2,2,2,0,0,0,0,2,2,2,2,2,1,1,0,0,0,0,2,2,1,1,2,2,2,1,1,0,2,2,2,1,1,2,2,0,0,2,2,2,0,1,1,1]},{"id":32,"path":"pkg/api/server/v1alpha2/log/gcs.go","lines":["/*","Copyright 2023 The Tekton Authors","","Licensed under the Apache License, Version 2.0 (the \"License\");","you may not use this file except in compliance with the License.","You may obtain a copy of the License at","","    http://www.apache.org/licenses/LICENSE-2.0","","Unless required by applicable law or agreed to in writing, software","distributed under the License is distributed on an \"AS IS\" BASIS,","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","See the License for the specific language governing permissions and","limitations under the License.","*/","","package log","","import (","\t\"path/filepath\"","","\t\"golang.org/x/oauth2/google\"","","\t\"context\"","\t\"fmt\"","\t\"io\"","\t\"os\"","","\tserver \"github.com/tektoncd/results/pkg/api/server/config\"","\t\"github.com/tektoncd/results/pkg/apis/v1alpha3\"","","\t\"gocloud.dev/blob/gcsblob\"","\t\"gocloud.dev/gcp\"",")","","type gcsStream struct {","\tctx    context.Context","\tconfig *server.Config","\tkey    string","\tclient *gcp.HTTPClient","}","","// NewGCSStream returns a log streamer for the GCS storage type.","func NewGCSStream(ctx context.Context, log *v1alpha3.Log, config *server.Config) (Stream, error) {","\tif log.Status.Path == \"\" {","\t\tfilePath, err := FilePath(log)","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\tlog.Status.Path = filePath","\t}","","\tfilePath := filepath.Join(config.LOGS_PATH, log.Status.Path)","","\tif config.STORAGE_EMULATOR_HOST != \"\" {","\t\tif err := os.Setenv(\"STORAGE_EMULATOR_HOST\", config.STORAGE_EMULATOR_HOST); err != nil {","\t\t\treturn nil, fmt.Errorf(\"failed to set STORAGE_EMULATOR_HOST: %w\", err)","\t\t}","\t}","\tclient, err := getGCSClient(ctx, config)","\tif err != nil {","\t\treturn nil, err","\t}","","\tgcs := \u0026gcsStream{","\t\tctx:    ctx,","\t\tconfig: config,","\t\tkey:    filePath,","\t\tclient: client,","\t}","","\treturn gcs, nil","}","","func getGCSClient(ctx context.Context, cfg *server.Config) (*gcp.HTTPClient, error) {","\tvar creds *google.Credentials","","\tif cfg.STORAGE_EMULATOR_HOST != \"\" {","\t\tcreds, _ = google.CredentialsFromJSON(ctx, []byte(`{\"type\": \"service_account\", \"project_id\": \"my-project-id\"}`))","\t} else {","\t\tvar err error","\t\tcreds, err = gcp.DefaultCredentials(ctx)","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t}","","\treturn gcp.NewHTTPClient(","\t\tgcp.DefaultTransport(),","\t\tgcp.CredentialsTokenSource(creds))","}","","func (*gcsStream) Type() string {","\treturn string(v1alpha3.GCSLogType)","}","","func (gcs *gcsStream) WriteTo(w io.Writer) (n int64, err error) {","\tbucket, err := gcsblob.OpenBucket(gcs.ctx, gcs.client, gcs.config.GCS_BUCKET_NAME, nil)","\tif err != nil {","\t\treturn 0, fmt.Errorf(\"could not open bucket: %v\", err)","\t}","\tdefer func() {","\t\tif cerr := bucket.Close(); cerr != nil \u0026\u0026 err == nil {","\t\t\terr = fmt.Errorf(\"could not close bucket: %w\", cerr)","\t\t}","\t}()","","\tr, err := bucket.NewReader(gcs.ctx, gcs.key, nil)","\tif err != nil {","\t\treturn 0, fmt.Errorf(\"could not create bucket reader: %v for the key: %s\", err, gcs.key)","\t}","\tn, err = r.WriteTo(w)","\tif err != nil {","\t\treturn 0, fmt.Errorf(\"could not read data from bucket: %v for the key: %s\", err, gcs.key)","\t}","\treturn n, nil","}","","func (gcs *gcsStream) ReadFrom(r io.Reader) (n int64, err error) {","\tbucket, err := gcsblob.OpenBucket(gcs.ctx, gcs.client, gcs.config.GCS_BUCKET_NAME, nil)","\tif err != nil {","\t\treturn 0, fmt.Errorf(\"could not open bucket: %v for the key: %s\", err, gcs.key)","\t}","\tdefer func() {","\t\tif cerr := bucket.Close(); cerr != nil \u0026\u0026 err == nil {","\t\t\terr = fmt.Errorf(\"could not close bucket: %w for the key: %s\", cerr, gcs.key)","\t\t}","\t}()","","\tw, err := bucket.NewWriter(gcs.ctx, gcs.key, nil)","\tif err != nil {","\t\treturn 0, fmt.Errorf(\"could not create bucket writer: %v for the key: %s\", err, gcs.key)","\t}","\tdefer func() {","\t\terr = w.Close()","\t\tif err != nil {","\t\t\terr = fmt.Errorf(\"could not flush data to bucket: %v for the key: %s\", err, gcs.key)","\t\t}","\t}()","","\tn, err = w.ReadFrom(r)","\tif err != nil {","\t\treturn 0, fmt.Errorf(\"could not write data to bucket: %v for the key: %s\", err, gcs.key)","\t}","\treturn n, nil","}","","func (gcs *gcsStream) Flush() error {","\treturn nil","}","","func (gcs *gcsStream) Delete() (err error) {","\tbucket, err := gcsblob.OpenBucket(gcs.ctx, gcs.client, gcs.config.GCS_BUCKET_NAME, nil)","\tif err != nil {","\t\treturn fmt.Errorf(\"could not open bucket: %v for the key: %s\", err, gcs.key)","\t}","\tdefer func() {","\t\tif cerr := bucket.Close(); cerr != nil \u0026\u0026 err == nil {","\t\t\terr = fmt.Errorf(\"could not close bucket: %w for the key: %s\", cerr, gcs.key)","\t\t}","\t}()","","\tif err := bucket.Delete(gcs.ctx, gcs.key); err != nil {","\t\treturn fmt.Errorf(\"could not delete bucket data: %v for the key: %s\", err, gcs.key)","\t}","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,0,0,1,1,1,0,2,2,2,1,1,2,2,1,1,0,0,2,2,1,1,2,2,1,1,2,0,0,2,2,2,1,1,2,2,1,1,0,0,2,2,1,1,2,2,2,1,1,0,0,2,2,1,1,2,0,0,1,1,1,0,2,2,2,1,1,2,2,1,1,0,0,2,1,1,2,0]},{"id":33,"path":"pkg/api/server/v1alpha2/log/log.go","lines":["package log","","import (","\t\"context\"","\t\"encoding/json\"","\t\"fmt\"","\t\"io\"","\t\"path/filepath\"","\t\"regexp\"","","\t\"github.com/tektoncd/results/pkg/api/server/config\"","\t\"github.com/tektoncd/results/pkg/api/server/db\"","\t\"github.com/tektoncd/results/pkg/apis/v1alpha3\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/status\"",")","","const (","\t// DefaultBufferSize is the default buffer size. This based on the recommended","\t// gRPC message size for streamed content, which ranges from 16 to 64 KiB. Choosing 32 KiB as a","\t// middle ground between the two.","\tDefaultBufferSize = 32 * 1024",")","","var (","\t// NameRegex matches valid name specs for a Result.","\tNameRegex = regexp.MustCompile(\"(^[a-z0-9_-]{1,63})/results/([a-z0-9_-]{1,63})/logs/([a-z0-9_-]{1,63}$)\")",")","","// ParseName splits a full Result name into its individual (parent, result, name)","// components.","func ParseName(raw string) (parent, result, name string, err error) {","\ts := NameRegex.FindStringSubmatch(raw)","\tif len(s) != 4 {","\t\treturn \"\", \"\", \"\", status.Errorf(codes.InvalidArgument, \"name must match %s\", NameRegex.String())","\t}","\treturn s[1], s[2], s[3], nil","}","","// FormatName takes in a parent (\"a/results/b\") and record name (\"c\") and","// returns the full resource name (\"a/results/b/logs/c\").","func FormatName(parent, name string) string {","\treturn fmt.Sprintf(\"%s/logs/%s\", parent, name)","}","","// Stream is an interface that defines the behavior of a streaming log service.","type Stream interface {","\tio.ReaderFrom","\tio.WriterTo","\tType() string","\tDelete() error","\tFlush() error","}","","// NewStream returns a LogStreamer for the given Log.","// LogStreamers do the following:","//","// 1. Write log data from their respective source to an io.Writer interface.","// 2. Read log data from a source, and store it in the respective backend if that behavior is supported.","//","// All LogStreamers support writing log data to an io.Writer from the provided source.","// LogStreamers do not need to receive and store data from the provided source.","//","// NewStream may mutate the Log object's status, to provide implementation information","// for reading and writing files.","func NewStream(ctx context.Context, log *v1alpha3.Log, config *config.Config) (Stream, error) {","\tswitch log.Spec.Type {","\tcase v1alpha3.FileLogType:","\t\treturn NewFileStream(ctx, log, config)","\tcase v1alpha3.S3LogType:","\t\treturn NewS3Stream(ctx, log, config)","\tcase v1alpha3.GCSLogType:","\t\treturn NewGCSStream(ctx, log, config)","\t}","\treturn nil, fmt.Errorf(\"log streamer type %s is not supported\", log.Spec.Type)","}","","// ToStorage converts log record to marshaled json bytes","func ToStorage(record *pb.Record, config *config.Config) ([]byte, error) {","\tlog := \u0026v1alpha3.Log{}","\tif len(record.GetData().Value) \u003e 0 {","\t\terr := json.Unmarshal(record.GetData().Value, log)","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t}","\tlog.Default()","","\tif log.Spec.Type == \"\" {","\t\tlog.Spec.Type = v1alpha3.LogType(config.LOGS_TYPE)","\t\tif len(log.Spec.Type) == 0 {","\t\t\treturn nil, fmt.Errorf(\"failed to set up log storage type to spec\")","\t\t}","\t}","\treturn json.Marshal(log)","}","","// ToStream returns three arguments.","// First one is a new log streamer created by log record.","// Second one is log API resource retrieved from log record.","// Third argument is an error.","func ToStream(ctx context.Context, record *db.Record, config *config.Config) (Stream, *v1alpha3.Log, error) {","\tif record.Type != v1alpha3.LogRecordType \u0026\u0026 record.Type != v1alpha3.LogRecordTypeV2 {","\t\treturn nil, nil, fmt.Errorf(\"record type %s cannot stream logs\", record.Type)","\t}","\tlog := \u0026v1alpha3.Log{}","\terr := json.Unmarshal(record.Data, log)","\tif err != nil {","\t\treturn nil, nil, fmt.Errorf(\"could not decode Log record: %w\", err)","\t}","\tstream, err := NewStream(ctx, log, config)","\treturn stream, log, err","}","","// FilePath returns file path to store log. This file path can be","// path in the real file system or virtual value depending on storage type.","func FilePath(log *v1alpha3.Log) (string, error) {","\tfilePath := filepath.Join(log.GetNamespace(), string(log.GetUID()), log.Name)","\tif filePath == \"\" {","\t\treturn \"\", fmt.Errorf(\"invalid file path\")","\t}","\treturn filePath, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,1,1,1,1,0,1,0,0,0,2,2,2,2,2,1,1,0,2,2,2,1,1,1,1,0,2,0,0,0,0,0,0,2,2,2,2,2,2,2,1,1,2,2,0,0,0,0,2,2,2,1,1,2,0]},{"id":34,"path":"pkg/api/server/v1alpha2/log/s3.go","lines":["package log","","import (","\t\"bufio\"","\t\"bytes\"","\t\"path/filepath\"","","\tv4 \"github.com/aws/aws-sdk-go-v2/aws/signer/v4\"","","\t\"context\"","\t\"io\"","","\t\"github.com/aws/aws-sdk-go-v2/aws\"","\t\"github.com/aws/aws-sdk-go-v2/config\"","\t\"github.com/aws/aws-sdk-go-v2/credentials\"","\t\"github.com/aws/aws-sdk-go-v2/service/s3\"","\t\"github.com/aws/aws-sdk-go-v2/service/s3/types\"","\tserver \"github.com/tektoncd/results/pkg/api/server/config\"","\t\"github.com/tektoncd/results/pkg/apis/v1alpha3\"",")","","const (","\tdefaultS3MultiPartSize = 1024 * 1024 * 5",")","","type s3Client interface {","\tAbortMultipartUpload(context.Context, *s3.AbortMultipartUploadInput, ...func(*s3.Options)) (*s3.AbortMultipartUploadOutput, error)","\tCompleteMultipartUpload(context.Context, *s3.CompleteMultipartUploadInput, ...func(*s3.Options)) (*s3.CompleteMultipartUploadOutput, error)","\tCreateMultipartUpload(context.Context, *s3.CreateMultipartUploadInput, ...func(*s3.Options)) (*s3.CreateMultipartUploadOutput, error)","\tDeleteObject(context.Context, *s3.DeleteObjectInput, ...func(*s3.Options)) (*s3.DeleteObjectOutput, error)","\tGetObject(context.Context, *s3.GetObjectInput, ...func(*s3.Options)) (*s3.GetObjectOutput, error)","\tUploadPart(context.Context, *s3.UploadPartInput, ...func(*s3.Options)) (*s3.UploadPartOutput, error)","}","","type s3Stream struct {","\tconfig        *server.Config","\tctx           context.Context","\tsize          int","\tbuffer        bytes.Buffer","\tclient        s3Client","\tbucket        string","\tkey           string","\tpartNumber    int32","\tpartSize      int64","\tuploadID      string","\tparts         []types.CompletedPart","\tmultiPartSize int64","}","","// NewS3Stream returns a log streamer for the S3 log storage type.","func NewS3Stream(ctx context.Context, log *v1alpha3.Log, config *server.Config) (Stream, error) {","\tif log.Status.Path == \"\" {","\t\tfilePath, err := FilePath(log)","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\tlog.Status.Path = filePath","\t}","","\tfilePath := filepath.Join(config.LOGS_PATH, log.Status.Path)","","\tclient, err := initConfig(ctx, config)","\tif err != nil {","\t\treturn nil, err","\t}","","\tmultipartUpload, err := client.CreateMultipartUpload(ctx,","\t\t\u0026s3.CreateMultipartUploadInput{","\t\t\tBucket: \u0026config.S3_BUCKET_NAME,","\t\t\tKey:    \u0026filePath,","\t\t},","\t)","\tif err != nil {","\t\treturn nil, err","\t}","","\tmultiPartSize := config.S3_MULTI_PART_SIZE","\tif multiPartSize == 0 {","\t\tmultiPartSize = defaultS3MultiPartSize","\t}","","\tsize := config.LOGS_BUFFER_SIZE","\tif size \u003c 1 {","\t\tsize = DefaultBufferSize","\t}","","\ts3s := \u0026s3Stream{","\t\tconfig:        config,","\t\tctx:           ctx,","\t\tsize:          size,","\t\tbucket:        config.S3_BUCKET_NAME,","\t\tkey:           filePath,","\t\tbuffer:        bytes.Buffer{},","\t\tuploadID:      *multipartUpload.UploadId,","\t\tclient:        client,","\t\tpartNumber:    1,","\t\tmultiPartSize: multiPartSize,","\t}","","\treturn s3s, nil","}","","func initConfig(ctx context.Context, cfg *server.Config) (*s3.Client, error) {","\tcredentialsOpt := config.WithCredentialsProvider(credentials.NewStaticCredentialsProvider(cfg.S3_ACCESS_KEY_ID, cfg.S3_SECRET_ACCESS_KEY, \"\"))","","\tvar awsConfig aws.Config","\tvar err error","\tif len(cfg.S3_ENDPOINT) \u003e 0 {","\t\tcustomResolver := aws.EndpointResolverWithOptionsFunc(func(_, region string, _ ...any) (aws.Endpoint, error) { //nolint:staticcheck","\t\t\tif region == cfg.S3_REGION {","\t\t\t\treturn aws.Endpoint{ //nolint:staticcheck","\t\t\t\t\tURL:               cfg.S3_ENDPOINT,","\t\t\t\t\tSigningRegion:     cfg.S3_REGION,","\t\t\t\t\tHostnameImmutable: cfg.S3_HOSTNAME_IMMUTABLE,","\t\t\t\t}, nil","\t\t\t}","\t\t\treturn aws.Endpoint{}, \u0026aws.EndpointNotFoundError{} //nolint:staticcheck","\t\t})","\t\tawsConfig, err = config.LoadDefaultConfig(ctx, config.WithRegion(cfg.S3_REGION), credentialsOpt, config.WithEndpointResolverWithOptions(customResolver)) //nolint:staticcheck","\t} else {","\t\tawsConfig, err = config.LoadDefaultConfig(ctx, config.WithRegion(cfg.S3_REGION), credentialsOpt)","\t}","","\tif err != nil {","\t\treturn nil, err","\t}","","\treturn s3.NewFromConfig(awsConfig, func(o *s3.Options) {","\t\to.UsePathStyle = true","\t}), nil","}","","func (*s3Stream) Type() string {","\treturn string(v1alpha3.S3LogType)","}","","func (s3s *s3Stream) WriteTo(w io.Writer) (n int64, err error) {","\toutPut, err := s3s.client.GetObject(s3s.ctx, \u0026s3.GetObjectInput{","\t\tBucket: \u0026s3s.bucket,","\t\tKey:    \u0026s3s.key,","\t})","\tif err != nil {","\t\treturn 0, err","\t}","","\tdefer func() {","\t\tif cerr := outPut.Body.Close(); cerr != nil \u0026\u0026 err == nil {","\t\t\terr = cerr","\t\t}","\t}()","","\treader := bufio.NewReaderSize(outPut.Body, s3s.size)","\tn, err = reader.WriteTo(w)","\tif err != nil {","\t\treturn 0, err","\t}","\treturn","}","","func (s3s *s3Stream) ReadFrom(r io.Reader) (int64, error) {","\tn, err := s3s.buffer.ReadFrom(r)","\tif err != nil {","\t\treturn 0, err","\t}","","\tsize := s3s.partSize + n","\tif size \u003e= s3s.multiPartSize {","\t\terr = s3s.uploadMultiPart(\u0026s3s.buffer, s3s.partNumber, size)","\t\tif err != nil {","\t\t\treturn 0, err","\t\t}","\t\ts3s.partSize = 0","\t\ts3s.buffer.Reset()","\t} else {","\t\ts3s.partSize = size","\t}","","\treturn s3s.partSize, err","}","","func (s3s *s3Stream) uploadMultiPart(reader io.Reader, partNumber int32, partSize int64) error {","\tif partSize == 0 {","\t\treturn nil","\t}","","\tpart, err := s3s.client.UploadPart(s3s.ctx, \u0026s3.UploadPartInput{","\t\tUploadId:      \u0026s3s.uploadID,","\t\tBucket:        \u0026s3s.bucket,","\t\tKey:           \u0026s3s.key,","\t\tPartNumber:    \u0026partNumber,","\t\tBody:          reader,","\t\tContentLength: \u0026partSize,","\t}, s3.WithAPIOptions(","\t\tv4.SwapComputePayloadSHA256ForUnsignedPayloadMiddleware,","\t))","","\tif err != nil {","\t\ts3s.client.AbortMultipartUpload(s3s.ctx, \u0026s3.AbortMultipartUploadInput{ //nolint:errcheck,gosec","\t\t\tBucket:   \u0026s3s.bucket,","\t\t\tKey:      \u0026s3s.key,","\t\t\tUploadId: \u0026s3s.uploadID,","\t\t})","\t\treturn err","\t}","","\ts3s.parts = append(s3s.parts, types.CompletedPart{PartNumber: \u0026partNumber, ETag: part.ETag})","\ts3s.partNumber++","","\treturn err","}","","func (s3s *s3Stream) Flush() error {","\tif err := s3s.uploadMultiPart(\u0026s3s.buffer, s3s.partNumber, int64(s3s.buffer.Len())); err != nil {","\t\treturn err","\t}","","\t_, err := s3s.client.CompleteMultipartUpload(s3s.ctx, \u0026s3.CompleteMultipartUploadInput{","\t\tBucket:   \u0026s3s.bucket,","\t\tKey:      \u0026s3s.key,","\t\tUploadId: \u0026s3s.uploadID,","\t\tMultipartUpload: \u0026types.CompletedMultipartUpload{","\t\t\tParts: s3s.parts,","\t\t},","\t})","\treturn err","}","","func (s3s *s3Stream) Delete() error {","\t_, err := s3s.client.DeleteObject(s3s.ctx, \u0026s3.DeleteObjectInput{","\t\tBucket: \u0026s3s.bucket,","\t\tKey:    \u0026s3s.key,","\t})","\treturn err","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1,1,1,0,1,1,1,0,0,1,1,1,0,2,2,2,2,2,2,1,1,0,2,2,1,1,0,0,2,2,2,1,1,2,0,0,2,2,2,1,1,0,2,2,2,2,1,1,2,2,2,2,2,0,2,0,0,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,0,2,2,2,2,0,0,2,2,1,1,0,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2]},{"id":35,"path":"pkg/api/server/v1alpha2/logs.go","lines":["package server","","import (","\t\"bytes\"","\t\"context\"","\t\"encoding/json\"","\t\"fmt\"","\t\"io\"","","\t\"github.com/golang/protobuf/ptypes/empty\"","\t\"github.com/google/cel-go/cel\"","\tcelenv \"github.com/tektoncd/results/pkg/api/server/cel\"","\t\"github.com/tektoncd/results/pkg/api/server/db/errors\"","\t\"github.com/tektoncd/results/pkg/api/server/db/pagination\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/result\"","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/status\"","\t\"gorm.io/gorm\"","","\t\"github.com/tektoncd/results/pkg/api/server/db\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/auth\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/log\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/record\"","\t\"github.com/tektoncd/results/pkg/apis/v1alpha3\"","\t\"github.com/tektoncd/results/pkg/logs\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"google.golang.org/protobuf/types/known/timestamppb\"",")","","// GetLog streams log record by log request","func (s *Server) GetLog(req *pb.GetLogRequest, srv pb.Logs_GetLogServer) error {","\tparent, res, name, err := log.ParseName(req.GetName())","\tif err != nil {","\t\ts.logger.Error(err)","\t\treturn status.Error(codes.InvalidArgument, \"Invalid Name\")","\t}","","\tif err := s.auth.Check(srv.Context(), parent, auth.ResourceLogs, auth.PermissionGet); err != nil {","\t\ts.logger.Error(err)","\t\t// unauthenticated status code and debug message produced by Check","\t\treturn err","\t}","","\trec, err := getRecord(s.db, parent, res, name)","\tif err != nil {","\t\ts.logger.Error(err)","\t\treturn err","\t}","\t// Check if the input record is referenced in any logs record in the result","\tif rec.Type != v1alpha3.LogRecordType \u0026\u0026 rec.Type != v1alpha3.LogRecordTypeV2 {","\t\trec, err = getLogRecord(s.db, parent, res, name)","\t\tif err != nil {","\t\t\ts.logger.Error(err)","\t\t\treturn err","\t\t}","\t}","","\tstream, object, err := log.ToStream(srv.Context(), rec, s.config)","\tif err != nil {","\t\ts.logger.Error(err)","\t\treturn status.Error(codes.Internal, \"Error streaming log\")","\t}","","\t// Handle v1alpha2 and earlier differently from v1alpha3 until v1alpha2 and earlier are deprecated","\tif object.APIVersion == \"results.tekton.dev/v1alpha3\" {","\t\tif !object.Status.IsStored || object.Status.Size == 0 {","\t\t\ts.logger.Errorf(\"no logs exist for %s\", req.GetName())","\t\t\treturn status.Error(codes.NotFound, \"Log doesn't exist\")","\t\t}","\t} else {","\t\t// For v1alpha2 checking log size is the best way to ensure if logs are stored","\t\t// this is however susceptible to race condition","\t\tif object.Status.Size == 0 {","\t\t\ts.logger.Errorf(\"no logs exist for %s\", req.GetName())","\t\t\treturn status.Error(codes.NotFound, \"Log doesn't exist\")","\t\t}","\t}","","\twriter := logs.NewBufferedHTTPWriter(srv, req.GetName(), s.config.LOGS_BUFFER_SIZE)","\tif _, err = stream.WriteTo(writer); err != nil {","\t\ts.logger.Error(err)","\t\treturn status.Error(codes.Internal, \"Error streaming log\")","\t}","\t_, err = writer.Flush()","\tif err != nil {","\t\ts.logger.Error(err)","\t\treturn status.Error(codes.Internal, \"Error streaming log\")","\t}","\treturn nil","}","","func getLogRecord(txn *gorm.DB, parent, result, name string) (*db.Record, error) {","\tstore := \u0026db.Record{}","\tq := txn.","\t\tWhere(\u0026db.Record{Result: db.Result{Parent: parent, Name: result}}).","\t\tWhere(\"data -\u003e 'spec' -\u003e 'resource' -\u003e\u003e 'uid' =  ?\", name).","\t\tFirst(store)","\tif err := errors.Wrap(q.Error); err != nil {","\t\treturn nil, err","\t}","\treturn store, nil","}","","// UpdateLog updates log record content","func (s *Server) UpdateLog(srv pb.Logs_UpdateLogServer) error {","\tvar name, parent, resultName, recordName string","\tvar bytesWritten int64","\tvar rec *db.Record","\tvar object *v1alpha3.Log","\tvar stream log.Stream","\t// fyi we cannot defer the flush call in case we need to return the error","\t// but instead we pass the stream into handleError to preserve the behavior of","\t// calling Flush regardless when we previously called Flush via defer","\tfor {","\t\t// the underlying grpc stream RecvMsg method blocks until this receives a message or it is done,","\t\trecv, err := srv.Recv()","\t\t// If we reach the end of the srv, we receive an io.EOF error","\t\tif err != nil {","\t\t\treturn s.handleReturn(srv, rec, object, bytesWritten, stream, err, true)","\t\t}","\t\t// Ensure that we are receiving logs for the same record","\t\tif name == \"\" {","\t\t\tname = recv.GetName()","\t\t\ts.logger.Debugf(\"receiving logs for %s\", name)","\t\t\tparent, resultName, recordName, err = log.ParseName(name)","\t\t\tif err != nil {","\t\t\t\treturn s.handleReturn(srv, rec, object, bytesWritten, stream, err, true)","\t\t\t}","","\t\t\tif err = s.auth.Check(srv.Context(), parent, auth.ResourceLogs, auth.PermissionUpdate); err != nil {","\t\t\t\treturn s.handleReturn(srv, rec, object, bytesWritten, stream, err, false)","\t\t\t}","\t\t}","\t\tif name != recv.GetName() {","\t\t\terr = fmt.Errorf(\"cannot put logs for multiple records in the same server\")","\t\t\treturn s.handleReturn(srv,","\t\t\t\trec,","\t\t\t\tobject,","\t\t\t\tbytesWritten,","\t\t\t\tstream,","\t\t\t\terr,","\t\t\t\tfalse)","\t\t}","","\t\tif rec == nil {","\t\t\trec, err = getRecord(s.db.WithContext(srv.Context()), parent, resultName, recordName)","\t\t\tif err != nil {","\t\t\t\treturn s.handleReturn(srv, rec, object, bytesWritten, stream, err, true)","\t\t\t}","\t\t}","","\t\tif stream == nil {","\t\t\tstream, object, err = log.ToStream(srv.Context(), rec, s.config)","\t\t\tif err != nil {","\t\t\t\treturn s.handleReturn(srv, rec, object, bytesWritten, stream, err, false)","\t\t\t}","\t\t}","","\t\tbuffer := bytes.NewBuffer(recv.GetData())","\t\tvar written int64","\t\twritten, err = stream.ReadFrom(buffer)","\t\tbytesWritten += written","","\t\tif err != nil {","\t\t\treturn s.handleReturn(srv, rec, object, bytesWritten, stream, err, true)","\t\t}","\t}","}","","func (s *Server) handleReturn(srv pb.Logs_UpdateLogServer, rec *db.Record, log *v1alpha3.Log, written int64, stream log.Stream, returnErr error, isRetryableErr bool) error {","\t// When the srv reaches the end, srv.Recv() returns an io.EOF error","\t// Therefore we should not return io.EOF if it is received in this function.","\t// Otherwise, we should return the original error and not mask any subsequent errors handling cleanup/return.","","\treturnErrorStr := \"\"","\tif returnErr != nil {","\t\treturnErrorStr = returnErr.Error()","\t}","","\t// If no database record or Log, return the original error","\tif rec == nil || log == nil {","\t\tif stream != nil {","\t\t\tif flushErr := stream.Flush(); flushErr != nil {","\t\t\t\ts.logger.Error(flushErr)","\t\t\t\treturn fmt.Errorf(\"got flush error %s with returnErr: %s\", flushErr.Error(), returnErrorStr)","\t\t\t}","\t\t}","\t\treturn returnErr","\t}","\tapiRec := record.ToAPI(rec)","\tapiRec.UpdateTime = timestamppb.Now()","\tlog.Status.Size = written","\tlog.Status.IsStored = returnErr == io.EOF","\tif returnErr != nil \u0026\u0026 returnErr != io.EOF {","\t\tlog.Status.ErrorOnStoreMsg = returnErr.Error()","\t\tlog.Status.IsRetryableErr = isRetryableErr","\t}","","\tdata, err := json.Marshal(log)","\tif err != nil {","\t\tif stream != nil {","\t\t\tif flushErr := stream.Flush(); flushErr != nil {","\t\t\t\ts.logger.Error(flushErr)","\t\t\t\treturn fmt.Errorf(\"got flush error %s with returnErr: %s\", flushErr.Error(), returnErrorStr)","\t\t\t}","\t\t}","\t\tif !isNilOrEOF(returnErr) {","\t\t\treturn returnErr","\t\t}","\t\treturn err","\t}","\tapiRec.Data = \u0026pb.Any{","\t\tType:  rec.Type,","\t\tValue: data,","\t}","","\t_, err = s.UpdateRecord(srv.Context(), \u0026pb.UpdateRecordRequest{","\t\tRecord: apiRec,","\t\tEtag:   rec.Etag,","\t})","","\tif err != nil {","\t\tif stream != nil {","\t\t\tif flushErr := stream.Flush(); flushErr != nil {","\t\t\t\ts.logger.Error(flushErr)","\t\t\t\treturn fmt.Errorf(\"got flush error %s with returnErr: %s\", flushErr.Error(), returnErrorStr)","\t\t\t}","\t\t}","\t\tif !isNilOrEOF(returnErr) {","\t\t\treturn returnErr","\t\t}","\t\treturn err","\t}","","\tif returnErr == io.EOF {","\t\tif stream != nil {","\t\t\tif flushErr := stream.Flush(); flushErr != nil {","\t\t\t\ts.logger.Error(flushErr)","\t\t\t\treturn flushErr","\t\t\t}","\t\t}","\t\ts.logger.Debugf(\"received %d bytes for %s\", written, apiRec.GetName())","\t\treturn srv.SendAndClose(\u0026pb.LogSummary{","\t\t\tRecord:        apiRec.Name,","\t\t\tBytesReceived: written,","\t\t})","\t}","\tif stream != nil {","\t\tif flushErr := stream.Flush(); flushErr != nil {","\t\t\ts.logger.Error(flushErr)","\t\t\treturn fmt.Errorf(\"got flush error %s with returnErr: %s\", flushErr.Error(), returnErrorStr)","\t\t}","\t}","\treturn returnErr","}","","func isNilOrEOF(err error) bool {","\treturn err == nil || err == io.EOF","}","","// ListLogs returns list log records","func (s *Server) ListLogs(ctx context.Context, req *pb.ListRecordsRequest) (*pb.ListRecordsResponse, error) {","\tif req.GetParent() == \"\" {","\t\treturn nil, status.Error(codes.InvalidArgument, \"Parent missing\")","\t}","\tparent, _, err := result.ParseName(req.GetParent())","\tif err != nil {","\t\ts.logger.Error(err)","\t\treturn nil, status.Error(codes.InvalidArgument, \"Invalid Name\")","\t}","\tif err := s.auth.Check(ctx, parent, auth.ResourceLogs, auth.PermissionList); err != nil {","\t\ts.logger.Debug(err)","\t\t// unauthenticated status code and debug message produced by Check","\t\treturn nil, err","","\t}","","\tuserPageSize, err := pageSize(int(req.GetPageSize()))","\tif err != nil {","\t\treturn nil, err","\t}","","\tstart, err := pageStart(req.GetPageToken(), req.GetFilter())","\tif err != nil {","\t\treturn nil, err","\t}","","\tsortOrder, err := orderBy(req.GetOrderBy())","\tif err != nil {","\t\treturn nil, err","\t}","","\tenv, err := recordCEL()","\tif err != nil {","\t\treturn nil, err","\t}","\tprg, err := celenv.ParseFilter(env, req.GetFilter())","\tif err != nil {","\t\treturn nil, err","\t}","\t// Fetch n+1 items to get the next token.","\trec, err := s.getFilteredPaginatedSortedLogRecords(ctx, req.GetParent(), start, userPageSize+1, prg, sortOrder)","\tif err != nil {","\t\treturn nil, err","\t}","","\t// If we returned the full n+1 items, use the last element as the next page","\t// token.","\tvar nextToken string","\tif len(rec) \u003e userPageSize {","\t\tnext := rec[len(rec)-1]","\t\tvar err error","\t\tnextToken, err = pagination.EncodeToken(next.GetUid(), req.GetFilter())","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\trec = rec[:len(rec)-1]","\t}","","\treturn \u0026pb.ListRecordsResponse{","\t\tRecords:       rec,","\t\tNextPageToken: nextToken,","\t}, nil","}","","// getFilteredPaginatedSortedLogRecords returns the specified number of results that","// match the given CEL program.","func (s *Server) getFilteredPaginatedSortedLogRecords(ctx context.Context, parent, start string, pageSize int, prg cel.Program, sortOrder string) ([]*pb.Record, error) {","\tparent, resultName, err := result.ParseName(parent)","\tif err != nil {","\t\treturn nil, err","\t}","","\trec := make([]*pb.Record, 0, pageSize)","\tbatcher := pagination.NewBatcher(pageSize, minPageSize, maxPageSize)","\tfor len(rec) \u003c pageSize {","\t\tbatchSize := batcher.Next()","\t\tdbrecords := make([]*db.Record, 0, batchSize)","\t\tq := s.db.WithContext(ctx).Where(\"type = ?\", v1alpha3.LogRecordType)","\t\tq = q.Where(\"id \u003e ?\", start)","\t\t// Specifying `-` allows users to read Records across Results.","\t\t// See https://google.aip.dev/159 for more details.","\t\tif parent != \"-\" {","\t\t\tq = q.Where(\"parent = ?\", parent)","\t\t}","\t\tif resultName != \"-\" {","\t\t\tq = q.Where(\"result_name = ?\", resultName)","\t\t}","\t\tif sortOrder != \"\" {","\t\t\tq = q.Order(sortOrder)","\t\t}","\t\tq = q.Limit(batchSize).Find(\u0026dbrecords)","\t\tif err := errors.Wrap(q.Error); err != nil {","\t\t\treturn nil, err","\t\t}","","\t\t// Only return results that match the filter.","\t\tfor _, r := range dbrecords {","\t\t\tapi := record.ToAPI(r)","\t\t\tok, err := record.Match(api, prg)","\t\t\tif err != nil {","\t\t\t\treturn nil, err","\t\t\t}","\t\t\tif !ok {","\t\t\t\tcontinue","\t\t\t}","","\t\t\t// Change resource name to log format","\t\t\tparent, resultName, recordName, err := record.ParseName(api.Name)","\t\t\tif err != nil {","\t\t\t\treturn nil, err","\t\t\t}","\t\t\tapi.Name = log.FormatName(result.FormatName(parent, resultName), recordName)","","\t\t\trec = append(rec, api)","\t\t\tif len(rec) \u003e= pageSize {","\t\t\t\treturn rec, nil","\t\t\t}","\t\t}","","\t\t// We fetched fewer results than requested - this means we've exhausted all items.","\t\tif len(dbrecords) \u003c batchSize {","\t\t\tbreak","\t\t}","","\t\t// Set params for next batch.","\t\tstart = dbrecords[len(dbrecords)-1].ID","\t\tbatcher.Update(len(dbrecords), batchSize)","\t}","\treturn rec, nil","}","","// DeleteLog deletes a given record and the stored log.","func (s *Server) DeleteLog(ctx context.Context, req *pb.DeleteLogRequest) (*empty.Empty, error) {","\tparent, res, name, err := log.ParseName(req.GetName())","\tif err != nil {","\t\treturn nil, err","\t}","\tif err := s.auth.Check(ctx, parent, auth.ResourceLogs, auth.PermissionDelete); err != nil {","\t\treturn \u0026empty.Empty{}, err","\t}","","\t// Check in the input record exists in the database","\trec, err := getRecord(s.db, parent, res, name)","\tif err != nil {","\t\treturn \u0026empty.Empty{}, err","\t}","\t// Check if the input record is referenced in any logs record","\tif rec.Type != v1alpha3.LogRecordType {","\t\trec, err = getLogRecord(s.db, parent, res, name)","\t\tif err != nil {","\t\t\treturn \u0026empty.Empty{}, err","\t\t}","\t}","","\tstreamer, _, err := log.ToStream(ctx, rec, s.config)","\tif err != nil {","\t\treturn nil, err","\t}","\terr = streamer.Delete()","\tif err != nil {","\t\treturn nil, err","\t}","","\treturn \u0026empty.Empty{}, errors.Wrap(s.db.WithContext(ctx).Delete(\u0026db.Record{}, rec).Error)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,1,1,1,0,2,1,1,1,1,0,2,2,1,1,1,0,2,1,1,1,1,1,0,0,2,2,1,1,1,0,0,2,2,1,1,1,1,1,1,1,1,1,1,0,0,2,2,1,1,1,2,2,1,1,1,2,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,1,1,0,2,1,1,0,2,1,1,1,1,1,1,1,1,1,0,2,2,2,1,1,0,0,2,2,2,1,1,0,0,2,2,2,2,2,2,1,1,0,0,0,2,2,2,2,2,2,2,2,2,0,0,2,1,1,1,1,1,0,1,0,2,2,2,2,2,1,1,1,0,2,2,1,1,1,1,1,0,1,1,1,1,0,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,0,1,1,1,1,0,0,2,2,2,1,1,1,0,2,2,2,2,2,0,1,1,1,1,1,0,1,0,0,1,1,1,0,0,2,2,1,1,2,2,2,2,2,2,1,1,1,1,1,0,2,2,2,2,0,2,2,1,1,0,2,2,2,2,0,2,2,1,1,2,2,2,2,0,2,2,1,1,0,0,0,2,2,2,2,2,2,1,1,2,0,0,2,2,2,2,0,0,0,0,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,0,2,2,2,2,1,1,2,2,0,0,0,2,2,1,1,2,2,2,2,2,2,0,0,0,2,2,0,0,0,1,1,0,2,0,0,0,2,2,2,1,1,2,1,1,0,0,2,2,2,2,0,2,1,1,1,1,0,0,2,2,1,1,2,2,1,1,0,2,0]},{"id":36,"path":"pkg/api/server/v1alpha2/ordering.go","lines":["// Copyright 2020 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package server","","import (","\t\"fmt\"","\t\"strings\"","","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/status\"",")","","var allowedOrderByFields = []string{\"created_time\", \"updated_time\"}","","// orderBy validates and returns a string formatted suitably for","// a sql order by clause.","func orderBy(fields string) (string, error) {","\tif strings.TrimSpace(fields) == \"\" {","\t\treturn \"\", nil","\t}","","\tvar orderBy []string","\tfor _, field := range strings.Split(fields, \",\") {","\t\tob, err := normalizeOrderByField(field)","\t\tif err != nil {","\t\t\treturn \"\", err","\t\t}","\t\torderBy = append(orderBy, ob)","\t}","","\treturn strings.Join(orderBy, \",\"), nil","}","","// normalizeOrderByField takes a field string, validating and formatting","// it for use in a sql query. An error is returned if the format of the string","// doesn't match either \"field_name\" or \"field_name direction\".","func normalizeOrderByField(field string) (string, error) {","\tf := strings.Fields(field)","\tfieldName := \"\"","\tdirection := \"\"","\tswitch len(f) {","\tcase 1:","\t\tfieldName = f[0]","\tcase 2:","\t\tfieldName = f[0]","\t\tdirection = f[1]","\tdefault:","\t\treturn \"\", status.Errorf(codes.InvalidArgument, \"invalid order_by %q\", field)","\t}","","\tif !isAllowedField(fieldName) {","\t\treturn \"\", status.Errorf(codes.InvalidArgument, \"order by %s not supported\", fieldName)","\t}","","\tif direction == \"\" {","\t\treturn fieldName, nil","\t}","\treturn orderByDirection(fieldName, direction)","}","","func isAllowedField(name string) bool {","\tfor i := range allowedOrderByFields {","\t\tif name == allowedOrderByFields[i] {","\t\t\treturn true","\t\t}","\t}","\treturn false","}","","func orderByDirection(field string, direction string) (string, error) {","\tswitch strings.ToLower(direction) {","\tcase \"asc\":","\t\treturn fmt.Sprintf(\"%s ASC\", field), nil","\tcase \"desc\":","\t\treturn fmt.Sprintf(\"%s DESC\", field), nil","\tdefault:","\t\treturn \"\", status.Errorf(codes.InvalidArgument, \"invalid sort direction %q\", direction)","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,2,2,2,2,2,2,2,0,0,2,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,0,2,2,2,2,0,0,2,2,2,2,2,0,2,0,0,2,2,2,2,2,2,2,2,0,0]},{"id":37,"path":"pkg/api/server/v1alpha2/pagination.go","lines":["// Copyright 2020 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package server","","import (","\t\"fmt\"","","\t\"github.com/tektoncd/results/pkg/api/server/db/pagination\"","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/status\"",")","","const (","\tminPageSize = 50","\tmaxPageSize = 10000",")","","func pageSize(in int) (int, error) {","\tif in \u003c 0 { //nolint:gocritic","\t\treturn 0, status.Error(codes.InvalidArgument, \"PageSize should be greater than 0\")","\t} else if in == 0 {","\t\treturn minPageSize, nil","\t} else if in \u003e maxPageSize {","\t\treturn maxPageSize, nil","\t}","\treturn in, nil","}","","func pageStart(token, filter string) (string, error) {","\tif token == \"\" {","\t\treturn \"\", nil","\t}","","\ttokenName, tokenFilter, err := pagination.DecodeToken(token)","\tif err != nil {","\t\treturn \"\", status.Error(codes.InvalidArgument, fmt.Sprintf(\"invalid PageToken: %v\", err))","\t}","\tif filter != tokenFilter {","\t\treturn \"\", status.Error(codes.InvalidArgument, \"filter does not match previous query\")","\t}","\treturn tokenName, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,0,2,2,2,2,2,2,2,2,0]},{"id":38,"path":"pkg/api/server/v1alpha2/plugin/plugin_logs.go","lines":["package plugin","","import (","\t\"bytes\"","\t\"context\"","\t\"encoding/json\"","\t\"errors\"","\t\"fmt\"","\t\"io\"","\t\"net/http\"","\t\"net/http/httputil\"","\t\"net/url\"","\t\"os\"","\t\"path\"","\t\"path/filepath\"","\t\"regexp\"","\t\"slices\"","\t\"sort\"","\t\"strconv\"","\t\"strings\"","\t\"time\"","","\t\"go.uber.org/zap\"","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/metadata\"","\t\"google.golang.org/grpc/status\"","\t\"gorm.io/gorm\"","","\t\"gocloud.dev/blob\"","","\t// Adding the driver for gcs.","\t_ \"gocloud.dev/blob/gcsblob\"","\t// Adding the driver for s3.","\t_ \"gocloud.dev/blob/s3blob\"","","\t\"github.com/tektoncd/results/pkg/api/server/db\"","\tdbErrors \"github.com/tektoncd/results/pkg/api/server/db/errors\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/auth\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/log\"","\t\"github.com/tektoncd/results/pkg/apis/v1alpha3\"","\t\"github.com/tektoncd/results/pkg/logs\"","\tpb3 \"github.com/tektoncd/results/proto/v1alpha3/results_go_proto\"","","\tpipelinev1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"",")","","const (","\tlokiQueryPath      = \"/loki/api/v1/query_range\"","\tsplunkQueryPath    = \"/services/search/v2/jobs\"","\ttypePipelineRun    = \"tekton.dev/v1.PipelineRun\"","\ttypeTaskRun        = \"tekton.dev/v1.TaskRun\"","\ttypeTaskRunV1Beta1 = \"tekton.dev/v1beta1.TaskRun\"","","\tlegacyLogType = \"v1alpha2LogType\"","\t// TODO: make this key configurable in a future release","\tdefaultBlobPathParams = \"/%s/%s/%s/\" // parent/result/record","","\t// TODO make these key configurable in a future release","\tpipelineRunUIDKey         = \"kubernetes.labels.tekton_dev_pipelineRunUID\"","\ttaskRunUIDKey             = \"kubernetes.labels.tekton_dev_taskRunUID\"","\tsplunkPollInterval        = 5 * time.Second","\tsplunkPollTimeoutDuration = 1 * time.Minute","\tsplunkTokenEnv            = \"SPLUNK_SEARCH_TOKEN\"","\tsplunkOutputFormat        = \"?output_mode=json\"",")","","var (","\topenBucket = func(ctx context.Context, urlString string) (*blob.Bucket, error) {","\t\tbucket, err := blob.OpenBucket(ctx, urlString)","\t\treturn bucket, err","\t}","\tclean = func(bucket *blob.Bucket, logger *zap.SugaredLogger) {","\t\terr := bucket.Close()","\t\tif err != nil {","\t\t\tlogger.Errorf(\"Got error while closing bucket %s\", err)","\t\t}","\t}",")","","type getLog func(s *LogServer, writer io.Writer, parent string, rec *db.Record) error","","// GetLog streams log record by log request","func (s *LogServer) GetLog(req *pb3.GetLogRequest, srv pb3.Logs_GetLogServer) error {","\tparent, res, name, err := log.ParseName(req.GetName())","\tif err != nil {","\t\ts.logger.Error(err)","\t\treturn status.Error(codes.InvalidArgument, \"Invalid Name\")","\t}","","\tif err := s.auth.Check(srv.Context(), parent, auth.ResourceLogs, auth.PermissionGet); err != nil {","\t\ts.logger.Error(err)","\t\t// unauthenticated status code and debug message produced by Check","\t\treturn err","\t}","","\trec, err := getRecord(s.db, parent, res, name)","\tif err != nil {","\t\ts.logger.Error(err)","\t\treturn err","\t}","","\tif rec == nil {","\t\ts.logger.Errorf(\"records not found: parent: %s, result: %s, name: %s\", parent, res, name)","\t\treturn status.Error(codes.Internal, \"Error streaming log\")","\t}","","\twriter := logs.NewBufferedHTTPWriter(srv, req.GetName(), s.config.LOGS_BUFFER_SIZE)","","\terr = s.getLog(s, writer, parent, rec)","\tif err != nil {","\t\ts.logger.Error(err)","\t}","","\t_, err = writer.Flush()","\tif err != nil {","\t\ts.logger.Error(err)","\t\treturn status.Error(codes.Internal, \"Error streaming log\")","\t}","\treturn nil","}","","func getLokiLogs(s *LogServer, writer io.Writer, parent string, rec *db.Record) error {","\tURL, err := url.Parse(s.config.LOGGING_PLUGIN_API_URL)","\tif err != nil {","\t\ts.logger.Error(err)","\t\treturn err","\t}","\tURL.Path = path.Join(URL.Path, s.config.LOGGING_PLUGIN_PROXY_PATH, lokiQueryPath)","","\tvar startTime, endTime, uidKey string","\tswitch rec.Type {","\tcase typePipelineRun:","\t\tuidKey = pipelineRunUIDKey","\t\tdata := \u0026pipelinev1.PipelineRun{}","\t\terr := json.Unmarshal(rec.Data, data)","\t\tif err != nil {","\t\t\terr = fmt.Errorf(\"failed to marshal pipelinerun data for fetching log, err: %s\", err.Error())","\t\t\ts.logger.Error(err)","\t\t\treturn err","\t\t}","","\t\tif data.Status.StartTime == nil {","\t\t\terr = errors.New(\"there's no startime in pipelinerun\")","\t\t\ts.logger.Error(err)","\t\t\treturn err","\t\t}","\t\tstartTime = strconv.FormatInt(data.Status.StartTime.UTC().Unix(), 10)","","\t\tif data.Status.CompletionTime == nil {","\t\t\terr = errors.New(\"there's no completion in pipelinerun\")","\t\t\ts.logger.Error(err)","\t\t\treturn err","\t\t}","\t\tendTime = strconv.FormatInt(data.Status.CompletionTime.Add(s.forwarderDelayDuration).UTC().Unix(), 10)","","\tcase typeTaskRun:","\t\tuidKey = taskRunUIDKey","\t\tdata := \u0026pipelinev1.TaskRun{}","\t\terr := json.Unmarshal(rec.Data, data)","\t\tif err != nil {","\t\t\terr = fmt.Errorf(\"failed to marshal taskrun data for fetching log, err: %s\", err.Error())","\t\t\ts.logger.Error(err)","\t\t\treturn err","\t\t}","\t\tif data.Status.StartTime == nil {","\t\t\terr = errors.New(\"there's no startime in taskrun\")","\t\t\ts.logger.Error(err)","\t\t\treturn err","\t\t}","\t\tstartTime = strconv.FormatInt(data.Status.StartTime.UTC().Unix(), 10)","","\t\tif data.Status.CompletionTime == nil {","\t\t\terr = errors.New(\"there's no completion in taskrun\")","\t\t\ts.logger.Error(err)","\t\t\treturn err","\t\t}","\t\tendTime = strconv.FormatInt(data.Status.CompletionTime.Add(s.forwarderDelayDuration).UTC().Unix(), 10)","","\tdefault:","\t\ts.logger.Errorf(\"record type is invalid, record ID: %v, Name: %v, result Name: %v, result ID:  %v\", rec.ID, rec.Name, rec.ResultName, rec.ResultID)","\t\treturn errors.New(\"record type is invalid\")","\t}","","\tparameters := url.Values{}","\tfor k, v := range s.queryParams {","\t\tparameters.Add(k, v)","\t}","\tquery := `{ ` + s.staticLabels + s.config.LOGGING_PLUGIN_NAMESPACE_KEY + `=\"` + parent + `\" }|json uid=\"` + uidKey + `\", message=\"message\" |uid=\"` + rec.Name + `\"| line_format \"{{.message}}\"`","\tif s.config.LOGGING_PLUGIN_CONTAINER_KEY != \"\" {","\t\tquery = `{ ` + s.staticLabels + s.config.LOGGING_PLUGIN_NAMESPACE_KEY + `=\"` + parent + `\" }|json uid=\"` + uidKey + `\", container=\"` + s.config.LOGGING_PLUGIN_CONTAINER_KEY + `\", message=\"message\" |uid=\"` + rec.Name + `\"| line_format \"container-{{.container}}: message={{.message}}\"`","\t}","\tparameters.Add(\"query\", query)","\tparameters.Add(\"end\", endTime)","\tparameters.Add(\"start\", startTime)","\tparameters.Add(\"limit\", strconv.FormatUint(uint64(s.queryLimit), 10))","","\tURL.RawQuery = parameters.Encode()","\ts.logger.Debugf(\"loki request url:%s\", URL.String())","","\treq, err := http.NewRequest(\"GET\", URL.String(), nil)","\tif err != nil {","\t\ts.logger.Errorf(\"new request to loki failed, err: %s:\", err.Error())","\t\treturn err","\t}","","\ttoken, err := s.tokenSource.Token()","\tif err != nil {","\t\ts.logger.Error(\"failed to fetch token\", err)","\t\treturn err","\t}","","\treq.Header.Set(\"Authorization\", \"Bearer \"+token.AccessToken)","\tresp, err := s.client.Do(req)","\tif err != nil {","\t\tdump, derr := httputil.DumpRequest(req, true)","\t\tif derr == nil {","\t\t\ts.logger.Debugf(\"Request Dump***:\\n %q\\n\", dump)","\t\t}","\t\ts.logger.Errorf(\"request to loki failed, err: %s, req: %v\", err.Error(), req)","\t\treturn status.Error(codes.Internal, \"Error streaming log\")","\t}","","\tif resp == nil {","\t\tdump, err := httputil.DumpRequest(req, true)","\t\tif err == nil {","\t\t\ts.logger.Debugf(\"Request Dump***:\\n %q\\n\", dump)","\t\t}","\t\ts.logger.Errorf(\"request to loki failed, received nil response\")","\t\ts.logger.Debugf(\"loki request url:%s\", URL.String())","\t\treturn status.Error(codes.Internal, \"Error streaming log\")","\t}","","\tif resp.StatusCode != http.StatusOK {","\t\ts.logger.Errorf(\"Loki API request failed with HTTP status code: %d\", resp.StatusCode)","\t\tdump, err := httputil.DumpRequest(req, true)","\t\tif err == nil {","\t\t\ts.logger.Debugf(\"Request Dump***:\\n %q\\n\", dump)","\t\t}","\t\tdump, err = httputil.DumpResponse(resp, true)","\t\tif err == nil {","\t\t\ts.logger.Debugf(\"Response Dump***:\\n %q\\n\", dump)","\t\t}","\t\treturn status.Error(codes.Internal, \"Error fetching log data\")","\t}","","\tdata, err := io.ReadAll(resp.Body)","\tif err != nil {","\t\ts.logger.Errorf(\"failed to read response body, err: %s\", err.Error())","\t\treturn status.Error(codes.Internal, \"Error streaming log\")","\t}","","\tvar lokiResponse struct {","\t\tStatus string `json:\"status\"`","\t\tError  string `json:\"error\"`","\t\tData   struct {","\t\t\tResult []struct {","\t\t\t\tStream struct {","\t\t\t\t\tMessage string `json:\"message\"`","\t\t\t\t} `json:\"stream\"`","\t\t\t\tValues [][]string `json:\"values\"`","\t\t\t} `json:\"result\"`","\t\t\tStats map[string]interface{} `json:\"stats\"`","\t\t} `json:\"data\"`","\t}","","\tif err := json.Unmarshal(data, \u0026lokiResponse); err != nil {","\t\ts.logger.Errorf(\"failed to unmarshal Loki response, err: %s, data: %s\", err.Error(), string(data))","\t\treturn status.Error(codes.Internal, fmt.Sprintf(\"Error processing fetched log data, err: %s, data: %s\",","\t\t\terr.Error(), string(data)))","\t}","","\ts.logger.Debugf(\"stats.summary %v\", lokiResponse.Data.Stats[\"summary\"])","","\tif lokiResponse.Status != \"success\" {","\t\ts.logger.Errorf(\"Loki API request failed with status: %s, error: %s\", lokiResponse.Status, lokiResponse.Error)","\t\treturn status.Error(codes.Internal, \"Error fetching log data from Loki\")","\t}","","\tvar logMessages [][]string","\tfor _, result := range lokiResponse.Data.Result {","\t\tlogMessages = append(logMessages, result.Values...)","\t}","","\tsort.Slice(logMessages, func(i, j int) bool {","\t\tif len(logMessages[i]) == 0 {","\t\t\treturn true","\t\t}","\t\tif len(logMessages[j]) == 0 {","\t\t\treturn false","\t\t}","\t\treturn logMessages[i][0] \u003c logMessages[j][0]","\t})","","\tvar orderMessages []string","\tfor _, val := range logMessages {","\t\tif len(val) \u003c= 1 {","\t\t\tcontinue","\t\t}","\t\torderMessages = append(orderMessages, val[1:]...)","\t}","","\tformattedLogs := strings.Join(orderMessages, \"\\n\")","","\tif _, err = writer.Write([]byte(formattedLogs)); err != nil {","\t\ts.logger.Errorf(\"failed to write log data, err: %s\", err.Error())","\t\treturn status.Error(codes.Internal, \"Error streaming log\")","\t}","","\treturn nil","}","","func getBlobLogs(s *LogServer, writer io.Writer, parent string, rec *db.Record) error {","\tu, err := url.Parse(s.config.LOGGING_PLUGIN_API_URL)","\tif err != nil {","\t\ts.logger.Error(err)","\t\treturn err","\t}","","\tlegacy := false","\tqueryParams := u.Query()","","\tfor k, v := range s.queryParams {","\t\tif k == legacyLogType \u0026\u0026 v == \"true\" {","\t\t\tlegacy = true","\t\t\tcontinue","\t\t}","\t\tqueryParams.Add(k, v)","\t}","\tu.RawQuery = queryParams.Encode()","","\tlogPath := []string{}","","\tctx := context.Background()","\ts.logger.Debugf(\"blob bucket: %s\", u.String())","\tbucket, err := openBucket(ctx, u.String())","\tif err != nil {","\t\ts.logger.Errorf(\"error opening bucket: %s\", err)","\t\treturn err","\t}","\tdefer clean(bucket, s.logger)","","\tswitch rec.Type {","\tcase typePipelineRun:","\t\terr := errors.New(\"pipelinerun not supported, please use taskrun\")","\t\ts.logger.Error(err)","\t\treturn err","\tcase typeTaskRunV1Beta1:","\t\tif legacy {","\t\t\tlogRec, err := getLogRecord(s.db, parent, rec.ResultID, rec.Name)","\t\t\tif err != nil {","\t\t\t\ts.logger.Debugf(\"error getting legacy log record: %s\", err)","\t\t\t}","\t\t\tif logRec != nil {","\t\t\t\tlog := \u0026v1alpha3.Log{}","\t\t\t\terr := json.Unmarshal(logRec.Data, log)","\t\t\t\tif err != nil {","\t\t\t\t\terr = fmt.Errorf(\"could not decode Log record: %w\", err)","\t\t\t\t\ts.logger.Error(err)","\t\t\t\t\treturn err","\t\t\t\t}","\t\t\t\tlogPath = append(logPath, filepath.Join(s.config.LOGS_PATH, log.Status.Path))","\t\t\t}","\t\t} else {","\t\t\ts.logger.Errorf(\"record type is invalid %s\", rec.Type)","\t\t\treturn fmt.Errorf(\"record type is invalid %s\", rec.Type)","\t\t}","\tcase typeTaskRun:","\t\ts.logger.Debugf(\"taskrun type\")","\t\titer := bucket.List(\u0026blob.ListOptions{","\t\t\tPrefix: strings.TrimPrefix(s.config.LOGS_PATH+fmt.Sprintf(defaultBlobPathParams, parent, rec.ResultName, rec.Name), \"/\"),","\t\t})","\t\ts.logger.Debugf(\"prefix: %s\", strings.TrimPrefix(s.config.LOGS_PATH+fmt.Sprintf(defaultBlobPathParams, parent, rec.ResultName, rec.Name), \"/\"))","\t\t// bucket.List returns the objects sorted alphabetically by key (name), we need that sorted by last modified time","\t\ttoSort := []*blob.ListObject{}","\t\tfor {","\t\t\tobj, err := iter.Next(ctx)","\t\t\tif err == io.EOF {","\t\t\t\tbreak","\t\t\t}","\t\t\tif err != nil {","\t\t\t\terr := fmt.Errorf(\"error listing log bucket objects: %w\", err)","\t\t\t\ts.logger.Error(err)","\t\t\t\treturn err","\t\t\t}","\t\t\ttoSort = append(toSort, obj)","\t\t}","\t\t// S3 objects ModTime is rounded to the second (not milliseconds), so objects stored in the same second are still ordered alphabetically","\t\tslices.SortFunc(toSort, func(a, b *blob.ListObject) int {","\t\t\treturn time.Time.Compare(a.ModTime, b.ModTime)","\t\t})","\t\tfor _, obj := range toSort {","\t\t\tlogPath = append(logPath, obj.Key)","\t\t}","","\t\ts.logger.Debugf(\"logPath: %v\", logPath)","\tcase v1alpha3.LogRecordType, v1alpha3.LogRecordTypeV2:","\t\tlog := \u0026v1alpha3.Log{}","\t\terr := json.Unmarshal(rec.Data, log)","\t\tif err != nil {","\t\t\terr = fmt.Errorf(\"could not decode Log record: %w\", err)","\t\t\ts.logger.Error(err)","\t\t\treturn err","\t\t}","\t\tlogPath = append(logPath, filepath.Join(s.config.LOGS_PATH, log.Status.Path))","\tdefault:","\t\ts.logger.Errorf(\"record type is invalid, record ID: %v, Name: %v, result Name: %v, result ID:  %v\", rec.ID, rec.Name, rec.ResultName, rec.ResultID)","\t\treturn fmt.Errorf(\"record type is invalid %s\", rec.Type)","\t}","","\tregex := s.config.LOGGING_PLUGIN_MULTIPART_REGEX","\tre, err := regexp.Compile(regex)","\tif err != nil {","\t\ts.logger.Errorf(\"failed to compile regexp: %s\", err)","\t\treturn err","\t}","\tmergedLogParts := mergeLogParts(logPath, re)","","\tfor _, parts := range mergedLogParts {","\t\tbaseName := re.ReplaceAllString(parts[0], \"\")","\t\ts.logger.Debugf(\"mergedLogParts key: %s value: %v\", baseName, parts)","\t\t_, file := filepath.Split(baseName)","\t\tif _, err := fmt.Fprint(writer, strings.TrimRight(file, \".log\")+\" :-\\n\"); err != nil {","\t\t\ts.logger.Errorf(\"error writing log header: %s\", err)","\t\t}","\t\tfor _, part := range parts {","\t\t\terr := func() error {","\t\t\t\trc, err := bucket.NewReader(ctx, part, nil)","\t\t\t\tif err != nil {","\t\t\t\t\ts.logger.Errorf(\"error creating bucket reader: %s for log part: %s\", err, part)","\t\t\t\t\treturn err","\t\t\t\t}","\t\t\t\tdefer func() {","\t\t\t\t\tif err := rc.Close(); err != nil {","\t\t\t\t\t\ts.logger.Errorf(\"error closing bucket reader: %s\", err)","\t\t\t\t\t}","\t\t\t\t}()","","\t\t\t\t_, err = rc.WriteTo(writer)","\t\t\t\tif err != nil {","\t\t\t\t\ts.logger.Errorf(\"error writing the logs: %s\", err)","\t\t\t\t}","\t\t\t\treturn nil","\t\t\t}()","\t\t\tif err != nil {","\t\t\t\ts.logger.Error(err)","\t\t\t\treturn err","\t\t\t}","\t\t\tif _, err := fmt.Fprint(writer, \"\\n\"); err != nil {","\t\t\t\ts.logger.Errorf(\"error writing newline: %s\", err)","\t\t\t}","\t\t}","\t}","","\treturn nil","}","","// mergeLogParts organizes in groups objects part of the same log","func mergeLogParts(logPath []string, re *regexp.Regexp) [][]string {","\tmerged := [][]string{}","\t// use extra mapping [log_base_name:index_of_slice_of_parts] to preserve the order of elements","\tbaseNameIndexes := map[string]int{}","\tindex := 0","\tfor _, log := range logPath {","\t\tbaseName := re.ReplaceAllString(log, \"\")","\t\tif existingIndex, ok := baseNameIndexes[baseName]; ok {","\t\t\tmerged[existingIndex] = append(merged[existingIndex], log)","\t\t} else {","\t\t\tbaseNameIndexes[baseName] = index","\t\t\tmerged = append(merged, []string{log})","\t\t\tindex++","\t\t}","\t}","\treturn merged","}","","// getSplunkLogs retrieves logs for a given record from a Splunk backend and writes them to the provided writer.","//","// It constructs a Splunk search job using the record's UID and namespace, submits the job, polls for completion, and fetches the resulting logs.","// The function requires a valid Splunk API URL, a search token from the environment, and an \"index\" query parameter.","// Returns an error if any step in the process fails, including job creation, polling, or log retrieval.","func getSplunkLogs(s *LogServer, writer io.Writer, parent string, rec *db.Record) error {","\tURL, err := url.Parse(s.config.LOGGING_PLUGIN_API_URL)","\tif err != nil {","\t\ts.logger.Error(err)","\t\treturn err","\t}","\tURL.Path = path.Join(URL.Path, splunkQueryPath)","","\ttoken := os.Getenv(splunkTokenEnv)","\tif token == \"\" {","\t\ts.logger.Error(\"splunk token not set SPLUNK_SEARCH_TOKEN\")","\t\treturn errors.New(\"splunk token not set\")","","\t}","","\tvar uidKey string","\tswitch rec.Type {","\tcase typePipelineRun:","\t\tuidKey = pipelineRunUIDKey","\tcase typeTaskRun:","\t\tuidKey = taskRunUIDKey","\tdefault:","\t\ts.logger.Errorf(\"record type is invalid, record ID: %v, Name: %v, result Name: %v, result ID:  %v, rec Type: %v\", rec.ID, rec.Name, rec.ResultName, rec.ResultID, rec.Type)","\t\treturn errors.New(\"record type is invalid\")","\t}","\tindex, ok := s.queryParams[\"index\"]","\tif !ok {","\t\ts.logger.Errorf(\"index not specified in queryParams: %v\\n\", s.queryParams)","\t\treturn errors.New(\"index not specified in query parameters\")","","\t}","\ts.logger.Debugf(\"splunk request url:%s\", URL.String()+splunkOutputFormat)","","\tquery := fmt.Sprintf(`search index=%s %s=%q %s=%q | table message structured.msg %s`,","\t\tindex, uidKey, rec.Name, s.config.LOGGING_PLUGIN_NAMESPACE_KEY, parent,","\t\ts.config.LOGGING_PLUGIN_CONTAINER_KEY)","","\tqueryData := url.Values{}","\tqueryData.Set(\"search\", query)","","\treq, err := http.NewRequest(\"POST\", URL.String()+splunkOutputFormat, bytes.NewReader([]byte(queryData.Encode())))","\tif err != nil {","\t\ts.logger.Errorf(\"new request to splunk failed, err: %s:\", err.Error())","\t\treturn err","\t}","","\treq.Header.Set(\"Content-Type\", \"application/x-www-form-urlencoded\")","\treq.Header.Set(\"Authorization\", \"Bearer \"+token)","\tresp, err := s.client.Do(req)","\tif err != nil {","\t\ts.logger.Errorf(\"request to splunk failed, err: %s, req: %v\", err.Error(), req)","\t\treturn status.Error(codes.Internal, \"Error streaming log\")","\t}","","\tif resp == nil {","\t\ts.logger.Errorf(\"request to splunk failed, received nil response\")","\t\ts.logger.Debugf(\"splunk request url:%s\", URL.String())","\t\treturn status.Error(codes.Internal, \"Error streaming log\")","\t}","\tdefer func() {","\t\tif err := resp.Body.Close(); err != nil {","\t\t\ts.logger.Errorf(\"error closing response body: %s\", err)","\t\t}","\t}()","","\tif resp.StatusCode != http.StatusCreated {","\t\ts.logger.Errorf(\"Splunk Job Creation API request failed with HTTP status code: %d\", resp.StatusCode)","\t\treturn status.Error(codes.Internal, \"Error fetching log data - search job creation failed\")","\t}","","\tdata, err := io.ReadAll(resp.Body)","\tif err != nil {","\t\ts.logger.Errorf(\"failed to read response body, err: %s\", err.Error())","\t\treturn status.Error(codes.Internal, \"Error streaming log\")","\t}","","\tvar searchResponse struct {","\t\tSID string `json:\"sid\"`","\t}","","\tif err := json.Unmarshal(data, \u0026searchResponse); err != nil {","\t\ts.logger.Errorf(\"failed to unmarshal Splunk Search response, err: %s, data: %s\", err.Error(), string(data))","\t\treturn status.Error(codes.Internal, fmt.Sprintf(\"Error processing fetched log data, err: %s, data: %s\",","\t\t\terr.Error(), string(data)))","\t}","","\tif err := pollSplunkJobStatus(s, URL.String()+\"/\"+searchResponse.SID+splunkOutputFormat, token); err != nil {","\t\ts.logger.Errorf(\"failed to poll splunk job status, err: %v\", err)","\t\treturn status.Error(codes.Internal, fmt.Sprintf(\"Error failed to poll splunk search job status, err: %s\", err.Error()))","\t}","","\treq, err = http.NewRequest(\"GET\", URL.String()+\"/\"+searchResponse.SID+\"/results?output_mode=json_rows\u0026count=0\", nil)","\tif err != nil {","\t\ts.logger.Errorf(\"new request to splunk failed, err: %s:\", err.Error())","\t\treturn err","\t}","","\treq.Header.Set(\"Authorization\", \"Bearer \"+token)","\tlresp, err := s.client.Do(req)","\tif err != nil {","\t\ts.logger.Errorf(\"request to fetch log from  splunk failed, err: %s, req: %v\", err.Error(), req)","\t\treturn status.Error(codes.Internal, \"Error streaming log\")","\t}","","\tif lresp == nil {","\t\ts.logger.Errorf(\"request to splunk failed, received nil response\")","\t\ts.logger.Debugf(\"splunk request url:%s\", URL.String())","\t\treturn status.Error(codes.Internal, \"Error streaming log\")","\t}","\tdefer func() {","\t\tif err := lresp.Body.Close(); err != nil {","\t\t\ts.logger.Errorf(\"error closing response body: %s\", err)","\t\t}","\t}()","","\tif lresp.StatusCode != http.StatusOK {","\t\ts.logger.Errorf(\"Splunk Fetch Log API request failed with HTTP status code: %d\", resp.StatusCode)","\t\treturn status.Error(codes.Internal, \"Error fetching log data - fetch log api failed\")","\t}","","\tdata, err = io.ReadAll(lresp.Body)","\tif err != nil {","\t\ts.logger.Errorf(\"failed to read response body, err: %s\", err.Error())","\t\treturn status.Error(codes.Internal, \"Error streaming log\")","\t}","","\tvar logData struct {","\t\tRows [][]string `json:\"rows\"`","\t}","\tif err := json.Unmarshal(data, \u0026logData); err != nil {","\t\ts.logger.Errorf(\"failed to unmarshal Splunk log data, err: %s, data: %s\", err.Error(), string(data))","\t\treturn fmt.Errorf(\"failed to unmarshal Splunk log data, err: %s\", err.Error())","\t}","","\tvar logMessages []string","\tstep := \"\"","\tfor _, msg := range logData.Rows {","\t\tif len(msg) != 3 {","\t\t\ts.logger.Errorf(\"mismatch in column data, should be 3 received: %v, %v\", len(msg), msg)","\t\t\tcontinue","\t\t}","\t\tif step != msg[2] {","\t\t\tstep = msg[2]","\t\t\tlogMessages = append(logMessages, step+\":-\")","\t\t}","\t\tlogMessages = append(logMessages, msg[0]+msg[1])","\t}","","\tformattedLogs := strings.Join(logMessages, \"\\n\")","\tif _, err = writer.Write([]byte(formattedLogs)); err != nil {","\t\ts.logger.Errorf(\"failed to write log data, err: %s\", err.Error())","\t\treturn status.Error(codes.Internal, \"Error streaming log\")","\t}","","\treturn nil","}","","// pollSplunkJobStatus polls the status of a Splunk search job until it is complete, fails, or times out.","// It sends periodic GET requests to the provided Splunk job status URL using the given token.","// Returns an error if the job fails, is canceled, or does not complete within the timeout period.","func pollSplunkJobStatus(s *LogServer, url, token string) error {","\tticker := time.NewTicker(splunkPollInterval)","\tdefer ticker.Stop()","","\ttimeout := time.After(splunkPollTimeoutDuration)","","\terrSplunkJobFailed := errors.New(\"splunk job failed\")","","\tfor {","\t\tselect {","\t\tcase \u003c-timeout:","\t\t\ts.logger.Errorf(\"timeout reached for splunk search job, url: %v\", url)","\t\t\treturn fmt.Errorf(\"timeout reached for splunk search job, url: %v\", url)","\t\tcase \u003c-ticker.C:","\t\t\terr := func() error {","\t\t\t\treq, err := http.NewRequest(\"GET\", url, nil)","\t\t\t\tif err != nil {","\t\t\t\t\ts.logger.Errorf(\"new request to splunk failed, err: %s:\", err.Error())","\t\t\t\t\treturn fmt.Errorf(\"new request to splunk failed: err: %s\", err.Error())","\t\t\t\t}","","\t\t\t\treq.Header.Set(\"Authorization\", \"Bearer \"+token)","\t\t\t\tresp, err := s.client.Do(req)","\t\t\t\tif err != nil {","\t\t\t\t\ts.logger.Errorf(\"request to splunk failed, err: %s, req: %v\", err.Error(), req)","\t\t\t\t\treturn fmt.Errorf(\"request to splunk failed, err: %s, req: %v\", err.Error(), req)","\t\t\t\t}","","\t\t\t\tif resp == nil {","\t\t\t\t\ts.logger.Errorf(\"request to splunk failed, received nil response,: %s\", url)","\t\t\t\t\treturn fmt.Errorf(\"request to splunk failed, received nil response,: %s\", url)","\t\t\t\t}","\t\t\t\tdefer func() {","\t\t\t\t\tif err := resp.Body.Close(); err != nil {","\t\t\t\t\t\ts.logger.Errorf(\"error closing response body: %s\", err)","\t\t\t\t\t}","\t\t\t\t}()","\t\t\t\tif resp.StatusCode != http.StatusOK {","\t\t\t\t\ts.logger.Errorf(\"Splunk Job Creation API request failed with HTTP status code: %d\", resp.StatusCode)","\t\t\t\t\treturn fmt.Errorf(\"splunk job creation API request failed with HTTP status code: %d\", resp.StatusCode)","\t\t\t\t}","","\t\t\t\tdata, err := io.ReadAll(resp.Body)","\t\t\t\tif err != nil {","\t\t\t\t\ts.logger.Errorf(\"failed to read response body, err: %s\", err.Error())","\t\t\t\t\treturn fmt.Errorf(\"failed to read response body, err: %s\", err.Error())","\t\t\t\t}","","\t\t\t\tvar jobResp struct {","\t\t\t\t\tEntry []struct {","\t\t\t\t\t\tContent struct {","\t\t\t\t\t\t\tDispatchState string `json:\"dispatchState\"`","\t\t\t\t\t\t} `json:\"content\"`","\t\t\t\t\t} `json:\"entry\"`","\t\t\t\t}","","\t\t\t\tif err := json.Unmarshal(data, \u0026jobResp); err != nil {","\t\t\t\t\ts.logger.Errorf(\"failed to unmarshal Splunk Search response, err: %s, data: %s\", err.Error(), string(data))","\t\t\t\t\treturn fmt.Errorf(\"failed to unmarshal Splunk Search response, err: %s, data: %s\", err.Error(), string(data))","\t\t\t\t}","\t\t\t\tif len(jobResp.Entry) == 0 {","\t\t\t\t\ts.logger.Errorf(\"empty Splunk Search response entry, data: %s\", string(data))","\t\t\t\t\treturn fmt.Errorf(\"empty Splunk Search response entry, data: %s\", string(data))","\t\t\t\t}","","\t\t\t\tswitch jobResp.Entry[0].Content.DispatchState {","\t\t\t\tcase \"DONE\":","\t\t\t\t\treturn nil","\t\t\t\tcase \"INTERNAL_CANCEL\", \"USER_CANCEL\", \"BAD_INPUT_CANCEL\", \"QUIT\", \"FAILED\":","\t\t\t\t\treturn fmt.Errorf(\"%w: state: %s\", errSplunkJobFailed, jobResp.Entry[0].Content.DispatchState)","\t\t\t\tdefault:","\t\t\t\t\treturn fmt.Errorf(\"waiting for job to be done: current state %s\", jobResp.Entry[0].Content.DispatchState)","\t\t\t\t}","\t\t\t}()","\t\t\tif err != nil {","\t\t\t\tif errors.Is(err, errSplunkJobFailed) {","\t\t\t\t\treturn err","\t\t\t\t}","\t\t\t\tcontinue","\t\t\t}","\t\t\treturn nil","\t\t}","\t}","}","","func (s *LogServer) setLogPlugin() bool {","\tswitch strings.ToLower(s.config.LOGS_TYPE) {","\tcase string(v1alpha3.LokiLogType):","\t\ts.IsLogPluginEnabled = true","\t\ts.getLog = getLokiLogs","\tcase string(v1alpha3.BlobLogType):","\t\ts.IsLogPluginEnabled = true","\t\ts.getLog = getBlobLogs","\tcase string(v1alpha3.SplunkLogType):","\t\ts.IsLogPluginEnabled = true","\t\ts.getLog = getSplunkLogs","\tdefault:","\t\t// TODO(xinnjie) when s.config.LOGS_TYPE is File also show this error log","\t\ts.IsLogPluginEnabled = false","\t\ts.logger.Warnf(\"Plugin Logs API Disable: unsupported type of logs given for plugin, \" +","\t\t\t\"legacy logging system might work\")","\t}","\treturn s.IsLogPluginEnabled","}","","// LogMux returns a http.Handler that serves the log plugin server","func (s *LogServer) LogMux() http.Handler {","\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {","\t\t// TODO: Create a new log handler","\t\tctx := r.Context()","\t\tmd := metadata.MD(r.Header)","\t\tctx = metadata.NewIncomingContext(ctx, md)","\t\tparent := r.PathValue(\"parent\")","\t\trecID := r.PathValue(\"recordID\")","\t\tres := r.PathValue(\"resultID\")","\t\ts.logger.Debugf(\"recordID: %s resultID: %s name: %s md: %+v\", recID, res, parent, r.Header)","\t\tif err := s.auth.Check(ctx, parent, auth.ResourceLogs, auth.PermissionGet); err != nil {","\t\t\ts.logger.Error(err)","\t\t\thttp.Error(w, \"Not Authorized\", http.StatusUnauthorized)","\t\t\treturn","\t\t}","\t\trec, err := getRecord(s.db, parent, res, recID)","\t\tif err != nil {","\t\t\ts.logger.Error(err)","\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)","\t\t\treturn","\t\t}","","\t\tif rec == nil {","\t\t\ts.logger.Errorf(\"records not found: parent: %s, result: %s, recID: %s\", parent, res, recID)","\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)","\t\t\treturn","\t\t}","","\t\terr = s.getLog(s, w, parent, rec)","\t\tif err != nil {","\t\t\ts.logger.Error(err)","\t\t\thttp.Error(w, \"Failed to stream logs err: \"+err.Error(), http.StatusInternalServerError)","\t\t\treturn","\t\t}","\t})","}","","func getRecord(txn *gorm.DB, parent, result, name string) (*db.Record, error) {","\tstore := \u0026db.Record{}","\tq := txn.","\t\tWhere(\u0026db.Record{Parent: parent, ResultName: result, Name: name}).","\t\tFirst(store)","\tif err := dbErrors.Wrap(q.Error); err != nil {","\t\treturn nil, err","\t}","\treturn store, nil","}","","func getLogRecord(txn *gorm.DB, parent, resultID, name string) (*db.Record, error) {","\tstore := \u0026db.Record{}","\tq := txn.","\t\tWhere(\u0026db.Record{Parent: parent, ResultID: resultID}).","\t\tWhere(\"data -\u003e 'spec' -\u003e 'resource' -\u003e\u003e 'uid' =  ?\", name).","\t\tFirst(store)","\tif err := dbErrors.Wrap(q.Error); err != nil {","\t\treturn nil, err","\t}","\treturn store, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,2,2,2,1,1,1,0,2,1,1,1,1,0,2,2,1,1,1,0,2,1,1,1,0,2,2,2,2,1,1,0,2,2,1,1,1,2,0,0,2,2,2,1,1,1,2,2,2,2,2,2,2,2,2,1,1,1,1,0,2,1,1,1,1,2,2,2,1,1,1,1,2,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,0,2,2,1,1,1,0,2,2,2,1,1,1,1,1,1,0,0,2,1,1,1,1,1,1,1,0,0,2,1,1,1,1,1,1,1,1,1,1,0,0,2,2,1,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,0,2,2,2,1,1,1,0,2,2,2,2,0,2,2,1,1,2,1,1,2,0,0,2,2,2,1,0,2,0,0,2,2,2,1,1,1,0,2,0,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,0,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,0,1,1,1,1,1,1,1,0,0,0,1,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,0,0,0,0,0,0,0,2,2,2,1,1,1,2,2,2,2,1,1,1,1,0,2,2,1,1,2,2,1,1,1,0,2,2,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,1,1,1,0,2,2,2,2,1,1,1,0,2,1,1,1,1,2,2,1,1,0,0,2,1,1,1,0,2,2,1,1,1,0,2,2,2,2,2,1,1,1,1,0,2,1,1,1,0,2,2,1,1,1,0,2,2,2,1,1,1,0,2,1,1,1,1,2,2,1,1,0,0,2,1,1,1,0,2,2,1,1,1,0,2,2,2,2,1,1,1,0,2,2,2,2,1,1,0,2,2,2,2,2,0,0,2,2,1,1,1,0,2,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,1,1,1,2,2,2,2,1,1,1,0,2,2,2,1,1,1,0,2,1,1,1,2,2,1,1,0,2,1,1,1,0,2,2,1,1,1,0,2,2,2,2,2,2,2,2,2,1,1,1,2,1,1,1,0,2,2,2,1,1,1,1,0,0,2,1,1,1,1,0,2,0,0,0,0,2,2,2,2,2,1,1,1,2,2,2,1,1,1,1,1,0,2,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,0,0,0,2,2,2,2,2,2,1,1,2,0,0,1,1,1,1,1,1,1,1,1,1,0]},{"id":39,"path":"pkg/api/server/v1alpha2/plugin/server.go","lines":["package plugin","","import (","\t\"crypto/tls\"","\t\"crypto/x509\"","\t\"fmt\"","\t\"net\"","\t\"net/http\"","\t\"strings\"","\t\"time\"","","\t\"github.com/tektoncd/results/pkg/api/server/config\"","\t\"go.uber.org/zap\"","","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/auth\"","\t\"golang.org/x/oauth2\"","\t\"gorm.io/gorm\"","","\tpb3 \"github.com/tektoncd/results/proto/v1alpha3/results_go_proto\"","","\t\"k8s.io/client-go/transport\"",")","","// LogServer is the server for the log plugin server","type LogServer struct {","\tpb3.UnimplementedLogsServer","","\tIsLogPluginEnabled bool","\tstaticLabels       string","","\tconfig *config.Config","\tlogger *zap.SugaredLogger","\tauth   auth.Checker","\tdb     *gorm.DB","\tclient *http.Client","","\tforwarderDelayDuration time.Duration","","\tqueryLimit uint","","\tqueryParams map[string]string","","\t// TODO: In future add support for non Oauth support","\ttokenSource oauth2.TokenSource","","\tgetLog getLog","}","","// NewLogServer returns a plugin log server","func NewLogServer(config *config.Config, logger *zap.SugaredLogger, auth auth.Checker, db *gorm.DB) (*LogServer, error) {","\ts := \u0026LogServer{","\t\tconfig: config,","\t\tlogger: logger,","\t\tauth:   auth,","\t\tdb:     db,","\t}","","\ts.logger.Debugf(\"LOGS_TYPE: %s\", strings.ToLower(s.config.LOGS_TYPE))","\t// If the logs type is not plugin supported,","\t//  we don't need to set up the LogPluginServer","\tif !s.setLogPlugin() {","\t\treturn s, nil","\t}","","\ts.logger.Info(\"Setting up LogPluginServer\")","","\tif s.config.LOGGING_PLUGIN_STATIC_LABELS != \"\" {","\t\tlabels := strings.Split(s.config.LOGGING_PLUGIN_STATIC_LABELS, \",\")","\t\tfor _, v := range labels {","\t\t\tlabel := strings.Split(v, \"=\")","\t\t\tif len(label) != 2 {","\t\t\t\treturn nil, fmt.Errorf(\"incorrect format for LOGGING_STATIC_LABELS: %s\", v)","\t\t\t}","\t\t\ts.staticLabels += label[0] + `=\"` + label[1] + `\",`","\t\t}","\t}","","\ts.client = \u0026http.Client{","\t\tTransport: \u0026http.Transport{","\t\t\tDial: (\u0026net.Dialer{","\t\t\t\tTimeout:   5 * time.Minute,","\t\t\t\tKeepAlive: 10 * time.Minute,","\t\t\t}).Dial,","\t\t\tTLSHandshakeTimeout:   10 * time.Second,","\t\t\tResponseHeaderTimeout: 10 * time.Second,","\t\t\tExpectContinueTimeout: 1 * time.Second,","\t\t},","\t}","","\tif s.config.LOGGING_PLUGIN_CA_CERT != \"\" {","\t\tcaCertPool := x509.NewCertPool()","\t\tcaCertPool.AppendCertsFromPEM([]byte(s.config.LOGGING_PLUGIN_CA_CERT))","\t\t// #nosec G402","\t\ts.client.Transport.(*http.Transport).TLSClientConfig = \u0026tls.Config{","\t\t\tRootCAs: caCertPool, //nolint:gosec  // needed when we have our own CA","\t\t}","\t} else if s.config.LOGGING_PLUGIN_TLS_VERIFICATION_DISABLE {","\t\ts.client.Transport.(*http.Transport).TLSClientConfig = \u0026tls.Config{","\t\t\tInsecureSkipVerify: true, //nolint:gosec  // needed for skipping tls verification","\t\t}","\t}","","\ts.forwarderDelayDuration = time.Duration(s.config.LOGGING_PLUGIN_FORWARDER_DELAY_DURATION) * time.Minute","","\ts.tokenSource = transport.NewCachedFileTokenSource(s.config.LOGGING_PLUGIN_TOKEN_PATH)","\ts.queryLimit = s.config.LOGGING_PLUGIN_QUERY_LIMIT","","\ts.queryParams = map[string]string{}","\tif s.config.LOGGING_PLUGIN_QUERY_PARAMS != \"\" {","\t\tfor _, v := range strings.Split(s.config.LOGGING_PLUGIN_QUERY_PARAMS, \"\u0026\") {","\t\t\tqueryParam := strings.Split(v, \"=\")","\t\t\tif len(queryParam) != 2 {","\t\t\t\treturn nil, fmt.Errorf(\"incorrect format for LOGGING_PLUGIN_QUERY_PARAMS: %s\", v)","\t\t\t}","\t\t\ts.queryParams[queryParam[0]] = queryParam[1]","\t\t}","\t}","","\treturn s, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,2,1,1,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,1,1,2,0,0,0,2,0]},{"id":40,"path":"pkg/api/server/v1alpha2/record/record.go","lines":["// Copyright 2020 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","// Package record provides utilities for manipulating and validating Records.","package record","","import (","\t\"encoding/json\"","\t\"fmt\"","\t\"regexp\"","","\t\"github.com/tektoncd/results/pkg/api/server/config\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/log\"","\t\"github.com/tektoncd/results/pkg/apis/v1alpha3\"","","\t\"github.com/google/cel-go/cel\"","\tv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\tresultscel \"github.com/tektoncd/results/pkg/api/server/cel\"","\t\"github.com/tektoncd/results/pkg/api/server/db\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/status\"","\t\"google.golang.org/protobuf/types/known/timestamppb\"",")","","const (","\ttypeSize = 768",")","","var (","\t// NameRegex matches valid name specs for a Result.","\tNameRegex = regexp.MustCompile(\"(^[a-z0-9_-]{1,63})/results/([a-z0-9_-]{1,63})/records/([a-z0-9_-]{1,63}$)\")",")","","// ParseName splits a full Result name into its individual (parent, result, name)","// components.","func ParseName(raw string) (parent, result, name string, err error) {","\ts := NameRegex.FindStringSubmatch(raw)","\tif len(s) != 4 {","\t\treturn \"\", \"\", \"\", status.Errorf(codes.InvalidArgument, \"name must match %s\", NameRegex.String())","\t}","\treturn s[1], s[2], s[3], nil","}","","// FormatName takes in a parent (\"a/results/b\") and record name (\"c\") and","// returns the full resource name (\"a/results/b/records/c\").","func FormatName(parent, name string) string {","\treturn fmt.Sprintf(\"%s/records/%s\", parent, name)","}","","// ToStorage converts an API Record into its corresponding database storage","// equivalent.","// parent,result,name should be the name parts (e.g. not containing \"/results/\" or \"/records/\").","func ToStorage(parent, resultName, resultID, name string, r *pb.Record, config *config.Config) (*db.Record, error) {","\tif err := validateData(r.GetData()); err != nil {","\t\treturn nil, status.Error(codes.InvalidArgument, err.Error())","\t}","","\tid := r.GetUid()","\tif id == \"\" {","\t\tid = r.GetId()","\t}","","\tdbr := \u0026db.Record{","\t\tParent:     parent,","\t\tResultName: resultName,","\t\tResultID:   resultID,","","\t\tID:   id,","\t\tName: name,","","\t\tType: r.GetData().GetType(),","\t\tData: r.GetData().GetValue(),","","\t\tEtag: r.Etag,","\t}","","\tif r.CreatedTime.IsValid() {","\t\tdbr.CreatedTime = r.CreatedTime.AsTime()","\t}","\tif r.CreateTime.IsValid() {","\t\tdbr.CreatedTime = r.CreateTime.AsTime()","\t}","\tif r.UpdatedTime.IsValid() {","\t\tdbr.UpdatedTime = r.UpdatedTime.AsTime()","\t}","\tif r.UpdateTime.IsValid() {","\t\tdbr.UpdatedTime = r.UpdateTime.AsTime()","\t}","\tif dbr.Type == v1alpha3.LogRecordType || dbr.Type == v1alpha3.LogRecordTypeV2 {","\t\tdata, err := log.ToStorage(r, config)","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\tdbr.Data = data","\t}","","\treturn dbr, nil","}","","// ToAPI converts a database storage Record into its corresponding API","// equivalent.","func ToAPI(r *db.Record) *pb.Record {","\tout := \u0026pb.Record{","\t\tName: fmt.Sprintf(\"%s/results/%s/records/%s\", r.Parent, r.ResultName, r.Name),","\t\tId:   r.ID,","\t\tUid:  r.ID,","\t\tEtag: r.Etag,","\t}","","\tif !r.CreatedTime.IsZero() {","\t\tout.CreatedTime = timestamppb.New(r.CreatedTime)","\t\tout.CreateTime = timestamppb.New(r.CreatedTime)","\t}","\tif !r.UpdatedTime.IsZero() {","\t\tout.UpdatedTime = timestamppb.New(r.UpdatedTime)","\t\tout.UpdateTime = timestamppb.New(r.UpdatedTime)","\t}","","\tif r.Data != nil {","\t\tout.Data = \u0026pb.Any{","\t\t\tType:  r.Type,","\t\t\tValue: r.Data,","\t\t}","\t}","","\treturn out","}","","// Match determines whether the given CEL filter matches the result.","func Match(r *pb.Record, prg cel.Program) (bool, error) {","\tif r == nil {","\t\treturn false, nil","\t}","","\tvar m map[string]any","\tif d := r.GetData().GetValue(); d != nil {","\t\tif err := json.Unmarshal(r.GetData().GetValue(), \u0026m); err != nil {","\t\t\treturn false, err","\t\t}","\t}","","\treturn resultscel.Match(prg, map[string]any{","\t\t\"name\":      r.GetName(),","\t\t\"data_type\": r.GetData().GetType(),","\t\t\"data\":      m,","\t})","}","","// UpdateEtag updates the etag field of a record according to its content.","// The record should at least have its `Id` and `UpdatedTime` fields set.","func UpdateEtag(r *db.Record) error {","\tif r.ID == \"\" {","\t\treturn fmt.Errorf(\"the ID field must be set\")","\t}","\tif r.UpdatedTime.IsZero() {","\t\treturn status.Error(codes.Internal, \"the UpdatedTime field must be set\")","\t}","\tr.Etag = fmt.Sprintf(\"%s-%v\", r.ID, r.UpdatedTime.UnixNano())","\treturn nil","}","","func validateData(m *pb.Any) error {","\tif err := ValidateType(m.GetType()); err != nil {","\t\treturn err","\t}","","\tif m == nil {","\t\treturn nil","\t}","\tswitch m.GetType() {","\tcase \"pipeline.tekton.dev/TaskRun\":","\t\treturn json.Unmarshal(m.GetValue(), \u0026v1.TaskRun{})","\tcase \"pipeline.tekton.dev/PipelineRun\":","\t\treturn json.Unmarshal(m.GetValue(), \u0026v1.PipelineRun{})","\tcase \"results.tekton.dev/v1alpha3.Log\":","\t\treturn json.Unmarshal(m.GetValue(), \u0026v1alpha3.Log{})","\tdefault:","\t\t// If it's not a well known type, just check that the message is a valid JSON document.","\t\treturn json.Unmarshal(m.GetValue(), \u0026json.RawMessage{})","\t}","}","","// ValidateType validates line t to ensure it can be stored in the database.","func ValidateType(t string) error {","\t// Certain DBs like sqlite will massage CHAR types to TEXT, so enforce","\t// this in our code for consistency.","\tif len(t) \u003e typeSize {","\t\treturn status.Errorf(codes.InvalidArgument, \"type must not exceed %d characters\", typeSize)","\t}","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,0,0,0,2,2,2,0,0,0,0,2,2,2,2,0,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,0,0,2,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,0,2,2,2,2,2,2,0,2,0,0,0,1,1,1,1,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,2,2,2,2,0,2,2,2,2,1,1,1,1,1,1,2,2,2,0,0,0,0,2,2,2,2,2,2,2,0]},{"id":41,"path":"pkg/api/server/v1alpha2/records.go","lines":["// Copyright 2020 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package server","","import (","\t\"context\"","","\t\"github.com/golang/protobuf/ptypes/empty\"","\t\"github.com/google/cel-go/cel\"","\t\"github.com/google/cel-go/checker/decls\"","\t\"github.com/tektoncd/results/pkg/api/server/db\"","\t\"github.com/tektoncd/results/pkg/api/server/db/errors\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/auth\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/lister\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/record\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/result\"","\t\"github.com/tektoncd/results/pkg/internal/protoutil\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/status\"","\t\"google.golang.org/protobuf/proto\"","\t\"google.golang.org/protobuf/types/known/timestamppb\"","\t\"gorm.io/gorm\"",")","","// CreateRecord creates a new record in the database.","func (s *Server) CreateRecord(ctx context.Context, req *pb.CreateRecordRequest) (*pb.Record, error) {","\tr := req.GetRecord()","","\t// Validate the incoming request","\tparent, resultName, name, err := record.ParseName(r.GetName())","\tif err != nil {","\t\treturn nil, err","\t}","\tif req.GetParent() != result.FormatName(parent, resultName) {","\t\treturn nil, status.Error(codes.InvalidArgument, \"requested parent does not match resource name\")","\t}","\tif err := s.auth.Check(ctx, parent, auth.ResourceRecords, auth.PermissionCreate); err != nil {","\t\treturn nil, err","\t}","","\t// Look up the result ID from the name. This does not have to happen","\t// transactionally with the insert since name\u003c-\u003eID mappings are immutable,","\t// and if the parent result is deleted mid-request, the insert should","\t// fail due to foreign key constraints.","\tresultID, err := s.getResultID(ctx, parent, resultName)","\tif err != nil {","\t\treturn nil, err","\t}","","\t// Populate Result with server provided fields.","\tprotoutil.ClearOutputOnly(r)","\tr.Id = uid()","\tr.Uid = r.Id","\tts := timestamppb.New(clock.Now())","\tr.CreatedTime = ts","\tr.CreateTime = ts","\tr.UpdatedTime = ts","\tr.UpdateTime = ts","","\tstore, err := record.ToStorage(parent, resultName, resultID, name, req.GetRecord(), s.config)","\tif err != nil {","\t\treturn nil, err","\t}","\tif err := record.UpdateEtag(store); err != nil {","\t\treturn nil, err","\t}","\tq := s.db.WithContext(ctx).","\t\tModel(store).","\t\tCreate(store).Error","\tif err := errors.Wrap(q); err != nil {","\t\treturn nil, err","\t}","","\treturn record.ToAPI(store), nil","}","","// resultID is a utility struct to extract partial Result data representing","// Result name \u003c-\u003e ID mappings.","type resultID struct {","\tName string","\tID   string","}","","func (s *Server) getResultIDImpl(ctx context.Context, parent, result string) (string, error) {","\tid := new(resultID)","\tq := s.db.WithContext(ctx).","\t\tModel(\u0026db.Result{}).","\t\tWhere(\u0026db.Result{Parent: parent, Name: result}).","\t\tFirst(id)","\tif err := errors.Wrap(q.Error); err != nil {","\t\treturn \"\", err","\t}","\treturn id.ID, nil","}","","// GetRecord returns a single Record.","func (s *Server) GetRecord(ctx context.Context, req *pb.GetRecordRequest) (*pb.Record, error) {","\tparent, result, name, err := record.ParseName(req.GetName())","\tif err != nil {","\t\treturn nil, err","\t}","\tif err := s.auth.Check(ctx, parent, auth.ResourceRecords, auth.PermissionGet); err != nil {","\t\treturn nil, err","\t}","","\tr, err := getRecord(s.db.WithContext(ctx), parent, result, name)","\tif err != nil {","\t\treturn nil, err","\t}","\treturn record.ToAPI(r), nil","}","","func getRecord(txn *gorm.DB, parent, result, name string) (*db.Record, error) {","\t// Note: set the Parent, ResultName and Name fields in the model used to","\t// query the database to take advantage of the records_by_name composite","\t// index. Although the Name is an unique value as well, leveraging the","\t// index speeds up the query significantly. See","\t// https://github.com/tektoncd/results/issues/336.","\tstore := \u0026db.Record{}","\tq := txn.","\t\tWhere(\u0026db.Record{Parent: parent, ResultName: result, Name: name}).","\t\tFirst(store)","\tif err := errors.Wrap(q.Error); err != nil {","\t\treturn nil, err","\t}","\treturn store, nil","}","","// ListRecords returns list records from the database.","func (s *Server) ListRecords(ctx context.Context, req *pb.ListRecordsRequest) (*pb.ListRecordsResponse, error) {","\tif req.GetParent() == \"\" {","\t\treturn nil, status.Error(codes.InvalidArgument, \"parent missing\")","\t}","","\t// Authentication","\tparent, resultName, err := result.ParseName(req.GetParent())","\tif err != nil {","\t\treturn nil, err","\t}","\tif err := s.auth.Check(ctx, parent, auth.ResourceRecords, auth.PermissionList); err != nil {","\t\treturn nil, err","\t}","","\trecordsLister, err := lister.OfRecords(s.recordsEnv, parent, resultName, req)","\tif err != nil {","\t\treturn nil, err","\t}","","\trecords, nextPageToken, err := recordsLister.List(ctx, s.db)","\tif err != nil {","\t\treturn nil, err","\t}","","\treturn \u0026pb.ListRecordsResponse{","\t\tRecords:       records,","\t\tNextPageToken: nextPageToken,","\t}, nil","}","","// UpdateRecord updates a record in the database.","func (s *Server) UpdateRecord(ctx context.Context, req *pb.UpdateRecordRequest) (*pb.Record, error) {","\tin := req.GetRecord()","","\tparent, result, name, err := record.ParseName(in.GetName())","\tif err != nil {","\t\treturn nil, err","\t}","\tif err := s.auth.Check(ctx, parent, auth.ResourceRecords, auth.PermissionUpdate); err != nil {","\t\treturn nil, err","\t}","","\tprotoutil.ClearOutputOnly(in)","","\tvar out *pb.Record","\terr = s.db.WithContext(ctx).Transaction(func(tx *gorm.DB) error {","\t\tr, err := getRecord(tx, parent, result, name)","\t\tif err != nil {","\t\t\treturn err","\t\t}","","\t\t// If the user provided the Etag field, then make sure the value of this field matches what saved in the database.","\t\t// See https://google.aip.dev/154 for more information.","\t\tif req.GetEtag() != \"\" \u0026\u0026 req.GetEtag() != r.Etag {","\t\t\treturn status.Error(codes.FailedPrecondition, \"the etag mismatches\")","\t\t}","","\t\t// Merge existing data with user request.","\t\tpb := record.ToAPI(r)","\t\t// TODO: field mask support.","\t\tproto.Merge(pb, in)","","\t\tupdateTime := timestamppb.New(clock.Now())","\t\tpb.UpdatedTime = updateTime","\t\tpb.UpdateTime = updateTime","","\t\t// Convert back to storage and store.","\t\ts, err := record.ToStorage(r.Parent, r.ResultName, r.ResultID, r.Name, pb, s.config)","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\tif err := record.UpdateEtag(s); err != nil {","\t\t\treturn err","\t\t}","\t\tif err := errors.Wrap(tx.Save(s).Error); err != nil {","\t\t\treturn err","\t\t}","","\t\tpb.Etag = s.Etag","\t\tout = pb","\t\treturn nil","\t})","\treturn out, err","}","","// DeleteRecord deletes a given record.","func (s *Server) DeleteRecord(ctx context.Context, req *pb.DeleteRecordRequest) (*empty.Empty, error) {","\tparent, result, name, err := record.ParseName(req.GetName())","\tif err != nil {","\t\treturn nil, err","\t}","\tif err := s.auth.Check(ctx, parent, auth.ResourceRecords, auth.PermissionDelete); err != nil {","\t\treturn \u0026empty.Empty{}, err","\t}","","\t// First get the current record. This ensures that we return NOT_FOUND if","\t// the entry is already deleted.","\t// This does not need to be done in the same transaction as to delete,","\t// since the identifiers are immutable.","\tr, err := getRecord(s.db, parent, result, name)","\tif err != nil {","\t\treturn \u0026empty.Empty{}, err","\t}","\treturn \u0026empty.Empty{}, errors.Wrap(s.db.WithContext(ctx).Delete(\u0026db.Record{}, r).Error)","}","","// recordCEL defines the CEL environment for querying Record data.","// Fields are broken up explicitly in order to support dynamic handling of the","// data field as a key-value document.","func recordCEL() (*cel.Env, error) {","\treturn cel.NewEnv(","\t\tcel.Types(\u0026pb.Record{}),","\t\tcel.Declarations(decls.NewVar(\"name\", decls.String)),","\t\tcel.Declarations(decls.NewVar(\"data_type\", decls.String)),","\t\tcel.Declarations(decls.NewVar(\"data\", decls.Dyn)),","\t)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,1,1,2,1,1,0,0,0,0,0,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,1,1,2,1,1,2,2,2,2,2,2,0,2,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,1,1,0,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,1,1,0,0,2,2,2,2,2,1,1,0,2,2,2,2,0,2,2,2,2,0,2,2,2,2,0,0,0,2,2,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,0,0,0,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,1,1,2,1,1,2,1,1,0,2,2,2,0,2,0,0,0,2,2,2,1,1,2,1,1,0,0,0,0,0,2,2,2,2,2,0,0,0,0,0,2,2,2,2,2,2,2,2]},{"id":42,"path":"pkg/api/server/v1alpha2/result/result.go","lines":["// Copyright 2020 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","// Package result provides utilities for manipulating and validating Results.","package result","","import (","\t\"fmt\"","\t\"regexp\"","\t\"time\"","","\t\"github.com/google/cel-go/cel\"","\tresultscel \"github.com/tektoncd/results/pkg/api/server/cel\"","\t\"github.com/tektoncd/results/pkg/api/server/db\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/record\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/status\"","\t\"google.golang.org/protobuf/types/known/timestamppb\"","\t\"knative.dev/pkg/ptr\"",")","","var (","\t// NameRegex matches valid name specs for a Result.","\tNameRegex = regexp.MustCompile(\"(^[a-z0-9_-]{1,63})/results/([a-z0-9_-]{1,63}$)\")",")","","// ParseName splits a full Result name into its individual (parent, name)","// components.","func ParseName(raw string) (parent, name string, err error) {","\ts := NameRegex.FindStringSubmatch(raw)","\tif len(s) != 3 {","\t\treturn \"\", \"\", status.Errorf(codes.InvalidArgument, \"name must match %s\", NameRegex.String())","\t}","\treturn s[1], s[2], nil","}","","// FormatName takes in a parent (\"a\") and result name (\"b\") and","// returns the full resource name (\"a/results/b\").","func FormatName(parent, name string) string {","\treturn fmt.Sprintf(\"%s/results/%s\", parent, name)","}","","// ToStorage converts an API Result into its corresponding database storage","// equivalent.","// parent,name should be the name parts (e.g. not containing \"/results/\").","func ToStorage(r *pb.Result) (*db.Result, error) {","\tparent, name, err := ParseName(r.GetName())","\tif err != nil {","\t\treturn nil, err","\t}","\tid := r.GetUid()","\tif id == \"\" {","\t\tid = r.GetId()","\t}","\tresult := \u0026db.Result{","\t\tParent:      parent,","\t\tID:          id,","\t\tName:        name,","\t\tAnnotations: r.Annotations,","\t\tEtag:        r.Etag,","\t}","","\tif r.CreatedTime.IsValid() {","\t\tresult.CreatedTime = r.CreatedTime.AsTime()","\t}","\tif r.CreateTime.IsValid() {","\t\tresult.CreatedTime = r.CreateTime.AsTime()","\t}","\tif r.UpdatedTime.IsValid() {","\t\tresult.UpdatedTime = r.UpdatedTime.AsTime()","\t}","\tif r.UpdateTime.IsValid() {","\t\tresult.UpdatedTime = r.UpdateTime.AsTime()","\t}","","\tif s := r.GetSummary(); s != nil {","\t\tif s.GetRecord() == \"\" || s.GetType() == \"\" {","\t\t\treturn nil, status.Errorf(codes.InvalidArgument, \"record and type fields required for RecordSummary\")","\t\t}","\t\tif !record.NameRegex.MatchString(s.GetRecord()) {","\t\t\treturn nil, status.Errorf(codes.InvalidArgument, \"invalid record format\")","\t\t}","\t\tif err := record.ValidateType(s.GetType()); err != nil {","\t\t\treturn nil, err","\t\t}","","\t\tsummary := db.RecordSummary{","\t\t\tRecord:      s.GetRecord(),","\t\t\tType:        s.GetType(),","\t\t\tStatus:      int32(s.GetStatus()),","\t\t\tAnnotations: s.Annotations,","\t\t}","\t\tif s.StartTime.IsValid() {","\t\t\tsummary.StartTime = ptr.Time(s.StartTime.AsTime())","\t\t}","\t\tif s.EndTime.IsValid() {","\t\t\tsummary.EndTime = ptr.Time(s.EndTime.AsTime())","\t\t}","\t\tresult.Summary = summary","\t}","","\treturn result, nil","}","","// ToAPI converts a database storage Result into its corresponding API","// equivalent.","func ToAPI(r *db.Result) *pb.Result {","\tvar summary *pb.RecordSummary","\tif r.Summary.Record != \"\" {","\t\tsummary = \u0026pb.RecordSummary{","\t\t\tRecord:      r.Summary.Record,","\t\t\tType:        r.Summary.Type,","\t\t\tStartTime:   newTS(r.Summary.StartTime),","\t\t\tEndTime:     newTS(r.Summary.EndTime),","\t\t\tStatus:      pb.RecordSummary_Status(r.Summary.Status),","\t\t\tAnnotations: r.Summary.Annotations,","\t\t}","\t}","","\treturn \u0026pb.Result{","\t\tName:        FormatName(r.Parent, r.Name),","\t\tId:          r.ID,","\t\tUid:         r.ID,","\t\tCreatedTime: timestamppb.New(r.CreatedTime),","\t\tCreateTime:  timestamppb.New(r.CreatedTime),","\t\tUpdatedTime: timestamppb.New(r.UpdatedTime),","\t\tUpdateTime:  timestamppb.New(r.UpdatedTime),","\t\tAnnotations: r.Annotations,","\t\tEtag:        r.Etag,","\t\tSummary:     summary,","\t}","}","","func newTS(t *time.Time) *timestamppb.Timestamp {","\tif t == nil {","\t\treturn nil","\t}","\treturn timestamppb.New(*t)","}","","// Match determines whether the given CEL filter matches the result.","func Match(r *pb.Result, prg cel.Program) (bool, error) {","\tif r == nil {","\t\treturn false, nil","\t}","\treturn resultscel.Match(prg, map[string]any{","\t\t\"result\": r,","\t})","}","","// UpdateEtag updates the etag field of a result according to its content.","// The result should at least have its `Id` and `UpdatedTime` fields set.","func UpdateEtag(r *db.Result) error {","\tif r.ID == \"\" {","\t\treturn fmt.Errorf(\"the ID field must be set\")","\t}","\tif r.UpdatedTime.IsZero() {","\t\treturn status.Error(codes.Internal, \"the UpdatedTime field must be set\")","\t}","\tr.Etag = fmt.Sprintf(\"%s-%v\", r.ID, r.UpdatedTime.UnixNano())","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,0,0,0,2,2,2,0,0,0,0,2,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,1,1,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,0,0,0,0,2,2,2,1,1,1,1,1,1,1,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,0,0,1,1,1,1,1,0,0,0,2,2,2,2,2,2,2,0,0,0,0,1,1,1,1,1,1,1,1,1,0]},{"id":43,"path":"pkg/api/server/v1alpha2/results.go","lines":["// Copyright 2020 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package server","","import (","\t\"context\"","\t\"log\"","","\t\"github.com/golang/protobuf/ptypes/empty\"","\t\"gorm.io/gorm\"","","\t\"github.com/tektoncd/results/pkg/api/server/db\"","\t\"github.com/tektoncd/results/pkg/api/server/db/errors\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/auth\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/lister\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/result\"","\t\"github.com/tektoncd/results/pkg/internal/protoutil\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/status\"","\t\"google.golang.org/protobuf/types/known/timestamppb\"",")","","// CreateResult creates a new result in the database.","func (s *Server) CreateResult(ctx context.Context, req *pb.CreateResultRequest) (*pb.Result, error) {","\tr := req.GetResult()","","\t// Validate the incoming request","\tparent, _, err := result.ParseName(r.GetName())","\tif err != nil {","\t\treturn nil, status.Error(codes.InvalidArgument, err.Error())","\t}","\tif req.GetParent() != parent {","\t\treturn nil, status.Error(codes.InvalidArgument, \"requested parent does not match resource name\")","\t}","\tif err := s.auth.Check(ctx, parent, auth.ResourceResults, auth.PermissionCreate); err != nil {","\t\treturn nil, err","\t}","","\t// Populate Result with server provided fields.","\tprotoutil.ClearOutputOnly(r)","\tid := uid()","\tr.Id = id","\tr.Uid = id","\tts := timestamppb.New(clock.Now())","\tr.CreatedTime = ts","\tr.CreateTime = ts","\tr.UpdatedTime = ts","\tr.UpdateTime = ts","","\tstore, err := result.ToStorage(r)","\tif err != nil {","\t\treturn nil, err","\t}","","\tif err := result.UpdateEtag(store); err != nil {","\t\treturn nil, err","\t}","","\tif err := errors.Wrap(s.db.WithContext(ctx).Create(store).Error); err != nil {","\t\treturn nil, err","\t}","\treturn result.ToAPI(store), nil","}","","// GetResult returns a single Result.","func (s *Server) GetResult(ctx context.Context, req *pb.GetResultRequest) (*pb.Result, error) {","\tparent, name, err := result.ParseName(req.GetName())","\tif err != nil {","\t\treturn nil, status.Error(codes.InvalidArgument, err.Error())","\t}","\tif err := s.auth.Check(ctx, parent, auth.ResourceResults, auth.PermissionGet); err != nil {","\t\treturn nil, err","\t}","\tstore, err := getResultByParentName(s.db, parent, name)","\tif err != nil {","\t\treturn nil, err","\t}","\treturn result.ToAPI(store), nil","}","","// UpdateResult updates a Result in the database.","func (s *Server) UpdateResult(ctx context.Context, req *pb.UpdateResultRequest) (*pb.Result, error) {","\t// Retrieve result from database by name","\tparent, name, err := result.ParseName(req.GetName())","\tif err != nil {","\t\treturn nil, status.Error(codes.InvalidArgument, err.Error())","\t}","\tif err := s.auth.Check(ctx, parent, auth.ResourceResults, auth.PermissionUpdate); err != nil {","\t\treturn nil, err","\t}","","\tvar out *pb.Result","\terr = s.db.WithContext(ctx).Transaction(func(tx *gorm.DB) error {","\t\tprev, err := getResultByParentName(tx, parent, name)","\t\tif err != nil {","\t\t\treturn status.Errorf(codes.NotFound, \"failed to find a result: %v\", err)","\t\t}","","\t\t// If the user provided the Etag field, then make sure the value of this field matches what saved in the database.","\t\t// See https://google.aip.dev/154 for more information.","\t\tif req.GetEtag() != \"\" \u0026\u0026 req.GetEtag() != prev.Etag {","\t\t\treturn status.Error(codes.FailedPrecondition, \"the etag mismatches\")","\t\t}","","\t\tnewpb := result.ToAPI(prev)","\t\treqpb := req.GetResult()","\t\tprotoutil.ClearOutputOnly(reqpb)","\t\t// Merge requested Result with previous Result to apply updates,","\t\t// making sure to filter out any OUTPUT_ONLY fields, and only","\t\t// updatable fields.","\t\t// We can't use proto.Merge, since empty fields in the req should take","\t\t// precedence, so set each updatable field here.","\t\tnewpb.Annotations = reqpb.GetAnnotations()","\t\tnewpb.Summary = reqpb.GetSummary()","\t\ttoDB, err := result.ToStorage(newpb)","\t\tif err != nil {","\t\t\treturn err","\t\t}","","\t\t// Set server-side provided fields","\t\ttoDB.UpdatedTime = clock.Now()","\t\tif err := result.UpdateEtag(toDB); err != nil {","\t\t\treturn err","\t\t}","","\t\t// Write result back to database.","\t\tif err = errors.Wrap(tx.Save(toDB).Error); err != nil {","\t\t\tlog.Printf(\"failed to save result into database: %v\", err)","\t\t\treturn err","\t\t}","\t\tout = result.ToAPI(toDB)","","\t\treturn nil","\t})","\treturn out, err","}","","// DeleteResult deletes a given result.","func (s *Server) DeleteResult(ctx context.Context, req *pb.DeleteResultRequest) (*empty.Empty, error) {","\tparent, name, err := result.ParseName(req.GetName())","\tif err != nil {","\t\treturn nil, err","\t}","\tif err := s.auth.Check(ctx, parent, auth.ResourceResults, auth.PermissionDelete); err != nil {","\t\treturn nil, err","\t}","","\t// First get the current result. This ensures that we return NOT_FOUND if","\t// the entry is already deleted.","\t// This does not need to be done in the same transaction as to delete,","\t// since the identifiers are immutable.","\tr := \u0026db.Result{}","\tget := s.db.WithContext(ctx).","\t\tWhere(\u0026db.Result{Parent: parent, Name: name}).","\t\tFirst(r)","\tif err := errors.Wrap(get.Error); err != nil {","\t\treturn \u0026empty.Empty{}, err","\t}","","\t// Delete the result.","\tdel := s.db.WithContext(ctx).Delete(\u0026db.Result{}, r)","\treturn \u0026empty.Empty{}, errors.Wrap(del.Error)","}","","// ListResults returns list results from the database.","func (s *Server) ListResults(ctx context.Context, req *pb.ListResultsRequest) (*pb.ListResultsResponse, error) {","\tif req.GetParent() == \"\" {","\t\treturn nil, status.Error(codes.InvalidArgument, \"parent missing\")","\t}","","\tif err := s.auth.Check(ctx, req.GetParent(), auth.ResourceResults, auth.PermissionList); err != nil {","\t\treturn nil, err","\t}","","\tresultsLister, err := lister.OfResults(s.resultsEnv, req)","\tif err != nil {","\t\treturn nil, err","\t}","","\tresults, nextPageToken, err := resultsLister.List(ctx, s.db)","\tif err != nil {","\t\treturn nil, err","\t}","","\treturn \u0026pb.ListResultsResponse{","\t\tResults:       results,","\t\tNextPageToken: nextPageToken,","\t}, nil","}","","func getResultByParentName(gdb *gorm.DB, parent, name string) (*db.Result, error) {","\tr := \u0026db.Result{}","\tq := gdb.","\t\tWhere(\u0026db.Result{Parent: parent, Name: name}).","\t\tFirst(r)","\tif err := errors.Wrap(q.Error); err != nil {","\t\treturn nil, err","\t}","\treturn r, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,1,1,0,2,2,2,2,0,0,0,2,2,2,2,2,2,1,1,2,2,2,2,2,0,0,0,2,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,0,0,0,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,1,1,0,0,2,1,1,1,2,2,2,0,2,0,0,0,2,2,2,1,1,2,1,1,0,0,0,0,0,2,2,2,2,2,2,2,0,0,2,2,0,0,0,2,2,2,2,0,2,1,1,0,2,2,2,2,0,2,2,2,2,0,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,0]},{"id":44,"path":"pkg/api/server/v1alpha2/server.go","lines":["// Copyright 2020 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package server","","import (","\t\"context\"","\t\"fmt\"","","\t\"github.com/google/cel-go/cel\"","\t\"github.com/tektoncd/results/pkg/api/server/config\"","\t\"go.uber.org/zap\"","","\t\"github.com/google/uuid\"","\tcw \"github.com/jonboulle/clockwork\"","\tresultscel \"github.com/tektoncd/results/pkg/api/server/cel\"","\tmodel \"github.com/tektoncd/results/pkg/api/server/db\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/auth\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/plugin\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"gorm.io/gorm\"",")","","var (","\tuid = func() string {","\t\treturn uuid.New().String()","\t}","\tclock cw.Clock = cw.NewRealClock()",")","","type getResultID func(ctx context.Context, parent, result string) (string, error)","","// Server with implementation of API server","type Server struct {","\tpb.UnimplementedResultsServer","\tpb.UnimplementedLogsServer","\tconfig          *config.Config","\tlogger          *zap.SugaredLogger","\tenv             *cel.Env","\tresultsEnv      *cel.Env","\trecordsEnv      *cel.Env","\tdb              *gorm.DB","\tauth            auth.Checker","\tLogPluginServer *plugin.LogServer","","\t// testing.","\tgetResultID getResultID","}","","// New set up environment for the api server","func New(config *config.Config, logger *zap.SugaredLogger, db *gorm.DB, opts ...Option) (*Server, error) {","\tenv, err := resultscel.NewEnv()","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"failed to create CEL environment: %w\", err)","\t}","\t// TODO: turn the func into a MustX that should panic on error.","\tresultsEnv, err := resultscel.NewResultsEnv()","\tif err != nil {","\t\treturn nil, err","\t}","\trecordsEnv, err := resultscel.NewRecordsEnv()","\tif err != nil {","\t\treturn nil, err","\t}","","\tsrv := \u0026Server{","\t\tdb:         db,","\t\tenv:        env,","\t\tresultsEnv: resultsEnv,","\t\trecordsEnv: recordsEnv,","\t\tconfig:     config,","\t\tlogger:     logger,","\t\t// Default open auth for easier testing.","\t\tauth: auth.AllowAll{},","\t}","","\t// Set default impls of overridable behavior","\tsrv.getResultID = srv.getResultIDImpl","","\tfor _, o := range opts {","\t\to(srv)","\t}","","\tif config.DB_ENABLE_AUTO_MIGRATION {","\t\tif err := db.AutoMigrate(\u0026model.Result{}, \u0026model.Record{}); err != nil {","\t\t\treturn nil, fmt.Errorf(\"error automigrating DB: %w\", err)","\t\t}","\t}","","\tpluginServer, err := plugin.NewLogServer(srv.config, srv.logger, srv.auth, srv.db)","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"failed to create log plugin server: %w\", err)","\t}","\tsrv.LogPluginServer = pluginServer","","\treturn srv, nil","}","","// Option is customization for server configuration.","type Option func(*Server)","","// WithAuth is an option to enable auth checker for Server","func WithAuth(c auth.Checker) Option {","\treturn func(s *Server) {","\t\ts.auth = c","\t}","}","","func withGetResultID(f getResultID) Option {","\treturn func(s *Server) {","\t\ts.getResultID = f","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,1,1,0,2,2,1,1,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,1,1,0,0,2,2,1,1,2,2,2,0,0,0,0,0,0,1,1,1,1,0,0,2,2,2,2,0]},{"id":45,"path":"pkg/api/server/v1alpha2/summary.go","lines":["package server","","import (","\t\"context\"","","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/auth\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/lister\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/result\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/status\"",")","","// GetRecordListSummary returns the summary and aggregation for a given list of records","func (s *Server) GetRecordListSummary(ctx context.Context, req *pb.RecordListSummaryRequest) (*pb.RecordListSummary, error) {","\tif req.GetParent() == \"\" {","\t\treturn nil, status.Error(codes.InvalidArgument, \"parent missing\")","\t}","","\tparent, resultName, err := result.ParseName(req.GetParent())","\tif err != nil {","\t\treturn nil, err","\t}","","\tif err := s.auth.Check(ctx, parent, auth.ResourceRecords, auth.PermissionGet); err != nil {","\t\treturn nil, err","\t}","","\trecordAggregator, err := lister.OfRecordList(s.recordsEnv, parent, resultName, req)","\tif err != nil {","\t\treturn nil, err","\t}","","\tagg, err := recordAggregator.Aggregate(ctx, s.db)","\tif err != nil {","\t\treturn nil, err","\t}","","\treturn agg, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,1,1,1,1,0,1,1,1,0,1,1,1,1,0,1,1,1,1,0,1,0]},{"id":46,"path":"pkg/apis/config/metrics.go","lines":["// Package config provides configuration types for Results APIs.","package config","","import (","\tcorev1 \"k8s.io/api/core/v1\"","\t\"knative.dev/pkg/metrics\"",")","","const (","\t// metricsTaskrunLevel determines to what level to aggregate metrics","\t// for taskrun","\tmetricsTaskrunLevelKey = \"metrics.taskrun.level\"","","\t// metricsPipelinerunLevel determines to what level to aggregate metrics","\t// for pipelinerun","\tmetricsPipelinerunLevelKey = \"metrics.pipelinerun.level\"","\t// metricsDurationTaskrunType determines what type of","\t// metrics to use for aggregating duration for taskrun","\tmetricsDurationTaskrunType = \"metrics.taskrun.duration-type\"","\t// metricsDurationPipelinerunType determines what type of","\t// metrics to use for aggregating duration for pipelinerun","\tmetricsDurationPipelinerunType = \"metrics.pipelinerun.duration-type\"","","\t// DefaultTaskrunLevel determines to what level to aggregate metrics","\t// when it isn't specified in configmap","\tDefaultTaskrunLevel = TaskrunLevelAtTask","\t// TaskrunLevelAtTask specify that aggregation will be done at task level","\tTaskrunLevelAtTask = \"task\"","\t// TaskrunLevelAtNS specify that aggregation will be done at namespace level","\tTaskrunLevelAtNS = \"namespace\"","\t// DefaultPipelinerunLevel determines to what level to aggregate metrics","\t// when it isn't specified in configmap","\tDefaultPipelinerunLevel = PipelinerunLevelAtPipeline","\t// PipelinerunLevelAtPipeline specify that aggregation will be done at","\t// pipeline level","\tPipelinerunLevelAtPipeline = \"pipeline\"","\t// PipelinerunLevelAtNS specify that aggregation will be done at","\t// namespace level","\tPipelinerunLevelAtNS = \"namespace\"","","\t// DefaultDurationTaskrunType determines what type","\t// of metrics to use when we don't specify one in","\t// configmap","\tDefaultDurationTaskrunType = \"histogram\"","\t// DurationTaskrunTypeHistogram specify that histogram","\t// type metrics need to be used for Duration of Taskrun","\tDurationTaskrunTypeHistogram = \"histogram\"","\t// DurationTaskrunTypeLastValue specify that lastValue or","\t// gauge type metrics need to be used for Duration of Taskrun","\tDurationTaskrunTypeLastValue = \"lastvalue\"","","\t// DefaultDurationPipelinerunType determines what type","\t// of metrics to use when we don't specify one in","\t// configmap","\tDefaultDurationPipelinerunType = \"histogram\"","\t// DurationPipelinerunTypeHistogram specify that histogram","\t// type metrics need to be used for Duration of Pipelinerun","\tDurationPipelinerunTypeHistogram = \"histogram\"","\t// DurationPipelinerunTypeLastValue specify that lastValue or","\t// gauge type metrics need to be used for Duration of Pipelinerun","\tDurationPipelinerunTypeLastValue = \"lastvalue\"",")","","// Metrics holds the configurations for the metrics","type Metrics struct {","\tTaskrunLevel            string","\tPipelinerunLevel        string","\tDurationTaskrunType     string","\tDurationPipelinerunType string","}","","// DeepCopy copying the receiver, creating a new Metrics.","// deepcopy-gen hasn't been introduced in results repo, so handcraft here for now","func (cfg *Metrics) DeepCopy() *Metrics {","\treturn \u0026Metrics{","\t\tTaskrunLevel:            cfg.TaskrunLevel,","\t\tPipelinerunLevel:        cfg.PipelinerunLevel,","\t\tDurationTaskrunType:     cfg.DurationTaskrunType,","\t\tDurationPipelinerunType: cfg.DurationPipelinerunType,","\t}","}","","// GetMetricsConfigName returns the name of the configmap containing all","// customizations for the storage bucket.","func GetMetricsConfigName() string {","\treturn metrics.ConfigMapName()","}","","// Equals returns true if two Configs are identical","func (cfg *Metrics) Equals(other *Metrics) bool {","\tif cfg == nil \u0026\u0026 other == nil {","\t\treturn true","\t}","","\tif cfg == nil || other == nil {","\t\treturn false","\t}","","\treturn other.TaskrunLevel == cfg.TaskrunLevel \u0026\u0026","\t\tother.PipelinerunLevel == cfg.PipelinerunLevel \u0026\u0026","\t\tother.DurationTaskrunType == cfg.DurationTaskrunType \u0026\u0026","\t\tother.DurationPipelinerunType == cfg.DurationPipelinerunType","}","","// newMetricsFromMap returns a Config given a map corresponding to a ConfigMap","func newMetricsFromMap(cfgMap map[string]string) (*Metrics, error) {","\ttc := Metrics{","\t\tTaskrunLevel:            DefaultTaskrunLevel,","\t\tPipelinerunLevel:        DefaultPipelinerunLevel,","\t\tDurationTaskrunType:     DefaultDurationTaskrunType,","\t\tDurationPipelinerunType: DefaultDurationPipelinerunType,","\t}","","\tif taskrunLevel, ok := cfgMap[metricsTaskrunLevelKey]; ok {","\t\ttc.TaskrunLevel = taskrunLevel","\t}","","\tif pipelinerunLevel, ok := cfgMap[metricsPipelinerunLevelKey]; ok {","\t\ttc.PipelinerunLevel = pipelinerunLevel","\t}","\tif durationTaskrun, ok := cfgMap[metricsDurationTaskrunType]; ok {","\t\ttc.DurationTaskrunType = durationTaskrun","\t}","\tif durationPipelinerun, ok := cfgMap[metricsDurationPipelinerunType]; ok {","\t\ttc.DurationPipelinerunType = durationPipelinerun","\t}","\treturn \u0026tc, nil","}","","// NewMetricsFromConfigMap returns a Config for the given configmap","func NewMetricsFromConfigMap(config *corev1.ConfigMap) (*Metrics, error) {","\treturn newMetricsFromMap(config.Data)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,0,0,0,2,2,2,0,0,1,1,1,1,0,1,1,1,0,1,1,1,1,0,0,0,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2]},{"id":47,"path":"pkg/apis/config/retention.go","lines":["package config","","import (","\t\"fmt\"","\t\"log\"","\t\"os\"","\t\"strconv\"","\t\"strings\"","\t\"time\"","","\t\"gopkg.in/yaml.v2\"","\tcorev1 \"k8s.io/api/core/v1\"",")","","const (","\trunAt            = \"runAt\"","\tmaxRetention     = \"maxRetention\"","\tdefaultRetention = \"defaultRetention\"","\tpoliciesKey      = \"policies\"","\tretentionCMName  = \"tekton-results-config-results-retention-policy\"","\t// DefaultRunAt is the default value for RunAt","\tDefaultRunAt = \"7 7 * * 7\"","\t// DefaultDefaultRetention is the default value for DefaultRetention","\tDefaultDefaultRetention = time.Hour * 24 * 30",")","","// ParseDuration parses a string into a time.Duration.","// It handles standard formats like \"24h\", \"90m\", as well as the \"d\" suffix for days.","// If no unit is specified, it defaults to days.","func ParseDuration(durationStr string) (time.Duration, error) {","\tif strings.HasSuffix(durationStr, \"d\") {","\t\tdays, err := strconv.Atoi(strings.TrimSuffix(durationStr, \"d\"))","\t\tif err != nil {","\t\t\treturn 0, err","\t\t}","\t\treturn time.Duration(days) * 24 * time.Hour, nil","\t}","\tif days, err := strconv.Atoi(durationStr); err == nil {","\t\treturn time.Duration(days) * 24 * time.Hour, nil","\t}","\treturn time.ParseDuration(durationStr)","}","","// Policy defines a single retention policy rule.","type Policy struct {","\tName      string   `yaml:\"name\"`","\tSelector  Selector `yaml:\"selector\"`","\tRetention string   `yaml:\"retention\"`","}","","// Selector defines the selection criteria for a policy.","type Selector struct {","\tMatchNamespaces  []string            `yaml:\"matchNamespaces\"`","\tMatchLabels      map[string][]string `yaml:\"matchLabels\"`","\tMatchAnnotations map[string][]string `yaml:\"matchAnnotations\"`","\tMatchStatuses    []string            `yaml:\"matchStatuses\"`","}","","// RetentionPolicy holds the configurations for the Retention Policy of the DB","type RetentionPolicy struct {","\tRunAt            string","\tDefaultRetention time.Duration","\tPolicies         []Policy","}","","// DeepCopy copying the receiver, creating a new RetentionPolicy.","// deepcopy-gen hasn't been introduced in results repo, so handcraft here for now","func (cfg *RetentionPolicy) DeepCopy() *RetentionPolicy {","\tif cfg == nil {","\t\treturn nil","\t}","\tnewCfg := \u0026RetentionPolicy{","\t\tRunAt:            cfg.RunAt,","\t\tDefaultRetention: cfg.DefaultRetention,","\t}","\tif cfg.Policies != nil {","\t\tnewCfg.Policies = make([]Policy, len(cfg.Policies))","\t\tfor i, p := range cfg.Policies {","\t\t\tnewCfg.Policies[i] = *p.DeepCopy()","\t\t}","\t}","\treturn newCfg","}","","// DeepCopy returns a deep copy of the Policy.","func (p *Policy) DeepCopy() *Policy {","\tif p == nil {","\t\treturn nil","\t}","\treturn \u0026Policy{","\t\tName:      p.Name,","\t\tSelector:  *p.Selector.DeepCopy(),","\t\tRetention: p.Retention,","\t}","}","","// DeepCopy returns a deep copy of the Selector.","func (s *Selector) DeepCopy() *Selector {","\tif s == nil {","\t\treturn nil","\t}","\tout := \u0026Selector{}","\tif s.MatchNamespaces != nil {","\t\tout.MatchNamespaces = append([]string(nil), s.MatchNamespaces...)","\t}","\tif s.MatchStatuses != nil {","\t\tout.MatchStatuses = append([]string(nil), s.MatchStatuses...)","\t}","\tif s.MatchLabels != nil {","\t\tout.MatchLabels = make(map[string][]string, len(s.MatchLabels))","\t\tfor k, v := range s.MatchLabels {","\t\t\tout.MatchLabels[k] = append([]string(nil), v...)","\t\t}","\t}","\tif s.MatchAnnotations != nil {","\t\tout.MatchAnnotations = make(map[string][]string, len(s.MatchAnnotations))","\t\tfor k, v := range s.MatchAnnotations {","\t\t\tout.MatchAnnotations[k] = append([]string(nil), v...)","\t\t}","\t}","\treturn out","}","","// Equals returns true if two Configs are identical","func (cfg *RetentionPolicy) Equals(other *RetentionPolicy) bool {","\tif cfg == nil \u0026\u0026 other == nil {","\t\treturn true","\t}","","\tif cfg == nil || other == nil {","\t\treturn false","\t}","","\treturn other.RunAt == cfg.RunAt \u0026\u0026","\t\tother.DefaultRetention == cfg.DefaultRetention","}","","func newRetentionPolicyFromMap(cfgMap map[string]string) (*RetentionPolicy, error) {","\trp := RetentionPolicy{","\t\tRunAt:            DefaultRunAt,","\t\tDefaultRetention: DefaultDefaultRetention,","\t}","","\tif schedule, ok := cfgMap[runAt]; ok {","\t\trp.RunAt = schedule","\t}","","\tif duration, ok := cfgMap[maxRetention]; ok {","\t\tlog.Println(\"WARNING: configuration key 'maxRetention' is deprecated and will be removed in a future release. See migration guide: https://github.com/tektoncd/results/blob/main/docs/retention-policy-agent/README.md#migrating-from-maxretention-to-defaultretention\")","\t\tv, err := ParseDuration(duration)","\t\tif err != nil {","\t\t\treturn nil, fmt.Errorf(\"incorrect configuration for maxRetention: %w\", err)","\t\t}","\t\trp.DefaultRetention = v","\t} else if duration, ok := cfgMap[defaultRetention]; ok {","\t\tv, err := ParseDuration(duration)","\t\tif err != nil {","\t\t\treturn nil, fmt.Errorf(\"incorrect configuration for defaultRetention: %w\", err)","\t\t}","\t\trp.DefaultRetention = v","\t}","","\tif policiesYAML, ok := cfgMap[policiesKey]; ok {","\t\tvar policies []Policy","\t\tif err := yaml.Unmarshal([]byte(policiesYAML), \u0026policies); err != nil {","\t\t\treturn nil, fmt.Errorf(\"failed to unmarshal policies: %w\", err)","\t\t}","\t\trp.Policies = policies","\t}","","\treturn \u0026rp, nil","}","","// NewRetentionPolicyFromConfigMap returns a Config for the given configmap","func NewRetentionPolicyFromConfigMap(config *corev1.ConfigMap) (*RetentionPolicy, error) {","\treturn newRetentionPolicyFromMap(config.Data)","}","","// GetRetentionPolicyConfigName returns the name of the configmap containing","// retention policy.","func GetRetentionPolicyConfigName() string {","\tif e := os.Getenv(\"CONFIG_RETENTION_POLICY_NAME\"); e != \"\" {","\t\treturn e","\t}","\treturn retentionCMName","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,1,1,2,0,2,2,2,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,0,1,0,0,0,1,1,1,1,0,1,1,1,0,1,1,0,0,2,2,2,2,2,2,2,1,1,0,2,2,2,2,1,1,2,2,2,2,1,1,2,0,0,2,2,2,2,2,2,0,0,2,0,0,0,2,2,2,0,0,0,2,2,1,1,2,0]},{"id":48,"path":"pkg/apis/config/store.go","lines":["package config","","import (","\t\"context\"","","\t\"knative.dev/pkg/configmap\"",")","","type cfgKey struct{}","","// Config holds the collection of configurations that we attach to contexts.","type Config struct {","\tMetrics *Metrics","}","","// FromContext extracts a Config from the provided context.","func FromContext(ctx context.Context) *Config {","\tx, ok := ctx.Value(cfgKey{}).(*Config)","\tif ok {","\t\treturn x","\t}","\treturn nil","}","","// ToContext attaches the provided Config to the provided context, returning the","// new context with the Config attached.","func ToContext(ctx context.Context, c *Config) context.Context {","\treturn context.WithValue(ctx, cfgKey{}, c)","}","","// Store is a typed wrapper around configmap.Untyped store to handle our configmaps.","type Store struct {","\t*configmap.UntypedStore","}","","// NewStore creates a new store of Configs and optionally calls functions when ConfigMaps are updated.","func NewStore(logger configmap.Logger, onAfterStore ...func(name string, value any)) *Store {","\tstore := \u0026Store{","\t\tUntypedStore: configmap.NewUntypedStore(","\t\t\t\"results\",","\t\t\tlogger,","\t\t\tconfigmap.Constructors{","\t\t\t\tGetMetricsConfigName():         NewMetricsFromConfigMap,","\t\t\t\tGetRetentionPolicyConfigName(): NewRetentionPolicyFromConfigMap,","\t\t\t},","\t\t\tonAfterStore...,","\t\t),","\t}","","\treturn store","}","","// ToContext attaches the current Config state to the provided context.","func (s *Store) ToContext(ctx context.Context) context.Context {","\treturn ToContext(ctx, s.Load())","}","","// Load creates a Config from the current config state of the Store.","func (s *Store) Load() *Config {","\tmetrics := s.UntypedLoad(GetMetricsConfigName())","\tif metrics == nil {","\t\tmetrics, _ = newMetricsFromMap(map[string]string{})","\t}","\treturn \u0026Config{","\t\tMetrics: metrics.(*Metrics).DeepCopy(),","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,1,0,0,0,0,2,2,2,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,0,0,2,2,2,2,2,2,2,2,0]},{"id":49,"path":"pkg/apis/v1alpha3/types.go","lines":["// Package v1alpha3 provides v1alpha3 API types for Tekton Results.","package v1alpha3","","import (","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/apimachinery/pkg/types\"",")","","// EventListRecordType represents the API resource type for EventSet records.","const EventListRecordType = \"results.tekton.dev/v1.EventList\"","","// LogRecordType represents the API resource type for Tekton log records.","const LogRecordType = \"results.tekton.dev/v1alpha3.Log\"","","// LogRecordTypeV2 represents the API resource type for Tekton log records deprecated now.","const LogRecordTypeV2 = \"results.tekton.dev/v1alpha2.Log\"","","// Log represents the API resource for Tekton results Log.","type Log struct {","\tmetav1.TypeMeta   `json:\",inline\"`","\tmetav1.ObjectMeta `json:\"metadata,omitempty\"`","","\tSpec   LogSpec   `json:\"spec\"`","\tStatus LogStatus `json:\"status,omitempty\"`","}","","// LogSpec represents the specification for the Tekton Log resource.","// It contains information about corresponding log resource.","type LogSpec struct {","\tResource Resource `json:\"resource\"`","\tType     LogType  `json:\"type\"`","}","","// Resource represents information to identify a Kubernetes API resource.","// It should be used to match the corresponding log to this resource.","type Resource struct {","\tKind      string    `json:\"kind,omitempty\"`","\tNamespace string    `json:\"namespace\"`","\tName      string    `json:\"name\"`","\tUID       types.UID `json:\"uid,omitempty\"`","}","","// LogType represents the log storage type.","// This information is useful to determine how the resource will be stored.","type LogType string","","const (","\t// FileLogType defines the log type for logs stored in the file system.","\tFileLogType LogType = \"File\"","","\t// S3LogType defines the log type for logs stored in the S3 object storage or S3 compatible alternatives.","\tS3LogType LogType = \"S3\"","","\t// GCSLogType defines the log type for logs stored in the GCS object storage or GCS compatible alternatives.","\tGCSLogType LogType = \"GCS\"","","\t// LokiLogType defines the log type for logs stored in the Loki.","\tLokiLogType LogType = \"loki\"","","\t// BlobLogType defines the log type for logs stored in the Blob - GCS, S3 compatible storage.","\tBlobLogType LogType = \"blob\"","","\t// SplunkLogType defines the log type for logs stored in the Splunk.","\tSplunkLogType LogType = \"splunk\"",")","","// LogStatus defines the current status of the log resource.","type LogStatus struct {","\tPath            string `json:\"path,omitempty\"`","\tSize            int64  `json:\"size\"`","\tIsStored        bool   `json:\"isStored\"`","\tErrorOnStoreMsg string `json:\"errorOnStoreMsg\"`","\tIsRetryableErr  bool   `json:\"isRetryableErr\"`","}","","// Default sets up default values for Log TypeMeta, such as API version and kind.","func (t *Log) Default() {","\tt.Kind = \"Log\"","\tt.APIVersion = \"results.tekton.dev/v1alpha3\"","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1]},{"id":50,"path":"pkg/cli/client/base.go","lines":["// Package client provides REST client functionality for the Results CLI.","package client","","import (","\t\"bytes\"","\t\"context\"","\t\"fmt\"","\t\"io\"","\t\"net/http\"","\t\"net/url\"","\t\"path\"","\t\"time\"","","\t\"k8s.io/client-go/transport\"","","\t\"google.golang.org/protobuf/encoding/protojson\"","\t\"google.golang.org/protobuf/proto\"",")","","// Error represents an error that occurred during a client operation","type Error struct {","\tMessage string","\tCode    int","}","","func (e *Error) Error() string {","\treturn fmt.Sprintf(\"client error: %s (code: %d)\", e.Message, e.Code)","}","","// NewError creates a new Error","func NewError(message string, code int) error {","\treturn \u0026Error{","\t\tMessage: message,","\t\tCode:    code,","\t}","}","","// Config for the HTTP client","type Config struct {","\tURL       *url.URL","\tTimeout   time.Duration","\tTransport *transport.Config","}","","// RESTClient handles HTTP communication with the server","type RESTClient struct {","\tbaseURL    *url.URL","\thttpClient *http.Client","}","","// NewRESTClient creates a new REST client.","func NewRESTClient(c *Config) (*RESTClient, error) {","\tif c == nil {","\t\treturn nil, fmt.Errorf(\"config cannot be nil\")","\t}","\tif c.URL == nil {","\t\treturn nil, fmt.Errorf(\"config.URL cannot be nil\")","\t}","\tif c.Transport == nil {","\t\treturn nil, fmt.Errorf(\"config.Transport cannot be nil\")","\t}","","\trt, err := transport.New(c.Transport)","\tif err != nil {","\t\treturn nil, err","\t}","","\treturn \u0026RESTClient{","\t\tbaseURL: c.URL,","\t\thttpClient: \u0026http.Client{","\t\t\tTransport: rt,","\t\t\tTimeout:   c.Timeout,","\t\t},","\t}, nil","}","","// DoRequest performs an HTTP request and handles the response","func (c *RESTClient) DoRequest(ctx context.Context, method, url string, in proto.Message) (*Response, error) {","\tvar body io.Reader","\tif in != nil {","\t\tdata, err := protojson.Marshal(in)","\t\tif err != nil {","\t\t\treturn nil, fmt.Errorf(\"failed to marshal request: %v\", err)","\t\t}","\t\tbody = bytes.NewReader(data)","\t}","","\treq, err := http.NewRequestWithContext(ctx, method, url, body)","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"failed to create request: %v\", err)","\t}","","\treq.Header.Set(\"Content-Type\", \"application/json\")","\treq.Header.Set(\"Accept\", \"application/json\")","","\tresp, err := c.httpClient.Do(req)","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"failed to send request: %v\", err)","\t}","\t// Workaround for golangci-lint returned value not checked complaint","\tdefer func() {","\t\t_ = resp.Body.Close()","\t}()","","\tif resp.StatusCode \u003e= 400 {","\t\tbody, _ := io.ReadAll(resp.Body)","\t\treturn nil, NewError(string(body), resp.StatusCode)","\t}","","\tdata, err := io.ReadAll(resp.Body)","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"failed to read response body: %v\", err)","\t}","","\treturn NewResponse(data), nil","}","","// BuildURL constructs a URL with the given path and query parameters","func (c *RESTClient) BuildURL(p string, params url.Values) string {","\tu := *c.baseURL","\tu.Path = path.Join(u.Path, p)","\tif params != nil {","\t\tu.RawQuery = params.Encode()","\t}","\treturn u.String()","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,1,1,2,2,2,0,2,2,1,1,0,2,2,2,2,2,2,2,0,0,0,2,2,2,1,1,1,1,1,0,0,2,2,1,1,0,2,2,2,2,2,1,1,0,2,2,2,0,2,1,1,1,0,2,2,1,1,0,2,0,0,0,2,2,2,2,2,2,2,0]},{"id":51,"path":"pkg/cli/client/logs/logs.go","lines":["// Package logs provides a client for interacting with Results logs.","package logs","","import (","\t\"context\"","\t\"fmt\"","\t\"io\"","\t\"net/http\"","\t\"strings\"","","\t\"github.com/tektoncd/results/pkg/cli/client\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"",")","","// Client is a client for interacting with logs.","type Client struct {","\t*client.RESTClient","}","","// NewClient creates a new logs client.","func NewClient(rc *client.RESTClient) *Client {","\treturn \u0026Client{RESTClient: rc}","}","","// GetLog gets a log by name.","func (c *Client) GetLog(ctx context.Context, req *pb.GetLogRequest) (io.Reader, error) {","\t// Create a pipe to stream the logs","\tpr, pw := io.Pipe()","","\t// Start a goroutine to handle the streaming response","\tgo func() {","\t\t// Workaround for golangci-lint returned value not checked complaint","\t\tdefer func() {","\t\t\t_ = pw.Close()","\t\t}()","","\t\t// Build the URL for the log request, replacing \"records\" with \"logs\" in the path","\t\turl := c.BuildURL(fmt.Sprintf(\"parents/%s\", strings.Replace(req.Name, \"records\", \"logs\", 1)), nil)","","\t\t// Make the request using the RESTClient's DoRequest method","\t\tresp, err := c.DoRequest(ctx, http.MethodGet, url, nil)","\t\tif err != nil {","\t\t\tpw.CloseWithError(fmt.Errorf(\"failed to get log: %v\", err))","\t\t\treturn","\t\t}","","\t\t// Write the log data to the pipe","\t\tif _, err := pw.Write(resp.Body()); err != nil {","\t\t\tpw.CloseWithError(fmt.Errorf(\"failed to write log data: %v\", err))","\t\t\treturn","\t\t}","\t}()","","\treturn pr, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,0,0,1,1,1,1,0,0,1,0]},{"id":52,"path":"pkg/cli/client/records/records.go","lines":["// Package records provides a client for interacting with Results records.","package records","","import (","\t\"context\"","\t\"fmt\"","\t\"net/http\"","\t\"net/url\"","","\t\"github.com/tektoncd/results/pkg/cli/client\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"",")","","// RecordClient defines the interface for record-related operations","type RecordClient interface {","\tListRecords(ctx context.Context, in *pb.ListRecordsRequest, fields string) (*pb.ListRecordsResponse, error)","}","","// recordClient implements the RecordClient interface","type recordClient struct {","\t*client.RESTClient","}","","// NewClient creates a new record client","func NewClient(rc *client.RESTClient) RecordClient {","\treturn \u0026recordClient{RESTClient: rc}","}","","// ListRecords makes request to get record list","func (c *recordClient) ListRecords(ctx context.Context, in *pb.ListRecordsRequest, fields string) (*pb.ListRecordsResponse, error) {","\tout := \u0026pb.ListRecordsResponse{}","","\t// Add query parameters","\tparams := url.Values{}","\tif in.Filter != \"\" {","\t\tparams.Set(\"filter\", in.Filter)","\t}","\tif in.OrderBy != \"\" {","\t\tparams.Set(\"order_by\", in.OrderBy)","\t}","\tif in.PageSize \u003e 0 {","\t\tparams.Set(\"page_size\", fmt.Sprintf(\"%d\", in.PageSize))","\t}","\tif in.PageToken != \"\" {","\t\tparams.Set(\"page_token\", in.PageToken)","\t}","","\t// Add fields parameter for partial response","\t// (Only add fields parameter if provided)","\tif fields != \"\" {","\t\tparams.Set(\"fields\", fields)","\t}","","\t// Construct the URL with parents prefix","\tbuildURL := c.BuildURL(fmt.Sprintf(\"parents/%s/records\", in.Parent), params)","","\t// Make the request","\tresp, err := c.DoRequest(ctx, http.MethodGet, buildURL, in)","","\tif err != nil {","\t\treturn out, err","\t}","","\t// Unmarshall the response","\terr = resp.ProtoUnmarshal(out)","\tif err != nil {","\t\treturn out, err","\t}","","\treturn out, err","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,2,2,2,2,2,2,1,1,2,1,1,2,2,2,2,1,1,0,0,0,2,1,1,0,0,2,2,2,2,2,2,2,2,0,0,2,2,1,1,0,2,0]},{"id":53,"path":"pkg/cli/client/response.go","lines":["package client","","import (","\t\"google.golang.org/protobuf/encoding/protojson\"","\t\"google.golang.org/protobuf/proto\"",")","","// Response represents a generic API response containing the raw body bytes.","type Response struct {","\tbody []byte","}","","// Body returns the raw response body as a byte slice.","func (r *Response) Body() []byte {","\treturn r.body","}","","// ProtoUnmarshal unmarshals the response body into the provided proto.Message.","func (r *Response) ProtoUnmarshal(out proto.Message) error {","\treturn protojson.Unmarshal(r.body, out)","}","","// NewResponse creates a new Response with the given body bytes.","func NewResponse(body []byte) *Response {","\treturn \u0026Response{","\t\tbody: body,","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,1,1,1,0,0,2,2,2,2,2]},{"id":54,"path":"pkg/cli/common/format.go","lines":["package common //nolint:revive // Package provides shared CLI utilities","","import (","\t\"fmt\"","\t\"time\"","","\t\"github.com/jonboulle/clockwork\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"",")","","// FormatAge returns a human-readable string representation of how long ago a timestamp occurred.","// The output format varies based on duration: seconds (\u003c1m), minutes (\u003c1h), hours (\u003c24h), or days.","func FormatAge(t *metav1.Time, c clockwork.Clock) string {","\tif t == nil {","\t\treturn \"\"","\t}","\tage := c.Since(t.Time)","\tif age \u003c time.Minute {","\t\treturn fmt.Sprintf(\"%ds ago\", int(age.Seconds()))","\t}","\tif age \u003c time.Hour {","\t\treturn fmt.Sprintf(\"%dm ago\", int(age.Minutes()))","\t}","\tif age \u003c 24*time.Hour {","\t\treturn fmt.Sprintf(\"%dh ago\", int(age.Hours()))","\t}","\treturn fmt.Sprintf(\"%dd ago\", int(age.Hours()/24))","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0]},{"id":55,"path":"pkg/cli/common/labels.go","lines":["package common","","import (","\t\"fmt\"","\t\"strings\"",")","","// ValidateLabels validates the format of the provided labels string.","// Labels should be in the format \"key=value\" or \"key=value,key2=value2\".","// Returns an error if the format is invalid.","func ValidateLabels(labels string) error {","\tlabelPairs := strings.Split(labels, \",\")","\tfor _, pair := range labelPairs {","\t\tparts := strings.Split(strings.TrimSpace(pair), \"=\")","\t\tif len(parts) != 2 {","\t\t\treturn fmt.Errorf(\"invalid label format: %s. Expected format: key=value\", pair)","\t\t}","","\t\t// Check for whitespace in key before trimming","\t\tif strings.ContainsAny(parts[0], \" \\t\") {","\t\t\treturn fmt.Errorf(\"label key cannot contain whitespace: %s\", parts[0])","\t\t}","","\t\tkey := strings.TrimSpace(parts[0])","\t\tvalue := strings.TrimSpace(parts[1])","","\t\tif key == \"\" {","\t\t\treturn fmt.Errorf(\"label key cannot be empty in pair: %s\", pair)","\t\t}","\t\tif value == \"\" {","\t\t\treturn fmt.Errorf(\"label value cannot be empty in pair: %s\", pair)","\t\t}","\t}","","\treturn nil","}","","// FilterOptions defines the interface for filter options","type FilterOptions interface {","\tGetLabel() string","\tGetResourceName() string","\tGetPipelineRun() string","\tGetResourceType() string","\tGetUID() string","}","","// BuildFilterString constructs the filter string for the ListRecordsRequest","func BuildFilterString(opts FilterOptions) string {","\tconst (","\t\tcontains = \"data.metadata.%s.contains(\\\"%s\\\")\"","\t\tequal    = \"data.metadata.%s[\\\"%s\\\"]==\\\"%s\\\"\"","\t\tdataType = \"data_type==\\\"%s\\\"\"","\t)","","\tvar filters []string","","\tswitch opts.GetResourceType() {","\tcase ResourceTypePipelineRun:","\t\t// Add data type filter for both v1 and v1beta1 PipelineRuns","\t\tfilters = append(filters, fmt.Sprintf(`(%s || %s)`,","\t\t\tfmt.Sprintf(dataType, \"tekton.dev/v1.PipelineRun\"),","\t\t\tfmt.Sprintf(dataType, \"tekton.dev/v1beta1.PipelineRun\")))","\tcase ResourceTypeTaskRun:","\t\t// Add data type filter for both v1 and v1beta1 TaskRuns","\t\tfilters = append(filters, fmt.Sprintf(`(%s || %s)`,","\t\t\tfmt.Sprintf(dataType, \"tekton.dev/v1.TaskRun\"),","\t\t\tfmt.Sprintf(dataType, \"tekton.dev/v1beta1.TaskRun\")))","\t}","","\t// Handle label filters","\tif opts.GetLabel() != \"\" {","\t\t// Split by comma to get individual label pairs","\t\tlabelPairs := strings.Split(opts.GetLabel(), \",\")","\t\tfor _, pair := range labelPairs {","\t\t\t// Split each pair by = to get key and value","\t\t\tparts := strings.Split(strings.TrimSpace(pair), \"=\")","\t\t\tif len(parts) == 2 {","\t\t\t\tkey := strings.TrimSpace(parts[0])","\t\t\t\tvalue := strings.TrimSpace(parts[1])","\t\t\t\tfilters = append(filters, fmt.Sprintf(equal, \"labels\", key, value))","\t\t\t}","\t\t}","\t}","","\t// Handle pipeline name filter","\tif opts.GetResourceName() != \"\" {","\t\tfilters = append(filters, fmt.Sprintf(contains, \"name\", opts.GetResourceName()))","\t}","","\t// Handle UID filter","\tif opts.GetUID() != \"\" {","\t\tfilters = append(filters, fmt.Sprintf(contains, \"uid\", opts.GetUID()))","\t}","","\t// Add PipelineRun filter if provided","\tif opts.GetPipelineRun() != \"\" {","\t\tfilters = append(filters, fmt.Sprintf(`data.metadata.labels['tekton.dev/pipelineRun'] == '%s'`, opts.GetPipelineRun()))","\t}","","\treturn strings.Join(filters, \" \u0026\u0026 \")","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,1,1,1,0,1,1,1,1,1,1,1,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,0,0,1,1,1,0,0,1,1,1,0,1,0]},{"id":56,"path":"pkg/cli/common/params.go","lines":["package common","","import (","\t\"github.com/tektoncd/results/pkg/cli/client\"","\t\"k8s.io/client-go/tools/clientcmd\"",")","","// ResultsParams holds configuration parameters for interacting with Kubernetes and API endpoints.","type ResultsParams struct {","\tkubeConfigPath string","\tkubeContext    string","\tnamespace      string","\thost           string","\ttoken          string","\tapiPath        string","\tskipTLSVerify  bool","","\t// Simple client storage","\trestClient *client.RESTClient","}","","var _ Params = (*ResultsParams)(nil)","","// KubeConfigPath returns the path to the Kubernetes configuration file.","func (p *ResultsParams) KubeConfigPath() string {","\treturn p.kubeConfigPath","}","","// KubeContext returns the Kubernetes context name.","func (p *ResultsParams) KubeContext() string {","\treturn p.kubeContext","}","","// SetKubeConfigPath sets the path to the Kubernetes configuration file.","//","// Parameters:","//   - path: The file path to the Kubernetes configuration.","func (p *ResultsParams) SetKubeConfigPath(path string) {","\tp.kubeConfigPath = path","}","","// SetKubeContext sets the Kubernetes context name.","//","// Parameters:","//   - context: The name of the Kubernetes context to use.","func (p *ResultsParams) SetKubeContext(context string) {","\tp.kubeContext = context","}","","// SetNamespace sets the Kubernetes namespace.","//","// Parameters:","//   - ns: The namespace to set. If empty, the default namespace from kubeconfig will be used.","func (p *ResultsParams) SetNamespace(ns string) {","\tif ns == \"\" {","\t\tloadingRules := clientcmd.NewDefaultClientConfigLoadingRules()","\t\tif p.kubeConfigPath != \"\" {","\t\t\tloadingRules.ExplicitPath = p.kubeConfigPath","\t\t}","\t\tconfigOverrides := \u0026clientcmd.ConfigOverrides{}","\t\tif p.kubeContext != \"\" {","\t\t\tconfigOverrides.CurrentContext = p.kubeContext","\t\t}","\t\tkubeConfig := clientcmd.NewNonInteractiveDeferredLoadingClientConfig(loadingRules, configOverrides)","\t\tnamespace, _, err := kubeConfig.Namespace()","\t\tif err == nil {","\t\t\tp.namespace = namespace","\t\t\treturn","\t\t}","\t}","\tp.namespace = ns","}","","// Namespace returns the current Kubernetes namespace.","func (p *ResultsParams) Namespace() string {","\treturn p.namespace","}","","// Host returns the API server host address.","func (p *ResultsParams) Host() string {","\treturn p.host","}","","// SetHost sets the API server host address.","//","// Parameters:","//   - host: The host address to set.","func (p *ResultsParams) SetHost(host string) {","\tp.host = host","}","","// Token returns the authentication token.","func (p *ResultsParams) Token() string {","\treturn p.token","}","","// SetToken sets the authentication token.","//","// Parameters:","//   - token: The authentication token to set.","func (p *ResultsParams) SetToken(token string) {","\tp.token = token","}","","// APIPath returns the API path prefix.","func (p *ResultsParams) APIPath() string {","\treturn p.apiPath","}","","// SetAPIPath sets the API path prefix.","//","// Parameters:","//   - path: The API path prefix to set.","func (p *ResultsParams) SetAPIPath(path string) {","\tp.apiPath = path","}","","// SkipTLSVerify returns whether TLS verification should be skipped.","func (p *ResultsParams) SkipTLSVerify() bool {","\treturn p.skipTLSVerify","}","","// SetSkipTLSVerify sets whether TLS verification should be skipped.","//","// Parameters:","//   - skip: Whether to skip TLS verification.","func (p *ResultsParams) SetSkipTLSVerify(skip bool) {","\tp.skipTLSVerify = skip","}","","// SetRESTClient injects a REST client","func (p *ResultsParams) SetRESTClient(client *client.RESTClient) {","\tp.restClient = client","}","","// RESTClient returns the injected REST client","func (p *ResultsParams) RESTClient() *client.RESTClient {","\treturn p.restClient","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,0,1,1,1,0,0,1,1,1,0,0,0,0,0,1,1,1,0,0,1,1,1,0,0,0,0,0,1,1,1,0,0,1,1,1,0,0,0,0,0,1,1,1,0,0,1,1,1,0,0,0,0,0,1,1,1,0,0,1,1,1,0,0,1,1,1]},{"id":57,"path":"pkg/cli/common/prerun/prerun.go","lines":["// Package prerun provides shared pre-run functions for CLI commands.","package prerun","","import (","\t\"fmt\"","","\t\"github.com/spf13/cobra\"","\t\"github.com/tektoncd/results/pkg/cli/client\"","\t\"github.com/tektoncd/results/pkg/cli/common\"","\t\"github.com/tektoncd/results/pkg/cli/config\"","\t\"github.com/tektoncd/results/pkg/cli/flags\"",")","","// PersistentPreRunE returns a function that can be used as a persistent pre-run","// function for Cobra commands. It initializes the provided parameters using","// the flags defined in the command.","//","// Parameters:","//   - p: A common.Params struct that will be initialized with values from command flags.","//","// Returns:","//   - A function that takes a *cobra.Command and a []string, and returns an error.","//     This function initializes the params using flags.InitParams and returns any error encountered.","func PersistentPreRunE(p common.Params) func(*cobra.Command, []string) error {","\treturn func(cmd *cobra.Command, _ []string) error {","\t\treturn flags.InitParams(p, cmd)","\t}","}","","// InitClient initializes the REST client for the command based on direct connection flags or kubeconfig.","//","// Parameters:","//   - p: common.Params containing configuration parameters.","//   - cmd: The cobra.Command being executed.","//","// Returns:","//   - *client.RESTClient: The initialized REST client.","//   - error: An error if client initialization fails.","func InitClient(p common.Params, cmd *cobra.Command) (*client.RESTClient, error) {","\t// Check if any of the direct connection flags are set","\t// if not fetch restConfig from k8s extension","\tif config.ServerConnectionFlagsChanged(cmd) {","\t\tcfg, err := config.BuildDirectClientConfig(p)","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\treturn client.NewRESTClient(cfg)","\t}","","\tc, err := config.NewConfig(p)","\tif err != nil {","\t\treturn nil, err","\t}","","\tif err := c.Validate(); err != nil {","\t\treturn nil, fmt.Errorf(\"invalid config. You may need to run the 'config set' command to configure the CLI: %w\", err)","\t}","\treturn client.NewRESTClient(c.Get())","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,0,1,1,1,1,0]},{"id":58,"path":"pkg/cli/common/utils.go","lines":["package common //nolint:revive // Package provides shared CLI utilities","","import (","\t\"errors\"","\t\"fmt\"","\t\"strings\"","","\t\"k8s.io/client-go/tools/clientcmd/api\"",")","","// BuildConfigContextInfo extracts cluster name, username, and constructs the config context name","// from a Kubernetes context.","//","// Parameters:","//   - context: The Kubernetes context object from kubeconfig","//","// Returns:","//   - configContextName: The config context name in format \"tekton-results-config/{cluster}/{user}\".","//   - clusterName: The cluster name from the context.","//   - userName: The extracted username (part before \"/\" if present).","//   - error: An error if the context is missing cluster/user information.","func BuildConfigContextInfo(context *api.Context) (configContextName, clusterName, userName string, err error) {","\tif context == nil {","\t\treturn \"\", \"\", \"\", errors.New(\"context is nil\")","\t}","","\tclusterName = context.Cluster","\tif clusterName == \"\" {","\t\treturn \"\", \"\", \"\", errors.New(\"no cluster specified in context\")","\t}","","\tuserName = context.AuthInfo","\tif userName == \"\" {","\t\treturn \"\", \"\", \"\", errors.New(\"no user specified in context\")","\t}","","\t// Extract just the username part before \"/\" for config context isolation","\t// In some cases user also has cluster name in the format \"user/cluster\"","\tif slashIndex := strings.Index(userName, \"/\"); slashIndex != -1 {","\t\tuserName = userName[:slashIndex]","\t}","","\t// Construct the config context name (tekton-results-config context for config storage)","\tconfigContextName = fmt.Sprintf(\"tekton-results-config/%s/%s\", clusterName, userName)","\treturn configContextName, clusterName, userName, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,1,1,1,1,0,1,1,1,1,0,0,0,1,1,1,0,0,1,1,0]},{"id":59,"path":"pkg/cli/config/config.go","lines":["// Package config provides configuration management for the Results CLI.","package config","","import (","\t\"encoding/json\"","\t\"errors\"","\t\"fmt\"","\t\"path\"","\t\"strconv\"","\t\"time\"","","\t\"github.com/tektoncd/results/pkg/cli/client\"","","\t\"github.com/AlecAivazis/survey/v2\"","\t\"github.com/spf13/cobra\"","\t\"github.com/tektoncd/results/pkg/cli/common\"","\t\"k8s.io/apimachinery/pkg/runtime\"","\t\"k8s.io/apimachinery/pkg/runtime/schema\"","\t\"k8s.io/client-go/rest\"","\t\"k8s.io/client-go/tools/clientcmd\"","\t\"k8s.io/client-go/tools/clientcmd/api\"",")","","// Constants defining various labels, names, and paths used in the Tekton Results configuration.","const (","\tExtensionName string = \"tekton-results\"","\tGroup         string = \"results.tekton.dev\"","\tVersion       string = \"v1alpha2\"","\tKind          string = \"Client\"","\tPath          string = \"apis\"",")","","// Config defines the interface for managing Tekton Results configuration.","type Config interface {","\tGet() *client.Config","\tGetObject() runtime.Object","\tSet(prompt bool, p common.Params) error","\tReset(p common.Params) error","\tValidate() error","}","","type config struct {","\tConfigAccess clientcmd.ConfigAccess","\tAPIConfig    *api.Config","\tRESTConfig   *rest.Config","\tClientConfig *client.Config","\tExtension    *Extension","}","","// NewConfig creates a new Config instance based on the provided parameters.","//","// It loads the kubeconfig, sets up the client configuration, and initializes","// the extension for Tekton Results.","//","// Parameters:","//   - p: common.Params containing configuration parameters such as kubeconfig path and context.","//","// Returns:","//   - Config: A new Config instance if successful.","//   - error: An error if any step in the configuration process fails.","func NewConfig(p common.Params) (Config, error) {","\tkubeconfigPath := clientcmd.RecommendedHomeFile","\tif p.KubeConfigPath() != \"\" {","\t\tkubeconfigPath = p.KubeConfigPath()","\t}","\t// Load kubeConfig","\tcc := getRawKubeConfigLoader(kubeconfigPath)","\tca := cc.ConfigAccess()","\tac, err := cc.RawConfig()","\tif err != nil {","\t\treturn nil, err","\t}","","\t// Get the desired context from user input","\tctx := p.KubeContext()","\tif ctx == \"\" {","\t\t// If no context is provided, use the current default context","\t\tctx = ac.CurrentContext","\t}","","\t// Validate if the specified context exists","\tif _, exists := ac.Contexts[ctx]; !exists {","\t\treturn nil, fmt.Errorf(\"context '%s' not found in kubeconfig\", ctx)","\t}","","\t// Create a REST config using the specified context","\toverriddenConfig := clientcmd.NewNonInteractiveClientConfig(ac, ctx, \u0026clientcmd.ConfigOverrides{}, ca)","\trc, err := overriddenConfig.ClientConfig()","\tif err != nil {","\t\treturn nil, err","\t}","","\tc := \u0026config{","\t\tConfigAccess: ca,","\t\tAPIConfig:    \u0026ac,","\t\tRESTConfig:   rc,","\t}","\tif err := c.LoadExtension(p); err != nil {","\t\treturn nil, err","\t}","","\treturn c, c.LoadClientConfig()","}","","// LoadClientConfig loads and configures the client configuration based on the current config state.","// It sets up the REST client configuration, including the GroupVersion, Host, APIPath, and authentication details.","// The function also configures TLS settings and timeout, and creates a common.Config with transport and URL information.","//","// Returns:","//   - error: An error if any step in the configuration process fails, nil otherwise.","func (c *config) LoadClientConfig() error {","\trc := rest.CopyConfig(c.RESTConfig)","","\tgv := c.Extension.TypeMeta.GroupVersionKind().GroupVersion()","\trc.GroupVersion = \u0026gv","","\tif c.Extension.Host != \"\" {","\t\trc.Host = c.Extension.Host","\t}","","\tif c.Extension.APIPath != \"\" {","\t\trc.APIPath = c.Extension.APIPath","\t}","","\tif c.Extension.Token != \"\" {","\t\trc.BearerToken = c.Extension.Token","\t}","\tif i, err := strconv.ParseBool(c.Extension.InsecureSkipTLSVerify); err == nil {","\t\tif i {","\t\t\trc.TLSClientConfig = rest.TLSClientConfig{}","\t\t}","\t\trc.Insecure = i","\t}","","\tif d, err := time.ParseDuration(c.Extension.Timeout); err != nil {","\t\trc.Timeout = d","\t}","","\ttc, err := rc.TransportConfig()","\tif err != nil {","\t\treturn err","\t}","","\trc.APIPath = path.Join(rc.APIPath, Path)","\tu, p, err := rest.DefaultServerUrlFor(rc)","\tif err != nil {","\t\treturn err","\t}","\tu.Path = p","","\tc.ClientConfig = \u0026client.Config{","\t\tTransport: tc,","\t\tURL:       u,","\t\tTimeout:   c.RESTConfig.Timeout,","\t}","","\treturn nil","}","","func (c *config) SetVersion() {","\tc.Extension.SetGroupVersionKind(schema.GroupVersionKind{","\t\tGroup:   Group,","\t\tVersion: Version,","\t\tKind:    Kind,","\t})","}","","// GetObject returns the runtime object representation of the configuration.","func (c *config) GetObject() runtime.Object {","\treturn c.Extension","}","","// Get retrieves the current common configuration.","func (c *config) Get() *client.Config {","\treturn c.ClientConfig","}","","func (c *config) Persist(p common.Params) error {","\t// Get the config context info for storing configuration","\tconfigContextName, clusterName, userName, err := c.getConfigContextInfo(p)","\tif err != nil {","\t\treturn err","\t}","","\t// Look for existing config context or create it","\tconfigContext, exists := c.APIConfig.Contexts[configContextName]","\tif !exists {","\t\tconfigContext = \u0026api.Context{","\t\t\tCluster:    clusterName,","\t\t\tAuthInfo:   userName,","\t\t\tNamespace:  \"default\",","\t\t\tExtensions: make(map[string]runtime.Object), // Initialize extensions","\t\t}","\t\tc.APIConfig.Contexts[configContextName] = configContext","\t}","","\t// Ensure Extensions map is initialized even for existing contexts","\tif configContext.Extensions == nil {","\t\tconfigContext.Extensions = make(map[string]runtime.Object)","\t}","","\t// Store/update extension in the config context","\textensionData, err := json.Marshal(c.Extension)","\tif err != nil {","\t\treturn fmt.Errorf(\"failed to marshal extension: %w\", err)","\t}","","\tconfigContext.Extensions[ExtensionName] = \u0026runtime.Unknown{","\t\tTypeMeta: c.Extension.TypeMeta,","\t\tRaw:      extensionData,","\t}","","\treturn clientcmd.ModifyConfig(c.ConfigAccess, *c.APIConfig, false)","}","","// getConfigContextInfo extracts cluster and user information from the current context and constructs","// the config context name for storing Tekton Results configuration.","//","// Parameters:","//   - p: common.Params containing configuration parameters, including the KubeContext.","//","// Returns:","//   - configContextName: The config context name in format \"tekton-results-config/{cluster}/{user}\".","//   - clusterName: The cluster name from the current context.","//   - userName: The username from the current context.","//   - error: An error if the current context is not set or missing cluster/user information.","func (c *config) getConfigContextInfo(p common.Params) (configContextName, clusterName, userName string, err error) {","\tctx := c.APIConfig.CurrentContext","\tif p.KubeContext() != \"\" {","\t\tctx = p.KubeContext()","\t}","","\t// Get the context to extract cluster and user info","\tcontext := c.APIConfig.Contexts[ctx]","\tif context == nil {","\t\treturn \"\", \"\", \"\", errors.New(\"current context is not set in kubeconfig\")","\t}","","\treturn common.BuildConfigContextInfo(context)","}","","// Set configures the Extension settings for the config object.","// It either prompts the user for input or uses provided parameters to set the values.","//","// Parameters:","//   - prompt: A boolean flag indicating whether to prompt the user for input.","//   - p: A common.Params object containing configuration parameters.","//","// Returns:","//   - error: An error if any step in the configuration process fails, nil otherwise.","func (c *config) Set(prompt bool, p common.Params) error {","\t// get data from prompt in enabled","\tif prompt {","\t\thost := c.Host()","\t\tif err := c.Prompt(\"Host\", \u0026c.Extension.Host, host); err != nil {","\t\t\treturn err","\t\t}","","\t\ttoken := c.Token()","\t\tif err, ok := token.(error); ok {","\t\t\treturn fmt.Errorf(\"failed to get token: %w\", err)","\t\t}","\t\tif err := c.Prompt(\"Token\", \u0026c.Extension.Token, token); err != nil {","\t\t\treturn err","\t\t}","","\t\tif err := c.Prompt(\"API Path\", \u0026c.Extension.APIPath, \"\"); err != nil {","\t\t\treturn err","\t\t}","\t\tif err := c.Prompt(\"Insecure Skip TLS Verify\", \u0026c.Extension.InsecureSkipTLSVerify, []string{\"false\", \"true\"}); err != nil {","\t\t\treturn err","\t\t}","\t} else {","\t\tif p.Host() != \"\" {","\t\t\tc.Extension.Host = p.Host()","\t\t}","\t\tif p.Token() != \"\" {","\t\t\tc.Extension.Token = p.Token()","\t\t}","\t\tif p.APIPath() != \"\" {","\t\t\tc.Extension.APIPath = p.APIPath()","\t\t}","\t\tif p.SkipTLSVerify() {","\t\t\tc.Extension.InsecureSkipTLSVerify = strconv.FormatBool(p.SkipTLSVerify())","\t\t}","\t}","","\treturn c.Persist(p)","}","","// Reset resets the Tekton Results extension configuration to its default state.//+","//","// Parameters:","//   - p: A common.Params object containing configuration parameters.","//","// Returns an error if the reset process fails, nil otherwise.","func (c *config) Reset(p common.Params) error {","\tc.Extension = new(Extension)","\tc.SetVersion()","\treturn c.Persist(p)","}","","func (c *config) Prompt(name string, value *string, data any) error {","\tvar p survey.Prompt","","\tm := name + \" : \"","","\tswitch d := data.(type) {","\tcase string:","\t\tp = \u0026survey.Input{","\t\t\tMessage: m,","\t\t\tDefault: d,","\t\t}","\tcase []string:","\t\tp = \u0026survey.Select{","\t\t\tMessage: m,","\t\t\tOptions: d,","\t\t}","\tdefault:","\t\tp = \u0026survey.Input{","\t\t\tMessage: m,","\t\t}","\t}","","\treturn survey.AskOne(p, value)","}","","// LoadExtension loads the Tekton Results extension configuration from the kubeconfig.","// It loads from a dedicated \"tekton-results-config\" context to ensure configuration","// persists regardless of current namespace context changes (e.g., 'oc project').","//","// Parameters:","//   - p: common.Params containing configuration parameters, including the KubeContext.","//","// Returns:","//   - error: An error if the current context is not set or if there's an issue unmarshaling the extension data.","func (c *config) LoadExtension(p common.Params) error {","\t// Get the config context info for loading configuration","\tconfigContextName, _, _, err := c.getConfigContextInfo(p)","\tif err != nil {","\t\treturn err","\t}","","\t// Check if config context exists","\tif configContext, exists := c.APIConfig.Contexts[configContextName]; exists {","\t\tif configContext.Extensions != nil {","\t\t\tif ext := configContext.Extensions[ExtensionName]; ext != nil {","\t\t\t\t// Load existing extension","\t\t\t\tc.Extension = new(Extension)","\t\t\t\tif err := json.Unmarshal(ext.(*runtime.Unknown).Raw, c.Extension); err != nil {","\t\t\t\t\treturn fmt.Errorf(\"failed to unmarshal extension: %w\", err)","\t\t\t\t}","\t\t\t\treturn nil","\t\t\t}","\t\t}","\t}","","\t// No config context or no extension found - create empty extension","\tc.Extension = new(Extension)","\tc.SetVersion()","\treturn nil","}","","// Host retrieves the host URL for the Tekton Results API based on external access detection.","// It automatically detects the platform and tries to find a healthy tekton-results-api-service.","//","// Returns:","//   - string: The detected host URL if successful, or empty string if detection fails.","func (c *config) Host() string {","\turl, err := getHostURL(c.RESTConfig)","\tif err != nil {","\t\treturn \"\" // Return empty string so user can enter manually","\t}","\treturn url // Return the detected URL as default","}","","// Token returns the bearer token from the REST configuration.","// It returns an error if the REST configuration is not properly initialized.","//","// Returns:","//   - any: The bearer token string if successful, or an error if the configuration is invalid.","func (c *config) Token() any {","\tif c.RESTConfig == nil {","\t\treturn fmt.Errorf(\"REST configuration is not initialized\")","\t}","\treturn c.RESTConfig.BearerToken","}","","// getRawKubeConfigLoader creates and returns a clientcmd.ClientConfig based on the provided kubeconfig path.","// This function is equivalent to ToRawKubeConfigLoader() and is used to load the kubeconfig file.","//","// Parameters:","//   - kubeconfigPath: A string representing the path to the kubeconfig file.","//","// Returns:","//   - clientcmd.ClientConfig: A non-interactive deferred loading client configuration","//     that uses the specified kubeconfig path and default overrides.","func getRawKubeConfigLoader(kubeconfigPath string) clientcmd.ClientConfig {","\t// Set explicit path for kubeconfig","\tloadingRules := \u0026clientcmd.ClientConfigLoadingRules{ExplicitPath: kubeconfigPath}","\tconfigOverrides := \u0026clientcmd.ConfigOverrides{}","","\t// Return the clientcmd.ClientConfig (equivalent to ToRawKubeConfigLoader)","\treturn clientcmd.NewNonInteractiveDeferredLoadingClientConfig(loadingRules, configOverrides)","}","","// ServerConnectionFlagsChanged returns true if any server connection flags are set.","func ServerConnectionFlagsChanged(cmd *cobra.Command) bool {","\treturn cmd.Flags().Changed(\"host\") ||","\t\tcmd.Flags().Changed(\"token\") ||","\t\tcmd.Flags().Changed(\"insecure-skip-tls-verify\") ||","\t\tcmd.Flags().Changed(\"api-path\")","}","","// BuildDirectClientConfig builds a client.Config from CLI flags (host, token, api-path, insecure-skip-tls-verify).","func BuildDirectClientConfig(p common.Params) (*client.Config, error) {","\thost := p.Host()","\ttoken := p.Token()","\tif host == \"\" || token == \"\" {","\t\treturn nil, errors.New(\"--host and --token flag must be set if using direct connection flags\")","\t}","\trc := \u0026rest.Config{","\t\tHost:        host,","\t\tBearerToken: token,","\t}","\tif p.APIPath() != \"\" {","\t\trc.APIPath = p.APIPath()","\t}","","\trc.Insecure = p.SkipTLSVerify()","\t// Optionally set timeout (default 60s)","\trc.Timeout = 60 * time.Second","","\trc.APIPath = path.Join(rc.APIPath, Path)","","\trc.GroupVersion = \u0026schema.GroupVersion{","\t\tGroup:   Group,","\t\tVersion: Version,","\t}","\tu, pth, err := rest.DefaultServerUrlFor(rc)","\tif err != nil {","\t\treturn nil, err","\t}","\tu.Path = pth","","\ttcfg, err := rc.TransportConfig()","\tif err != nil {","\t\treturn nil, err","\t}","","\treturn \u0026client.Config{","\t\tURL:       u,","\t\tTimeout:   rc.Timeout,","\t\tTransport: tcfg,","\t}, nil","}","","// Validate validates the configuration of the client.","// It checks if the client configuration and extension are properly set up.","//","// Parameters:","//   - c: A Config interface containing the client configuration and extension.","//","// Returns:","//   - error: An error if the configuration is invalid, nil otherwise.","func (c *config) Validate() error {","\t// Check if the configuration is properly set up","\tclientConfig := c.Get()","\tif clientConfig == nil || clientConfig.URL == nil {","\t\treturn fmt.Errorf(\"client configuration missing: URL not set\")","\t}","","\t// Check if essential configuration values are missing","\textensionObj := c.GetObject()","\textension, ok := extensionObj.(*Extension)","\tif !ok {","\t\treturn fmt.Errorf(\"invalid extension type: expected *Extension, got %T\", extensionObj)","\t}","","\tif extension.Host == \"\" {","\t\treturn fmt.Errorf(\"API server host not configured: host field is empty\")","\t}","","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,2,2,2,2,2,2,0,0,2,2,2,2,2,0,0,2,2,2,0,0,2,2,2,1,1,0,2,2,2,2,2,2,1,1,0,2,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,0,2,2,2,0,2,2,2,2,2,2,2,2,0,0,2,2,2,0,2,2,1,1,0,2,2,2,1,1,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,0,0,2,2,2,0,0,2,2,2,0,2,2,2,2,1,1,0,0,2,2,2,2,2,2,2,2,2,2,0,0,2,1,1,0,0,2,2,1,1,0,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,0,2,2,1,1,0,2,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,0,1,1,1,1,1,1,1,0,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,0,0,0,0,0,0,0,0,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,1,1,0,0,2,2,2,2,2,2,1,1,2,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,2,2,2,2,2,1,0,0,0,0,0,0,0,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,0,0,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,1,1,1,1,1,0,1,1,1,0,1,0]},{"id":60,"path":"pkg/cli/config/extension.go","lines":["package config","","import \"k8s.io/apimachinery/pkg/runtime\"","","// Extension represents the configuration for an API extension.","// It includes details necessary for connecting to and authenticating with an external API.","type Extension struct {","\t// TypeMeta is embedded to provide API version and kind information.","\truntime.TypeMeta `json:\",inline\"`","","\t// APIPath is the path to the API endpoint.","\tAPIPath string `json:\"api-path\"`","","\t// Host is the hostname or IP address of the API server.","\tHost string `json:\"host\"`","","\t// Token is the authentication token used for API requests.","\tToken string `json:\"token\"`","","\t// Timeout specifies the maximum duration for API requests.","\t// It is optional and can be omitted.","\tTimeout string `json:\"timeout,omitempty\"`","","\t// InsecureSkipTLSVerify indicates whether to skip TLS certificate verification.","\t// It is optional and can be omitted. If set, it should be used with caution.","\tInsecureSkipTLSVerify string `json:\"insecure-skip-tls-verify,omitempty\"`","}","","// DeepCopy is an autogenerated deep copy function, copying the receiver, creating a new Extension.","func (in *Extension) DeepCopy() *Extension {","\tif in == nil {","\t\treturn nil","\t}","\tout := new(Extension)","\tin.DeepCopyInto(out)","\treturn out","}","","// DeepCopyObject is an autogenerated deep copy function, copying the receiver, creating a new runtime.Object.","func (in *Extension) DeepCopyObject() runtime.Object {","\treturn in.DeepCopy()","}","","// DeepCopyInto is an autogenerated deep copy function, copying the receiver, writing into out. in must be non-nil.","func (in *Extension) DeepCopyInto(out *Extension) {","\t*out = *in","\tout.TypeMeta = in.TypeMeta","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,1,1,2,2,2,0,0,0,2,2,2,0,0,2,2,2,2]},{"id":61,"path":"pkg/cli/config/host.go","lines":["package config","","import (","\t\"errors\"","\t\"fmt\"","\t\"net\"","\t\"net/url\"","\t\"strings\"","\t\"time\"","","\t\"k8s.io/client-go/rest\"",")","","// getHostURL retrieves the external access URL for Tekton Results API.","// It automatically detects the platform and tries to connect to the standard tekton-results-api-service endpoint.","//","// Parameters:","//   - c: A pointer to a rest.Config struct containing the Kubernetes REST configuration.","//","// Returns:","//   - A string containing the external access URL.","//   - An error if any step in the process fails.","func getHostURL(c *rest.Config) (string, error) {","\tif c == nil {","\t\treturn \"\", errors.New(\"nil REST config provided\")","\t}","","\tplatform := DetectPlatform(c)","","\tswitch platform {","\tcase PlatformOpenShift:","\t\treturn tryConnectToRoute(c)","\tcase PlatformKubernetes:","\t\treturn \"\", fmt.Errorf(\"kubernetes ingress not supported\")","\tdefault:","\t\treturn \"\", errors.New(\"unable to detect platform type\")","\t}","}","","// tryConnectToRoute attempts to construct and test OpenShift route URLs to check the server's health","func tryConnectToRoute(c *rest.Config) (string, error) {","\tclusterDomain, err := extractClusterDomain(c.Host)","\tif err != nil {","\t\treturn \"\", fmt.Errorf(\"failed to extract cluster domain\")","\t}","","\t// OpenShift route patterns: tekton-results-api-service-{namespace}.apps.{cluster-domain}","\tnamespace := \"openshift-pipelines\"","\tserviceName := \"tekton-results-api-service\"","","\t// Try HTTPS first (most common for OpenShift routes)","\thttpsURL := fmt.Sprintf(\"https://%s-%s.apps.%s\", serviceName, namespace, clusterDomain)","\tif isURLReachable(httpsURL) {","\t\treturn httpsURL, nil","\t}","","\t// Try HTTP as fallback","\thttpURL := fmt.Sprintf(\"http://%s-%s.apps.%s\", serviceName, namespace, clusterDomain)","\tif isURLReachable(httpURL) {","\t\treturn httpURL, nil","\t}","\treturn \"\", fmt.Errorf(\"no reachable route found\")","}","","// extractClusterDomain extracts the cluster domain from the Kubernetes API server URL","// Example: https://api.mycluster.example.com:6443 -\u003e mycluster.example.com","func extractClusterDomain(apiServerURL string) (string, error) {","\tif apiServerURL == \"\" {","\t\treturn \"\", errors.New(\"empty API server URL\")","\t}","","\t// Parse the URL","\tu, err := url.Parse(apiServerURL)","\tif err != nil {","\t\treturn \"\", fmt.Errorf(\"failed to parse URL\")","\t}","","\thostname := u.Hostname()","\tif hostname == \"\" {","\t\treturn \"\", errors.New(\"failed to extract hostname\")","\t}","","\t// For OpenShift/K8s, API server is typically: api.{cluster-domain}","\t// Extract {cluster-domain} part","\tif strings.HasPrefix(hostname, \"api.\") {","\t\treturn strings.TrimPrefix(hostname, \"api.\"), nil","\t}","","\t// If it doesn't start with \"api.\", try to extract domain differently","\t// Handle cases like: k8s-api-server.cluster.example.com -\u003e cluster.example.com","\tparts := strings.Split(hostname, \".\")","\tif len(parts) \u003e= 2 {","\t\t// Take the last two parts as domain (example.com)","\t\t// or more if it looks like a full domain","\t\tif len(parts) \u003e= 3 {","\t\t\treturn strings.Join(parts[1:], \".\"), nil // Skip first part","\t\t}","\t\treturn strings.Join(parts, \".\"), nil","\t}","","\treturn \"\", fmt.Errorf(\"unable to extract cluster domain\")","}","","// isURLReachable checks if a URL is reachable with a simple TCP connection test","func isURLReachable(testURL string) bool {","\t// Parse URL to extract hostname and determine port","\tparsedURL, err := url.Parse(testURL)","\tif err != nil {","\t\treturn false","\t}","","\t// Get hostname and add appropriate port","\thostname := parsedURL.Hostname()","\tvar port string","\tswitch parsedURL.Scheme {","\tcase \"https\":","\t\tport = \"443\"","\tcase \"http\":","\t\tport = \"80\"","\tdefault:","\t\treturn false // unsupported scheme","\t}","","\t// Create host:port for dialing","\thostPort := hostname + \":\" + port","","\t// Test TCP connectivity","\tconn, err := net.DialTimeout(\"tcp\", hostPort, 5*time.Second)","\tif conn != nil {","\t\t_ = conn.Close()","\t}","\treturn err == nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,1,1,0,2,2,2,1,1,1,1,2,2,0,0,0,0,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,0,0,0,0,1,1,1,1,0,0,1,1,1,1,0,1,1,1,1,0,0,0,1,1,1,0,0,0,1,1,1,1,1,1,1,1,0,0,1,0,0,0,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,0]},{"id":62,"path":"pkg/cli/config/platform.go","lines":["package config","","import (","\t\"k8s.io/client-go/discovery\"","\t\"k8s.io/client-go/rest\"",")","","// PlatformType represents the type of Kubernetes platform","type PlatformType string","","// Platform types for identifying the underlying Kubernetes platform","const (","\tPlatformUnknown    PlatformType = \"Unknown\"","\tPlatformOpenShift  PlatformType = \"OpenShift\"","\tPlatformKubernetes PlatformType = \"Kubernetes\"",")","","// DetectPlatform determines if we're running on OpenShift or Kubernetes","// Checks for OpenShift-specific API groups","func DetectPlatform(c *rest.Config) PlatformType {","\tif c == nil {","\t\treturn PlatformUnknown","\t}","","\tdiscoveryClient, err := discovery.NewDiscoveryClientForConfig(c)","\tif err != nil {","\t\treturn PlatformKubernetes // Default to Kubernetes if discovery fails","\t}","","\t// Check for OpenShift-specific API groups using a map for efficient lookup","\topenShiftAPIGroups := map[string]bool{","\t\t\"route.openshift.io\":    true, // Routes (core OpenShift feature)","\t\t\"image.openshift.io\":    true, // Image streams","\t\t\"apps.openshift.io\":     true, // DeploymentConfigs","\t\t\"security.openshift.io\": true, // Security Context Constraints","\t\t\"project.openshift.io\":  true, // Projects","\t\t\"user.openshift.io\":     true, // Users and groups","\t\t\"oauth.openshift.io\":    true, // OAuth","\t\t\"config.openshift.io\":   true, // Cluster configuration","\t}","","\tapiGroupList, err := discoveryClient.ServerGroups()","\tif err != nil {","\t\treturn PlatformUnknown","\t}","","\t// Check if any OpenShift API groups are present","\tfor _, group := range apiGroupList.Groups {","\t\tif openShiftAPIGroups[group.Name] {","\t\t\treturn PlatformOpenShift","\t\t}","\t}","","\t// No OpenShift API groups found - this is Kubernetes","\treturn PlatformKubernetes","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,2,2,1,1,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,1,1,1,1,0,0,0,1,0]},{"id":63,"path":"pkg/cli/dev/client/client.go","lines":["// Package client provides gRPC client utilities for the dev CLI.","package client","","import (","\t\"context\"","\t\"crypto/tls\"","\t\"crypto/x509\"","\t\"fmt\"","\t\"io\"","\t\"os\"","\t\"time\"","","\t\"github.com/tektoncd/results/pkg/cli/dev/config\"","","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\tpb3 \"github.com/tektoncd/results/proto/v1alpha3/results_go_proto\"","\t\"golang.org/x/oauth2\"","\t\"google.golang.org/grpc\"","\t\"google.golang.org/grpc/credentials\"","\t\"google.golang.org/grpc/credentials/oauth\"","\tv1 \"k8s.io/api/authentication/v1\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/client-go/kubernetes\"","","\t// Load auth plugins","\t_ \"k8s.io/client-go/plugin/pkg/client/auth\"","\t\"k8s.io/client-go/tools/clientcmd\"",")","","//lint:ignore SA1019","","// Factory contains the configuration for creating a k8s client.","type Factory struct {","\tk8s kubernetes.Interface","\tcfg *config.Config","}","","// NewDefaultFactory creates a new Factory with the default configuration.","func NewDefaultFactory() (*Factory, error) {","\tcfg := config.GetConfig()","","\trules := clientcmd.NewDefaultClientConfigLoadingRules()","\tkubeconfig := clientcmd.NewNonInteractiveDeferredLoadingClientConfig(rules, nil)","\tclientconfig, err := kubeconfig.ClientConfig()","\tif err != nil {","\t\treturn nil, err","\t}","\tif cfg.ServiceAccount != nil \u0026\u0026 cfg.ServiceAccount.Name != \"\" \u0026\u0026","\t\tcfg.ServiceAccount.Namespace == \"\" {","\t\tns, _, err := kubeconfig.Namespace()","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\tcfg.ServiceAccount.Namespace = ns","\t}","\tclient, err := kubernetes.NewForConfig(clientconfig)","\tif err != nil {","\t\treturn nil, err","\t}","","\treturn \u0026Factory{","\t\tk8s: client,","\t\tcfg: cfg,","\t}, nil","}","","// ResultsClient creates a new Results gRPC client for the given factory settings.","// TODO: Refactor this with watcher client code?","func (f *Factory) ResultsClient(ctx context.Context, overrideAPIAddr string) (pb.ResultsClient, error) {","\ttoken, err := f.token(ctx)","\tif err != nil {","\t\treturn nil, err","\t}","","\tvar creds credentials.TransportCredentials","\tif f.cfg.Insecure {","\t\tcreds = credentials.NewTLS(\u0026tls.Config{","\t\t\t//nolint:gosec // needed for --insecure flag","\t\t\tInsecureSkipVerify: true,","\t\t})","\t} else {","\t\tcerts, err := f.certs()","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\tcreds = credentials.NewClientTLSFromCert(certs, f.cfg.SSL.ServerNameOverride)","\t}","","\tctx, cancel := context.WithTimeout(ctx, 10*time.Second)","\tdefer cancel()","\taddr := f.cfg.Address","\tif overrideAPIAddr != \"\" {","\t\taddr = overrideAPIAddr","\t}","\tconn, err := grpc.DialContext(ctx, addr, grpc.WithBlock(), //nolint:staticcheck","\t\tgrpc.WithTransportCredentials(creds),","\t\tgrpc.WithDefaultCallOptions(grpc.PerRPCCredentials(oauth.TokenSource{","\t\t\tTokenSource: oauth2.StaticTokenSource(\u0026oauth2.Token{AccessToken: token}),","\t\t})),","\t)","\tif err != nil {","\t\tfmt.Printf(\"Dial: %v\\n\", err)","\t\treturn nil, err","\t}","\treturn pb.NewResultsClient(conn), nil","}","","// DefaultResultsClient creates a new results client.","// Will dial overrideAPIAddr if overrideAPIAddr is not empty","func DefaultResultsClient(ctx context.Context, overrideAPIAddr string) (pb.ResultsClient, error) {","\tf, err := NewDefaultFactory()","","\tif err != nil {","\t\treturn nil, err","\t}","","\tclient, err := f.ResultsClient(ctx, overrideAPIAddr)","","\tif err != nil {","\t\treturn nil, err","\t}","","\treturn client, nil","}","","// LogClient creates a new Results gRPC client for the given factory settings.","// TODO: Refactor this with watcher client code?","func (f *Factory) LogClient(ctx context.Context, overrideAPIAddr string) (pb.LogsClient, error) {","\ttoken, err := f.token(ctx)","\tif err != nil {","\t\treturn nil, err","\t}","","\tvar creds credentials.TransportCredentials","\tif f.cfg.Insecure {","\t\tcreds = credentials.NewTLS(\u0026tls.Config{","\t\t\t//nolint:gosec // needed for --insecure flag","\t\t\tInsecureSkipVerify: true,","\t\t})","\t} else {","\t\tcerts, err := f.certs()","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\tcreds = credentials.NewClientTLSFromCert(certs, f.cfg.SSL.ServerNameOverride)","\t}","","\tctx, cancel := context.WithTimeout(ctx, 10*time.Second)","\tdefer cancel()","\taddr := f.cfg.Address","\tif overrideAPIAddr != \"\" {","\t\taddr = overrideAPIAddr","\t}","\tconn, err := grpc.DialContext(ctx, addr, grpc.WithBlock(), //nolint:staticcheck","\t\tgrpc.WithTransportCredentials(creds),","\t\tgrpc.WithDefaultCallOptions(grpc.PerRPCCredentials(oauth.TokenSource{","\t\t\tTokenSource: oauth2.StaticTokenSource(\u0026oauth2.Token{AccessToken: token}),","\t\t})),","\t)","\tif err != nil {","\t\tfmt.Printf(\"Dial: %v\\n\", err)","\t\treturn nil, err","\t}","\treturn pb.NewLogsClient(conn), nil","}","","// DefaultLogsClient creates a new default logs client.","func DefaultLogsClient(ctx context.Context, overrideAPIAddr string) (pb.LogsClient, error) {","\tf, err := NewDefaultFactory()","","\tif err != nil {","\t\treturn nil, err","\t}","","\tclient, err := f.LogClient(ctx, overrideAPIAddr)","","\tif err != nil {","\t\treturn nil, err","\t}","","\treturn client, nil","}","","func (f *Factory) certs() (*x509.CertPool, error) {","\tcerts, err := x509.SystemCertPool()","\tif err != nil {","\t\treturn nil, err","\t}","\tif path := f.cfg.SSL.RootsFilePath; path != \"\" {","\t\tf, err := os.Open(path) //nolint:gosec // Path is from user-controlled configuration","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\tdefer func() {","\t\t\t_ = f.Close()","\t\t}()","\t\tb, err := io.ReadAll(f)","\t\tif err != nil {","\t\t\treturn nil, fmt.Errorf(\"unable to read TLS cert file: %v\", err)","\t\t}","\t\tif ok := certs.AppendCertsFromPEM(b); !ok {","\t\t\treturn nil, fmt.Errorf(\"unable to add cert to pool\")","\t\t}","\t}","\treturn certs, nil","}","","// PluginLogsClient creates a new Results gRPC client for the given factory settings.","func (f *Factory) PluginLogsClient(ctx context.Context, overrideAPIAddr string) (pb3.LogsClient, error) {","\ttoken, err := f.token(ctx)","\tif err != nil {","\t\treturn nil, err","\t}","","\tvar creds credentials.TransportCredentials","\tif f.cfg.Insecure {","\t\tcreds = credentials.NewTLS(\u0026tls.Config{","\t\t\t//nolint:gosec // needed for --insecure flag","\t\t\tInsecureSkipVerify: true,","\t\t})","\t} else {","\t\tcerts, err := f.certs()","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\tcreds = credentials.NewClientTLSFromCert(certs, f.cfg.SSL.ServerNameOverride)","\t}","","\tctx, cancel := context.WithTimeout(ctx, 10*time.Second)","\tdefer cancel()","\taddr := f.cfg.Address","\tif overrideAPIAddr != \"\" {","\t\taddr = overrideAPIAddr","\t}","\tconn, err := grpc.DialContext(ctx, addr, grpc.WithBlock(), //nolint:staticcheck","\t\tgrpc.WithTransportCredentials(creds),","\t\tgrpc.WithDefaultCallOptions(grpc.PerRPCCredentials(oauth.TokenSource{","\t\t\tTokenSource: oauth2.StaticTokenSource(\u0026oauth2.Token{AccessToken: token}),","\t\t})),","\t)","\tif err != nil {","\t\tfmt.Printf(\"Dial: %v\\n\", err)","\t\treturn nil, err","\t}","\treturn pb3.NewLogsClient(conn), nil","}","","// DefaultPluginLogsClient creates a new default logs client.","func DefaultPluginLogsClient(ctx context.Context, overrideAPIAddr string) (pb3.LogsClient, error) {","\tf, err := NewDefaultFactory()","","\tif err != nil {","\t\treturn nil, err","\t}","","\tclient, err := f.PluginLogsClient(ctx, overrideAPIAddr)","","\tif err != nil {","\t\treturn nil, err","\t}","","\treturn client, nil","}","","func (f *Factory) token(ctx context.Context) (string, error) {","\tif f.cfg == nil {","\t\treturn \"\", nil","\t}","","\tif t := f.cfg.Token; t != \"\" {","\t\treturn t, nil","\t}","","\tif sa := f.cfg.ServiceAccount; sa != nil {","\t\tt, err := f.k8s.CoreV1().ServiceAccounts(sa.Namespace).CreateToken(ctx, sa.Name, \u0026v1.TokenRequest{}, metav1.CreateOptions{})","\t\tif err != nil {","\t\t\treturn \"\", fmt.Errorf(\"error getting service account token: %w\", err)","\t\t}","\t\treturn t.Status.Token, nil","\t}","","\treturn \"\", nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,0,0,0,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,0,1,1,1,1,1,0,1,0,0,0,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,0,1,1,1,1,1,0,1,0,0,2,2,2,1,1,2,2,2,1,1,2,2,2,2,2,1,1,2,1,1,0,2,0,0,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,0,1,1,1,1,1,0,1,0,0,2,2,2,2,0,2,2,2,0,2,2,2,1,1,2,0,0,1,0]},{"id":64,"path":"pkg/cli/dev/config/config.go","lines":["// Package config provides configuration management for the dev CLI.","package config","","import (","\t\"log\"","","\t\"github.com/spf13/viper\"",")","","const (","\t// EnvSSLRootFilePath is the environment variable name for the path to","\t// local SSL cert to use for requests.","\tEnvSSLRootFilePath = \"TKN_RESULTS_SSL_ROOTS_FILE_PATH\"","\t// EnvSSLServerNameOverride is the environment variable name for the SSL server","\t// name override.","\tEnvSSLServerNameOverride = \"TKN_RESULTS_SSL_SERVER_NAME_OVERRIDE\"",")","","var (","\tenv = map[string]string{","\t\tEnvSSLRootFilePath:       \"Path to local SSL cert to use.\",","\t\tEnvSSLServerNameOverride: \"SSL server name override (useful if using with a proxy such as kubectl port-forward).\",","\t}","\tcfg *Config",")","","// Config contains configuration information for the Results CLI.","type Config struct {","\t// Address is the server address to connect to.","\tAddress string","","\t// Token is the bearer token to use for authentication. Takes priority over ServiceAccount.","\tToken string","\t// ServiceAccount is the Kubernetes Service Account to use to authenticate with the Results API.","\t// When specified, the client will fetch a bearer token from the Kubernetes API and use that token","\t// for all Results API requests.","\tServiceAccount *ServiceAccount `mapstructure:\"service_account\"`","","\t// SSL contains SSL configuration information.","\tSSL SSLConfig","\t// Portforward enable auto portforwarding to tekton-results-api-service","\t// When Address is set and Portforward is true, tkn-results will portforward tekton-results-api-service automatically","\tPortforward bool","\t// Insecure determines whether to use insecure GRPC tls communication","\tInsecure bool","","\t// v1alpha2","\tUseV1Alpha2 bool","}","","// SSLConfig contains SSL configuration information.","type SSLConfig struct {","\tRootsFilePath      string `mapstructure:\"roots_file_path\"`","\tServerNameOverride string `mapstructure:\"server_name_override\"`","}","","// ServiceAccount contains information about a Kubernetes ServiceAccount.","type ServiceAccount struct {","\tNamespace string","\tName      string","}","","// Init sets defaults and reads in config from the config file.","func Init() {","\tviper.SetConfigName(\"results\")","\tviper.SetConfigType(\"yaml\")","\tviper.AddConfigPath(\"$HOME/.config/tkn\")","\terr := setConfig()","\tif err != nil {","\t\tlog.Fatal(\"error setting up flags and config\", err)","\t}","}","","func setConfig() error {","\tfor k := range env {","\t\tif err := viper.BindEnv(k); err != nil {","\t\t\treturn err","\t\t}","\t}","","\t// Config should be evaluated in the following order (last wins):","\t// 1. Environment variables","\t// 2. Config File","\t// 3. Flags","","\t// Initial config is contains the env variables,","\t// so that the unmarshal can take priority if those values are set.","\tcfg = \u0026Config{","\t\tSSL: SSLConfig{","\t\t\tRootsFilePath:      viper.GetString(EnvSSLRootFilePath),","\t\t\tServerNameOverride: viper.GetString(EnvSSLServerNameOverride),","\t\t},","\t}","","\tif err := viper.ReadInConfig(); err == nil {","\t\tif err := viper.Unmarshal(cfg); err != nil {","\t\t\treturn err","\t\t}","\t} else {","\t\tif _, ok := err.(viper.ConfigFileNotFoundError); !ok {","\t\t\treturn err","\t\t}","\t}","","\t// Flags should override other values.","\tif s := viper.GetString(\"addr\"); s != \"\" {","\t\tcfg.Address = s","\t}","\tif s := viper.GetString(\"authtoken\"); s != \"\" {","\t\tcfg.Token = viper.GetString(\"authtoken\")","\t}","\tif s := viper.GetString(\"sa\"); s != \"\" {","\t\tcfg.ServiceAccount = \u0026ServiceAccount{}","\t\tcfg.ServiceAccount.Name = viper.GetString(\"sa\")","\t\tif s := viper.GetString(\"sa-ns\"); s != \"\" {","\t\t\tcfg.ServiceAccount.Namespace = viper.GetString(\"sa-ns\")","\t\t}","","\t}","","\tcfg.Portforward = viper.GetBool(\"portforward\")","\tcfg.Insecure = viper.GetBool(\"insecure\")","\tcfg.UseV1Alpha2 = viper.GetBool(\"v1alpha2\")","\treturn nil","}","","// GetConfig returns the current config.","func GetConfig() *Config {","\treturn cfg","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,2,2,2,1,1,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,0,0,0,2,2,2,2,1,1,2,1,1,1,1,1,0,0,0,2,2,2,2,0,0,0,2,2,2]},{"id":65,"path":"pkg/cli/dev/flags/flags.go","lines":["// Package flags provides shared flag definitions and parameters for CLI commands.","package flags","","import (","\t\"github.com/jonboulle/clockwork\"","\t\"github.com/spf13/cobra\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\tpb3 \"github.com/tektoncd/results/proto/v1alpha3/results_go_proto\"",")","","// Params contains a ResultsClient and LogsClient","type Params struct {","\tResultsClient    pb.ResultsClient","\tLogsClient       pb.LogsClient","\tPluginLogsClient pb3.LogsClient","","\tClock clockwork.Clock","}","","// ListOptions is used on commands that list Results, Records or Logs","type ListOptions struct {","\tFilter    string","\tLimit     int32","\tPageToken string","\tFormat    string","}","","// AddListFlags is a helper function that adds common flags for commands that list things","func AddListFlags(options *ListOptions, cmd *cobra.Command) {","\tcmd.Flags().StringVarP(\u0026options.Filter, \"filter\", \"f\", \"\", \"[To be deprecated] CEL Filter\")","\tcmd.Flags().Int32VarP(\u0026options.Limit, \"limit\", \"l\", 0, \"[To be deprecated] number of items to return. Response may be truncated due to server limits.\")","\tcmd.Flags().StringVarP(\u0026options.PageToken, \"page\", \"p\", \"\", \"[To be deprecated] pagination token to use for next page\")","\tcmd.Flags().StringVarP(\u0026options.Format, \"output\", \"o\", \"tab\", \"[To be deprecated] output format. Valid values: tab|textproto|json\")","}","","// GetOptions used on commands that get a single Result, Record or Log","type GetOptions struct {","\tFormat string","}","","// AddGetFlags is a helper function that adds common flags for get commands","func AddGetFlags(options *GetOptions, cmd *cobra.Command) {","\tcmd.Flags().StringVarP(\u0026options.Format, \"output\", \"o\", \"json\", \"[To be deprecated] output format. Valid values: textproto|json\")","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,1,1,1]},{"id":66,"path":"pkg/cli/dev/format/format.go","lines":["// Package format provides output formatting utilities for the CLI.","package format","","import (","\t\"fmt\"","\t\"io\"","\t\"strings\"","\t\"text/tabwriter\"","\t\"time\"","","\t\"github.com/hako/durafmt\"","\t\"github.com/jonboulle/clockwork\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"google.golang.org/protobuf/encoding/protojson\"","\t\"google.golang.org/protobuf/encoding/prototext\"","\t\"google.golang.org/protobuf/proto\"","\t\"google.golang.org/protobuf/types/known/timestamppb\"",")","","// PrintProto prints the given proto message to the given writer in the given format.","// Valid formats are: tab, textproto, json","func PrintProto(w io.Writer, m proto.Message, format string) error {","\tswitch format {","\tcase \"tab\":","\t\ttw := tabwriter.NewWriter(w, 40, 2, 2, ' ', 0)","\t\tswitch t := m.(type) {","\t\tcase *pb.ListResultsResponse:","\t\t\tif _, err := fmt.Fprintln(tw, strings.Join([]string{\"Name\", \"Start\", \"Update\"}, \"\\t\")); err != nil {","\t\t\t\treturn err","\t\t\t}","\t\t\tfor _, r := range t.GetResults() {","\t\t\t\tif _, err := fmt.Fprintln(tw, strings.Join([]string{","\t\t\t\t\tr.GetName(),","\t\t\t\t\tr.GetCreateTime().AsTime().Truncate(time.Second).Local().String(),","\t\t\t\t\tr.GetUpdateTime().AsTime().Truncate(time.Second).Local().String(),","\t\t\t\t}, \"\\t\")); err != nil {","\t\t\t\t\treturn err","\t\t\t\t}","\t\t\t}","\t\tcase *pb.ListRecordsResponse:","\t\t\tif _, err := fmt.Fprintln(tw, strings.Join([]string{\"Name\", \"Type\", \"Start\", \"Update\"}, \"\\t\")); err != nil {","\t\t\t\treturn err","\t\t\t}","\t\t\tfor _, r := range t.GetRecords() {","\t\t\t\tif _, err := fmt.Fprintln(tw, strings.Join([]string{","\t\t\t\t\tr.GetName(),","\t\t\t\t\tr.GetData().GetType(),","\t\t\t\t\tr.GetCreateTime().AsTime().Truncate(time.Second).Local().String(),","\t\t\t\t\tr.GetUpdateTime().AsTime().Truncate(time.Second).Local().String(),","\t\t\t\t}, \"\\t\")); err != nil {","\t\t\t\t\treturn err","\t\t\t\t}","\t\t\t}","\t\t}","\t\tif err := tw.Flush(); err != nil {","\t\t\treturn err","\t\t}","\tcase \"textproto\":","\t\topts := prototext.MarshalOptions{","\t\t\tMultiline: true,","\t\t}","\t\tb, err := opts.Marshal(m)","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\tif _, err := w.Write(b); err != nil {","\t\t\treturn err","\t\t}","\tcase \"json\":","\t\topts := protojson.MarshalOptions{","\t\t\tMultiline: true,","\t\t}","\t\tb, err := opts.Marshal(m)","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\tif _, err := w.Write(b); err != nil {","\t\t\treturn err","\t\t}","\tdefault:","\t\treturn fmt.Errorf(\"unknown output format %q\", format)","\t}","\treturn nil","}","","// Age returns the age of the given timestamp in a human-readable format.","func Age(timestamp *timestamppb.Timestamp, c clockwork.Clock) string {","\tif timestamp == nil {","\t\treturn \"---\"","\t}","\tt := timestamp.AsTime()","\tif t.IsZero() {","\t\treturn \"---\"","\t}","\tduration := c.Since(t)","\treturn durafmt.ParseShort(duration).String() + \" ago\"","}","","// Duration returns the duration between two timestamps in a human-readable format.","func Duration(timestamp1, timestamp2 *timestamppb.Timestamp) string {","\tif timestamp1 == nil || timestamp2 == nil {","\t\treturn \"---\"","\t}","\tt1 := timestamp1.AsTime()","\tt2 := timestamp2.AsTime()","\tif t1.IsZero() || t2.IsZero() {","\t\treturn \"---\"","\t}","\tduration := t2.Sub(t1)","\treturn duration.String()","}","","// Status returns the status of the given record summary in a human-readable format.","func Status(status pb.RecordSummary_Status) string {","\tswitch status {","\tcase pb.RecordSummary_SUCCESS:","\t\treturn \"Succeeded\"","\tcase pb.RecordSummary_FAILURE:","\t\treturn \"Failed\"","\tcase pb.RecordSummary_TIMEOUT:","\t\treturn \"Timed Out\"","\tcase pb.RecordSummary_CANCELLED:","\t\treturn \"Cancelled\"","\t}","\treturn \"Unknown\"","}","","// Namespace returns the namespace of the given result name.","func Namespace(resultName string) string {","\treturn strings.Split(resultName, \"/\")[0]","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,0,1,0,0,0,1,1,1]},{"id":67,"path":"pkg/cli/dev/portforward/portforward.go","lines":["// Package portforward provides Kubernetes port-forwarding utilities for the CLI.","package portforward","","import (","\t\"context\"","\t\"fmt\"","\t\"log\"","\t\"net\"","\t\"net/http\"","\t\"os\"","","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/apimachinery/pkg/labels\"","\t\"k8s.io/client-go/kubernetes\"","\t\"k8s.io/client-go/rest\"","\t\"k8s.io/client-go/tools/clientcmd\"","\t\"k8s.io/client-go/tools/portforward\"","\t\"k8s.io/client-go/transport/spdy\"",")","","// PortForward provides port-forwarding functionality to results api service,","// so cli users would not bother to open a new terminal and type `kubectrl port-forward` manually","type PortForward struct {","\tclientConfig *rest.Config","\tclientSet    kubernetes.Interface","}","","// NewPortForward create a new PortForward to do port-forwarding staff","func NewPortForward() (*PortForward, error) {","\tkubeConfig := clientcmd.NewNonInteractiveDeferredLoadingClientConfig(clientcmd.NewDefaultClientConfigLoadingRules(), nil)","\tclientConfig, err := kubeConfig.ClientConfig()","\tif err != nil {","\t\treturn nil, err","\t}","\tclientSet, err := kubernetes.NewForConfig(clientConfig)","\tif err != nil {","\t\treturn nil, err","\t}","","\treturn \u0026PortForward{clientSet: clientSet, clientConfig: clientConfig}, nil","}","","// ForwardPortBackground do port-forwarding in background.","// stopChan control when port-forwarding stops, port specify which port on localhost port-forwarding will occupy","func (pf *PortForward) ForwardPortBackground(stopChan \u003c-chan struct{}, port int) error {","\tresultsAPIService, err := pf.clientSet.CoreV1().Services(\"tekton-pipelines\").Get(context.TODO(), \"tekton-results-api-service\", metav1.GetOptions{})","\tif err != nil {","\t\treturn err","\t}","\tresultsPodSelector := labels.Set(resultsAPIService.Spec.Selector)","\tpods, err := pf.clientSet.CoreV1().Pods(\"tekton-pipelines\").List(context.TODO(), metav1.ListOptions{","\t\tLabelSelector: resultsPodSelector.String(),","\t})","\tif err != nil {","\t\treturn err","\t}","\tresultsAPIPod := pods.Items[0]","\treq := pf.clientSet.CoreV1().RESTClient().Post().Namespace(\"tekton-pipelines\").Resource(\"pods\").Name(resultsAPIPod.Name).SubResource(\"portforward\")","","\ttransport, upgrader, err := spdy.RoundTripperFor(pf.clientConfig)","\tif err != nil {","\t\treturn err","\t}","\tdialer := spdy.NewDialer(upgrader, \u0026http.Client{Transport: transport}, \"POST\", req.URL())","","\treadyChan := make(chan struct{})","\tfw, err := portforward.NewOnAddresses(dialer, []string{\"localhost\"}, []string{fmt.Sprintf(\"%d:8080\", port)}, stopChan, readyChan, nil, os.Stderr)","\tif err != nil {","\t\treturn err","\t}","\tgo func() {","\t\terr := fw.ForwardPorts()","\t\tif err != nil {","\t\t\tlog.Fatalf(\"err forward ports: %v\", err)","\t\t}","\t}()","","\t// wait for port-forward ready","\t\u003c-readyChan","\treturn nil","}","","// PickFreePort asks the kernel for a free open port that is ready to use.","func PickFreePort() (port int, err error) {","\tvar a *net.TCPAddr","\ta, err = net.ResolveTCPAddr(\"tcp\", \"localhost:0\")","\tif err != nil {","\t\treturn 0, err","\t}","\tvar l *net.TCPListener","\tl, err = net.ListenTCP(\"tcp\", a)","\tif err != nil {","\t\treturn 0, err","\t}","\tdefer func() {","\t\t_ = l.Close()","\t}()","\treturn l.Addr().(*net.TCPAddr).Port, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0]},{"id":68,"path":"pkg/cli/flags/flags.go","lines":["// Package flags provides flag definitions for the Results CLI.","package flags","","import (","\t\"github.com/spf13/cobra\"","\t\"github.com/tektoncd/cli/pkg/formatted\"","\t\"github.com/tektoncd/results/pkg/cli/common\"",")","","const (","\tkubeConfig            = \"kubeconfig\"","\tcontext               = \"context\"","\tnamespace             = \"namespace\"","\thost                  = \"host\"","\ttoken                 = \"token\"","\tapiPath               = \"api-path\"","\tinsecureSkipTLSVerify = \"insecure-skip-tls-verify\"",")","","// ResultsOptions all global Results options","type ResultsOptions struct {","\tKubeConfig, Context, Namespace, Host, Token, APIPath string","\tInsecureSkipTLSVerify                                bool","}","","// AddResultsOptions amends the provided command by adding flags required to initialize a cli.Param.","// It adds persistent flags for kubeconfig, context, namespace, host, token, API path, and TLS verification.","//","// Parameters:","//   - cmd: A pointer to a cobra.Command to which the flags will be added.","//","// This function does not return any value.","func AddResultsOptions(cmd *cobra.Command) {","\tcmd.PersistentFlags().StringP(","\t\tkubeConfig, \"k\", \"\",","\t\t\"kubectl config file (default: $HOME/.kube/config)\")","","\tcmd.PersistentFlags().StringP(","\t\tcontext, \"c\", \"\",","\t\t\"name of the kubeconfig context to use (default: kubectl config current-context)\")","","\tcmd.PersistentFlags().StringP(","\t\tnamespace, \"n\", \"\",","\t\t\"namespace to use (default: from $KUBECONFIG)\")","\t_ = cmd.RegisterFlagCompletionFunc(namespace,","\t\tfunc(_ *cobra.Command, args []string, _ string) ([]string, cobra.ShellCompDirective) {","\t\t\treturn formatted.BaseCompletion(\"namespace\", args)","\t\t},","\t)","","\tcmd.PersistentFlags().StringP(","\t\thost, \"\", \"\",","\t\t\"host to use (default: value provided in config set command)\")","","\tcmd.PersistentFlags().StringP(","\t\ttoken, \"\", \"\",","\t\t\"bearer token to use (default: value provided in config set command)\")","","\tcmd.PersistentFlags().StringP(","\t\tapiPath, \"\", \"\",","\t\t\"api path to use (default: value provided in config set command)\")","","\tcmd.PersistentFlags().BoolP(","\t\tinsecureSkipTLSVerify, \"\", false,","\t\t\"skip server's certificate validation for requests (default: false)\")","","}","","// GetResultsOptions retrieves the global Results Options that are not passed to subcommands.","// It extracts flag values from the provided command and returns them as a ResultsOptions struct.","//","// Parameters:","//   - cmd: A pointer to a cobra.Command from which to retrieve flag values.","//","// Returns:","//   - ResultsOptions: A struct containing the extracted flag values for kubeconfig, context,","//     namespace, host, token, API path, and TLS verification settings.","func GetResultsOptions(cmd *cobra.Command) ResultsOptions {","\tkcPath, _ := cmd.Flags().GetString(kubeConfig)","\tkubeContext, _ := cmd.Flags().GetString(context)","\tns, _ := cmd.Flags().GetString(namespace)","\th, _ := cmd.Flags().GetString(host)","\tt, _ := cmd.Flags().GetString(token)","\tap, _ := cmd.Flags().GetString(apiPath)","\tskipTLSVerify, _ := cmd.Flags().GetBool(insecureSkipTLSVerify)","\treturn ResultsOptions{","\t\tKubeConfig:            kcPath,","\t\tContext:               kubeContext,","\t\tNamespace:             ns,","\t\tHost:                  h,","\t\tToken:                 t,","\t\tAPIPath:               ap,","\t\tInsecureSkipTLSVerify: skipTLSVerify,","\t}","}","","// InitParams initializes cli.Params based on flags defined in the command.","//","// This function retrieves flag values from the provided cobra.Command and sets","// the corresponding values in the common.Params object. It handles flags for","// kubeconfig, context, namespace, host, token, API path, and TLS verification.","//","// Parameters:","//   - p: A common.Params object to be initialized with flag values.","//   - cmd: A *cobra.Command object containing the flags to be processed.","//","// Returns:","//   - An error if any flag retrieval operation fails, nil otherwise.","//","// Note: This function uses cmd.Flags() instead of cmd.PersistentFlags() to","// access flags, which may break symmetry with AddResultsOptions. This is because","// it could be a subcommand trying to access flags defined by the parent command.","func InitParams(p common.Params, cmd *cobra.Command) error {","\t// First set kubeconfig and context","\tkcPath, err := cmd.Flags().GetString(kubeConfig)","\tif err != nil {","\t\treturn err","\t}","\tp.SetKubeConfigPath(kcPath)","","\tkubeContext, err := cmd.Flags().GetString(context)","\tif err != nil {","\t\treturn err","\t}","\tp.SetKubeContext(kubeContext)","","\t// Then set namespace, which will use the kubeconfig and context if needed","\tns, err := cmd.Flags().GetString(namespace)","\tif err != nil {","\t\treturn err","\t}","\tp.SetNamespace(ns)","","\t// Set other flags","\th, err := cmd.Flags().GetString(host)","\tif err != nil {","\t\treturn err","\t}","\tif h != \"\" {","\t\tp.SetHost(h)","\t}","","\tt, err := cmd.Flags().GetString(token)","\tif err != nil {","\t\treturn err","\t}","\tif t != \"\" {","\t\tp.SetToken(t)","\t}","","\tap, err := cmd.Flags().GetString(apiPath)","\tif err != nil {","\t\treturn err","\t}","\tif ap != \"\" {","\t\tp.SetAPIPath(ap)","\t}","","\tskipTLSVerify, err := cmd.Flags().GetBool(insecureSkipTLSVerify)","\tif err != nil {","\t\treturn err","\t}","\tp.SetSkipTLSVerify(skipTLSVerify)","","\treturn nil","}","","// AnyResultsFlagChanged checks if any of the Results flags (host, token, api-path, insecure-skip-tls-verify)","// have been changed from their default values.","//","// Parameters:","//   - cmd: A pointer to a cobra.Command to check the flags.","//","// Returns:","//   - bool: true if any of the Results flags have been changed, false otherwise.","func AnyResultsFlagChanged(cmd *cobra.Command) bool {","\treturn cmd.Flags().Changed(host) ||","\t\tcmd.Flags().Changed(token) ||","\t\tcmd.Flags().Changed(apiPath) ||","\t\tcmd.Flags().Changed(insecureSkipTLSVerify)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,1,1,2,2,2,2,1,1,2,2,2,2,2,1,1,2,2,2,2,2,1,1,2,2,2,0,2,2,1,1,2,2,2,0,2,2,1,1,2,2,2,0,2,2,1,1,2,2,2,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1]},{"id":69,"path":"pkg/cli/options/describe.go","lines":["// Package options provides shared option structs for CLI commands.","package options","","import \"github.com/tektoncd/results/pkg/cli/client\"","","// DescribeOptions contains options for describing a resource.","type DescribeOptions struct {","\tClient       *client.RESTClient","\tUID          string","\tResourceType string","\tResourceName string","}","","// GetLabel implements FilterOptions interface","func (o *DescribeOptions) GetLabel() string {","\treturn \"\" // Label field is not relevant in the describe commands","}","","// GetResourceName implements FilterOptions interface","func (o *DescribeOptions) GetResourceName() string {","\treturn o.ResourceName","}","","// GetPipelineRun implements FilterOptions interface","func (o *DescribeOptions) GetPipelineRun() string {","\treturn \"\" // PipelineRun field is not relevant in the describe commands","}","","// GetResourceType implements FilterOptions interface","func (o *DescribeOptions) GetResourceType() string {","\treturn o.ResourceType","}","","// GetUID implements FilterOptions interface","func (o *DescribeOptions) GetUID() string {","\treturn o.UID","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,1,1,1,0,0,1,1,1,0,0,1,1,1,0,0,1,1,1]},{"id":70,"path":"pkg/cli/options/list.go","lines":["package options","","import (","\t\"github.com/tektoncd/results/pkg/cli/client\"",")","","// ListOptions holds the options for listing resources","type ListOptions struct {","\tClient        *client.RESTClient","\tLimit         int32","\tAllNamespaces bool","\tLabel         string","\tPipelineRun   string","\tSinglePage    bool","\tResourceName  string","\tResourceType  string","}","","// GetLabel implements FilterOptions interface","func (o *ListOptions) GetLabel() string {","\treturn o.Label","}","","// GetResourceName implements FilterOptions interface","func (o *ListOptions) GetResourceName() string {","\treturn o.ResourceName","}","","// GetPipelineRun implements FilterOptions interface","func (o *ListOptions) GetPipelineRun() string {","\treturn o.PipelineRun","}","","// GetResourceType implements FilterOptions interface","func (o *ListOptions) GetResourceType() string {","\treturn o.ResourceType","}","","// GetUID implements FilterOptions interface","func (o *ListOptions) GetUID() string {","\treturn \"\"","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,1,1,1,0,0,1,1,1,0,0,1,1,1,0,0,1,1,1]},{"id":71,"path":"pkg/cli/options/logs.go","lines":["package options","","import \"github.com/tektoncd/results/pkg/cli/client\"","","// LogsOptions contains options for fetching logs for a resource.","type LogsOptions struct {","\tClient       *client.RESTClient","\tUID          string","\tResourceType string","\tResourceName string","}","","// GetLabel implements FilterOptions interface","func (o *LogsOptions) GetLabel() string {","\treturn \"\" // Label field is not relevant in the logs commands","}","","// GetResourceName implements FilterOptions interface","func (o *LogsOptions) GetResourceName() string {","\treturn o.ResourceName","}","","// GetPipelineRun implements FilterOptions interface","func (o *LogsOptions) GetPipelineRun() string {","\treturn \"\" // PipelineRun field is not relevant in the logs commands","}","","// GetResourceType implements FilterOptions interface","func (o *LogsOptions) GetResourceType() string {","\treturn o.ResourceType","}","","// GetUID implements FilterOptions interface","func (o *LogsOptions) GetUID() string {","\treturn o.UID","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,1,1,1,0,0,1,1,1,0,0,1,1,1,0,0,1,1,1]},{"id":72,"path":"pkg/cli/testutils/cobra.go","lines":["package testutils","","import (","\t\"bytes\"","","\t\"github.com/spf13/cobra\"",")","","// ExecuteCommand executes the root command passing the args and returns","// the output as a string and error","func ExecuteCommand(c *cobra.Command, args ...string) (string, error) {","\tbuf := new(bytes.Buffer)","\tc.SetOut(buf)","\tc.SetErr(buf)","\tc.SetArgs(args)","\tc.SilenceUsage = true","","\t_, err := c.ExecuteC()","","\treturn buf.String(), err","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1]},{"id":73,"path":"pkg/cli/testutils/kubeconfig.go","lines":["// Package testutils provides test utility functions for the CLI package","package testutils","","import (","\t\"encoding/json\"","\t\"fmt\"","\t\"os\"","\t\"path/filepath\"","\t\"testing\"","","\t\"github.com/tektoncd/results/pkg/cli/common\"","\t\"k8s.io/apimachinery/pkg/runtime\"","\t\"k8s.io/client-go/tools/clientcmd\"",")","","// CreateTestKubeconfig creates a temporary kubeconfig file for testing","// If namespace is empty, no namespace is set in the context","func CreateTestKubeconfig(t *testing.T, namespace string) string {","\tt.Helper()","","\tdir := t.TempDir()","\tkubeconfigPath := filepath.Join(dir, \"kubeconfig.yaml\")","","\t// Build context section with optional namespace","\tcontextSection := `    cluster: test-cluster","    user: test-user`","\tif namespace != \"\" {","\t\tcontextSection = fmt.Sprintf(`    cluster: test-cluster","    namespace: %s","    user: test-user`, namespace)","\t}","","\tkubeconfigContent := fmt.Sprintf(`apiVersion: v1","clusters:","- cluster:","    server: http://test-host","  name: test-cluster","contexts:","- context:","%s","  name: test-context","current-context: test-context","kind: Config","preferences: {}","users:","- name: test-user","`, contextSection)","","\tif err := os.WriteFile(kubeconfigPath, []byte(kubeconfigContent), 0600); err != nil {","\t\tt.Fatalf(\"Failed to write kubeconfig: %v\", err)","\t}","","\treturn kubeconfigPath","}","","// ReadKubeconfigExtensionRaw reads the tekton-results extension from a kubeconfig file as raw data","// This avoids import cycles by not depending on config package types","func ReadKubeconfigExtensionRaw(t *testing.T, kubeconfigPath, extensionName string) ([]byte, error) {","\tt.Helper()","","\tconfigLoadingRules := \u0026clientcmd.ClientConfigLoadingRules{ExplicitPath: kubeconfigPath}","\tapiConfig, err := configLoadingRules.Load()","\tif err != nil {","\t\treturn nil, err","\t}","","\tcontext := apiConfig.Contexts[apiConfig.CurrentContext]","\tif context == nil {","\t\treturn nil, nil","\t}","","\t// Use the shared function from common package to build context info","\tconfigContextName, _, _, err := common.BuildConfigContextInfo(context)","\tif err != nil {","\t\treturn nil, err","\t}","","\t// Look for config context using direct lookup","\tconfigContext, exists := apiConfig.Contexts[configContextName]","\tif !exists || configContext.Extensions == nil {","\t\treturn nil, nil","\t}","","\textensionData, exists := configContext.Extensions[extensionName]","\tif !exists {","\t\treturn nil, nil","\t}","","\t// Return the raw extension data","\treturn extensionData.(*runtime.Unknown).Raw, nil","}","","// ReadKubeconfigExtension reads and unmarshals a kubeconfig extension into the provided target","// The target parameter should be a pointer to the struct you want to unmarshal into","// Returns true if the extension was found and unmarshaled successfully, false if not found","func ReadKubeconfigExtension(t *testing.T, kubeconfigPath, extensionName string, target interface{}) (bool, error) {","\tt.Helper()","","\trawData, err := ReadKubeconfigExtensionRaw(t, kubeconfigPath, extensionName)","\tif err != nil {","\t\treturn false, err","\t}","\tif rawData == nil {","\t\treturn false, nil","\t}","","\t// Unmarshal the raw data into the target","\tif err := json.Unmarshal(rawData, target); err != nil {","\t\treturn false, err","\t}","","\treturn true, nil","}","","// UpdateKubeconfigNamespace updates the namespace of the current context in a kubeconfig file","func UpdateKubeconfigNamespace(t *testing.T, kubeconfigPath, newNamespace string) {","\tt.Helper()","","\t// Load the kubeconfig","\tconfigLoadingRules := \u0026clientcmd.ClientConfigLoadingRules{ExplicitPath: kubeconfigPath}","\tapiConfig, err := configLoadingRules.Load()","\tif err != nil {","\t\tt.Fatalf(\"Failed to load kubeconfig: %v\", err)","\t}","","\t// Update the namespace of the current context","\tcurrentContext := apiConfig.Contexts[apiConfig.CurrentContext]","\tif currentContext == nil {","\t\tt.Fatalf(\"Current context not found in kubeconfig\")","\t}","","\tcurrentContext.Namespace = newNamespace","","\t// Write the updated kubeconfig","\tif err := clientcmd.WriteToFile(*apiConfig, kubeconfigPath); err != nil {","\t\tt.Fatalf(\"Failed to write updated kubeconfig: %v\", err)","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,0,0,1,1,1,1,1,1,1,1,0,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,1,0,1,1,1,1,0,0,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,0,1,0,0,0,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,0,1,1,1,1,1,1,0]},{"id":74,"path":"pkg/cli/testutils/mock_rest_client.go","lines":["package testutils","","import (","\t\"encoding/json\"","\t\"fmt\"","\t\"net/http\"","\t\"net/http/httptest\"","\t\"net/url\"","\t\"strings\"","\t\"time\"","","\tv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"github.com/tektoncd/results/pkg/cli/client\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"google.golang.org/protobuf/encoding/protojson\"","\t\"k8s.io/client-go/transport\"",")","","// filterRecordsByNamespace filters records based on the namespace in the URL path","func filterRecordsByNamespace(records []*pb.Record, urlPath string) []*pb.Record {","\t// Extract parent from URL path like \"/parents/{parent}/results/-/records\"","\t// parent can be:","\t// - \"namespace\" for specific namespace (e.g., \"production\", \"default\")","\t// - \"-\" for all namespaces","","\tparts := strings.Split(urlPath, \"/\")","\tvar parent string","\tfor i, part := range parts {","\t\tif part == \"parents\" \u0026\u0026 i+1 \u003c len(parts) {","\t\t\tparent = parts[i+1]","\t\t\tbreak","\t\t}","\t}","","\t// If parent is \"-\", return all records (all namespaces mode)","\tif parent == \"-\" {","\t\treturn records","\t}","","\t// Parent is the namespace directly (URL structure: /parents/{namespace}/results/-/records)","\ttargetNamespace := parent","","\t// Filter records by namespace","\tvar filteredRecords []*pb.Record","\tfor _, record := range records {","\t\t// Parse the PipelineRun data to get the namespace","\t\tvar pipelineRun v1.PipelineRun","\t\tif err := json.Unmarshal(record.Data.Value, \u0026pipelineRun); err == nil {","\t\t\tif pipelineRun.Namespace == targetNamespace {","\t\t\t\tfilteredRecords = append(filteredRecords, record)","\t\t\t}","\t\t}","\t}","","\treturn filteredRecords","}","","// filterRecordsByPipelineName filters records based on pipeline name from the filter query parameter","func filterRecordsByPipelineName(records []*pb.Record, filterQuery string) []*pb.Record {","\t// Parse the filter query to extract pipeline name filter","\t// Example filter: \"(data_type==\\\"tekton.dev/v1.PipelineRun\\\" || data_type==\\\"tekton.dev/v1beta1.PipelineRun\\\") \u0026\u0026 data.metadata.name.contains(\\\"build-pipeline\\\")\"","","\t// Look for the pattern: data.metadata.name.contains(\"pipeline-name\")","\tnameFilterPrefix := \"data.metadata.name.contains(\\\"\"","\tnameFilterSuffix := \"\\\")\"","","\tstartIdx := strings.Index(filterQuery, nameFilterPrefix)","\tif startIdx == -1 {","\t\t// No pipeline name filter found, return all records","\t\treturn records","\t}","","\tstartIdx += len(nameFilterPrefix)","\tendIdx := strings.Index(filterQuery[startIdx:], nameFilterSuffix)","\tif endIdx == -1 {","\t\t// Malformed filter, return all records","\t\treturn records","\t}","","\tpipelineName := filterQuery[startIdx : startIdx+endIdx]","","\t// Filter records by pipeline name (contains match)","\tvar filteredRecords []*pb.Record","\tfor _, record := range records {","\t\t// Parse the PipelineRun data to get the name","\t\tvar pipelineRun v1.PipelineRun","\t\tif err := json.Unmarshal(record.Data.Value, \u0026pipelineRun); err == nil {","\t\t\tif strings.Contains(pipelineRun.Name, pipelineName) {","\t\t\t\tfilteredRecords = append(filteredRecords, record)","\t\t\t}","\t\t}","\t}","","\treturn filteredRecords","}","","// filterRecordsByLabels filters records based on label filters from the filter query parameter","func filterRecordsByLabels(records []*pb.Record, filterQuery string) []*pb.Record {","\t// Parse the filter query to extract label filters","\t// Example filter: \"data.metadata.labels[\\\"app\\\"]==\\\"myapp\\\" \u0026\u0026 data.metadata.labels[\\\"env\\\"]==\\\"prod\\\"\"","","\t// Look for the pattern: data.metadata.labels[\"key\"]==\"value\"","\tlabelFilterPrefix := \"data.metadata.labels[\\\"\"","\tlabelFilterSuffix := \"\\\"]==\"","","\t// Extract all label filters from the query","\tvar labelFilters []struct {","\t\tkey   string","\t\tvalue string","\t}","","\tsearchStart := 0","\tfor {","\t\tstartIdx := strings.Index(filterQuery[searchStart:], labelFilterPrefix)","\t\tif startIdx == -1 {","\t\t\tbreak","\t\t}","\t\tstartIdx += searchStart + len(labelFilterPrefix)","","\t\t// Find the end of the key","\t\tkeyEndIdx := strings.Index(filterQuery[startIdx:], labelFilterSuffix)","\t\tif keyEndIdx == -1 {","\t\t\tbreak","\t\t}","","\t\tkey := filterQuery[startIdx : startIdx+keyEndIdx]","\t\tvalueStartIdx := startIdx + keyEndIdx + len(labelFilterSuffix)","","\t\t// Find the value (enclosed in quotes)","\t\tif valueStartIdx \u003e= len(filterQuery) || filterQuery[valueStartIdx] != '\"' {","\t\t\tbreak","\t\t}","\t\tvalueStartIdx++ // Skip opening quote","","\t\tvalueEndIdx := strings.Index(filterQuery[valueStartIdx:], \"\\\"\")","\t\tif valueEndIdx == -1 {","\t\t\tbreak","\t\t}","","\t\tvalue := filterQuery[valueStartIdx : valueStartIdx+valueEndIdx]","\t\tlabelFilters = append(labelFilters, struct {","\t\t\tkey   string","\t\t\tvalue string","\t\t}{key: key, value: value})","","\t\tsearchStart = valueStartIdx + valueEndIdx + 1","\t}","","\t// If no label filters found, return all records","\tif len(labelFilters) == 0 {","\t\treturn records","\t}","","\t// Filter records by labels (all labels must match)","\tvar filteredRecords []*pb.Record","\tfor _, record := range records {","\t\t// Parse the PipelineRun data to get the labels","\t\tvar pipelineRun v1.PipelineRun","\t\tif err := json.Unmarshal(record.Data.Value, \u0026pipelineRun); err == nil {","\t\t\t// Check if all label filters match","\t\t\tallMatch := true","\t\t\tfor _, labelFilter := range labelFilters {","\t\t\t\tif pipelineRun.Labels == nil {","\t\t\t\t\tallMatch = false","\t\t\t\t\tbreak","\t\t\t\t}","\t\t\t\tif labelValue, exists := pipelineRun.Labels[labelFilter.key]; !exists || labelValue != labelFilter.value {","\t\t\t\t\tallMatch = false","\t\t\t\t\tbreak","\t\t\t\t}","\t\t\t}","\t\t\tif allMatch {","\t\t\t\tfilteredRecords = append(filteredRecords, record)","\t\t\t}","\t\t}","\t}","","\treturn filteredRecords","}","","// MockRESTClientFromRecords creates a mock REST client with comprehensive filtering support","func MockRESTClientFromRecords(records []*pb.Record) (*client.RESTClient, error) {","\t// Create HTTP server that handles multiple endpoints with filtering","\tserver := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {","\t\tw.Header().Set(\"Content-Type\", \"application/json\")","","\t\tswitch {","\t\tcase r.Method == \"GET\" \u0026\u0026 strings.Contains(r.URL.Path, \"/records\"):","\t\t\t// Handle ListRecords with namespace and pipeline name filtering","\t\t\tfilteredRecords := filterRecordsByNamespace(records, r.URL.Path)","","\t\t\t// Apply filtering from query parameters","\t\t\tif filter := r.URL.Query().Get(\"filter\"); filter != \"\" {","\t\t\t\tfilteredRecords = filterRecordsByPipelineName(filteredRecords, filter)","\t\t\t\tfilteredRecords = filterRecordsByLabels(filteredRecords, filter)","\t\t\t}","","\t\t\t// Create response with filtered records (no pagination)","\t\t\tgrpcResp := \u0026pb.ListRecordsResponse{","\t\t\t\tRecords:       filteredRecords,","\t\t\t\tNextPageToken: \"\", // No pagination support","\t\t\t}","\t\t\tjsonData, err := protojson.Marshal(grpcResp)","\t\t\tif err != nil {","\t\t\t\thttp.Error(w, \"Failed to marshal records response\", http.StatusInternalServerError)","\t\t\t\treturn","\t\t\t}","\t\t\tif _, err := w.Write(jsonData); err != nil {","\t\t\t\thttp.Error(w, \"Failed to write response\", http.StatusInternalServerError)","\t\t\t}","\t\t\treturn","","\t\tcase r.Method == \"GET\" \u0026\u0026 strings.Contains(r.URL.Path, \"/logs\"):","\t\t\t// Handle logs endpoints - can be extended as needed","\t\t\t// For now, return empty response","\t\t\tlogsResp := \u0026pb.ListRecordsResponse{Records: nil}","\t\t\tjsonData, err := protojson.Marshal(logsResp)","\t\t\tif err != nil {","\t\t\t\thttp.Error(w, \"Failed to marshal logs response\", http.StatusInternalServerError)","\t\t\t\treturn","\t\t\t}","\t\t\tif _, err := w.Write(jsonData); err != nil {","\t\t\t\thttp.Error(w, \"Failed to write response\", http.StatusInternalServerError)","\t\t\t}","\t\t\treturn","\t\t}","","\t\thttp.NotFound(w, r)","\t}))","","\tserverURL, err := url.Parse(server.URL + \"/apis/results.tekton.dev/v1alpha2\")","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"failed to parse server URL: %w\", err)","\t}","","\tconfig := \u0026client.Config{","\t\tURL:       serverURL,","\t\tTimeout:   30 * time.Second,","\t\tTransport: \u0026transport.Config{},","\t}","","\trestClient, err := client.NewRESTClient(config)","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"failed to create REST client: %w\", err)","\t}","","\treturn restClient, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,0,0,1,1,1,1,1,1,0,1,1,1,1,1,0,0,1,1,1,1,1,1,1,0,0,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,0,0,1,1,1,0,0,0,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,0,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,1,0]},{"id":75,"path":"pkg/cli/testutils/params.go","lines":["package testutils","","import (","\t\"github.com/tektoncd/results/pkg/cli/client\"","\t\"github.com/tektoncd/results/pkg/cli/common\"",")","","// Params implements common.Params interface for testing","// This allows injecting fake/mock clients during tests","type Params struct {","\tkubeConfigPath string","\tkubeContext    string","\tnamespace      string","\thost           string","\ttoken          string","\tapiPath        string","\tskipTLSVerify  bool","","\t// Simple client storage for testing","\trestClient *client.RESTClient","}","","// Ensure Params implements the interface","var _ common.Params = (*Params)(nil)","","// NewParams creates a new test Params with sensible defaults","func NewParams() *Params {","\treturn \u0026Params{","\t\thost:      \"http://localhost:8080\",","\t\tnamespace: \"default\",","\t}","}","","// SetKubeConfigPath sets the kubeconfig file path","func (p *Params) SetKubeConfigPath(path string) { p.kubeConfigPath = path }","","// KubeConfigPath returns the kubeconfig file path","func (p *Params) KubeConfigPath() string { return p.kubeConfigPath }","","// SetKubeContext sets the kubernetes context","func (p *Params) SetKubeContext(context string) { p.kubeContext = context }","","// KubeContext returns the kubernetes context","func (p *Params) KubeContext() string { return p.kubeContext }","","// SetNamespace sets the kubernetes namespace, preserving default if empty","func (p *Params) SetNamespace(ns string) {","\t// For testing, simulate the kubeconfig resolution behavior:","\t// If empty string is provided, keep the existing namespace (simulates kubeconfig default)","\tif ns != \"\" {","\t\tp.namespace = ns","\t}","\t// If ns is empty, keep the existing namespace (set in NewParams() as \"default\")","}","","// Namespace returns the kubernetes namespace","func (p *Params) Namespace() string { return p.namespace }","","// SetHost sets the API host","func (p *Params) SetHost(host string) { p.host = host }","","// Host returns the API host","func (p *Params) Host() string { return p.host }","","// SetToken sets the authentication token","func (p *Params) SetToken(token string) { p.token = token }","","// Token returns the authentication token","func (p *Params) Token() string { return p.token }","","// SetAPIPath sets the API path","func (p *Params) SetAPIPath(path string) { p.apiPath = path }","","// APIPath returns the API path","func (p *Params) APIPath() string { return p.apiPath }","","// SetSkipTLSVerify sets whether to skip TLS verification","func (p *Params) SetSkipTLSVerify(skip bool) { p.skipTLSVerify = skip }","","// SkipTLSVerify returns whether to skip TLS verification","func (p *Params) SkipTLSVerify() bool { return p.skipTLSVerify }","","// SetRESTClient injects a REST client for testing purposes","func (p *Params) SetRESTClient(client *client.RESTClient) {","\tp.restClient = client","}","","// RESTClient returns the injected REST client for testing","func (p *Params) RESTClient() *client.RESTClient {","\treturn p.restClient","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,1,1,1,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,1,1,0,0,1,1,1]},{"id":76,"path":"pkg/cli/testutils/utils.go","lines":["package testutils","","import (","\t\"time\"","","\t\"github.com/jonboulle/clockwork\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"google.golang.org/protobuf/types/known/timestamppb\"",")","","const (","\t// Common test constants","\tdefaultNamespace     = \"default\"","\tdefaultConditionType = \"Succeeded\"","\tpipelineRunKind      = \"tekton.dev/v1.PipelineRun\"",")","","// TimePtr converts time.Time to *time.Time","func TimePtr(t time.Time) *time.Time {","\treturn \u0026t","}","","// CreateTestRecord creates a test record with all possible options","// Use empty string for namespace to use default, nil for labels to omit them","func CreateTestRecord(clock clockwork.Clock, name, uid, namespace string, startTime, endTime *time.Time, conditionStatus string, labels map[string]string) *pb.Record {","\t// Use default namespace if not specified","\tif namespace == \"\" {","\t\tnamespace = defaultNamespace","\t}","","\tconditionType := defaultConditionType","\tcreateTime := clock.Now().Add(-5 * time.Minute)","","\tvar completionTimeJSON string","\tif endTime != nil {","\t\tcompletionTimeJSON = `\"completionTime\": \"` + endTime.Format(time.RFC3339) + `\",`","\t}","","\tvar startTimeJSON string","\tif startTime != nil {","\t\tstartTimeJSON = `\"startTime\": \"` + startTime.Format(time.RFC3339) + `\",`","\t}","","\t// Build labels JSON","\tvar labelsJSON string","\tif len(labels) \u003e 0 {","\t\tlabelsJSON = `,\"labels\": {`","\t\tfirst := true","\t\tfor key, value := range labels {","\t\t\tif !first {","\t\t\t\tlabelsJSON += \",\"","\t\t\t}","\t\t\tlabelsJSON += `\"` + key + `\": \"` + value + `\"`","\t\t\tfirst = false","\t\t}","\t\tlabelsJSON += `}`","\t}","","\treturn \u0026pb.Record{","\t\tName:       namespace + \"/results/\" + name + \"/records/\" + name,","\t\tUid:        \"record-\" + uid,","\t\tCreateTime: timestamppb.New(createTime),","\t\tData: \u0026pb.Any{","\t\t\tType: pipelineRunKind,","\t\t\tValue: []byte(`{","\t\t\t\t\"apiVersion\": \"tekton.dev/v1\",","\t\t\t\t\"kind\": \"PipelineRun\",","\t\t\t\t\"metadata\": {","\t\t\t\t\t\"name\": \"` + name + `\",","\t\t\t\t\t\"namespace\": \"` + namespace + `\",","\t\t\t\t\t\"uid\": \"` + uid + `\"` + labelsJSON + `","\t\t\t\t},","\t\t\t\t\"status\": {","\t\t\t\t\t` + startTimeJSON + `","\t\t\t\t\t` + completionTimeJSON + `","\t\t\t\t\t\"conditions\": [","\t\t\t\t\t\t{","\t\t\t\t\t\t\t\"type\": \"` + conditionType + `\",","\t\t\t\t\t\t\t\"status\": \"` + conditionStatus + `\"","\t\t\t\t\t\t}","\t\t\t\t\t]","\t\t\t\t}","\t\t\t}`),","\t\t},","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,1,1,1,1,1,0,1,1,1,1,1,1,1,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,0,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0]},{"id":77,"path":"pkg/converter/convert.go","lines":["/*","Copyright 2024 The Tekton Authors","","Licensed under the Apache License, Version 2.0 (the \"License\");","you may not use this file except in compliance with the License.","You may obtain a copy of the License at","","    http://www.apache.org/licenses/LICENSE-2.0","","Unless required by applicable law or agreed to in writing, software","distributed under the License is distributed on an \"AS IS\" BASIS,","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","See the License for the specific language governing permissions and","limitations under the License.","*/","","// Package converter provides utilities for converting between Result types.","package converter","","import (","\t\"context\"","\t\"encoding/json\"","","\t\"github.com/google/martian/v3/log\"","\tv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1beta1\"","\t\"github.com/tektoncd/results/pkg/api/server/db\"","\t\"github.com/tektoncd/results/pkg/api/server/db/errors\"","\t\"go.uber.org/zap\"","\t\"gorm.io/gorm\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"knative.dev/pkg/logging\"",")","","// New provides an instance of converter which converts v1beta1 to v1 API in db","// revive:disable:unexported-return","func New(logger *zap.SugaredLogger, db *gorm.DB, limit int) *convertor {","\treturn \u0026convertor{","\t\tdb:     db,","\t\tlogger: logger,","\t\tlimit:  limit,","\t}","}","","type convertor struct {","\tlogger *zap.SugaredLogger","\tdb     *gorm.DB","\tlimit  int","}","","func (c *convertor) Start(ctx context.Context) {","\tc.convert(ctx, \"tekton.dev/v1beta1.TaskRun\")","\tc.convert(ctx, \"tekton.dev/v1beta1.PipelineRun\")","}","","func (c *convertor) convert(ctx context.Context, recType string) {","\tlogger := logging.FromContext(ctx)","\tvar conv func(ctx context.Context, record *db.Record) error","\tswitch recType {","\tcase \"tekton.dev/v1beta1.TaskRun\":","\t\tconv = c.convertTR","\tcase \"tekton.dev/v1beta1.PipelineRun\":","\t\tconv = c.convertPR","\tdefault:","\t\tlogger.Error(\"Incorrect Type of record for conversion\")","\t}","\tpending := true","\tfor pending {","\t\trecords, err := c.getRecords(ctx, recType)","\t\tif err != nil {","\t\t\tlogger.Errorf(\"failed to fetch records\", err)","\t\t\tcontinue","\t\t}","\t\tif len(records) == 0 {","\t\t\tlog.Infof(\"No %s in db\", recType)","\t\t\tpending = false","\t\t}","\t\tfor i := range records {","\t\t\terr := conv(ctx, \u0026records[i])","\t\t\tif err != nil {","\t\t\t\tlogger.Errorf(\"failed to convert record name:%s id: %s  err: %v\", records[i].Name, records[i].ID, err.Error())","\t\t\t}","\t\t}","\t\terr = c.setRecords(ctx, records)","\t\tif err != nil {","\t\t\tlogger.Errorf(\"failed to set records %v\", err.Error())","\t\t\tcontinue","\t\t}","\t}","}","","func (c *convertor) convertTR(ctx context.Context, record *db.Record) error {","\tvar tr v1beta1.TaskRun //nolint:staticcheck","\terr := json.Unmarshal(record.Data, \u0026tr)","\tif err != nil {","\t\treturn err","\t}","","\ttrV1 := \u0026v1.TaskRun{","\t\tTypeMeta: metav1.TypeMeta{","\t\t\tAPIVersion: \"tekton.dev/v1\",","\t\t\tKind:       \"TaskRun\"},","\t}","","\tif err := tr.ConvertTo(ctx, trV1); err != nil {","\t\treturn err","\t}","","\tdata, _ := json.Marshal(trV1)","\trecord.Data = data","\trecord.Type = \"tekton.dev/v1.TaskRun\"","\treturn nil","}","","func (c *convertor) convertPR(ctx context.Context, record *db.Record) error {","\tvar pr v1beta1.PipelineRun //nolint:staticcheck","\terr := json.Unmarshal(record.Data, \u0026pr)","\tif err != nil {","\t\treturn err","\t}","","\ttrV1 := \u0026v1.PipelineRun{","\t\tTypeMeta: metav1.TypeMeta{","\t\t\tAPIVersion: \"tekton.dev/v1\",","\t\t\tKind:       \"PipelineRun\"},","\t}","","\tif err := pr.ConvertTo(ctx, trV1); err != nil {","\t\treturn err","\t}","","\tdata, _ := json.Marshal(trV1)","\trecord.Data = data","\trecord.Type = \"tekton.dev/v1.PipelineRun\"","\treturn nil","}","","func (c *convertor) getRecords(ctx context.Context, recType string) ([]db.Record, error) {","\ttxn := c.db.WithContext(ctx)","\trecords := []db.Record{}","\tq := txn.Limit(c.limit).Find(\u0026records, \u0026db.Record{Type: recType})","\tif err := errors.Wrap(q.Error); err != nil {","\t\treturn records, err","\t}","\treturn records, nil","}","","func (c *convertor) setRecords(ctx context.Context, records []db.Record) error {","\tvar err error","\ttxn := c.db.WithContext(ctx)","\ttransaction := txn.Begin()","\tfor i := range records {","\t\ttransaction.Model(\u0026records[i]).Updates(db.Record{Data: records[i].Data, Type: records[i].Type})","\t}","\terr = transaction.Commit().Error","\treturn err","","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,0,0,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,0,0]},{"id":78,"path":"pkg/internal/jsonutil/jsonutil.go","lines":["// Package jsonutil provides JSON utility functions for testing.","package jsonutil","","import (","\t\"encoding/json\"","\t\"testing\"",")","","// AnyBytes returns the marshalled bytes of an Any proto wrapping the given","// message, or causes the test to fail.","func AnyBytes(t testing.TB, v any) []byte {","\tt.Helper()","\tb, err := json.Marshal(v)","\tif err != nil {","\t\tt.Fatalf(\"error marshalling Any proto: %v\", err)","\t}","\treturn b","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0]},{"id":79,"path":"pkg/internal/protoutil/protoutil.go","lines":["// Copyright 2020 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","// Package protoutil provides utilities for manipulating protos in tests.","package protoutil","","import (","\t\"testing\"","","\t\"github.com/google/go-cmp/cmp\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\tfbpb \"google.golang.org/genproto/googleapis/api/annotations\"","\t\"google.golang.org/protobuf/proto\"","\t\"google.golang.org/protobuf/reflect/protoreflect\"","\t\"google.golang.org/protobuf/testing/protocmp\"","\t\"google.golang.org/protobuf/types/descriptorpb\"","\t\"google.golang.org/protobuf/types/known/anypb\"",")","","// Any wraps a proto message in an Any proto, or causes the test to fail.","func Any(t testing.TB, m proto.Message) *anypb.Any {","\tt.Helper()","\ta, err := anypb.New(m)","\tif err != nil {","\t\tt.Fatalf(\"error wrapping Any proto: %v\", err)","\t}","\treturn a","}","","// AnyBytes returns the marshalled bytes of an Any proto wrapping the given","// message, or causes the test to fail.","func AnyBytes(t testing.TB, m proto.Message) []byte {","\tt.Helper()","\tb, err := proto.Marshal(Any(t, m))","\tif err != nil {","\t\tt.Fatalf(\"error marshalling Any proto: %v\", err)","\t}","\treturn b","}","","// ClearOutputOnly clears any proto fields marked as OUTPUT_ONLY.","func ClearOutputOnly(pb proto.Message) {","\tm := pb.ProtoReflect()","\tm.Range(func(fd protoreflect.FieldDescriptor, _ protoreflect.Value) bool {","\t\topts := fd.Options().(*descriptorpb.FieldOptions)","\t\tfor _, b := range proto.GetExtension(opts, fbpb.E_FieldBehavior).([]fbpb.FieldBehavior) {","\t\t\tif b == fbpb.FieldBehavior_OUTPUT_ONLY {","\t\t\t\tm.Clear(fd)","\t\t\t}","\t\t}","\t\treturn true","\t})","}","","// IgnoreResultOutputOnly ignores all fields marked OUTPUT_ONLY during cmp","// comparisons.","func IgnoreResultOutputOnly() cmp.Option {","\t// We might be able to something fancy with protocmp / cmp to filter","\t// by the actual extension value, but for now this is straightforward","\t// and works.","\treturn protocmp.IgnoreFields(\u0026pb.Result{}, \"update_time\", \"updated_time\", \"etag\", \"uid\", \"id\", \"create_time\", \"created_time\")","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,0,0,0,2,2,2,2,2,2,2,2,0,2,0,0,0,0,0,1,1,1,1,1,1]},{"id":80,"path":"pkg/logs/writer.go","lines":["// Package logs provides log writing utilities.","package logs","","import (","\t\"bytes\"","","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"google.golang.org/genproto/googleapis/api/httpbody\"",")","","const (","\t// DefaultBufferSize is the default buffer size. This based on the recommended","\t// gRPC message size for streamed content, which ranges from 16 to 64 KiB. Choosing 32 KiB as a","\t// middle ground between the two.","\tDefaultBufferSize = 64 * 1024",")","","// LogSender is an interface that defines the contract for sending log data.","type LogSender interface {","\tSend(*pb.Log) error","}","","// HTTPSender is an interface that defines the contract for sending log data in HTTP body.","type HTTPSender interface {","\tSend(*httpbody.HttpBody) error","}","","// BufferedLog is in memory buffered log sender.","type BufferedLog struct {","\tsender any","\tname   string","\tsize   int","\tbuffer bytes.Buffer","}","","// NewBufferedWriter returns an io.Writer that writes log chunk messages to the gRPC sender for the","// named Tekton result. The chunk size determines the maximum size of a single sent message - if","// less than zero, this defaults to DefaultBufferSize.","func NewBufferedWriter(sender LogSender, name string, size int) *BufferedLog {","\tif size \u003c 1 {","\t\tsize = DefaultBufferSize","\t}","","\treturn \u0026BufferedLog{","\t\tsender: sender,","\t\tname:   name,","\t\tsize:   size,","\t\tbuffer: *bytes.NewBuffer(make([]byte, 0)),","\t}","}","","// NewBufferedHTTPWriter returns an io.Writer that writes log chunk messages to the gRPC sender for the","// named Tekton result. The chunk size determines the maximum size of a single sent message - if","// less than zero, this defaults to DefaultBufferSize.","func NewBufferedHTTPWriter(sender HTTPSender, name string, size int) *BufferedLog {","\tif size \u003c 1 {","\t\tsize = DefaultBufferSize","\t}","","\treturn \u0026BufferedLog{","\t\tsender: sender,","\t\tname:   name,","\t\tsize:   size,","\t\tbuffer: *bytes.NewBuffer(make([]byte, 0)),","\t}","}","","// Write sends bytes to the buffer and/or consumer (e.g., gRPC stream).","// This method combines the bytes from the buffer with a new portion of p bytes in memory.","// Bytes larger than the buffer size will be truncated and sent to the consumer,","// while the remaining bytes will be stored in the buffer.","func (w *BufferedLog) Write(p []byte) (n int, err error) {","\tallBts := w.buffer.Bytes()","\tallBts = append(allBts, p...)","","\tbtsLength := len(allBts)","\tremainBytes := btsLength % w.size","","\tamountChunks := (btsLength - remainBytes) / w.size","","\tfor i := 0; i \u003c amountChunks; i++ {","\t\toffSet := i * w.size","\t\t_, err = w.sendBytes(allBts[offSet : offSet+w.size])","\t\tif err != nil {","\t\t\treturn 0, err","\t\t}","\t}","","\tvar b []byte","\tif remainBytes \u003e 0 {","\t\tb = allBts[(amountChunks * w.size):]","\t}","","\tw.buffer.Reset()","","\tif _, err = w.buffer.Write(b); err != nil {","\t\treturn 0, err","\t}","","\treturn len(p), err","}","","// Flush sends all remaining bytes in the buffer to consumer.","func (w *BufferedLog) Flush() (int, error) {","\tif len(w.buffer.Bytes()) \u003e 0 {","\t\treturn w.sendBytes(w.buffer.Bytes())","\t}","\treturn 0, nil","}","","// sendBytes sends the provided byte array over gRPC.","func (w *BufferedLog) sendBytes(p []byte) (int, error) {","\tvar err error","\tswitch t := w.sender.(type) {","\tcase HTTPSender:","\t\terr = t.Send(\u0026httpbody.HttpBody{","\t\t\tContentType: \"text/plain\",","\t\t\tData:        p,","\t\t})","\tcase LogSender:","\t\terr = t.Send(\u0026pb.Log{","\t\t\tName: w.name,","\t\t\tData: p,","\t\t})","\t}","\tif err != nil {","\t\treturn 0, err","\t}","\treturn len(p), nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,1,1,0,2,2,2,2,2,2,0,0,0,0,0,2,2,1,1,0,2,2,2,2,2,2,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,0,2,2,2,2,0,2,2,2,1,1,0,2,0,0,0,2,2,2,2,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,1,1,2,0]},{"id":81,"path":"pkg/metrics/metrics.go","lines":["// Package metrics provides unified metrics recording for Tekton Results.","// It includes metrics for tracking runs not stored and storage latency for both PipelineRuns and TaskRuns.","package metrics","","import (","\t\"context\"","\t\"fmt\"","\t\"sync\"","\t\"time\"","","\t\"github.com/jonboulle/clockwork\"","","\tpipelinev1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"github.com/tektoncd/results/pkg/apis/config\"","\t\"go.opencensus.io/stats\"","\t\"go.opencensus.io/stats/view\"","\t\"go.opencensus.io/tag\"","\t\"go.uber.org/zap\"","\t\"knative.dev/pkg/metrics\"",")","","var (","\tregisterMutex sync.Mutex","","\tregisteredAt       *time.Time","\trunsNotStoredCount = stats.Int64(\"runs_not_stored_count\", \"total number of runs which were deleted without being stored\", stats.UnitDimensionless)","\trunsNotStoredView  *view.View","","\t// Storage latency metric (shared)","\trunStorageLatency     = stats.Float64(\"run_storage_latency_seconds\", \"time from run completion to successful storage\", stats.UnitSeconds)","\trunStorageLatencyView *view.View","","\t// Common tags","\tnamespaceTag = tag.MustNewKey(\"namespace\")","\tkindTag      = tag.MustNewKey(\"kind\")",")","","// Recorder is used to record metrics for both PipelineRuns and TaskRuns","type Recorder struct {","\tclock clockwork.Clock","}","","// NewRecorder creates a new metrics recorder instance","func NewRecorder() *Recorder {","\treturn \u0026Recorder{clock: clockwork.NewRealClock()}","}","","func registerViews(logger *zap.SugaredLogger) error {","\trunsNotStoredView = \u0026view.View{","\t\tDescription: runsNotStoredCount.Description(),","\t\tTagKeys:     []tag.Key{kindTag, namespaceTag},","\t\tMeasure:     runsNotStoredCount,","\t\tAggregation: view.Count(),","\t}","","\t// Storage latency view","\trunStorageLatencyView = \u0026view.View{","\t\tDescription: runStorageLatency.Description(),","\t\tTagKeys:     []tag.Key{kindTag, namespaceTag},","\t\tMeasure:     runStorageLatency,","\t\tAggregation: view.Distribution(0.1, 0.5, 1, 2, 5, 10, 30, 60, 120, 300, 600, 1800),","\t}","","\tlogger.Debug(\"registering shared metrics views\")","\treturn view.Register(runsNotStoredView, runStorageLatencyView)","}","","func unregisterViews(logger *zap.SugaredLogger) {","\tlogger.Debug(\"unregistering shared metrics views\")","\tvar viewsToUnregister []*view.View","\tif runsNotStoredView != nil {","\t\tviewsToUnregister = append(viewsToUnregister, runsNotStoredView)","\t}","\tif runStorageLatencyView != nil {","\t\tviewsToUnregister = append(viewsToUnregister, runStorageLatencyView)","\t}","\tview.Unregister(viewsToUnregister...)","\tregisteredAt = nil","}","","// IdempotentRegisterViews ensures all shared views are registered exactly once.","// If views are already registered, it does nothing.","// If views were previously registered but may be stale, it unregisters and re-registers them.","func IdempotentRegisterViews(logger *zap.SugaredLogger) {","\tregisterMutex.Lock()","\tdefer registerMutex.Unlock()","\tif registeredAt != nil {","\t\treturn","\t}","\tunregisterViews(logger)","\tif err := registerViews(logger); err != nil {","\t\tlogger.Errorf(\"Failed to register View %v \", err)","\t} else {","\t\tnow := time.Now()","\t\tregisteredAt = \u0026now","\t}","}","","// CountRunNotStored records a run that was not stored due to deletion or timeout","func CountRunNotStored(ctx context.Context, namespace, kind string) error {","\tctx, err := tag.New(","\t\tctx,","\t\ttag.Insert(kindTag, kind),","\t\ttag.Insert(namespaceTag, namespace),","\t)","\tif err != nil {","\t\treturn fmt.Errorf(\"unable to create tags for %s metric: %w\", runsNotStoredCount.Name(), err)","\t}","","\tmetrics.Record(ctx, runsNotStoredCount.M(1))","\treturn nil","}","","// OnStore returns a function that checks if metrics are configured for a config.Store, and registers it if so","func OnStore(logger *zap.SugaredLogger) func(name string, value any) {","\treturn func(name string, value any) {","\t\tif name != config.GetMetricsConfigName() {","\t\t\treturn","\t\t}","\t\t_, ok := value.(*config.Metrics)","\t\tif !ok {","\t\t\tlogger.Error(\"Failed to do type insertion for extracting metrics config\")","\t\t\treturn","\t\t}","\t\t// For shared metrics, we use idempotent registration","\t\tIdempotentRegisterViews(logger)","\t}","}","","// RecordStorageLatency records the storage latency metric for both PipelineRuns and TaskRuns","func (r *Recorder) RecordStorageLatency(ctx context.Context, object interface{}) error {","\tvar (","\t\tcompletionTime *time.Time","\t\tnamespace      string","\t\tkind           string","\t)","","\t// Extract completion time and metadata using type switch","\tswitch o := object.(type) {","\tcase *pipelinev1.PipelineRun:","\t\tif o.Status.CompletionTime == nil {","\t\t\treturn nil","\t\t}","\t\tcompletionTime = \u0026o.Status.CompletionTime.Time","\t\tnamespace = o.Namespace","\t\tkind = \"pipelinerun\"","","\tcase *pipelinev1.TaskRun:","\t\tif o.Status.CompletionTime == nil {","\t\t\treturn nil","\t\t}","\t\tcompletionTime = \u0026o.Status.CompletionTime.Time","\t\tnamespace = o.Namespace","\t\tkind = \"taskrun\"","","\tdefault:","\t\treturn fmt.Errorf(\"unsupported object type: %T\", object)","\t}","","\t// Calculate latency from completion to now","\tnow := r.clock.Now()","\tlatency := now.Sub(*completionTime)","","\t// Create tags","\ttags := []tag.Mutator{","\t\ttag.Insert(kindTag, kind),","\t\ttag.Insert(namespaceTag, namespace),","\t}","","\tctx, err := tag.New(ctx, tags...)","\tif err != nil {","\t\treturn fmt.Errorf(\"error creating tagged context: %w\", err)","\t}","","\t// Record the metric","\tmetrics.Record(ctx, runStorageLatency.M(float64(latency/time.Second)))","","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,2,2,2,2,2,2,2,1,1,0,2,2,0,0,0,1,1,1,1,1,1,1,1,1,1,0,1,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,0,1,1,0,0,0,2,2,2,2,2,2,2,2,2,2,2,1,1,0,0,2,2,2,0]},{"id":82,"path":"pkg/pipelinerunmetrics/metrics.go","lines":["// Package pipelinerunmetrics provides metrics collection for PipelineRun resources.","package pipelinerunmetrics","","import (","\t\"context\"","\t\"errors\"","\t\"time\"","","\t\"github.com/jonboulle/clockwork\"","","\tpipelinev1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"github.com/tektoncd/results/pkg/apis/config\"","\tsharedMetrics \"github.com/tektoncd/results/pkg/metrics\"","\t\"go.opencensus.io/stats\"","\t\"go.opencensus.io/stats/view\"","\t\"go.opencensus.io/tag\"","\t\"go.uber.org/zap\"","\tcorev1 \"k8s.io/api/core/v1\"","\t\"knative.dev/pkg/apis\"","\t\"knative.dev/pkg/metrics\"",")","","var (","\tprDeleteCount     = stats.Int64(\"pipelinerun_delete_count\", \"total number of deleted pipelineruns\", stats.UnitDimensionless)","\tprDeleteCountView *view.View","","\tprDeleteDuration     = stats.Float64(\"pipelinerun_delete_duration_seconds\", \"the pipelinerun deletion time in seconds\", stats.UnitSeconds)","\tprDeleteDurationView *view.View","","\tpipelineTag  = tag.MustNewKey(\"pipeline\")","\tnamespaceTag = tag.MustNewKey(\"namespace\")","\tstatusTag    = tag.MustNewKey(\"status\")",")","","// Recorder is used to actually record PipelineRun metrics","type Recorder struct {","\tclock clockwork.Clock","}","","// NewRecorder creates a new metrics recorder instance","// to log the PipelineRun related metrics","func NewRecorder() *Recorder {","\treturn \u0026Recorder{clock: clockwork.NewRealClock()}","}","","func registerView(logger *zap.SugaredLogger, cfg *config.Metrics) error {","\tvar tags []tag.Key","\tswitch cfg.PipelinerunLevel {","\tcase config.PipelinerunLevelAtPipeline:","\t\ttags = []tag.Key{pipelineTag}","\tcase config.PipelinerunLevelAtNS:","\t\ttags = []tag.Key{}","\tdefault:","\t\treturn errors.New(\"invalid config for PipelinerunLevel: \" + cfg.PipelinerunLevel)","\t}","\tprDeleteCountView = \u0026view.View{","\t\tDescription: prDeleteCount.Description(),","\t\tTagKeys:     []tag.Key{statusTag, namespaceTag},","\t\tMeasure:     prDeleteCount,","\t\tAggregation: view.Count(),","\t}","","\tvar distribution *view.Aggregation","\tswitch cfg.DurationPipelinerunType {","\tcase config.DurationPipelinerunTypeLastValue:","\t\tdistribution = view.LastValue()","\tcase config.DurationPipelinerunTypeHistogram:","\t\tdistribution = view.Distribution(10, 30, 60, 300, 900, 1800, 3600, 5400, 10800, 21600, 43200, 86400)","\t}","","\tprDeleteDurationView = \u0026view.View{","\t\tDescription: prDeleteDuration.Description(),","\t\tTagKeys:     append([]tag.Key{statusTag, namespaceTag}, tags...),","\t\tMeasure:     prDeleteDuration,","\t\tAggregation: distribution,","\t}","","\tlogger.Debug(\"registering pipelinerun metrics view\")","\treturn view.Register(prDeleteDurationView, prDeleteCountView)","}","","func unregisterView(logger *zap.SugaredLogger) {","\tlogger.Debug(\"unregistering pipelinerun metrics view\")","\tfor _, v := range []*view.View{prDeleteDurationView, prDeleteCountView} {","\t\tif v != nil {","\t\t\tview.Unregister(v)","\t\t}","\t}","}","","// MetricsOnStore returns a function that checks if metrics are configured for a config.Store, and registers it if so","func MetricsOnStore(logger *zap.SugaredLogger) func(name string,","\tvalue any) {","\treturn func(name string, value any) {","\t\tif name != config.GetMetricsConfigName() {","\t\t\treturn","\t\t}","\t\tcfg, ok := value.(*config.Metrics)","\t\tif !ok {","\t\t\tlogger.Error(\"Failed to do type insertion for extracting metrics config\")","\t\t\treturn","\t\t}","\t\tunregisterView(logger)","\t\terr := registerView(logger, cfg)","\t\tif err != nil {","\t\t\tlogger.Errorf(\"Failed to register View %v \", err)","\t\t\treturn","\t\t}","\t\tsharedMetrics.IdempotentRegisterViews(logger)","\t}","}","","// DurationAndCountDeleted counts for deleted number and records duration PipelineRuns","func (r *Recorder) DurationAndCountDeleted(ctx context.Context, cfg *config.Metrics, pr *pipelinev1.PipelineRun) error {","\tpipelineName := \"anonymous\"","\tnow := r.clock.Now()","","\tif pr.Spec.PipelineRef != nil \u0026\u0026 pr.Spec.PipelineRef.Name != \"\" {","\t\tpipelineName = pr.Spec.PipelineRef.Name","\t}","","\t// Metrics status tag meaning should be consistent with pipeline repo definition","\t// TODO(xinnjie) metrics status query function should be defined in pipeline repo, and use that function here","\tstatus := \"success\"","\tdeleteDuration := time.Duration(0)","","\tif cond := pr.Status.GetCondition(apis.ConditionSucceeded); cond.Status == corev1.ConditionFalse {","\t\tstatus = \"failed\"","\t\t// Use failedTime to compute delete duration in case of completion time being nil","\t\tfailedTime := cond.LastTransitionTime.Inner.Time","\t\tif !failedTime.After(now) {","\t\t\tdeleteDuration = now.Sub(failedTime)","\t\t}","\t\tif cond.Reason == \"Cancelled\" {","\t\t\tstatus = \"cancelled\"","\t\t}","\t}","","\tvar tags []tag.Mutator","\tif cfg.PipelinerunLevel == config.PipelinerunLevelAtPipeline {","\t\ttags = []tag.Mutator{tag.Insert(pipelineTag, pipelineName)}","\t}","\tctx, err := tag.New(ctx, append([]tag.Mutator{tag.Insert(namespaceTag, pr.Namespace), tag.Insert(statusTag, status)}, tags...)...)","\tif err != nil {","\t\treturn err","\t}","","\tif pr.Status.CompletionTime != nil \u0026\u0026 !pr.Status.CompletionTime.After(now) {","\t\tdeleteDuration = now.Sub(pr.Status.CompletionTime.Time)","\t}","","\tmetrics.Record(ctx, prDeleteCount.M(1))","\tmetrics.Record(ctx, prDeleteDuration.M(float64(deleteDuration/time.Second)))","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,2,2,2,2,2,1,1,1,1,0,2,2,2,2,2,2,2,2,2,2,2,1,1,0,0,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,2,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,1,1,0,2,2,2,0,2,2,2,0]},{"id":83,"path":"pkg/retention/config.go","lines":["/*","Copyright 2024 The Tekton Authors","","Licensed under the Apache License, Version 2.0 (the \"License\");","you may not use this file except in compliance with the License.","You may obtain a copy of the License at","","    http://www.apache.org/licenses/LICENSE-2.0","","Unless required by applicable law or agreed to in writing, software","distributed under the License is distributed on an \"AS IS\" BASIS,","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","See the License for the specific language governing permissions and","limitations under the License.","*/","","// Package retention provides data retention policy management.","package retention","","import (","\t\"log\"","\t\"sync\"","","\t\"github.com/tektoncd/results/pkg/apis/config\"","\t\"go.uber.org/zap\"","\t\"gorm.io/gorm\"","","\t\"github.com/robfig/cron/v3\"","\t\"knative.dev/pkg/controller\"","\t\"knative.dev/pkg/injection\"","\t\"knative.dev/pkg/injection/sharedmain\"","\t\"knative.dev/pkg/logging\"","\t\"knative.dev/pkg/signals\"",")","","const (","\t// ResultsRetentionPolicyAgent is the name of the logger for the retention policy agent cmd","\tResultsRetentionPolicyAgent = \"results-retention-policy-agent\"",")","","// Agent have all the information needed to run retention job","type Agent struct {","\tconfig.RetentionPolicy","","\tmutex sync.Mutex","","\tLogger *zap.SugaredLogger","","\tdb *gorm.DB","","\tcron *cron.Cron","}","","// NewAgent returns the Retention Policy Agent","func NewAgent(db *gorm.DB) (*Agent, error) {","","\tcfg := injection.ParseAndGetRESTConfigOrDie()","","\tctx := signals.NewContext()","\tctx = injection.WithConfig(ctx, cfg)","\tctx, informers := injection.Default.SetupInformers(ctx, cfg)","","\tif err := controller.StartInformers(ctx.Done(), informers...); err != nil {","\t\tlog.Fatal(\"failed to start informers:\", err)","\t}","","\tlogger := logging.FromContext(ctx)","\tctx = logging.WithLogger(ctx, logger)","","\tcmw := sharedmain.SetupConfigMapWatchOrDie(ctx, logger)","","\tagent := Agent{","\t\tLogger: logger,","\t\tdb:     db,","\t}","\tconfigStore := config.NewStore(logger.Named(\"config-store\"), agent.AgentOnStore(logger))","\tconfigStore.WatchConfigs(cmw)","","\tif err := cmw.Start(ctx.Done()); err != nil {","\t\tlogger.Fatalw(\"Failed to start configuration manager\", zap.Error(err))","\t}","\treturn \u0026agent, nil","}","","// AgentOnStore returns a function that checks if agent are configured for a config.Store, and registers it if so","func (a *Agent) AgentOnStore(logger *zap.SugaredLogger) func(name string,","\tvalue interface{}) {","\treturn func(name string, value interface{}) {","\t\tif name == config.GetRetentionPolicyConfigName() {","\t\t\tcfg, ok := value.(*config.RetentionPolicy)","\t\t\tif !ok {","\t\t\t\tlogger.Error(\"Failed to do type assertion for extracting retention policy config\")","\t\t\t\treturn","\t\t\t}","\t\t\ta.setAgentConfig(cfg)","\t\t\ta.stop()","\t\t\ta.start()","\t\t}","\t}","}","","func (a *Agent) setAgentConfig(cfg *config.RetentionPolicy) {","\ta.mutex.Lock()","\tdefer a.mutex.Unlock()","\ta.RetentionPolicy = *cfg","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1]},{"id":84,"path":"pkg/retention/job.go","lines":["/*","Copyright 2024 The Tekton Authors","","Licensed under the Apache License, Version 2.0 (the \"License\");","you may not use this file except in compliance with the License.","You may obtain a copy of the License at","","    http://www.apache.org/licenses/LICENSE-2.0","","Unless required by applicable law or agreed to in writing, software","distributed under the License is distributed on an \"AS IS\" BASIS,","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","See the License for the specific language governing permissions and","limitations under the License.","*/","","package retention","","import (","\t\"fmt\"","\t\"strings\"","\t\"time\"","","\t\"github.com/robfig/cron/v3\"","\t\"github.com/tektoncd/results/pkg/api/server/db/errors\"","\t\"github.com/tektoncd/results/pkg/apis/config\"",")","","func (a *Agent) start() {","\tc := cron.New()","\t_, err := c.AddFunc(a.RunAt, a.job)","\tif err != nil {","\t\ta.Logger.Fatalf(\"failed to add function for cronjob %s\", err.Error())","\t}","\ta.cron = c","\ta.cron.Start()","}","","func (a *Agent) stop() {","\tif a.cron == nil {","\t\treturn","\t}","\ta.cron.Stop()","}","","func (a *Agent) job() {","\ta.Logger.Infof(\"retention job started at: %s, retention policy: %+v\", time.Now().String(), a.RetentionPolicy)","","\tcaseStatement, err := buildCaseStatement(a.Policies, a.DefaultRetention)","\tif err != nil {","\t\ta.Logger.Errorf(\"failed to build case statement: %v\", err)","\t\treturn","\t}","","\t// First, clean up PipelineRun results.","\ta.cleanupResults(caseStatement, \"tekton.dev/v1.PipelineRun\")","","\t// Second, clean up top-level TaskRun results.","\ta.cleanupResults(caseStatement, \"tekton.dev/v1.TaskRun\")","","\ta.Logger.Infof(\"retention job finished at: %s\", time.Now().String())","}","","func (a *Agent) cleanupResults(caseStatement, recordType string) {","\tvar additionalFilter string","\tif recordType == \"tekton.dev/v1.TaskRun\" {","\t\tadditionalFilter = `AND NOT EXISTS (","\t\t\tSELECT 1 FROM records pr","\t\t\tWHERE pr.type = 'tekton.dev/v1.PipelineRun'","\t\t\t  AND pr.parent = r.parent","\t\t\t  AND pr.result_id = r.result_id","\t\t)`","\t}","","\tdeleteQuery := fmt.Sprintf(`","        DELETE FROM results","        WHERE id IN (","            SELECT result_id FROM (","                SELECT","                    r.result_id,","                    r.updated_time,","                    %s AS expiration_time","                FROM records r","                WHERE r.type = '%s' %s","            ) AS subquery","            WHERE updated_time \u003c expiration_time","        )","    `, caseStatement, recordType, additionalFilter)","","\tif err := errors.Wrap(a.db.Exec(deleteQuery).Error); err != nil {","\t\ta.Logger.Errorf(\"failed to delete results for record type %s: %s\", recordType, err.Error())","\t}","}","","func buildCaseStatement(policies []config.Policy, defaultRetention time.Duration) (string, error) {","\tif len(policies) == 0 {","\t\treturn fmt.Sprintf(\"NOW() - INTERVAL '%f seconds'\", defaultRetention.Seconds()), nil","\t}","\tvar caseClauses []string","\tfor _, policy := range policies {","\t\twhereClause, err := buildWhereClause(policy.Selector)","\t\tif err != nil {","\t\t\treturn \"\", err","\t\t}","\t\tretentionDuration, err := config.ParseDuration(policy.Retention)","\t\tif err != nil {","\t\t\treturn \"\", err","\t\t}","\t\tcaseClauses = append(caseClauses, fmt.Sprintf(\"WHEN %s THEN NOW() - INTERVAL '%f seconds'\", whereClause, retentionDuration.Seconds()))","\t}","","\tdefaultRetentionSeconds := defaultRetention.Seconds()","\tcaseClauses = append(caseClauses, fmt.Sprintf(\"ELSE NOW() - INTERVAL '%f seconds'\", defaultRetentionSeconds))","","\treturn fmt.Sprintf(\"CASE %s END\", strings.Join(caseClauses, \" \")), nil","}","","func buildWhereClause(selector config.Selector) (string, error) {","\tvar conditions []string","\tif len(selector.MatchNamespaces) \u003e 0 {","\t\tconditions = append(conditions, fmt.Sprintf(\"parent IN (%s)\", quoteAndJoin(selector.MatchNamespaces)))","\t}","\tfor key, values := range selector.MatchLabels {","\t\tconditions = append(conditions, fmt.Sprintf(\"data-\u003e'metadata'-\u003e'labels'-\u003e\u003e'%s' IN (%s)\", key, quoteAndJoin(values)))","\t}","\tfor key, values := range selector.MatchAnnotations {","\t\tconditions = append(conditions, fmt.Sprintf(\"data-\u003e'metadata'-\u003e'annotations'-\u003e\u003e'%s' IN (%s)\", key, quoteAndJoin(values)))","\t}","\tif len(selector.MatchStatuses) \u003e 0 {","\t\tconditions = append(conditions, fmt.Sprintf(\"data-\u003e'status'-\u003e'conditions'-\u003e0-\u003e\u003e'reason' IN (%s)\", quoteAndJoin(selector.MatchStatuses)))","\t}","\tif len(conditions) == 0 {","\t\treturn \"1=1\", nil // No specific selectors, so match all.","\t}","\treturn strings.Join(conditions, \" AND \"), nil","}","","func quoteAndJoin(items []string) string {","\tquoted := make([]string, len(items))","\tfor i, item := range items {","\t\tquoted[i] = fmt.Sprintf(\"'%s'\", item)","\t}","\treturn strings.Join(quoted, \",\")","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,0,0,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,0,2,2,2,2,2,2,2,2,1,1,2,2,1,1,2,0,0,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,0]},{"id":85,"path":"pkg/taskrunmetrics/metrics.go","lines":["// Package taskrunmetrics provides metrics collection for TaskRun resources.","package taskrunmetrics","","import (","\t\"context\"","\t\"errors\"","\t\"time\"","","\t\"github.com/jonboulle/clockwork\"","","\t\"github.com/tektoncd/pipeline/pkg/apis/pipeline\"","\tpipelinev1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"github.com/tektoncd/results/pkg/apis/config\"","\tsharedMetrics \"github.com/tektoncd/results/pkg/metrics\"","\t\"go.opencensus.io/stats\"","\t\"go.opencensus.io/stats/view\"","\t\"go.opencensus.io/tag\"","\t\"go.uber.org/zap\"","\tcorev1 \"k8s.io/api/core/v1\"","\t\"knative.dev/pkg/apis\"","\t\"knative.dev/pkg/metrics\"",")","","var (","\ttrDeleteCount        = stats.Int64(\"taskrun_delete_count\", \"total number of deleted taskruns\", stats.UnitDimensionless)","\ttrDeleteCountView    *view.View","\ttrDeleteDuration     = stats.Float64(\"taskrun_delete_duration_seconds\", \"the taskrun deletion time in seconds\", stats.UnitSeconds)","\ttrDeleteDurationView *view.View","","\tpipelineTag  = tag.MustNewKey(\"pipeline\")","\ttaskTag      = tag.MustNewKey(\"task\")","\tnamespaceTag = tag.MustNewKey(\"namespace\")","\tstatusTag    = tag.MustNewKey(\"status\")",")","","// Recorder is used to actually record TaskRun metrics","type Recorder struct {","\tclock clockwork.Clock","}","","// NewRecorder creates a new metrics recorder instance","// to log the TaskRun related metrics","func NewRecorder() *Recorder {","\treturn \u0026Recorder{clock: clockwork.NewRealClock()}","}","","func registerViews(logger *zap.SugaredLogger, cfg *config.Metrics) error {","\tvar tags []tag.Key","\tswitch cfg.TaskrunLevel {","\tcase config.TaskrunLevelAtTask:","\t\ttags = []tag.Key{taskTag}","\tcase config.TaskrunLevelAtNS:","\t\ttags = []tag.Key{}","\tdefault:","\t\treturn errors.New(\"invalid config for TaskrunLevel: \" + cfg.TaskrunLevel)","\t}","","\tswitch cfg.PipelinerunLevel {","\tcase config.PipelinerunLevelAtPipeline:","\t\ttags = append(tags, pipelineTag)","\tcase config.PipelinerunLevelAtNS:","\tdefault:","\t\treturn errors.New(\"invalid config for PipelinerunLevel: \" + cfg.PipelinerunLevel)","\t}","","\ttrDeleteCountView = \u0026view.View{","\t\tDescription: trDeleteCount.Description(),","\t\tTagKeys:     []tag.Key{statusTag, namespaceTag},","\t\tMeasure:     trDeleteCount,","\t\tAggregation: view.Count(),","\t}","","\tvar distribution *view.Aggregation","\tswitch cfg.DurationPipelinerunType {","\tcase config.DurationTaskrunTypeLastValue:","\t\tdistribution = view.LastValue()","\tcase config.DurationTaskrunTypeHistogram:","\t\tdistribution = view.Distribution(10, 30, 60, 300, 900, 1800, 3600, 5400, 10800, 21600, 43200, 86400)","\t}","","\ttrDeleteDurationView = \u0026view.View{","\t\tDescription: trDeleteDuration.Description(),","\t\tTagKeys:     append([]tag.Key{statusTag, namespaceTag}, tags...),","\t\tMeasure:     trDeleteDuration,","\t\tAggregation: distribution,","\t}","","\tlogger.Debug(\"registering taskrun metrics view\")","\treturn view.Register(trDeleteDurationView, trDeleteCountView)","}","","func unregisterViews(logger *zap.SugaredLogger) {","\tlogger.Debug(\"unregistering taskrun metrics view\")","\tfor _, v := range []*view.View{trDeleteDurationView, trDeleteCountView} {","\t\tif v != nil {","\t\t\tview.Unregister(v)","\t\t}","\t}","}","","// MetricsOnStore returns a function that checks if metrics are configured for a config.Store, and registers it if so","func MetricsOnStore(logger *zap.SugaredLogger) func(name string, value any) {","\treturn func(name string, value any) {","\t\tif name != config.GetMetricsConfigName() {","\t\t\treturn","\t\t}","\t\tcfg, ok := value.(*config.Metrics)","\t\tif !ok {","\t\t\tlogger.Error(\"Failed to do type insertion for extracting metrics config\")","\t\t\treturn","\t\t}","\t\tunregisterViews(logger)","\t\terr := registerViews(logger, cfg)","\t\tif err != nil {","\t\t\tlogger.Errorf(\"Failed to register View %v \", err)","\t\t\treturn","\t\t}","\t\tsharedMetrics.IdempotentRegisterViews(logger)","\t}","}","","// DurationAndCountDeleted counts deleted number and record duration for TaskRuns","func (r *Recorder) DurationAndCountDeleted(ctx context.Context, cfg *config.Metrics, tr *pipelinev1.TaskRun) error {","\ttaskName := \"anonymous\"","\tpipelineName := \"anonymous\"","\tnow := r.clock.Now()","","\tif tr.Spec.TaskRef != nil \u0026\u0026 tr.Spec.TaskRef.Name != \"\" {","\t\ttaskName = tr.Spec.TaskRef.Name","\t}","","\tstatus := \"success\"","\tdeleteDuration := time.Duration(0)","\tif cond := tr.Status.GetCondition(apis.ConditionSucceeded); cond.Status == corev1.ConditionFalse {","\t\tstatus = \"failed\"","","\t\t// Use failedTime to compute delete duration in case of completion time being nil","\t\tfailedTime := cond.LastTransitionTime.Inner.Time","\t\tif !failedTime.After(now) {","\t\t\tdeleteDuration = now.Sub(failedTime)","\t\t}","\t\tif cond.Reason == pipelinev1.TaskRunSpecStatusCancelled {","\t\t\tstatus = \"cancelled\"","\t\t}","\t}","","\ttags := []tag.Mutator{tag.Insert(namespaceTag, tr.Namespace), tag.Insert(statusTag, status)}","","\tif ok, pipeline, _ := isPartOfPipeline(tr); ok {","\t\tpipelineName = pipeline","\t}","","\ttags = append(tags, r.insertPipelineTag(cfg, pipelineName)...)","\ttags = append(tags, r.insertTaskTag(cfg, taskName)...)","","\tctx, err := tag.New(ctx, tags...)","\tif err != nil {","\t\treturn err","\t}","","\tif tr.Status.CompletionTime != nil \u0026\u0026 !tr.Status.CompletionTime.After(now) {","\t\tdeleteDuration = now.Sub(tr.Status.CompletionTime.Time)","\t}","\tmetrics.Record(ctx, trDeleteCount.M(1))","\tmetrics.Record(ctx, trDeleteDuration.M(float64(deleteDuration/time.Second)))","\treturn nil","}","","func (r *Recorder) insertPipelineTag(cfg *config.Metrics, pipeline string) []tag.Mutator {","\tvar tags []tag.Mutator","\tswitch cfg.PipelinerunLevel {","\tcase config.PipelinerunLevelAtPipeline:","\t\ttags = []tag.Mutator{tag.Insert(pipelineTag, pipeline)}","\tcase config.PipelinerunLevelAtNS:","\t}","\treturn tags","}","","func (r *Recorder) insertTaskTag(cfg *config.Metrics, task string) []tag.Mutator {","\tvar tags []tag.Mutator","\tswitch cfg.TaskrunLevel {","\tcase config.TaskrunLevelAtTask:","\t\ttags = []tag.Mutator{tag.Insert(taskTag, task)}","\tcase config.TaskrunLevelAtNS:","\t}","\treturn tags","}","","// IsPartOfPipeline return true if TaskRun is a part of a Pipeline.","// It also returns the name of Pipeline and PipelineRun","func isPartOfPipeline(tr *pipelinev1.TaskRun) (bool, string, string) {","\tpipelineLabel, hasPipelineLabel := tr.Labels[pipeline.PipelineLabelKey]","\tpipelineRunLabel, hasPipelineRunLabel := tr.Labels[pipeline.PipelineRunLabelKey]","","\tif hasPipelineLabel \u0026\u0026 hasPipelineRunLabel {","\t\treturn true, pipelineLabel, pipelineRunLabel","\t}","","\treturn false, \"\", \"\"","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,2,2,2,2,2,1,1,1,1,0,0,2,2,2,0,1,1,0,0,2,2,2,2,2,2,2,2,2,2,2,1,1,0,0,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,1,1,0,0,2,2,2,2,2,0,2,2,2,2,2,1,1,0,2,2,2,2,2,2,0,0,2,2,2,2,2,0,0,2,0,0,2,2,2,2,2,0,0,2,0,0,0,0,2,2,2,2,2,2,2,0,2,0]},{"id":86,"path":"pkg/watcher/convert/convert.go","lines":["/*","Copyright 2020 The Tekton Authors","","Licensed under the Apache License, Version 2.0 (the \"License\");","you may not use this file except in compliance with the License.","You may obtain a copy of the License at","","    http://www.apache.org/licenses/LICENSE-2.0","","Unless required by applicable law or agreed to in writing, software","distributed under the License is distributed on an \"AS IS\" BASIS,","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","See the License for the specific language governing permissions and","limitations under the License.","*/","","// Package convert provides a method to convert Pipeline v1 API objects to Results","// API proto objects.","package convert","","import (","\t\"encoding/json\"","\t\"fmt\"","","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/record\"","\t\"github.com/tektoncd/results/pkg/apis/v1alpha3\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/apimachinery/pkg/types\"","","\tpipelineV1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","","\t\"github.com/tektoncd/pipeline/pkg/client/clientset/versioned/scheme\"","\t\"github.com/tektoncd/pipeline/pkg/pod\"","\trpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"k8s.io/apimachinery/pkg/runtime\"","\t\"k8s.io/apimachinery/pkg/runtime/schema\"","\t\"knative.dev/pkg/apis\"",")","","// ToProto converts k8s object to proto object.","func ToProto(in runtime.Object) (*rpb.Any, error) {","\tif in == nil {","\t\treturn nil, nil","\t}","","\tb, err := json.Marshal(in)","\tif err != nil {","\t\treturn nil, err","\t}","","\treturn \u0026rpb.Any{","\t\tType:  TypeName(in),","\t\tValue: b,","\t}, nil","}","","// ToLogProto converts k8s object to log proto object.","func ToLogProto(in metav1.Object, kind, name string) (*rpb.Any, error) {","\tif in == nil {","\t\treturn nil, nil","\t}","\t_, _, uid, err := record.ParseName(name)","\tif err != nil {","\t\treturn nil, err","\t}","\tlog := \u0026v1alpha3.Log{","\t\tObjectMeta: metav1.ObjectMeta{","\t\t\tNamespace: in.GetNamespace(),","\t\t\tName:      fmt.Sprintf(\"%s-log\", in.GetName()),","\t\t\tUID:       types.UID(uid),","\t\t},","\t\tSpec: v1alpha3.LogSpec{","\t\t\tResource: v1alpha3.Resource{","\t\t\t\tKind:      kind,","\t\t\t\tNamespace: in.GetNamespace(),","\t\t\t\tName:      in.GetName(),","\t\t\t\tUID:       in.GetUID(),","\t\t\t},","\t\t},","\t}","\tlog.Default()","\tb, err := json.Marshal(log)","\tif err != nil {","\t\treturn nil, err","\t}","\treturn \u0026rpb.Any{","\t\tType:  v1alpha3.LogRecordType,","\t\tValue: b,","\t}, nil","}","","// TypeName returns a string representation of type Object type.","// We do not know of any formalized spec for identifying objects across API","// versions. Standard GVK string formatting does not produce something that's","// payload friendly (i.e. includes spaces).","// To get around this we append API Version + Kind","// (e.g. tekton.dev/v1.TaskRun).","func TypeName(in runtime.Object) string {","\tgvk := in.GetObjectKind().GroupVersionKind()","\tif gvk.Empty() {","\t\t// GVK not explicitly set in the object, fall back to scheme-based","\t\t// lookup.","\t\tvar err error","\t\tgvk, err = InferGVK(in)","\t\t// Avoid returning back \".\" if the GVK doesn't contain any info.","\t\tif err != nil || gvk.Empty() {","\t\t\treturn \"\"","\t\t}","\t}","\tv, k := gvk.ToAPIVersionAndKind()","\treturn fmt.Sprintf(\"%s.%s\", v, k)","}","","// InferGVK infers the GroupVersionKind from the Object via schemes. Currently","// only the Tekton scheme is supported.","func InferGVK(o runtime.Object) (schema.GroupVersionKind, error) {","\tgvks, _, err := scheme.Scheme.ObjectKinds(o)","\tif err != nil {","\t\treturn schema.GroupVersionKind{}, err","\t}","\t// This could potentially match a few different ones (not exactly sure","\t// when this would happen), but generally shouldn't because we're using","\t// the direct types from the Tekton package.","\tif len(gvks) == 0 {","\t\treturn schema.GroupVersionKind{}, fmt.Errorf(\"could not determine GroupVersionKind for object\")","\t}","\treturn gvks[0], nil","}","","// Status maps a Run condition to a general Record status.","func Status(ca apis.ConditionAccessor) rpb.RecordSummary_Status {","\tc := ca.GetCondition(apis.ConditionSucceeded)","\tif c == nil {","\t\treturn rpb.RecordSummary_UNKNOWN","\t}","","\tswitch pipelineV1.TaskRunReason(c.Reason) {","\tcase pipelineV1.TaskRunReasonSuccessful:","\t\treturn rpb.RecordSummary_SUCCESS","\tcase pipelineV1.TaskRunReasonFailed:","\t\treturn rpb.RecordSummary_FAILURE","\tcase pipelineV1.TaskRunReasonTimedOut:","\t\treturn rpb.RecordSummary_TIMEOUT","\tcase pipelineV1.TaskRunReasonCancelled:","\t\treturn rpb.RecordSummary_CANCELLED","\tcase pipelineV1.TaskRunReasonRunning, pipelineV1.TaskRunReasonStarted:","\t\treturn rpb.RecordSummary_UNKNOWN","\t}","","\tswitch pipelineV1.PipelineRunReason(c.Reason) {","\tcase pipelineV1.PipelineRunReasonSuccessful, pipelineV1.PipelineRunReasonCompleted:","\t\treturn rpb.RecordSummary_SUCCESS","\tcase pipelineV1.PipelineRunReasonFailed:","\t\treturn rpb.RecordSummary_FAILURE","\tcase pipelineV1.PipelineRunReasonTimedOut:","\t\treturn rpb.RecordSummary_TIMEOUT","\tcase pipelineV1.PipelineRunReasonCancelled:","\t\treturn rpb.RecordSummary_CANCELLED","\tcase pipelineV1.PipelineRunReasonRunning, pipelineV1.PipelineRunReasonStarted, pipelineV1.PipelineRunReasonPending, pipelineV1.PipelineRunReasonStopping, pipelineV1.PipelineRunReasonCancelledRunningFinally, pipelineV1.PipelineRunReasonStoppedRunningFinally:","\t\treturn rpb.RecordSummary_UNKNOWN","\t}","","\tswitch c.Reason {","\tcase pod.ReasonFailedResolution, pod.ReasonFailedValidation, pod.ReasonTaskFailedValidation, pod.ReasonResourceVerificationFailed, pod.ReasonExceededResourceQuota, pod.ReasonExceededNodeResources, pod.ReasonPullImageFailed, pod.ReasonCreateContainerConfigError, pod.ReasonPodCreationFailed, pod.ReasonPodAdmissionFailed:","\t\treturn rpb.RecordSummary_FAILURE","\tcase pod.ReasonPodPending:","\t\treturn rpb.RecordSummary_UNKNOWN","\t}","\treturn rpb.RecordSummary_UNKNOWN","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,2,2,1,1,0,2,2,2,2,0,0,0,2,2,1,1,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,2,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,1,1,0,2,2,0,0,0,0,2,2,2,2,2,0,0,0,2,1,1,2,0,0,0,2,2,2,2,2,0,2,2,2,1,1,1,1,1,1,1,1,0,0,2,1,1,1,1,2,2,1,1,1,1,0,0,2,2,2,1,1,0,2,0]},{"id":87,"path":"pkg/watcher/grpc/creds.go","lines":["// Copyright 2020 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package grpc","","import (","\t\"context\"","\t\"fmt\"","","\t\"google.golang.org/api/idtoken\"","\t\"google.golang.org/grpc/credentials\"",")","","// Google provides an authenticated transport for use against Google APIs","// (e.g. Cloud Run, Identity Aware Proxy, API Gateway, etc.) using the","// incoming gRPC server URI as the token audience.","// See https://pkg.go.dev/google.golang.org/api/idtoken for more details.","func Google(opts ...idtoken.ClientOption) credentials.PerRPCCredentials {","\treturn \u0026googleCreds{","\t\topts: opts,","\t}","}","","type googleCreds struct {","\topts []idtoken.ClientOption","}","","// GetRequestMetadata gets the current request metadata, refreshing","// tokens if required. This should be called by the transport layer on","// each request, and the data should be populated in headers or other","// context. If a status code is returned, it will be used as the status","// for the RPC. uri is the URI of the entry point for the request.","// When supported by the underlying implementation, ctx can be used for","// timeout and cancellation. Additionally, RequestInfo data will be","// available via ctx to this call.","func (c *googleCreds) GetRequestMetadata(ctx context.Context, uri ...string) (map[string]string, error) {","\tout := map[string]string{}","\tfor _, u := range uri {","\t\ttokenSource, err := idtoken.NewTokenSource(ctx, u, c.opts...)","\t\tif err != nil {","\t\t\treturn nil, fmt.Errorf(\"idtoken.NewTokenSource(%s): %v\", u, err)","\t\t}","\t\ttoken, err := tokenSource.Token()","\t\tif err != nil {","\t\t\treturn nil, fmt.Errorf(\"TokenSource.Token(%s): %v\", u, err)","\t\t}","\t\tout[\"authorization\"] = \"Bearer \" + token.AccessToken","\t}","\treturn out, nil","}","","// RequireTransportSecurity indicates whether the credentials requires","// transport security.","func (googleCreds) RequireTransportSecurity() bool {","\treturn true","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,0,0,1,1,1]},{"id":88,"path":"pkg/watcher/logs/client.go","lines":["// Package logs provides log streaming utilities for the watcher.","package logs","","import (","\t\"context\"","\t\"errors\"","","\tv1alpha2pb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"google.golang.org/grpc\"","\treflectionv1 \"google.golang.org/grpc/reflection/grpc_reflection_v1\"","\t\"knative.dev/pkg/logging\"",")","","// Key is key to store LogsClient in the context","type Key struct{}","","const (","\tlogsServiceName = \"tekton.results.v1alpha2.Logs\"",")","","// WithContext includes the Logs client to the context.","func WithContext(ctx context.Context, conn *grpc.ClientConn) (context.Context, error) {","\treflectionInfo, err := reflectionv1.NewServerReflectionClient(conn).ServerReflectionInfo(ctx)","\tif err != nil {","\t\treturn ctx, err","\t}","","\terr = reflectionInfo.Send(\u0026reflectionv1.ServerReflectionRequest{","\t\tMessageRequest: \u0026reflectionv1.ServerReflectionRequest_ListServices{","\t\t\tListServices: \"*\",","\t\t},","\t})","\tif err != nil {","\t\treturn ctx, err","\t}","","\tresponse, err := reflectionInfo.Recv()","\tif err != nil {","\t\treturn ctx, err","\t}","\tfor _, service := range response.GetListServicesResponse().GetService() {","\t\tif service.Name == logsServiceName {","\t\t\treturn context.WithValue(ctx, Key{}, v1alpha2pb.NewLogsClient(conn)), nil","\t\t}","\t}","\treturn ctx, errors.New(\"logs service not enabled in server\")","}","","// Get extracts the Logs client from the context.","func Get(ctx context.Context) v1alpha2pb.LogsClient {","\tuntyped := ctx.Value(Key{})","\tif untyped == nil {","\t\tlogging.FromContext(ctx).Info(","\t\t\t\"Unable to fetch Logs Client from context, either disabled from config or disabled from server side\")","\t\treturn nil","\t}","\treturn untyped.(v1alpha2pb.LogsClient)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,1,0,0,0,1,1,1,1,1,1,1,1,0]},{"id":89,"path":"pkg/watcher/reconciler/annotation/annotation.go","lines":["// Copyright 2020 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","// Package annotation provides utilities for managing Result annotations on Tekton resources.","package annotation","","import (","\t\"context\"","\t\"encoding/json\"","\t\"fmt\"","\t\"strings\"","","\t\"github.com/tektoncd/results/pkg/watcher/reconciler/client\"","\tapierrors \"k8s.io/apimachinery/pkg/api/errors\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/apimachinery/pkg/runtime\"","\t\"k8s.io/apimachinery/pkg/types\"","\t\"knative.dev/pkg/logging\"",")","","const (","\t// annotationPrefix - all annotations managed by watcher should have this prefix","\tannotationPrefix = \"results.tekton.dev/\"","","\t// Result identifier.","\tResult = annotationPrefix + \"result\"","","\t// Record identifier.","\tRecord = annotationPrefix + \"record\"","","\t// Log identifier.","\tLog = annotationPrefix + \"log\"","","\t// EventList identifier.","\tEventList = annotationPrefix + \"eventlist\"","","\t// Stored is an annotation that signals to the controller that a given object","\t// has been stored by the Results API.","\tStored = annotationPrefix + \"stored\"","","\t// ResultAnnotations is an annotation that integrators should add to objects in order to store","\t// arbitrary keys/values into the Result.Annotations field.","\tResultAnnotations = annotationPrefix + \"resultAnnotations\"","","\t// RecordSummaryAnnotations is an annotation that integrators should add to objects","\t// in order to store arbitrary keys/values into the Result.Summary.Annotations field.","\t// This allows for additional information to be associated with the summary of a record.","\tRecordSummaryAnnotations = annotationPrefix + \"recordSummaryAnnotations\"","","\t// ChildReadyForDeletion is an annotation that signals to the controller that a given child object","\t// (e.g. TaskRun owned by a PipelineRun) is done and up to date in the","\t// API server and therefore, ready to be garbage collected.","\tChildReadyForDeletion = annotationPrefix + \"childReadyForDeletion\"","","\t// FieldManager identifier to be used with Server-Side Apply patches","\tfieldManager = \"tekton-results-watcher\"",")","","// Annotation is wrapper for Kubernetes resource annotations stored in the metadata.","type Annotation struct {","\tName  string","\tValue string","}","","// Server-side apply patch structure","type applyPatch struct {","\tAPIVersion string   `json:\"apiVersion\"`","\tKind       string   `json:\"kind\"`","\tMetadata   metadata `json:\"metadata\"`","}","","type metadata struct {","\tName        string            `json:\"name\"`","\tNamespace   string            `json:\"namespace\"`","\tAnnotations map[string]string `json:\"annotations\"`","}","","// Patch builds and applies a patch with the given annotations to the object using the provided object client.","func Patch(","\tctx context.Context,","\tobject metav1.Object,","\tobjectClient client.ObjectClient,","\tannotations ...Annotation,",") error {","","\tlogger := logging.FromContext(ctx)","","\t// Get the API version and kind from the object","\tvar apiVersion, kind string","\tif runtimeObj, ok := object.(runtime.Object); ok {","\t\tif gvk := runtimeObj.GetObjectKind().GroupVersionKind(); !gvk.Empty() {","\t\t\tkind = gvk.Kind","\t\t\tapiVersion = gvk.GroupVersion().String()","\t\t}","\t}","\t// If we couldn't determine the kind or apiVersion, fail","\tif kind == \"\" || apiVersion == \"\" {","\t\tlogger.Errorf(\"could not determine apiVersion and kind from object %s/%s\", object.GetNamespace(), object.GetName())","\t\treturn fmt.Errorf(\"could not determine apiVersion and kind from object %s/%s\", object.GetNamespace(), object.GetName())","\t}","","\tif IsPatched(object, annotations...) {","\t\tlogger.Debugf(\"Skipping CRD annotation patch: annotations are already set ObjectName: %s\", object.GetName())","\t\treturn nil","\t}","","\tdata := applyPatch{","\t\tAPIVersion: apiVersion,","\t\tKind:       kind,","\t\tMetadata: metadata{","\t\t\tName:        object.GetName(),","\t\t\tNamespace:   object.GetNamespace(),","\t\t\tAnnotations: map[string]string{},","\t\t},","\t}","","\t// Copy existing managed annotations from the object","\t// Only include annotations that we manage (results.tekton.dev/* annotations)","\t// to avoid conflicts with other controllers using server-side apply","\tcurrentAnnotations := object.GetAnnotations()","\tfor key, value := range currentAnnotations {","\t\tif strings.HasPrefix(key, annotationPrefix) {","\t\t\tdata.Metadata.Annotations[key] = value","\t\t}","\t}","","\t// Add/overwrite with new annotations","\tfor _, annotation := range annotations {","\t\tif len(annotation.Value) != 0 {","\t\t\tdata.Metadata.Annotations[annotation.Name] = annotation.Value","\t\t}","\t}","\tpatch, err := json.Marshal(data)","\tif err != nil {","\t\treturn err","\t}","","\tforce := false","\tpatchOptions := metav1.PatchOptions{","\t\tFieldManager: fieldManager,","\t\tForce:        \u0026force,","\t}","\terr = objectClient.Patch(ctx, object.GetName(), types.ApplyPatchType, patch, patchOptions)","\tif apierrors.IsConflict(err) {","\t\t// Since we only update the list of annotations we manage, there shouldn't be any conflicts unless","\t\t// another controller/client is updating our annotations. We log the issue and force patch.","\t\t// TODO: We can expose the error as a metric","\t\tlogger.Warnf(\"failed to patch object %s with annotations %v due to Server-Side Apply patch conflict, using force patch.\", object.GetName(), data.Metadata.Annotations)","\t\tforce = true","\t\terr = objectClient.Patch(ctx, object.GetName(), types.ApplyPatchType, patch, patchOptions)","\t}","","\t// After successful patch, update in-memory object","\tif err == nil {","\t\tcurrentAnnotations := object.GetAnnotations()","\t\tif currentAnnotations == nil {","\t\t\tcurrentAnnotations = make(map[string]string)","\t\t}","\t\tfor _, ann := range annotations {","\t\t\tcurrentAnnotations[ann.Name] = ann.Value","\t\t}","\t\tobject.SetAnnotations(currentAnnotations)","\t}","","\treturn err","}","","// IsPatched returns true if the object in question contains all relevant","// annotations or false otherwise.","func IsPatched(object metav1.Object, annotations ...Annotation) bool {","\tobjAnnotations := object.GetAnnotations()","\tfor _, annotation := range annotations {","\t\tif objAnnotations[annotation.Name] != annotation.Value {","\t\t\treturn false","\t\t}","\t}","\treturn true","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,0,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,2,0,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,0,0,2,0,0,0,0,2,2,2,2,2,2,0,2,0]},{"id":90,"path":"pkg/watcher/reconciler/client/client.go","lines":["// Copyright 2024 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","// Package client provides utilities for interacting with watcher reconciler clients.","package client","","import (","\t\"context\"","","\tpipelinev1 \"github.com/tektoncd/pipeline/pkg/client/clientset/versioned/typed/pipeline/v1\"","\tv1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/apimachinery/pkg/types\"",")","","// ObjectClient is a shim around generated k8s clients to handle objects in","// type agnostic ways.","// This might be able to be replaced with generics later?","type ObjectClient interface {","\tDelete(ctx context.Context, name string, opts v1.DeleteOptions) error","\tPatch(ctx context.Context, name string, pt types.PatchType, data []byte, opts v1.PatchOptions, subresources ...string) error","}","","// TaskRunClient implements the dynamic ObjectClient for TaskRuns.","type TaskRunClient struct {","\tpipelinev1.TaskRunInterface","}","","// Patch patches TaskRun k8s resource","func (c *TaskRunClient) Patch(ctx context.Context, name string, pt types.PatchType, data []byte, opts v1.PatchOptions, subresources ...string) error {","\t_, err := c.TaskRunInterface.Patch(ctx, name, pt, data, opts, subresources...)","\treturn err","}","","// PipelineRunClient implements the dynamic ObjectClient for PipelineRuns.","type PipelineRunClient struct {","\tpipelinev1.PipelineRunInterface","}","","// Patch patches pipelineRun Kubernetes resource.","func (c *PipelineRunClient) Patch(ctx context.Context, name string, pt types.PatchType, data []byte, opts v1.PatchOptions, subresources ...string) error {","\t_, err := c.PipelineRunInterface.Patch(ctx, name, pt, data, opts, subresources...)","\treturn err","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,1,1,1,1]},{"id":91,"path":"pkg/watcher/reconciler/config.go","lines":["// Copyright 2021 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","// Package reconciler provides the core reconciliation logic for the watcher.","package reconciler","","import (","\t\"time\"","","\t\"k8s.io/apimachinery/pkg/labels\"",")","","// Config defines shared reconciler configuration options.","type Config struct {","\t// Configures whether Tekton CRD objects should be updated with Result","\t// annotations during reconcile. Useful to enable for dry run modes.","\tDisableAnnotationUpdate bool","","\t// CompletedResourceGracePeriod is the time to wait before deleting completed resources.","\t// 0 implies the duration","\tCompletedResourceGracePeriod time.Duration","","\t// Label selector to match resources against in order to determine","\t// whether completed resources are eligible for deletion. The default","\t// value is labels.Everything() which matches any resource.","\tlabelSelector labels.Selector","","\t// How long the controller waits to reprocess keys on certain events","\t// (e.g. an object doesn't match the provided label selectors).","\tRequeueInterval time.Duration","","\t// Check owner reference when deleting objects. By default, objects having owner references set won't be deleted.","\tCheckOwner bool","","\t// UpdateLogTimeout is the time we provide for storing logs before aborting","\tUpdateLogTimeout *time.Duration","","\t// DynamicReconcileTimeout is the time we provide for the dynamic reconciler to process an event","\tDynamicReconcileTimeout *time.Duration","","\t// Whether to Store Events related to Taskrun and Pipelineruns","\tStoreEvent bool","","\t// StoreDeadline is the time we provide for the PipelineRun and TaskRun resources","\t// to be stored before aborting and clearing the finalizer in case of delete event","\tStoreDeadline *time.Duration","","\t// FinalizerRequeueInterval is the duration after which finalizer reconciler","\t// is scheduled to run for processing Runs not yet stored.","\tFinalizerRequeueInterval time.Duration","","\t// ForwardBuffer is the time we provide for the TaskRun Logs to finish streaming","\t// by a forwarder. Since there's no way to check if log has been streamed, we","\t// always wait for this much amount of duration","\tForwardBuffer *time.Duration","","\t// Collect logs with timestamps","\tLogsTimestamps bool","","\t// SummaryLabels are labels which should be part of the summary of the result","\tSummaryLabels string","","\t// SummaryAnnotations are annotations which should be part of the summary of the result","\tSummaryAnnotations string","","\t// DisableStoringIncompleteRuns disables the collection of incomplete Runs data","\tDisableStoringIncompleteRuns bool","}","","// GetDisableAnnotationUpdate returns whether annotation updates should be","// disabled. This is safe to call for missing configs.","func (c *Config) GetDisableAnnotationUpdate() bool {","\tif c == nil {","\t\treturn false","\t}","\treturn c.DisableAnnotationUpdate","}","","// GetCompletedResourceGracePeriod returns the grace period to wait for","// deleting Run objects.","// If value \u003c 0, objects will be deleted immediately.","// If value = 0 (or not explicitly set), then objects will not be deleted.","// If value \u003e 0, objects will be deleted with a grace period option of the","// duration.","func (c *Config) GetCompletedResourceGracePeriod() time.Duration {","\tif c == nil {","\t\treturn 0","\t}","\treturn c.CompletedResourceGracePeriod","}","","// GetLabelSelector returns the label selector to match resources against in","// order to determine whether they're eligible for deletion. If no selector was","// configured via the SetLabelSelector method, returns a selector that always","// matches any resource.","func (c *Config) GetLabelSelector() labels.Selector {","\tif c.labelSelector == nil {","\t\treturn labels.Everything()","\t}","\treturn c.labelSelector","}","","// SetLabelSelector sets a label selector to match resources against in order to","// determine whether they're eligible for deletion. The syntax obeys the same","// format accepted by list operations performed on the Kubernetes API server.","func (c *Config) SetLabelSelector(selector string) error {","\tparsedSelector, err := labels.Parse(selector)","\tif err != nil {","\t\treturn err","\t}","\tc.labelSelector = parsedSelector","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,0,0,0,0,0,0,0,2,2,1,1,2,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1,0]},{"id":92,"path":"pkg/watcher/reconciler/dynamic/dynamic.go","lines":["// Copyright 2020 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","// Package dynamic provides dynamic reconciliation for Tekton resources.","package dynamic","","import (","\t\"bytes\"","\t\"context\"","\t\"encoding/json\"","\t\"fmt\"","\t\"strings\"","\t\"sync\"","\t\"time\"","","\t\"github.com/fatih/color\"","\t\"github.com/jonboulle/clockwork\"","\t\"github.com/tektoncd/cli/pkg/cli\"","\ttknlog \"github.com/tektoncd/cli/pkg/log\"","\ttknopts \"github.com/tektoncd/cli/pkg/options\"","\tpipelinev1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/log\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/record\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/result\"","\t\"github.com/tektoncd/results/pkg/logs\"","\t\"github.com/tektoncd/results/pkg/watcher/convert\"","\t\"github.com/tektoncd/results/pkg/watcher/reconciler\"","\t\"github.com/tektoncd/results/pkg/watcher/reconciler/annotation\"","\t\"github.com/tektoncd/results/pkg/watcher/reconciler/client\"","\t\"github.com/tektoncd/results/pkg/watcher/results\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"go.uber.org/zap\"","\tv1 \"k8s.io/api/core/v1\"","\t\"k8s.io/apimachinery/pkg/api/errors\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/apimachinery/pkg/labels\"","\t\"k8s.io/client-go/kubernetes\"","\t\"knative.dev/pkg/apis\"","\t\"knative.dev/pkg/controller\"","\t\"knative.dev/pkg/logging\"",")","","var (","\tclock = clockwork.NewRealClock()",")","","// Reconciler implements common reconciler behavior across different Tekton Run","// Object types.","type Reconciler struct {","\t// KubeClientSet allows us to talk to the k8s for core APIs","\tKubeClientSet kubernetes.Interface","","\tresultsClient          *results.Client","\tobjectClient           client.ObjectClient","\tcfg                    *reconciler.Config","\tIsReadyForDeletionFunc IsReadyForDeletion","\tAfterDeletion          AfterDeletion","\tAfterStorage           AfterStorage","}","","func init() {","\t// Disable colorized output from the tkn CLI.","\tcolor.NoColor = true","}","","// IsReadyForDeletion is a predicate function which indicates whether the object","// being reconciled is ready to be garbage collected. Besides the reqirements","// that are already enforced by this reconciler, callers may define more","// specific constraints by providing a function that has the below signature to","// the Reconciler instance. For instance, the controller that reconciles","// PipelineRuns can verify whether all dependent TaskRuns are up to date in the","// API server before deleting all objects in cascade.","type IsReadyForDeletion func(ctx context.Context, object results.Object) (bool, error)","","// AfterDeletion is the function called after object is deleted","type AfterDeletion func(ctx context.Context, object results.Object) error","","// AfterStorage is called after an object has been successfully stored","type AfterStorage func(ctx context.Context, object results.Object, storageSuccess bool) error","","// NewDynamicReconciler creates a new dynamic Reconciler.","func NewDynamicReconciler(kubeClientSet kubernetes.Interface, rc pb.ResultsClient, lc pb.LogsClient, oc client.ObjectClient, cfg *reconciler.Config) *Reconciler {","\treturn \u0026Reconciler{","\t\tresultsClient: results.NewClient(rc, lc, cfg),","\t\tKubeClientSet: kubeClientSet,","\t\tobjectClient:  oc,","\t\tcfg:           cfg,","\t\t// Always true predicate.","\t\tIsReadyForDeletionFunc: func(_ context.Context, _ results.Object) (bool, error) {","\t\t\treturn true, nil","\t\t},","\t}","}","","// Reconcile handles result/record uploading for the given Run object.","// If enabled, the object may be deleted upon successful result upload.","func (r *Reconciler) Reconcile(ctx context.Context, o results.Object) error {","\tvar ctxCancel context.CancelFunc","\t// context with timeout does not work with the partial end to end flow that exists with unit tests;","\t// this field will always be set for real","\tif r.cfg != nil \u0026\u0026 r.cfg.UpdateLogTimeout != nil {","\t\tctx, ctxCancel = context.WithTimeout(ctx, *r.cfg.UpdateLogTimeout)","\t}","\t// we dont defer the dynamicCancle because golang defers follow a LIFO pattern","\t// and we want to have our context analysis defer function be able to distinguish between","\t// the context channel being closed because of Canceled or DeadlineExceeded","\tlogger := logging.FromContext(ctx)","\tdefer func() {","\t\tif ctx == nil {","\t\t\treturn","\t\t}","\t\tctxErr := ctx.Err()","\t\tif ctxErr == nil {","\t\t\tlogger.Warn(\"Leaving dynamic Reconciler somehow but the context channel is not closed\")","\t\t\treturn","\t\t}","\t\tif ctxErr == context.Canceled {","\t\t\tlogger.Debug(\"Leaving dynamic Reconciler normally with context properly canceled\")","\t\t\treturn","\t\t}","\t\tif ctxErr == context.DeadlineExceeded {","\t\t\tlogger.Warn(\"Leaving dynamic Reconciler only after context timeout\")","\t\t\treturn","\t\t}","\t\tlogger.Warnw(\"Leaving dynamic Reconciler with unexpected error\", zap.String(\"error\", ctxErr.Error()))","\t}()","","\tif o.GetObjectKind().GroupVersionKind().Empty() {","\t\tgvk, err := convert.InferGVK(o)","\t\tif err != nil {","\t\t\tif ctxCancel != nil {","\t\t\t\tctxCancel()","\t\t\t}","\t\t\treturn err","\t\t}","\t\to.GetObjectKind().SetGroupVersionKind(gvk)","\t\tlogger.Debugf(\"Post SetGroupVersionKind: %s\", o.GetObjectKind().GroupVersionKind().String())","\t}","","\t// Upsert record.","\tstartTime := time.Now()","\tres, rec, err := r.resultsClient.Put(ctx, o)","\ttimeTakenField := zap.Int64(\"results.tekton.dev/time-taken-ms\", time.Since(startTime).Milliseconds())","","\tif err != nil {","\t\tlogger.Debugw(\"Error upserting record to API server\", zap.Error(err), timeTakenField)","","\t\tif ctxCancel != nil {","\t\t\tctxCancel()","\t\t}","\t\treturn fmt.Errorf(\"error upserting record: %w\", err)","\t}","","\t// Update logs if enabled.","\tif r.resultsClient.LogsClient != nil {","\t\tif r.cfg == nil || r.cfg.UpdateLogTimeout == nil {","\t\t\t// single threaded for unit tests given fragility of fake k8s client","\t\t\tif err = r.sendLog(ctx, o); err != nil {","\t\t\t\tlogger.Errorw(\"Error sending log\", zap.Error(err))","\t\t\t}","","\t\t} else {","\t\t\t// so while performance was acceptable with development level storage mechanisms like minio, latency proved","\t\t\t// intolerable for even basic amounts of log storage; moving off of the reconciler thread again, and","\t\t\t// completely divesting from its context, now using the background context and a separate timer to provide","\t\t\t// for timeout capability","\t\t\tgo func() {","\t\t\t\t// TODO need to leverage the log status API noting log storage completion to coordinate with pruning","\t\t\t\tbackgroundCtx, cancel := context.WithCancel(context.Background())","\t\t\t\t// need this to get grpc to clean up its threads","\t\t\t\tdefer cancel()","\t\t\t\ttimeout := 30 * time.Second","\t\t\t\t// context with timeout does not work with the partial end to end flow that exists with unit tests;","\t\t\t\t// this field will always be set for real","\t\t\t\tif r.cfg != nil \u0026\u0026 r.cfg.DynamicReconcileTimeout != nil {","\t\t\t\t\t// given what we have seen in stress testing, we track this timeout separately from the reconciler's timeout","\t\t\t\t\ttimeout = *r.cfg.DynamicReconcileTimeout","\t\t\t\t}","\t\t\t\teventTicker := time.NewTicker(timeout)","\t\t\t\t// make buffered for golang GC","\t\t\t\tstopCh := make(chan bool, 1)","\t\t\t\tonce := sync.Once{}","","\t\t\t\tgo func() {","\t\t\t\t\tif err = r.sendLog(backgroundCtx, o); err != nil {","\t\t\t\t\t\tlogger.Errorw(\"Error sending log\", zap.Error(err))","\t\t\t\t\t}","\t\t\t\t\tonce.Do(func() { close(stopCh) })","\t\t\t\t\t// TODO once we have the log status available, report the error there for retry if needed","\t\t\t\t}()","","\t\t\t\tselect {","\t\t\t\tcase \u003c-eventTicker.C:","\t\t\t\t\tonce.Do(func() { close(stopCh) })","\t\t\t\t\tlogger.Warn(\"Leaving sendLogs thread only after timeout\")","","\t\t\t\tcase \u003c-stopCh:","\t\t\t\t\t// this is safe to call twice, as it does not need to close its buffered channel","\t\t\t\t\teventTicker.Stop()","\t\t\t\t}","\t\t\t}()","","\t\t}","\t}","","\t// CreateEvents if enabled","\tif r.cfg.StoreEvent {","\t\tif err := r.storeEvents(ctx, o); err != nil {","\t\t\tlogger.Errorw(\"Error storing eventlist\", zap.Error(err))","\t\t\tif ctxCancel != nil {","\t\t\t\tctxCancel()","\t\t\t}","\t\t\treturn err","\t\t}","\t\tlogger.Debug(\"Successfully store eventlist\")","\t}","\tlogger = logger.With(zap.String(\"results.tekton.dev/result\", res.Name),","\t\tzap.String(\"results.tekton.dev/record\", rec.Name))","\tlogger.Debugw(\"Record has been successfully upserted into API server\", timeTakenField)","","\trecordAnnotation := annotation.Annotation{Name: annotation.Record, Value: rec.GetName()}","\tresultAnnotation := annotation.Annotation{Name: annotation.Result, Value: res.GetName()}","\tif err = r.addResultsAnnotations(ctx, o, recordAnnotation, resultAnnotation); err != nil {","\t\t// no grpc calls from addResultsAnnotation","\t\tif ctxCancel != nil {","\t\t\tctxCancel()","\t\t}","\t\treturn err","\t}","","\tif err = r.addChildReadyForDeletionAnnotations(ctx, o); err != nil {","\t\tif ctxCancel != nil {","\t\t\tctxCancel()","\t\t}","\t\treturn err","\t}","","\tif err = r.deleteUponCompletion(ctx, o); err != nil {","\t\t// no grpc calls from deleteUponCompletion","\t\tif ctxCancel != nil {","\t\t\tctxCancel()","\t\t}","\t\treturn err","\t}","\tif ctxCancel != nil {","\t\tdefer ctxCancel()","\t}","\treturn r.addStoredAnnotations(ctx, o)","}","","// addResultsAnnotations adds Results annotations to the object in question if","// annotation patching is enabled.","func (r *Reconciler) addResultsAnnotations(ctx context.Context, o results.Object, annotations ...annotation.Annotation) error {","\tlogger := logging.FromContext(ctx)","\tif r.cfg.GetDisableAnnotationUpdate() { //nolint:gocritic","\t\tlogger.Debug(\"Skipping CRD annotation patch: annotation update is disabled\")","\t} else {","\t\terr := annotation.Patch(ctx, o, r.objectClient, annotations...)","\t\tif err != nil {","\t\t\treturn fmt.Errorf(\"error patching object: %w\", err)","\t\t}","\t}","\treturn nil","}","","// deleteUponCompletion deletes the object in question when the following","// conditions are met:","// * The resource deletion is enabled in the config (the grace period is greater","// than 0).","// * The object is done, and it isn't owned by other object.","// * The configured grace period has elapsed since the object's completion.","// * The object satisfies all label requirements defined in the supplied config.","// * The assigned IsReadyForDeletionFunc returns true.","func (r *Reconciler) deleteUponCompletion(ctx context.Context, o results.Object) error {","\tlogger := logging.FromContext(ctx)","","\tgracePeriod := r.cfg.GetCompletedResourceGracePeriod()","\tlogger = logger.With(zap.Duration(\"results.tekton.dev/gracePeriod\", gracePeriod))","\tif gracePeriod == 0 {","\t\tlogger.Info(\"Skipping resource deletion: deletion is disabled\")","\t\treturn nil","\t}","","\tif !isDone(o) {","\t\tlogger.Debug(\"Skipping resource deletion: object is not done yet\")","\t\treturn nil","\t}","","\tif ownerReferences := o.GetOwnerReferences(); len(ownerReferences) \u003e 0 {","\t\t// do not delete if the object is owned by a PipelineRun object","\t\t// This can be removed once the PipelineRun controller is patched to stop updating the PipelineRun object","\t\t// when child TaskRuns are deleted","\t\tfor _, or := range ownerReferences {","\t\t\tif or.Kind == \"PipelineRun\" {","\t\t\t\tlogger.Debugw(\"Resource is owned by a PipelineRun, deferring deletion to parent PipelineRun\", zap.Any(\"tekton.dev/PipelineRun\", or.Name))","\t\t\t\treturn nil","\t\t\t}","\t\t}","\t\t// do not delete if CheckOwner flag is enabled and the object has some owner references","\t\tif r.cfg.CheckOwner {","\t\t\tlogger.Debugw(\"Resource is owned by another object, deferring deletion to parent resource(s)\", zap.Any(\"results.tekton.dev/ownerReferences\", ownerReferences))","\t\t\treturn nil","\t\t}","\t}","","\tcompletionTime, err := getCompletionTime(o)","\tif err != nil {","\t\treturn err","\t}","","\t// This isn't probable since the object is done, but defensive","\t// programming never hurts.","\tif completionTime == nil {","\t\tlogger.Debug(\"Object's completion time isn't set yet - requeuing to process later\")","\t\treturn controller.NewRequeueAfter(gracePeriod)","\t}","","\tif timeSinceCompletion := clock.Since(*completionTime); timeSinceCompletion \u003c gracePeriod {","\t\trequeueAfter := gracePeriod - timeSinceCompletion","\t\tlogger.Debugw(\"Object is not ready for deletion yet - requeuing to process later\", zap.Duration(\"results.tekton.dev/requeueAfter\", requeueAfter))","\t\treturn controller.NewRequeueAfter(requeueAfter)","\t}","","\t// Verify whether this object matches the provided label selectors","\tif selectors := r.cfg.GetLabelSelector(); !selectors.Matches(labels.Set(o.GetLabels())) {","\t\tlogger.Debugw(\"Object doesn't match the required label selectors - requeuing to process later\", zap.String(\"results.tekton.dev/label-selectors\", selectors.String()))","\t\treturn controller.NewRequeueAfter(r.cfg.RequeueInterval)","\t}","","\tif isReady, err := r.IsReadyForDeletionFunc(ctx, o); err != nil {","\t\treturn err","\t} else if !isReady {","\t\treturn controller.NewRequeueAfter(r.cfg.RequeueInterval)","\t}","","\tlogger.Infow(\"Deleting object\", zap.String(\"results.tekton.dev/uid\", string(o.GetUID())),","\t\tzap.Int64(\"results.tekton.dev/time-taken-seconds\", int64(time.Since(*completionTime).Seconds())))","","\tif err := r.objectClient.Delete(ctx, o.GetName(), metav1.DeleteOptions{","\t\tPreconditions: metav1.NewUIDPreconditions(string(o.GetUID())),","\t}); err != nil \u0026\u0026 !errors.IsNotFound(err) {","\t\tlogger.Debugw(\"Error deleting object\", zap.Error(err))","\t\treturn fmt.Errorf(\"error deleting object: %w\", err)","\t}","","\tlogger.Debugw(\"Object has been successfully deleted\", zap.Int64(\"results.tekton.dev/time-taken-seconds\", int64(time.Since(*completionTime).Seconds())))","\tif r.AfterDeletion != nil {","\t\terr = r.AfterDeletion(ctx, o)","\t\tif err != nil {","\t\t\tlogger.Errorw(\"Failed to record deletion metrics\", zap.Error(err))","\t\t}","\t}","\treturn nil","}","","func isDone(o results.Object) bool {","\treturn !o.GetStatusCondition().GetCondition(apis.ConditionSucceeded).IsUnknown()","}","","// getCompletionTime returns the completion time of the object (PipelineRun or","// TaskRun) in question.","func getCompletionTime(object results.Object) (*time.Time, error) {","\tvar completionTime *time.Time","","\tswitch o := object.(type) {","","\tcase *pipelinev1.PipelineRun:","\t\tif o.Status.CompletionTime != nil {","\t\t\tcompletionTime = \u0026o.Status.CompletionTime.Time","\t\t}","","\tcase *pipelinev1.TaskRun:","\t\tif o.Status.CompletionTime != nil {","\t\t\tcompletionTime = \u0026o.Status.CompletionTime.Time","\t\t}","","\tdefault:","\t\treturn nil, controller.NewPermanentError(fmt.Errorf(\"error getting completion time from incoming object: unrecognized type %T\", o))","\t}","\treturn completionTime, nil","}","","// sendLog streams logs to the API server","func (r *Reconciler) sendLog(ctx context.Context, o results.Object) error {","\tlogger := logging.FromContext(ctx)","\tcondition := o.GetStatusCondition().GetCondition(apis.ConditionSucceeded)","\tGVK := o.GetObjectKind().GroupVersionKind()","\tif !GVK.Empty() \u0026\u0026","\t\t(GVK.Kind == \"TaskRun\" || GVK.Kind == \"PipelineRun\") \u0026\u0026","\t\tcondition != nil \u0026\u0026","\t\tcondition.Type == \"Succeeded\" \u0026\u0026","\t\t!condition.IsUnknown() {","","\t\trec, err := r.resultsClient.GetLogRecord(ctx, o)","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\tif rec != nil {","\t\t\t// we had already started logs streaming","\t\t\tparent, resName, recName, err := record.ParseName(rec.GetName())","\t\t\tif err != nil {","\t\t\t\treturn err","\t\t\t}","\t\t\tlogName := log.FormatName(result.FormatName(parent, resName), recName)","\t\t\t// Update log annotation if it doesn't exist","\t\t\treturn r.addResultsAnnotations(ctx, o, annotation.Annotation{Name: annotation.Log, Value: logName})","\t\t}","","\t\t// Create a log record if the object has/supports logs.","\t\trec, err = r.resultsClient.PutLog(ctx, o)","\t\tif err != nil {","\t\t\treturn err","\t\t}","","\t\tparent, resName, recName, err := record.ParseName(rec.GetName())","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\tlogName := log.FormatName(result.FormatName(parent, resName), recName)","","\t\tvar logType string","\t\tswitch o.GetObjectKind().GroupVersionKind().Kind {","\t\tcase \"TaskRun\":","\t\t\tlogType = tknlog.LogTypeTask","\t\tcase \"PipelineRun\":","\t\t\tlogType = tknlog.LogTypePipeline","\t\t}","","\t\tif err := r.addResultsAnnotations(ctx, o, annotation.Annotation{Name: annotation.Log, Value: logName}); err != nil {","\t\t\treturn err","\t\t}","","\t\tlogger.Debug(\"Streaming log started\")","","\t\terr = r.streamLogs(ctx, o, logType, logName)","\t\tif err != nil {","\t\t\tlogger.Errorw(\"Error streaming log\", zap.Error(err))","\t\t\t// TODO once we have the log status available, report the error there for retry if needed","\t\t}","\t\tlogger.Info(\"Streaming log completed\")","","\t}","","\treturn nil","}","","func (r *Reconciler) streamLogs(ctx context.Context, o results.Object, logType, logName string) error {","\tlogger := logging.FromContext(ctx)","\tlogsClient, err := r.resultsClient.UpdateLog(ctx)","\tif err != nil {","\t\treturn fmt.Errorf(\"failed to create UpdateLog client: %w\", err)","\t}","","\twriter := logs.NewBufferedWriter(logsClient, logName, logs.DefaultBufferSize)","","\tinMemWriteBufferStdout := bytes.NewBuffer(make([]byte, 0))","\tinMemWriteBufferStderr := bytes.NewBuffer(make([]byte, 0))","\ttknParams := \u0026cli.TektonParams{}","\ttknParams.SetNamespace(o.GetNamespace())","\t// KLUGE: tkn reader.Read() will raise an error if a step in the TaskRun failed and there is no","\t// Err writer in the Stream object. This will result in some \"error\" messages being written to","\t// the log.  That, coupled with the fact that the tkn client wrappers and oftent masks errors","\t// makes it impossible to differentiate between retryable and permanent k8s errors wrt retrying","\t// reconciliation in this controller","","\treader, err := tknlog.NewReader(logType, \u0026tknopts.LogOptions{","\t\tAllSteps:        true,","\t\tParams:          tknParams,","\t\tPipelineRunName: o.GetName(),","\t\tTaskrunName:     o.GetName(),","\t\tTimestamps:      r.cfg.LogsTimestamps,","\t\tStream: \u0026cli.Stream{","\t\t\tOut: inMemWriteBufferStdout,","\t\t\tErr: inMemWriteBufferStderr,","\t\t},","\t})","\tif err != nil {","\t\treturn fmt.Errorf(\"failed to create tkn reader: %w\", err)","\t}","\tlogChan, errChan, err := reader.Read()","\tif err != nil {","\t\treturn fmt.Errorf(\"error reading from tkn reader: %w\", err)","\t}","","\ttknlog.NewWriter(logType, true).Write(\u0026cli.Stream{","\t\tOut: inMemWriteBufferStdout,","\t\tErr: inMemWriteBufferStderr,","\t}, logChan, errChan)","","\t// pull the first error that occurred and return on that; reminder - per https://golang.org/ref/spec#Channel_types","\t// channels act as FIFO queues","\tchanErr, ok := \u003c-errChan","\tif ok \u0026\u0026 chanErr != nil {","\t\treturn fmt.Errorf(\"error occurred while calling tkn client write: %w\", chanErr)","\t}","","\tbufStdout := inMemWriteBufferStdout.Bytes()","\tcntStdout, writeStdOutErr := writer.Write(bufStdout)","\tif writeStdOutErr != nil {","\t\tlogger.Warnw(\"streamLogs in mem bufStdout write err\", zap.String(\"error\", writeStdOutErr.Error()))","\t}","\tif cntStdout != len(bufStdout) {","\t\tlogger.Warnw(\"streamLogs bufStdout write len inconsistent\",","\t\t\tzap.Int(\"in\", len(bufStdout)),","\t\t\tzap.Int(\"out\", cntStdout),","\t\t)","","\t}","\tbufStderr := inMemWriteBufferStderr.Bytes()","\t// we do not write these errors to the results api server","","\t// TODO we may need somehow discern the precise nature of the errors here and adjust how","\t// we return accordingly","\tif len(bufStderr) \u003e 0 {","\t\terrStr := string(bufStderr)","\t\tlogger.Warnw(\"tkn client std error output\",","\t\t\tzap.String(\"name\", o.GetName()),","\t\t\tzap.String(\"errStr\", errStr))","\t}","","\t_, flushErr := writer.Flush()","\tif flushErr != nil {","\t\tlogger.Warnw(\"flush ret err\", zap.String(\"error\", flushErr.Error()))","\t\tlogger.Error(flushErr)","\t\treturn flushErr","\t}","\t// so we use CloseAndRecv vs. just CloseSent to achieve a few things:","\t// 1) CloseAndRecv calls CloseSend under the covers, followed by a Recv call to obtain a LogSummary","\t// 2) LogSummary appears to have some stats on the state of operations","\t// 3) It also appears to be the best form of \"confirmation\" that the asynchronous operation of UpdateLog on the api","\t// server side has reached a terminal state","\t// 4) Hence, creating a child context which we cancel hopefully does not interrupt the UpdateLog call when this method exits,","\t// 5) However, we need the context cancel to close out the last goroutine launched in newClientStreamWithParams that does","\t// the final clean, otherwise we end up with our now familiar goroutine leak, which in the end is a memory leak","","\t// comparing closeErr with io.EOF does not work; and I could not find code / desc etc. constants in the grpc code that handled","\t// the wrapped EOF error we expect to get from grpc when things are \"OK\"","\tif logSummary, closeErr := logsClient.CloseAndRecv(); closeErr != nil \u0026\u0026 !strings.Contains(closeErr.Error(), \"EOF\") {","\t\tlogger.Warnw(\"CloseAndRecv ret err\",","\t\t\tzap.String(\"name\", o.GetName()),","\t\t\tzap.String(\"error\", closeErr.Error()))","\t\tif logSummary != nil {","\t\t\tlogger.Errorw(\"CloseAndRecv\", zap.String(\"logSummary\", logSummary.String()))","\t\t}","\t\tlogger.Error(closeErr)","\t\treturn closeErr","\t}","","\tlogger.Debug(\"Exiting streamLogs\")","","\treturn nil","}","","// storeEvents streams logs to the API server","func (r *Reconciler) storeEvents(ctx context.Context, o results.Object) error {","\tlogger := logging.FromContext(ctx)","\tcondition := o.GetStatusCondition().GetCondition(apis.ConditionSucceeded)","\tGVK := o.GetObjectKind().GroupVersionKind()","\tif !GVK.Empty() \u0026\u0026","\t\t(GVK.Kind == \"TaskRun\" || GVK.Kind == \"PipelineRun\") \u0026\u0026","\t\tcondition != nil \u0026\u0026","\t\t!condition.IsUnknown() {","","\t\trec, err := r.resultsClient.GetEventListRecord(ctx, o)","\t\tif err != nil {","\t\t\treturn err","\t\t}","","\t\tif rec != nil {","\t\t\t// It means we have already stored events","\t\t\teventListName := rec.GetName()","\t\t\t// Update Events annotation if it doesn't exist","\t\t\treturn r.addResultsAnnotations(ctx, o, annotation.Annotation{Name: annotation.EventList, Value: eventListName})","\t\t}","","\t\tevents, err := r.KubeClientSet.CoreV1().Events(o.GetNamespace()).List(ctx, metav1.ListOptions{","\t\t\tFieldSelector: \"involvedObject.uid=\" + string(o.GetUID()),","\t\t})","\t\tif err != nil {","\t\t\tlogger.Errorf(\"Failed to store events - retrieve\", zap.String(\"err\", err.Error()))","\t\t\treturn err","\t\t}","","\t\ttr, ok := o.(*pipelinev1.TaskRun)","","\t\tif ok {","\t\t\tpodName := tr.Status.PodName","\t\t\tpodEvents, err := r.KubeClientSet.CoreV1().Events(o.GetNamespace()).List(ctx, metav1.ListOptions{","\t\t\t\tFieldSelector: \"involvedObject.name=\" + podName,","\t\t\t})","\t\t\tif err != nil {","\t\t\t\tlogger.Errorf(\"Failed to fetch taskrun pod events\",","\t\t\t\t\tzap.String(\"podname\", podName),","\t\t\t\t\tzap.String(\"err\", err.Error()),","\t\t\t\t)","\t\t\t}","\t\t\tif podEvents != nil \u0026\u0026 len(podEvents.Items) \u003e 0 {","\t\t\t\tevents.Items = append(events.Items, podEvents.Items...)","\t\t\t}","","\t\t}","","\t\tdata := filterEventList(events)","\t\teventList, err := json.Marshal(data)","\t\tif err != nil {","\t\t\tlogger.Errorf(\"Failed to store events - marshal\", zap.String(\"err\", err.Error()))","\t\t\treturn err","\t\t}","","\t\trec, err = r.resultsClient.PutEventList(ctx, o, eventList)","\t\tif err != nil {","\t\t\treturn err","\t\t}","","\t\tif err := r.addResultsAnnotations(ctx, o, annotation.Annotation{Name: annotation.EventList, Value: rec.GetName()}); err != nil {","\t\t\treturn err","\t\t}","","\t}","","\treturn nil","}","","func filterEventList(events *v1.EventList) *v1.EventList {","\tif events == nil || len(events.Items) == 0 {","\t\treturn events","\t}","","\tfor i, event := range events.Items {","\t\t// Only taking Name, Namespace and CreationTimeStamp for ObjectMeta","\t\tevents.Items[i].ObjectMeta = metav1.ObjectMeta{","\t\t\tName:              event.Name,","\t\t\tNamespace:         event.Namespace,","\t\t\tCreationTimestamp: event.CreationTimestamp,","\t\t}","\t}","","\treturn events","}","","// addStoredAnnotations adds stored annotations to the object in question if","// annotation patching is enabled.","func (r *Reconciler) addStoredAnnotations(ctx context.Context, o results.Object) error {","\tlogger := logging.FromContext(ctx)","","\tif r.resultsClient.LogsClient != nil {","\t\treturn nil","\t}","","\tif r.cfg.GetDisableAnnotationUpdate() { //nolint:gocritic","\t\tlogger.Debug(\"Skipping CRD annotation patch: annotation update is disabled\")","\t\treturn nil","\t}","","\tstored := annotation.Annotation{Name: annotation.Stored, Value: \"false\"}","\tGVK := o.GetObjectKind().GroupVersionKind()","","\tif GVK.Empty() {","\t\tlogger.Debugf(\"Skipping CRD annotation patch: ObjectKind is empty ObjectName: %s\", o.GetName())","\t\treturn nil","\t}","","\t// Checking if the object operation by other controllers is done","\tswitch GVK.Kind {","\tcase \"TaskRun\":","\t\ttaskRun, ok := o.(*pipelinev1.TaskRun)","\t\tif !ok {","\t\t\treturn fmt.Errorf(\"failed to cast object to TaskRun\")","\t\t}","\t\tif taskRun.IsDone() {","\t\t\tstored = annotation.Annotation{Name: annotation.Stored, Value: \"true\"}","\t\t}","\tcase \"PipelineRun\":","\t\tpipelineRun, ok := o.(*pipelinev1.PipelineRun)","\t\tif !ok {","\t\t\treturn fmt.Errorf(\"failed to cast object to PipelineRun\")","\t\t}","\t\tif pipelineRun.IsDone() {","\t\t\tstored = annotation.Annotation{Name: annotation.Stored, Value: \"true\"}","\t\t}","\tdefault:","\t\treturn nil","\t}","","\terr := annotation.Patch(ctx, o, r.objectClient, stored)","\tif err != nil {","\t\tlogger.Errorf(\"error patching object with stored annotation: %w ObjectName: %s\", err, o.GetName())","\t\treturn fmt.Errorf(\"error patching object with stored annotation: %w ObjectName: %s\", err, o.GetName())","\t}","","\t// Call AfterStorage callback if this is the first time we're marking it as stored after completion","\t// This ensures storage latency metrics are recorded exactly once per object when it transitions","\t// from \"not stored after completion\" to \"stored after completion\"","\tif stored.Value == \"true\" \u0026\u0026 r.AfterStorage != nil {","\t\tlogger.Debugw(\"Object stored after completion\",","\t\t\tzap.String(\"object\", o.GetName()),","\t\t)","\t\tif err := r.AfterStorage(ctx, o, true); err != nil {","\t\t\tlogger.Warnw(\"Failed to call AfterStorage callback\", zap.Error(err))","\t\t}","\t}","","\treturn nil","}","","// addChildReadyForDeletionAnnotations set the ChildReadyForDeletion annotation","// on objects which have an owner and are done.","func (r *Reconciler) addChildReadyForDeletionAnnotations(ctx context.Context, o results.Object) error {","\tlogger := logging.FromContext(ctx)","\tif r.cfg.GetDisableAnnotationUpdate() { //nolint:gocritic","\t\tlogger.Debug(\"Skipping CRD ChildReadyForDeletion annotation patch: annotation update is disabled\")","\t\treturn nil","\t}","","\tif len(o.GetOwnerReferences()) == 0 {","\t\treturn nil","\t}","","\tdoneObj, ok := o.(interface{ IsDone() bool })","\tif !ok {","\t\tlogger.Errorf(\"Object %s does not have IsDone() method\", o.GetName())","\t\treturn fmt.Errorf(\"object does not have IsDone() method\")","\t}","\tif !doneObj.IsDone() {","\t\tlogger.Debug(\"Skipping ChildReadyForDeletion annotation patch: object is not done yet\")","\t\treturn nil","\t}","","\tchildReadyForDeletion := annotation.Annotation{Name: annotation.ChildReadyForDeletion, Value: \"true\"}","\terr := annotation.Patch(ctx, o, r.objectClient, childReadyForDeletion)","\tif err != nil {","\t\tlogger.Errorf(\"error patching object with ChildReadyForDeletion annotation: %w ObjectName: %s\", err, o.GetName())","\t\treturn fmt.Errorf(\"error patching object with ChildReadyForDeletion annotation: %w ObjectName: %s\", err, o.GetName())","\t}","","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,2,2,2,2,2,1,1,0,0,0,2,2,2,1,1,2,2,2,2,2,1,1,1,1,1,1,1,1,1,0,0,2,1,1,1,1,1,1,0,1,1,0,0,0,2,2,2,2,2,1,1,1,1,1,1,0,0,0,2,2,2,2,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,0,1,1,1,0,0,0,0,0,0,0,2,2,1,1,1,1,1,0,2,0,2,2,2,2,2,2,2,1,1,1,1,1,0,0,2,1,1,1,1,0,0,2,2,2,1,1,2,0,2,1,1,2,0,0,0,0,2,2,2,2,2,2,2,1,1,0,2,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,0,2,1,1,1,0,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,0,0,2,2,1,1,0,0,0,2,1,1,1,0,2,2,2,2,2,0,0,2,2,2,2,0,2,2,2,2,2,0,2,2,2,2,2,2,1,1,1,0,2,2,1,1,1,1,0,2,0,0,2,2,2,0,0,0,2,2,2,2,0,1,1,1,1,0,2,2,2,2,0,1,1,0,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,2,1,1,2,2,2,0,0,0,2,2,1,1,0,2,2,1,1,2,2,2,2,2,2,2,2,0,0,2,1,1,0,2,2,2,2,2,2,2,2,0,0,0,2,0,0,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,1,1,1,0,0,0,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,0,2,2,2,2,1,1,1,0,2,2,2,2,2,2,2,2,1,1,1,1,1,2,1,1,0,0,0,2,2,2,1,1,1,0,2,2,1,1,0,2,1,1,0,0,0,2,0,0,2,2,2,2,0,1,1,1,1,1,1,1,1,0,1,0,0,0,0,2,2,2,2,2,2,0,1,1,1,1,0,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,0,0,1,0,0,0,0,2,2,2,2,2,2,0,2,2,2,0,2,2,1,1,1,2,1,1,1,0,2,2,2,1,1,1,0,2,0]},{"id":93,"path":"pkg/watcher/reconciler/leaderelection/leader_election.go","lines":["// Copyright 2022 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","// Package leaderelection provides a few utilities to help us to enable leader","// election support in the Watcher controllers.","package leaderelection","","import (","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/apimachinery/pkg/labels\"","\t\"k8s.io/apimachinery/pkg/types\"","\t\"knative.dev/pkg/reconciler\"",")","","// Lister is a generic signature of Lister.List functions, by allowing us to","// support various listers in the NewLeaderAwareFuncs function below.","type Lister[O metav1.Object] func(labels.Selector) ([]O, error)","","// NewLeaderAwareFuncs returns a new reconciler.LeaderAwareFuncs object to be","// used in our controllers.","func NewLeaderAwareFuncs[O metav1.Object](lister Lister[O]) reconciler.LeaderAwareFuncs {","\treturn reconciler.LeaderAwareFuncs{","\t\tPromoteFunc: func(bucket reconciler.Bucket, enqueue func(reconciler.Bucket, types.NamespacedName)) error {","\t\t\tobjects, err := lister(labels.Everything())","\t\t\tif err != nil {","\t\t\t\treturn err","\t\t\t}","\t\t\tfor _, object := range objects {","\t\t\t\tenqueue(bucket, types.NamespacedName{","\t\t\t\t\tNamespace: object.GetNamespace(),","\t\t\t\t\tName:      object.GetName(),","\t\t\t\t})","\t\t\t}","\t\t\treturn nil","\t\t},","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0]},{"id":94,"path":"pkg/watcher/reconciler/pipelinerun/controller.go","lines":["// Copyright 2020 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","// Package pipelinerun provides the PipelineRun reconciler controller.","package pipelinerun","","import (","\t\"context\"","","\tpipelinerunreconciler \"github.com/tektoncd/pipeline/pkg/client/injection/reconciler/pipeline/v1/pipelinerun\"","\t\"github.com/tektoncd/results/pkg/apis/config\"","\t\"github.com/tektoncd/results/pkg/metrics\"","\t\"github.com/tektoncd/results/pkg/pipelinerunmetrics\"","\t\"github.com/tektoncd/results/pkg/watcher/logs\"","\t\"github.com/tektoncd/results/pkg/watcher/reconciler\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"knative.dev/pkg/configmap\"","\t\"knative.dev/pkg/controller\"","\t\"knative.dev/pkg/logging\"","","\tpipelineclient \"github.com/tektoncd/pipeline/pkg/client/injection/client\"","\tpipelineruninformer \"github.com/tektoncd/pipeline/pkg/client/injection/informers/pipeline/v1/pipelinerun\"","\ttaskruninformer \"github.com/tektoncd/pipeline/pkg/client/injection/informers/pipeline/v1/taskrun\"","\tkubeclient \"knative.dev/pkg/client/injection/kube/client\"",")","","// NewController creates a Controller for watching PipelineRuns.","func NewController(ctx context.Context, resultsClient pb.ResultsClient, cmw configmap.Watcher) *controller.Impl {","\treturn NewControllerWithConfig(ctx, resultsClient, \u0026reconciler.Config{}, cmw)","}","","// NewControllerWithConfig creates a Controller for watching PipelineRuns by config.","func NewControllerWithConfig(ctx context.Context, resultsClient pb.ResultsClient, cfg *reconciler.Config, cmw configmap.Watcher) *controller.Impl {","\tpipelineRunInformer := pipelineruninformer.Get(ctx)","\tpipelineRunLister := pipelineRunInformer.Lister()","\tlogger := logging.FromContext(ctx)","\tconfigStore := config.NewStore(logger.Named(\"config-store\"),","\t\tmetrics.OnStore(logger),","\t\tpipelinerunmetrics.MetricsOnStore(logger))","\tconfigStore.WatchConfigs(cmw)","","\tc := \u0026Reconciler{","\t\tkubeClientSet:      kubeclient.Get(ctx),","\t\tresultsClient:      resultsClient,","\t\tlogsClient:         logs.Get(ctx),","\t\tpipelineRunLister:  pipelineRunLister,","\t\ttaskRunLister:      taskruninformer.Get(ctx).Lister(),","\t\tpipelineClient:     pipelineclient.Get(ctx),","\t\tcfg:                cfg,","\t\tconfigStore:        configStore,","\t\tmetrics:            metrics.NewRecorder(),","\t\tpipelineRunMetrics: pipelinerunmetrics.NewRecorder(),","\t}","","\timpl := pipelinerunreconciler.NewImpl(ctx, c, func(_ *controller.Impl) controller.Options {","\t\treturn controller.Options{","\t\t\t// This results pipelinerun reconciler shouldn't mutate the pipelinerun's status.","\t\t\tSkipStatusUpdates: true,","\t\t\tConfigStore:       configStore,","\t\t\tFinalizerName:     \"results.tekton.dev/pipelinerun\",","\t\t}","\t})","","\t_, err := pipelineRunInformer.Informer().AddEventHandler(controller.HandleAll(impl.Enqueue))","\tif err != nil {","\t\tlogger.Panicf(\"Couldn't register PipelineRun informer event handler: %w\", err)","\t}","","\treturn impl","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1,0]},{"id":95,"path":"pkg/watcher/reconciler/pipelinerun/reconciler.go","lines":["// Copyright 2020 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package pipelinerun","","import (","\t\"context\"","\t\"fmt\"","\t\"time\"","","\t\"github.com/tektoncd/results/pkg/apis/config\"","\t\"github.com/tektoncd/results/pkg/metrics\"","\t\"github.com/tektoncd/results/pkg/pipelinerunmetrics\"","","\tpipelinev1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"github.com/tektoncd/pipeline/pkg/client/clientset/versioned\"","\tpipelinerunreconciler \"github.com/tektoncd/pipeline/pkg/client/injection/reconciler/pipeline/v1/pipelinerun\"","\tpipelinev1listers \"github.com/tektoncd/pipeline/pkg/client/listers/pipeline/v1\"","\t\"github.com/tektoncd/results/pkg/watcher/reconciler\"","\tresultsannotation \"github.com/tektoncd/results/pkg/watcher/reconciler/annotation\"","\t\"github.com/tektoncd/results/pkg/watcher/reconciler/client\"","\t\"github.com/tektoncd/results/pkg/watcher/reconciler/dynamic\"","\t\"github.com/tektoncd/results/pkg/watcher/results\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"go.uber.org/zap\"","\tapierrors \"k8s.io/apimachinery/pkg/api/errors\"","\t\"k8s.io/client-go/kubernetes\"","\t\"knative.dev/pkg/controller\"","\t\"knative.dev/pkg/logging\"","\tknativereconciler \"knative.dev/pkg/reconciler\"",")","","// Reconciler represents pipelineRun watcher logic","type Reconciler struct {","","\t// kubeClientSet allows us to talk to the k8s for core APIs","\tkubeClientSet kubernetes.Interface","","\tresultsClient      pb.ResultsClient","\tlogsClient         pb.LogsClient","\tpipelineRunLister  pipelinev1listers.PipelineRunLister","\ttaskRunLister      pipelinev1listers.TaskRunLister","\tpipelineClient     versioned.Interface","\tcfg                *reconciler.Config","\tmetrics            *metrics.Recorder","\tpipelineRunMetrics *pipelinerunmetrics.Recorder","\tconfigStore        *config.Store","}","","// Check that our Reconciler implements pipelinerunreconciler.Interface and pipelinerunreconciler.Finalizer","var _ pipelinerunreconciler.Interface = (*Reconciler)(nil)","var _ pipelinerunreconciler.Finalizer = (*Reconciler)(nil)","","// ReconcileKind makes new watcher reconcile cycle to handle PipelineRun.","func (r *Reconciler) ReconcileKind(ctx context.Context, pr *pipelinev1.PipelineRun) knativereconciler.Event {","\tlogger := logging.FromContext(ctx).With(zap.String(\"results.tekton.dev/kind\", \"PipelineRun\"))","","\tlogger.Infof(\"Initiating reconciliation for PipelineRun '%s/%s'\", pr.Namespace, pr.Name)","","\tif r.cfg.DisableStoringIncompleteRuns {","\t\t// Skip if pipelinerun is not done","\t\tif !pr.IsDone() {","\t\t\tlogger.Debugf(\"pipelinerun %s/%s is not done and incomplete runs are disabled, skipping storing\", pr.Namespace, pr.Name)","\t\t\treturn nil","\t\t}","","\t\t// Skip if pipelinerun is already stored","\t\tif pr.Annotations != nil \u0026\u0026 pr.Annotations[resultsannotation.Stored] == \"true\" {","\t\t\tlogger.Debugf(\"pipelinerun %s/%s is already stored, skipping\", pr.Namespace, pr.Name)","\t\t\treturn nil","\t\t}","\t}","","\tpipelineRunClient := \u0026client.PipelineRunClient{","\t\tPipelineRunInterface: r.pipelineClient.TektonV1().PipelineRuns(pr.Namespace),","\t}","","\tdyn := dynamic.NewDynamicReconciler(r.kubeClientSet, r.resultsClient, r.logsClient, pipelineRunClient, r.cfg)","\t// Tell the dynamic reconciler to wait until all underlying TaskRuns are","\t// ready for deletion before deleting the PipelineRun. This guarantees","\t// that the TaskRuns will not be deleted before their final state being","\t// properly archived into the API server.","\tdyn.IsReadyForDeletionFunc = r.areAllUnderlyingTaskRunsReadyForDeletion","\tdyn.AfterDeletion = func(ctx context.Context, object results.Object) error {","\t\tpr, ok := object.(*pipelinev1.PipelineRun)","\t\tif !ok {","\t\t\treturn fmt.Errorf(\"expected PipelineRun, got %T\", object)","\t\t}","\t\treturn r.pipelineRunMetrics.DurationAndCountDeleted(ctx, r.configStore.Load().Metrics, pr)","\t}","\tdyn.AfterStorage = func(ctx context.Context, object results.Object, _ bool) error {","\t\tpr, ok := object.(*pipelinev1.PipelineRun)","\t\tif !ok {","\t\t\treturn fmt.Errorf(\"expected PipelineRun, got %T\", object)","\t\t}","\t\treturn r.metrics.RecordStorageLatency(ctx, pr)","\t}","","\treturn dyn.Reconcile(logging.WithLogger(ctx, logger), pr)","}","","func (r *Reconciler) areAllUnderlyingTaskRunsReadyForDeletion(ctx context.Context, object results.Object) (bool, error) {","\tpipelineRun, ok := object.(*pipelinev1.PipelineRun)","\tif !ok {","\t\treturn false, fmt.Errorf(\"unexpected object (must not happen): want %T, but got %T\", \u0026pipelinev1.PipelineRun{}, object)","\t}","","\tlogger := logging.FromContext(ctx)","","\tif len(pipelineRun.Status.ChildReferences) \u003e 0 {","\t\tfor _, reference := range pipelineRun.Status.ChildReferences {","\t\t\ttaskRun, err := r.taskRunLister.TaskRuns(pipelineRun.Namespace).Get(reference.Name)","\t\t\tif err != nil {","\t\t\t\tif apierrors.IsNotFound(err) {","\t\t\t\t\t// Let's assume that the TaskRun in","\t\t\t\t\t// question is gone and therefore, we","\t\t\t\t\t// can safely ignore it.","\t\t\t\t\tlogger.Debugf(\"TaskRun %s/%s associated with PipelineRun %s is no longer available. Skipping.\", pipelineRun.Namespace, reference.Name, pipelineRun.Name)","\t\t\t\t\tcontinue","\t\t\t\t}","\t\t\t\treturn false, fmt.Errorf(\"error reading TaskRun from the indexer: %w\", err)","\t\t\t}","\t\t\tif !isMarkedAsReadyForDeletion(taskRun) {","\t\t\t\tlogger.Debugf(\"TaskRun %s/%s associated with PipelineRun %s isn't yet ready to be deleted - the annotation %s is missing\", taskRun.Namespace, taskRun.Name, pipelineRun.Name, resultsannotation.ChildReadyForDeletion)","\t\t\t\treturn false, nil","\t\t\t}","\t\t}","\t}","","\treturn true, nil","}","","func isMarkedAsReadyForDeletion(taskRun *pipelinev1.TaskRun) bool {","\tif taskRun.Annotations == nil {","\t\treturn false","\t}","\tif _, found := taskRun.Annotations[resultsannotation.ChildReadyForDeletion]; found {","\t\treturn true","\t}","\treturn false","}","","// FinalizeKind implements pipelinerunreconciler.Finalizer","// We utilize finalizers to ensure that we get a crack at storing every pipelinerun","// that we see flowing through the system.  If we don't add a finalizer, it could","// get cleaned up before we see the final state and store it.","func (r *Reconciler) FinalizeKind(ctx context.Context, pr *pipelinev1.PipelineRun) knativereconciler.Event {","\t// Reconcile the pipelinerun to ensure that it is stored in the database","\trerr := r.ReconcileKind(ctx, pr)","","\treturn r.finalize(ctx, pr, rerr)","}","","func (r *Reconciler) finalize(ctx context.Context, pr *pipelinev1.PipelineRun, rerr error) knativereconciler.Event {","\t// If logsClient isn't nil, it means we have logging storage enabled","\t// and we can't use finalizers to coordinate deletion.","\tif r.logsClient != nil {","\t\treturn nil","\t}","","\t// If annotation update is disabled, we can't use finalizers to coordinate deletion.","\tif r.cfg.DisableAnnotationUpdate {","\t\treturn nil","\t}","","\t// Check to make sure the PipelineRun is finished.","\tif !pr.IsDone() {","\t\tlogging.FromContext(ctx).Debugf(\"pipelinerun %s/%s is still running\", pr.Namespace, pr.Name)","\t\treturn nil","\t}","","\tvar storeDeadline, now time.Time","","\t// Check if the store deadline is configured","\tif r.cfg.StoreDeadline != nil {","\t\tif pr.Status.CompletionTime == nil {","\t\t\tlogging.FromContext(ctx).Infof(\"removing finalizer without wait, no completion time set for pipelinerun %s/%s\",","\t\t\t\tpr.Namespace, pr.Name)","\t\t\treturn nil","\t\t}","\t\tnow = time.Now().UTC()","\t\tstoreDeadline = pr.Status.CompletionTime.UTC().Add(*r.cfg.StoreDeadline)","\t\tif now.After(storeDeadline) {","\t\t\tlogging.FromContext(ctx).Debugf(\"store deadline: %s now: %s, completion time: %s\", storeDeadline.String(), now.String(),","\t\t\t\tpr.Status.CompletionTime.UTC().String())","\t\t\tlogging.FromContext(ctx).Debugf(\"store deadline has passed for pipelinerun %s/%s\", pr.Namespace, pr.Name)","\t\t\t_, ok := pr.Annotations[resultsannotation.Stored]","\t\t\tif !ok {","\t\t\t\tlogging.FromContext(ctx).Errorf(\"pipelinerun not stored: %s/%s, uid: %s,\",","\t\t\t\t\tpr.Namespace, pr.Name, pr.UID)","\t\t\t\tif err := metrics.CountRunNotStored(ctx, pr.Namespace, \"PipelineRun\"); err != nil {","\t\t\t\t\tlogging.FromContext(ctx).Errorf(\"error counting PipelineRun as not stored: %w\", err)","\t\t\t\t}","\t\t\t}","\t\t\treturn nil // Proceed with deletion","\t\t}","\t}","","\tif pr.Annotations == nil {","\t\tlogging.FromContext(ctx).Debugf(\"pipelinerun %s/%s annotations are missing, now: %s, storeDeadline: %s\",","\t\t\tpr.Namespace, pr.Name, now.String(), storeDeadline.String())","\t\treturn controller.NewRequeueAfter(r.cfg.FinalizerRequeueInterval)","\t}","","\tstored, ok := pr.Annotations[resultsannotation.Stored]","\tif !ok {","\t\tlogging.FromContext(ctx).Debugf(\"stored annotation is missing on pipelinerun %s/%s, now: %s, storeDeadline: %s\",","\t\t\tpr.Namespace, pr.Name, now.String(), storeDeadline.String())","\t\treturn controller.NewRequeueAfter(r.cfg.FinalizerRequeueInterval)","\t}","\tif rerr != nil || stored != \"true\" {","\t\tlogging.FromContext(ctx).Debugf(\"stored annotation is not true on pipelinerun %s/%s, now: %s, storeDeadline: %s\",","\t\t\tpr.Namespace, pr.Name, now.String(), storeDeadline.String())","\t\treturn controller.NewRequeueAfter(r.cfg.FinalizerRequeueInterval)","\t}","","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,0,0,1,0,0,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,0,1,0,2,2,2,2,0,0,0,2,0,0,2,2,2,2,2,2,2,1,0,0,0,0,0,0,1,1,1,1,1,1,0,2,2,2,2,1,1,0,0,2,1,1,0,0,2,2,2,2,0,2,2,2,2,2,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,0,0,0,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,0,2,0]},{"id":96,"path":"pkg/watcher/reconciler/taskrun/controller.go","lines":["// Copyright 2020 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","// Package taskrun provides the TaskRun reconciler controller.","package taskrun","","import (","\t\"context\"","","\ttaskrunreconciler \"github.com/tektoncd/pipeline/pkg/client/injection/reconciler/pipeline/v1/taskrun\"","\t\"github.com/tektoncd/results/pkg/apis/config\"","\t\"github.com/tektoncd/results/pkg/metrics\"","\t\"github.com/tektoncd/results/pkg/taskrunmetrics\"","\t\"github.com/tektoncd/results/pkg/watcher/logs\"","\t\"github.com/tektoncd/results/pkg/watcher/reconciler\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"knative.dev/pkg/configmap\"","\t\"knative.dev/pkg/controller\"","\t\"knative.dev/pkg/logging\"","","\tpipelineclient \"github.com/tektoncd/pipeline/pkg/client/injection/client\"","\ttaskruninformer \"github.com/tektoncd/pipeline/pkg/client/injection/informers/pipeline/v1/taskrun\"","\tkubeclient \"knative.dev/pkg/client/injection/kube/client\"",")","","// NewController creates a Controller for watching TaskRuns.","func NewController(ctx context.Context, resultsClient pb.ResultsClient, cmw configmap.Watcher) *controller.Impl {","\treturn NewControllerWithConfig(ctx, resultsClient, \u0026reconciler.Config{}, cmw)","}","","// NewControllerWithConfig creates a Controller for watching TaskRuns by config.","func NewControllerWithConfig(ctx context.Context, resultsClient pb.ResultsClient, cfg *reconciler.Config, cmw configmap.Watcher) *controller.Impl {","\tinformer := taskruninformer.Get(ctx)","\tlister := informer.Lister()","\tlogger := logging.FromContext(ctx)","\tconfigStore := config.NewStore(logger.Named(\"config-store\"),","\t\tmetrics.OnStore(logger),","\t\ttaskrunmetrics.MetricsOnStore(logger))","\tconfigStore.WatchConfigs(cmw)","","\tc := \u0026Reconciler{","\t\tkubeClientSet:  kubeclient.Get(ctx),","\t\tresultsClient:  resultsClient,","\t\tlogsClient:     logs.Get(ctx),","\t\ttaskRunLister:  lister,","\t\tpipelineClient: pipelineclient.Get(ctx),","\t\tcfg:            cfg,","\t\tconfigStore:    configStore,","\t\tmetrics:        metrics.NewRecorder(),","\t\ttaskRunMetrics: taskrunmetrics.NewRecorder(),","\t}","","\timpl := taskrunreconciler.NewImpl(ctx, c, func(_ *controller.Impl) controller.Options {","\t\treturn controller.Options{","\t\t\t// This results taskrun reconciler shouldn't mutate the taskrun's status.","\t\t\tSkipStatusUpdates: true,","\t\t\tConfigStore:       configStore,","\t\t\tFinalizerName:     \"results.tekton.dev/taskrun\",","\t\t}","\t})","","\t_, err := informer.Informer().AddEventHandler(controller.HandleAll(impl.Enqueue))","\tif err != nil {","\t\tlogger.Panicf(\"Couldn't register TaskRun informer event handler: %w\", err)","\t}","","\treturn impl","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1,0]},{"id":97,"path":"pkg/watcher/reconciler/taskrun/reconciler.go","lines":["package taskrun","","import (","\t\"context\"","\t\"fmt\"","\t\"time\"","","\t\"github.com/tektoncd/results/pkg/apis/config\"","\t\"github.com/tektoncd/results/pkg/metrics\"","\t\"github.com/tektoncd/results/pkg/taskrunmetrics\"","","\tpipelinev1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"github.com/tektoncd/pipeline/pkg/client/clientset/versioned\"","\ttaskrunreconciler \"github.com/tektoncd/pipeline/pkg/client/injection/reconciler/pipeline/v1/taskrun\"","\tv1 \"github.com/tektoncd/pipeline/pkg/client/listers/pipeline/v1\"","\t\"github.com/tektoncd/results/pkg/watcher/reconciler\"","","\tresultsannotation \"github.com/tektoncd/results/pkg/watcher/reconciler/annotation\"","\t\"github.com/tektoncd/results/pkg/watcher/reconciler/client\"","\t\"github.com/tektoncd/results/pkg/watcher/reconciler/dynamic\"","\t\"github.com/tektoncd/results/pkg/watcher/results\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"go.uber.org/zap\"","\t\"k8s.io/client-go/kubernetes\"","\t\"knative.dev/pkg/controller\"","\t\"knative.dev/pkg/logging\"","\tknativereconciler \"knative.dev/pkg/reconciler\"",")","","// Reconciler represents taskRun watcher logic","type Reconciler struct {","","\t// kubeClientSet allows us to talk to the k8s for core APIs","\tkubeClientSet kubernetes.Interface","","\tresultsClient  pb.ResultsClient","\tlogsClient     pb.LogsClient","\ttaskRunLister  v1.TaskRunLister","\tpipelineClient versioned.Interface","\tcfg            *reconciler.Config","\tmetrics        *metrics.Recorder","\ttaskRunMetrics *taskrunmetrics.Recorder","\tconfigStore    *config.Store","}","","// Check that our Reconciler implements taskrunreconciler.Interface and taskrunreconciler.Finalizer","var _ taskrunreconciler.Interface = (*Reconciler)(nil)","var _ taskrunreconciler.Finalizer = (*Reconciler)(nil)","","// ReconcileKind makes new watcher reconcile cycle to handle TaskRun.","func (r *Reconciler) ReconcileKind(ctx context.Context, tr *pipelinev1.TaskRun) knativereconciler.Event {","\tlogger := logging.FromContext(ctx).With(zap.String(\"results.tekton.dev/kind\", \"TaskRun\"))","","\tif r.cfg.DisableStoringIncompleteRuns {","\t\t// Skip if taskrun is not done","\t\tif !tr.IsDone() {","\t\t\tlogger.Debugf(\"taskrun %s/%s is not done and incomplete runs are disabled, skipping storing\", tr.Namespace, tr.Name)","\t\t\treturn nil","\t\t}","","\t\t// Skip if taskrun is already stored","\t\tif tr.Annotations != nil \u0026\u0026 tr.Annotations[resultsannotation.Stored] == \"true\" {","\t\t\tlogger.Debugf(\"taskrun %s/%s is already stored, skipping\", tr.Namespace, tr.Name)","\t\t\treturn nil","\t\t}","\t}","","\ttaskRunClient := \u0026client.TaskRunClient{","\t\tTaskRunInterface: r.pipelineClient.TektonV1().TaskRuns(tr.Namespace),","\t}","","\tdyn := dynamic.NewDynamicReconciler(r.kubeClientSet, r.resultsClient, r.logsClient, taskRunClient, r.cfg)","\tdyn.AfterDeletion = func(ctx context.Context, object results.Object) error {","\t\ttr, ok := object.(*pipelinev1.TaskRun)","\t\tif !ok {","\t\t\treturn fmt.Errorf(\"expected TaskRun, got %T\", object)","\t\t}","\t\treturn r.taskRunMetrics.DurationAndCountDeleted(ctx, r.configStore.Load().Metrics, tr)","\t}","\tdyn.AfterStorage = func(ctx context.Context, o results.Object, _ bool) error {","\t\ttr, ok := o.(*pipelinev1.TaskRun)","\t\tif !ok {","\t\t\treturn fmt.Errorf(\"expected TaskRun, got %T\", o)","\t\t}","\t\treturn r.metrics.RecordStorageLatency(ctx, tr)","\t}","\treturn dyn.Reconcile(logging.WithLogger(ctx, logger), tr)","}","","// FinalizeKind implements pipelinerunreconciler.Finalizer","// We utilize finalizers to ensure that we get a crack at storing every taskrun","// that we see flowing through the system.  If we don't add a finalizer, it could","// get cleaned up before we see the final state and store it.","func (r *Reconciler) FinalizeKind(ctx context.Context, tr *pipelinev1.TaskRun) knativereconciler.Event {","\t// Reconcile the taskrun to ensure that it is stored in the database","\trerr := r.ReconcileKind(ctx, tr)","","\treturn r.finalize(ctx, tr, rerr)","}","","func (r *Reconciler) finalize(ctx context.Context, tr *pipelinev1.TaskRun, rerr error) knativereconciler.Event {","\t// If logsClient isn't nil, it means we have logging storage enabled","\t// and we can't use finalizers to coordinate deletion.","\tif r.logsClient != nil {","\t\treturn nil","\t}","","\t// If annotation update is disabled, we can't use finalizers to coordinate deletion.","\tif r.cfg.DisableAnnotationUpdate {","\t\treturn nil","\t}","","\t// Check the TaskRun has finished.","\tif !tr.IsDone() {","\t\tlogging.FromContext(ctx).Debugf(\"taskrun %s/%s is still running\", tr.Namespace, tr.Name)","\t\treturn nil","\t}","","\tnow := time.Now().UTC()","","\t// Check if the forwarding buffer is configured and passed","\tif r.cfg.ForwardBuffer != nil {","\t\tif tr.Status.CompletionTime == nil {","\t\t\tlogging.FromContext(ctx).Infof(\"removing finalizer without wait, no completion time set for taskrun %s/%s\",","\t\t\t\ttr.Namespace, tr.Name)","\t\t\treturn nil","\t\t}","\t\tbuffer := tr.Status.CompletionTime.UTC().Add(*r.cfg.ForwardBuffer)","\t\tif !now.After(buffer) {","\t\t\tlogging.FromContext(ctx).Debugf(\"log forwarding buffer wait for taskrun %s/%s\", tr.Namespace, tr.Name)","\t\t\treturn controller.NewRequeueAfter(r.cfg.FinalizerRequeueInterval)","\t\t}","\t}","","\tvar storeDeadline time.Time","","\t// Check if the store deadline is configured","\tif r.cfg.StoreDeadline != nil {","\t\tif tr.Status.CompletionTime == nil {","\t\t\tlogging.FromContext(ctx).Infof(\"removing finalizer without wait, no completion time set for taskrun %s/%s\",","\t\t\t\ttr.Namespace, tr.Name)","\t\t\treturn nil","\t\t}","\t\tstoreDeadline = tr.Status.CompletionTime.UTC().Add(*r.cfg.StoreDeadline)","\t\tif now.After(storeDeadline) {","\t\t\tlogging.FromContext(ctx).Debugf(\"store deadline: %s now: %s, completion time: %s\", storeDeadline.String(), now.String(),","\t\t\t\ttr.Status.CompletionTime.UTC().String())","\t\t\tlogging.FromContext(ctx).Debugf(\"store deadline has passed for taskrun %s/%s\", tr.Namespace, tr.Name)","\t\t\t_, ok := tr.Annotations[resultsannotation.Stored]","\t\t\tif !ok {","\t\t\t\tlogging.FromContext(ctx).Errorf(\"taskrun not stored: %s/%s, uid: %s,\",","\t\t\t\t\ttr.Namespace, tr.Name, tr.UID)","\t\t\t\tif err := metrics.CountRunNotStored(ctx, tr.Namespace, \"TaskRun\"); err != nil {","\t\t\t\t\tlogging.FromContext(ctx).Errorf(\"error counting TaskRun as not stored: %w\", err)","\t\t\t\t}","\t\t\t}","\t\t\treturn nil // Proceed with deletion","\t\t}","\t}","","\tif tr.Annotations == nil {","\t\tlogging.FromContext(ctx).Debugf(\"taskrun %s/%s annotations are missing, now: %s, storeDeadline: %s\",","\t\t\ttr.Namespace, tr.Name, now.String(), storeDeadline.String())","\t\treturn controller.NewRequeueAfter(r.cfg.FinalizerRequeueInterval)","\t}","","\tstored, ok := tr.Annotations[resultsannotation.Stored]","\tif !ok {","\t\tlogging.FromContext(ctx).Debugf(\"stored annotation is missing on taskrun %s/%s, now: %s, storeDeadline: %s\",","\t\t\ttr.Namespace, tr.Name, now.String(), storeDeadline.String())","\t\treturn controller.NewRequeueAfter(r.cfg.FinalizerRequeueInterval)","\t}","\tif rerr != nil || stored != \"true\" {","\t\tlogging.FromContext(ctx).Debugf(\"stored annotation is not true on taskrun %s/%s, now: %s, storeDeadline: %s\",","\t\t\ttr.Namespace, tr.Name, now.String(), storeDeadline.String())","\t\treturn controller.NewRequeueAfter(r.cfg.FinalizerRequeueInterval)","\t}","","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,0,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,0,1,0,0,0,0,0,0,1,1,1,1,1,1,0,2,2,2,2,1,1,0,0,2,1,1,0,0,2,2,2,2,0,2,2,2,2,1,1,1,1,1,1,1,1,1,1,0,0,2,2,2,2,2,1,1,1,1,2,2,2,2,2,2,2,2,2,2,1,1,0,2,0,0,0,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,0,2,0]},{"id":98,"path":"pkg/watcher/results/eventlist.go","lines":["// Package results provides the Results client for the watcher.","package results","","import (","\t\"context\"","","\t\"github.com/google/uuid\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/record\"","\t\"github.com/tektoncd/results/pkg/apis/v1alpha3\"","\t\"github.com/tektoncd/results/pkg/watcher/reconciler/annotation\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"google.golang.org/grpc\"","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/status\"",")","","// PutEventList adds the given Object to the Results API.","// If the parent result is missing or the object is not yet associated with a","// result, one is created automatically.","func (c *Client) PutEventList(ctx context.Context, o Object, eventList []byte, opts ...grpc.CallOption) (*pb.Record, error) {","\tres, err := c.ensureResult(ctx, o, opts...)","\tif err != nil {","\t\treturn nil, err","\t}","\treturn c.createEventListRecord(ctx, res, o, eventList, opts...)","}","","// createEventListRecord creates a record for eventlist.","func (c *Client) createEventListRecord(ctx context.Context, result *pb.Result, o Object, eventList []byte, opts ...grpc.CallOption) (*pb.Record, error) {","\tname, err := getEventListRecordName(result, o)","\tif err != nil {","\t\treturn nil, err","\t}","\trec, err := c.GetRecord(ctx, \u0026pb.GetRecordRequest{Name: name}, opts...)","\tif err != nil \u0026\u0026 status.Code(err) != codes.NotFound {","\t\treturn nil, err","\t}","\tif rec != nil {","\t\treturn rec, nil","\t}","\treturn c.CreateRecord(ctx, \u0026pb.CreateRecordRequest{","\t\tParent: result.GetName(),","\t\tRecord: \u0026pb.Record{","\t\t\tName: name,","\t\t\tData: \u0026pb.Any{","\t\t\t\tType:  v1alpha3.EventListRecordType,","\t\t\t\tValue: eventList,","\t\t\t},","\t\t},","\t})","}","","// getEventListRecordName gets the eventlist name to use for the given object.","// The name is derived from a known Tekton annotation if available, else","// the object's UID is used to create MD5 UUID.","func getEventListRecordName(result *pb.Result, o Object) (string, error) {","\tname, ok := o.GetAnnotations()[annotation.EventList]","\tif ok {","\t\treturn name, nil","\t}","\tuid, err := uuid.Parse(result.GetUid())","\tif err != nil {","\t\treturn \"\", nil","\t}","\treturn FormatEventListName(result.GetName(), uid, o), nil","}","","// FormatEventListName generates record name for EventList given resultName,","// result UUID and object - taskrun/pipelinerun.","func FormatEventListName(resultName string, resultUID uuid.UUID, o Object) string {","\treturn record.FormatName(resultName,","\t\tuuid.NewMD5(resultUID, []byte(o.GetUID()+\"eventlist\")).String())","}","","// GetEventListRecord returns eventlist record using gRPC clients.","func (c *Client) GetEventListRecord(ctx context.Context, o Object) (*pb.Record, error) {","\tres, err := c.ensureResult(ctx, o)","\tif err != nil {","\t\treturn nil, err","\t}","\tname, err := getEventListRecordName(res, o)","\tif err != nil {","\t\treturn nil, err","\t}","\trec, err := c.GetRecord(ctx, \u0026pb.GetRecordRequest{Name: name})","\tif err != nil \u0026\u0026 status.Code(err) == codes.NotFound {","\t\treturn nil, nil","\t}","\treturn rec, err","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0]},{"id":99,"path":"pkg/watcher/results/logs.go","lines":["package results","","import (","\t\"context\"","","\t\"github.com/google/uuid\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/log\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/record\"","\t\"github.com/tektoncd/results/pkg/watcher/convert\"","\t\"github.com/tektoncd/results/pkg/watcher/reconciler/annotation\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"google.golang.org/grpc\"","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/status\"",")","","// PutLog adds the given Object to the Results API.","// If the parent result is missing or the object is not yet associated with a","// result, one is created automatically.","func (c *Client) PutLog(ctx context.Context, o Object, opts ...grpc.CallOption) (*pb.Record, error) {","\tres, err := c.ensureResult(ctx, o, opts...)","\tif err != nil {","\t\treturn nil, err","\t}","\treturn c.createLogRecord(ctx, res, o, opts...)","}","","// createLogRecord creates a record for logs.","func (c *Client) createLogRecord(ctx context.Context, result *pb.Result, o Object, opts ...grpc.CallOption) (*pb.Record, error) {","\tname, err := getLogRecordName(result, o)","\tif err != nil {","\t\treturn nil, err","\t}","\tkind := o.GetObjectKind().GroupVersionKind().Kind","\trec, err := c.GetRecord(ctx, \u0026pb.GetRecordRequest{Name: name}, opts...)","\tif err != nil \u0026\u0026 status.Code(err) != codes.NotFound {","\t\treturn nil, err","\t}","\tif rec != nil {","\t\treturn rec, nil","\t}","\tdata, err := convert.ToLogProto(o, kind, name)","\tif err != nil {","\t\treturn nil, err","\t}","\treturn c.CreateRecord(ctx, \u0026pb.CreateRecordRequest{","\t\tParent: result.GetName(),","\t\tRecord: \u0026pb.Record{","\t\t\tName: name,","\t\t\tData: data,","\t\t},","\t})","}","","// getLogRecordName gets the log name to use for the given object.","// The name is derived from a known Tekton annotation if available, else","// the object's UID is used to create MD5 UUID.","func getLogRecordName(result *pb.Result, o Object) (string, error) {","\tname, ok := o.GetAnnotations()[annotation.Log]","\tif ok {","\t\t_, _, name, err := log.ParseName(name)","\t\tif err == nil {","\t\t\treturn record.FormatName(result.GetName(), name), nil","\t\t}","\t}","\tuid, err := uuid.Parse(result.GetUid())","\tif err != nil {","\t\treturn \"\", nil","\t}","\treturn record.FormatName(result.GetName(), uuid.NewMD5(uid, []byte(o.GetUID())).String()), nil","}","","// GetLogRecord returns log record using gRPC clients.","func (c *Client) GetLogRecord(ctx context.Context, o Object) (*pb.Record, error) {","\tres, err := c.ensureResult(ctx, o)","\tif err != nil {","\t\treturn nil, err","\t}","\tname, err := getLogRecordName(res, o)","\tif err != nil {","\t\treturn nil, err","\t}","\trec, err := c.GetRecord(ctx, \u0026pb.GetRecordRequest{Name: name})","\tif err != nil \u0026\u0026 status.Code(err) == codes.NotFound {","\t\treturn nil, nil","\t}","\treturn rec, err","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,1,1,2,0,0,0,2,2,2,1,1,2,2,2,1,1,2,2,2,2,2,1,1,2,2,2,2,2,2,2,0,0,0,0,0,2,2,2,1,1,1,1,0,2,2,1,1,2,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0]},{"id":100,"path":"pkg/watcher/results/results.go","lines":["// Copyright 2021 The Tekton Authors","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//      http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package results","","import (","\t\"context\"","\t\"encoding/json\"","\t\"fmt\"","\t\"strings\"","","\t\"go.uber.org/zap\"","","\t\"github.com/google/go-cmp/cmp\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/record\"","\t\"github.com/tektoncd/results/pkg/api/server/v1alpha2/result\"","\t\"github.com/tektoncd/results/pkg/watcher/convert\"","\t\"github.com/tektoncd/results/pkg/watcher/reconciler\"","\t\"github.com/tektoncd/results/pkg/watcher/reconciler/annotation\"","\tpb \"github.com/tektoncd/results/proto/v1alpha2/results_go_proto\"","\t\"google.golang.org/grpc\"","\t\"google.golang.org/grpc/codes\"","\t\"google.golang.org/grpc/status\"","\t\"google.golang.org/protobuf/testing/protocmp\"","\t\"google.golang.org/protobuf/types/known/timestamppb\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/apimachinery/pkg/runtime\"","\t\"knative.dev/pkg/apis\"","\t\"knative.dev/pkg/controller\"","\t\"knative.dev/pkg/logging\"",")","","const (","\t// objectName is used to store the name of the object in the result summary","\tobjectName = \"object.metadata.name\"",")","","// Client is a wrapper around a Results client that provides helpful utilities","// for performing result operations that require multiple RPCs or data specific","// operations.","type Client struct {","\tpb.ResultsClient","\tpb.LogsClient","\treconciler.Config","}","","// NewClient returns a new results client for the particular kind.","func NewClient(resultsClient pb.ResultsClient, logsClient pb.LogsClient, reconcilerConfig *reconciler.Config) *Client {","\treturn \u0026Client{","\t\tResultsClient: resultsClient,","\t\tLogsClient:    logsClient,","\t\tConfig:        *reconcilerConfig,","\t}","}","","// Object is a union type of different base k8s Object interfaces.","// This is similar in spirit to","// https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.9.4/pkg/client#Object,","// but is defined as its own type to avoid an extra dependency.","type Object interface {","\tmetav1.Object","\truntime.Object","\tStatusConditionGetter","}","","// StatusConditionGetter defines status for Object interface","type StatusConditionGetter interface {","\tGetStatusCondition() apis.ConditionAccessor","}","","// Put adds the given Object to the Results API.","// If the parent result is missing or the object is not yet associated with a","// result, one is created automatically.","// If the Object is already associated with a Record, the existing Record is","// updated - otherwise a new Record is created.","func (c *Client) Put(ctx context.Context, o Object, opts ...grpc.CallOption) (*pb.Result, *pb.Record, error) {","\t// Make sure parent Result exists (or create one)","\tres, err := c.ensureResult(ctx, o, opts...)","\tif err != nil {","\t\treturn nil, nil, err","\t}","","\t// Create or update the rec.","\trec, err := c.upsertRecord(ctx, res.GetName(), o, opts...)","\tif err != nil {","\t\treturn nil, nil, err","\t}","","\treturn res, rec, nil","}","","// ensureResult gets the Result corresponding to the Object, creates a new","// one, or updates the existing Result with new Object details if necessary.","func (c *Client) ensureResult(ctx context.Context, o Object, opts ...grpc.CallOption) (*pb.Result, error) {","\tresName := resultName(o)","\tcurr, err := c.GetResult(ctx, \u0026pb.GetResultRequest{Name: resName}, opts...)","\tif err != nil \u0026\u0026 status.Code(err) != codes.NotFound {","\t\treturn nil, status.Errorf(status.Code(err), \"GetResult(%s): %v\", resName, err)","\t}","","\tres := \u0026pb.Result{","\t\tName: resName,","\t}","\trecName := recordName(resName, o)","\ttopLevel := isTopLevelRecord(o)","\tlogger := logging.FromContext(ctx).With(zap.String(annotation.Result, resName),","\t\tzap.String(annotation.Record, recName),","\t\tzap.Bool(\"results.tekton.dev/top-level-record\", topLevel))","","\tif topLevel {","\t\t// If the object corresponds to a top level record  - include a RecordSummary.","\t\tres.Summary = \u0026pb.RecordSummary{","\t\t\tRecord:    recName,","\t\t\tType:      convert.TypeName(o),","\t\t\tStatus:    convert.Status(o.GetStatusCondition()),","\t\t\tStartTime: getTimestamp(o.GetStatusCondition().GetCondition(apis.ConditionReady)),","\t\t\tEndTime:   getTimestamp(o.GetStatusCondition().GetCondition(apis.ConditionSucceeded)),","\t\t}","\t}","","\t// Set the Result.Annotations and Result.Summary.Annotations fields if","\t// the object in question contains the required annotations.","\tres.Annotations = map[string]string{}","\tif value, found := o.GetAnnotations()[annotation.ResultAnnotations]; found {","\t\tresultAnnotations, err := parseAnnotations(annotation.ResultAnnotations, value)","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\tvar annotations map[string]string","\t\tif curr != nil \u0026\u0026 len(curr.Annotations) != 0 {","\t\t\tcopyKeys(resultAnnotations, curr.Annotations)","\t\t\tannotations = curr.Annotations","\t\t} else {","\t\t\tannotations = resultAnnotations","\t\t}","\t\tres.Annotations = annotations","\t}","","\tif topLevel {","\t\tif value, found := o.GetAnnotations()[annotation.RecordSummaryAnnotations]; found {","\t\t\trecordSummaryAnnotations, err := parseAnnotations(annotation.RecordSummaryAnnotations, value)","\t\t\tif err != nil {","\t\t\t\treturn nil, err","\t\t\t}","\t\t\tvar annotations map[string]string","\t\t\tif curr != nil \u0026\u0026 curr.Summary != nil \u0026\u0026 len(curr.Summary.Annotations) != 0 {","\t\t\t\tcopyKeys(recordSummaryAnnotations, curr.Summary.Annotations)","\t\t\t\tannotations = curr.Summary.Annotations","\t\t\t} else {","\t\t\t\tannotations = recordSummaryAnnotations","\t\t\t}","\t\t\tres.Summary.Annotations = annotations","\t\t}","\t\t// Set the Result.Summary.Labels fields if the object in question contains the required labels.","\t\tsummaryLabels := strings.Split(c.SummaryLabels, \",\")","\t\tif len(summaryLabels) \u003e 0 \u0026\u0026 summaryLabels[0] != \"\" {","\t\t\tfor _, v := range summaryLabels {","\t\t\t\tif value, found := o.GetLabels()[v]; found {","\t\t\t\t\tres.Annotations[v] = value","\t\t\t\t}","\t\t\t}","\t\t}","\t\tsummaryAnnotations := strings.Split(c.SummaryAnnotations, \",\")","\t\tif len(summaryAnnotations) \u003e 0 \u0026\u0026 summaryAnnotations[0] != \"\" {","\t\t\tfor _, v := range summaryAnnotations {","\t\t\t\tif value, found := o.GetLabels()[v]; found {","\t\t\t\t\tres.Annotations[v] = value","\t\t\t\t}","\t\t\t}","\t\t}","\t\tres.Annotations[objectName] = o.GetName()","\t}","","\t// Regardless of whether the object is a top level record or not,","\t// if the Result doesn't exist yet just create it and return.","\tif status.Code(err) == codes.NotFound {","\t\tlogger.Debug(\"Result doesn't exist yet - creating\")","\t\treq := \u0026pb.CreateResultRequest{","\t\t\tParent: parentName(o),","\t\t\tResult: res,","\t\t}","\t\tcreated, err := c.CreateResult(ctx, req, opts...)","\t\tif err != nil {","\t\t\tif status.Code(err) == codes.AlreadyExists {","\t\t\t\tlogger.Debug(\"Result was created concurrently - refetching\")","\t\t\t\treturn c.GetResult(ctx, \u0026pb.GetResultRequest{","\t\t\t\t\tName: resName,","\t\t\t\t}, opts...)","\t\t\t}","\t\t\treturn nil, err","\t\t}","\t\treturn created, nil","\t}","","\t// From here on, we're checking to see if there are any updates that need","\t// to be made to the Record.","","\tif !topLevel {","\t\t// If the object isn't top level there's nothing else to do because we","\t\t// won't be modifying the RecordSummary.","\t\tlogger.Debug(\"No further actions to be done on the Result: the object is not a top level record\")","\t\treturn curr, nil","\t}","","\t// If this object is a top level record, only update if there's been a","\t// change to the RecordSummary (only looking at the summary also helps us","\t// avoid OUTPUT_ONLY fields in the Result)","\tif cmp.Equal(curr.GetSummary(), res.GetSummary(), protocmp.Transform()) {","\t\tlogger.Debug(\"No further actions to be done on the Result: no differences found\")","\t\treturn curr, nil","\t}","\treq := \u0026pb.UpdateResultRequest{","\t\tName:   resName,","\t\tResult: res,","\t}","\treturn c.UpdateResult(ctx, req, opts...)","}","","// parseAnnotations attempts to return the provided value as a map of strings.","func parseAnnotations(annotationKey, value string) (map[string]string, error) {","\tvar data map[string]interface{}","\tif err := json.Unmarshal([]byte(value), \u0026data); err != nil {","\t\treturn nil, controller.NewPermanentError(fmt.Errorf(\"error parsing annotation %s: %w\", annotationKey, err))","\t}","\tannotations := map[string]string{}","\tfor i, v := range data {","\t\tannotations[i] = fmt.Sprint(v)","\t}","\treturn annotations, nil","}","","func copyKeys(in, out map[string]string) {","\tfor key, value := range in {","\t\tout[key] = value","\t}","}","","func getTimestamp(c *apis.Condition) *timestamppb.Timestamp {","\tif c == nil || c.IsFalse() {","\t\treturn nil","\t}","\treturn timestamppb.New(c.LastTransitionTime.Inner.Time)","}","","// resultName gets the result name to use for the given object.","// The name is derived from a known Tekton annotation if available, else","// the object's name is used.","func resultName(o metav1.Object) string {","\t// Special case result annotations, since this should already be the","\t// full result identifier.","\tif v, ok := o.GetAnnotations()[annotation.Result]; ok {","\t\treturn v","\t}","","\tvar part string","\tif v, ok := o.GetLabels()[\"triggers.tekton.dev/triggers-eventid\"]; ok {","\t\t// Don't prefix trigger events. These are 1) not CRD types, 2) are","\t\t// intended to be unique identifiers already, and 3) should be applied","\t\t// to all objects created via trigger templates, so there's no need to","\t\t// prefix these to avoid collision.","\t\tpart = v","\t} else if len(o.GetOwnerReferences()) \u003e 0 {","\t\tfor _, owner := range o.GetOwnerReferences() {","\t\t\tif strings.EqualFold(owner.Kind, \"pipelinerun\") {","\t\t\t\tpart = string(owner.UID)","\t\t\t\tbreak","\t\t\t}","\t\t}","\t}","","\tif part == \"\" {","\t\tpart = defaultName(o)","\t}","\treturn result.FormatName(o.GetNamespace(), part)","}","","func recordName(parent string, o Object) string {","\t// Attempt to read the record name from annotations only if the object","\t// in question is a top-level record (i.e. it isn't owned by another","\t// object). Otherwise, the annotation containing the record name maybe","\t// was propagated by the owner what causes conflicts while upserting the","\t// object into the API. For further details, please see","\t// https://github.com/tektoncd/results/issues/296.","\tif isTopLevelRecord(o) {","\t\tif name, ok := o.GetAnnotations()[annotation.Record]; ok {","\t\t\treturn name","\t\t}","\t}","\treturn record.FormatName(parent, defaultName(o))","}","","// parentName returns the parent's name of the result in question. If the","// results annotation is set, returns the first segment of the result","// name. Otherwise, returns the object's namespace.","func parentName(o metav1.Object) string {","\tif value, found := o.GetAnnotations()[annotation.Result]; found {","\t\tif parts := strings.Split(value, \"/\"); len(parts) != 0 {","\t\t\treturn parts[0]","\t\t}","\t}","\treturn o.GetNamespace()","}","","// upsertRecord updates or creates a record for the object. If there has been","// no change in the Record data, the existing Record is returned.","func (c *Client) upsertRecord(ctx context.Context, parent string, o Object, opts ...grpc.CallOption) (*pb.Record, error) {","\trecName := recordName(parent, o)","\tlogger := logging.FromContext(ctx).With(zap.String(annotation.Record, recName))","\tdata, err := convert.ToProto(o)","\tif err != nil {","\t\treturn nil, err","\t}","","\tcurr, err := c.GetRecord(ctx, \u0026pb.GetRecordRequest{Name: recName}, opts...)","\tif err != nil \u0026\u0026 status.Code(err) != codes.NotFound {","\t\treturn nil, err","\t}","\tif curr != nil {","\t\t// Data already exists for the Record - update it iff there is a diff of Data.","\t\tif cmp.Equal(data, curr.GetData(), protocmp.Transform()) {","\t\t\tlogger.Debug(\"No further actions to be done on the Record: no changes found\")","\t\t\treturn curr, nil","\t\t}","","\t\tlogger.Debug(\"Updating Record\")","\t\tcurr.Data = data","\t\treturn c.UpdateRecord(ctx, \u0026pb.UpdateRecordRequest{","\t\t\tRecord: curr,","\t\t\tEtag:   curr.GetEtag(),","\t\t}, opts...)","\t}","","\tlogger.Debug(\"Record doesn't exist yet - creating\")","\treturn c.CreateRecord(ctx, \u0026pb.CreateRecordRequest{","\t\tParent: parent,","\t\tRecord: \u0026pb.Record{","\t\t\tName: recName,","\t\t\tData: data,","\t\t},","\t}, opts...)","}","","// defaultName is the default Result/Record name that should be used if one is","// not already associated to the Object.","func defaultName(o metav1.Object) string {","\treturn string(o.GetUID())","}","","// isTopLevelRecord determines whether an Object is a top level Record - e.g. a","// Record that should be considered the primary record for the result for purposes","// of timing, status, etc. For example, if a Result contains records for a PipelineRun","// and TaskRun, the PipelineRun should take precedence.","// We define an Object to be top level if it does not have any OwnerReferences.","func isTopLevelRecord(o Object) bool {","\treturn len(o.GetOwnerReferences()) == 0","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,1,1,0,0,2,2,1,1,0,2,0,0,0,0,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,2,1,1,2,2,1,1,2,2,2,2,0,0,2,2,2,2,1,1,2,2,1,1,2,2,2,2,0,0,2,2,1,1,1,1,0,0,2,2,1,1,1,1,0,0,2,0,0,0,0,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,0,2,0,0,0,0,0,2,1,1,1,1,1,0,0,0,0,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,1,1,2,2,2,2,2,0,0,1,1,1,1,0,0,2,2,2,2,1,0,0,0,0,0,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,0,2,0,0,0,0,0,2,2,2,2,2,0,2,0,0,0,0,2,2,2,2,2,1,1,0,2,2,1,1,2,2,2,2,2,2,0,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,0,0,0,0,2,2,2,0,0,0,0,0,0,2,2,2]}],"tree":{"name":".","type":"dir","children":[{"name":"internal","type":"dir","children":[{"name":"fieldmask","type":"dir","children":[{"name":"fieldmask.go","type":"file","fileId":0}]}]},{"name":"pkg","type":"dir","children":[{"name":"api","type":"dir","children":[{"name":"server","type":"dir","children":[{"name":"cel","type":"dir","children":[{"name":"cel.go","type":"file","fileId":1},{"name":"env.go","type":"file","fileId":2}]},{"name":"cel2sql","type":"dir","children":[{"name":"concat.go","type":"file","fileId":3},{"name":"convert.go","type":"file","fileId":4},{"name":"functions.go","type":"file","fileId":5},{"name":"index.go","type":"file","fileId":6},{"name":"interpreter.go","type":"file","fileId":7},{"name":"operators.go","type":"file","fileId":8},{"name":"select.go","type":"file","fileId":9},{"name":"type_coercion.go","type":"file","fileId":10}]},{"name":"config","type":"dir","children":[{"name":"config.go","type":"file","fileId":11}]},{"name":"db","type":"dir","children":[{"name":"errors","type":"dir","children":[{"name":"postgres","type":"dir","children":[{"name":"postgres.go","type":"file","fileId":13}]},{"name":"sqlite","type":"dir","children":[{"name":"sqlite.go","type":"file","fileId":14}]},{"name":"errors.go","type":"file","fileId":12}]},{"name":"pagination","type":"dir","children":[{"name":"pagination.go","type":"file","fileId":17}]},{"name":"log_level.go","type":"file","fileId":15},{"name":"model.go","type":"file","fileId":16}]},{"name":"features","type":"dir","children":[{"name":"features.go","type":"file","fileId":18}]},{"name":"logger","type":"dir","children":[{"name":"logger.go","type":"file","fileId":19}]},{"name":"v1alpha2","type":"dir","children":[{"name":"auth","type":"dir","children":[{"name":"impersonation","type":"dir","children":[{"name":"impersonation.go","type":"file","fileId":20}]},{"name":"nop.go","type":"file","fileId":21},{"name":"rbac.go","type":"file","fileId":22}]},{"name":"lister","type":"dir","children":[{"name":"aggregator.go","type":"file","fileId":24},{"name":"filter.go","type":"file","fileId":25},{"name":"limit.go","type":"file","fileId":26},{"name":"lister.go","type":"file","fileId":27},{"name":"offset.go","type":"file","fileId":28},{"name":"order.go","type":"file","fileId":29},{"name":"page_token.go","type":"file","fileId":30}]},{"name":"log","type":"dir","children":[{"name":"file.go","type":"file","fileId":31},{"name":"gcs.go","type":"file","fileId":32},{"name":"log.go","type":"file","fileId":33},{"name":"s3.go","type":"file","fileId":34}]},{"name":"plugin","type":"dir","children":[{"name":"plugin_logs.go","type":"file","fileId":38},{"name":"server.go","type":"file","fileId":39}]},{"name":"record","type":"dir","children":[{"name":"record.go","type":"file","fileId":40}]},{"name":"result","type":"dir","children":[{"name":"result.go","type":"file","fileId":42}]},{"name":"handlers.go","type":"file","fileId":23},{"name":"logs.go","type":"file","fileId":35},{"name":"ordering.go","type":"file","fileId":36},{"name":"pagination.go","type":"file","fileId":37},{"name":"records.go","type":"file","fileId":41},{"name":"results.go","type":"file","fileId":43},{"name":"server.go","type":"file","fileId":44},{"name":"summary.go","type":"file","fileId":45}]}]}]},{"name":"apis","type":"dir","children":[{"name":"config","type":"dir","children":[{"name":"metrics.go","type":"file","fileId":46},{"name":"retention.go","type":"file","fileId":47},{"name":"store.go","type":"file","fileId":48}]},{"name":"v1alpha3","type":"dir","children":[{"name":"types.go","type":"file","fileId":49}]}]},{"name":"cli","type":"dir","children":[{"name":"client","type":"dir","children":[{"name":"logs","type":"dir","children":[{"name":"logs.go","type":"file","fileId":51}]},{"name":"records","type":"dir","children":[{"name":"records.go","type":"file","fileId":52}]},{"name":"base.go","type":"file","fileId":50},{"name":"response.go","type":"file","fileId":53}]},{"name":"common","type":"dir","children":[{"name":"prerun","type":"dir","children":[{"name":"prerun.go","type":"file","fileId":57}]},{"name":"format.go","type":"file","fileId":54},{"name":"labels.go","type":"file","fileId":55},{"name":"params.go","type":"file","fileId":56},{"name":"utils.go","type":"file","fileId":58}]},{"name":"config","type":"dir","children":[{"name":"config.go","type":"file","fileId":59},{"name":"extension.go","type":"file","fileId":60},{"name":"host.go","type":"file","fileId":61},{"name":"platform.go","type":"file","fileId":62}]},{"name":"dev","type":"dir","children":[{"name":"client","type":"dir","children":[{"name":"client.go","type":"file","fileId":63}]},{"name":"config","type":"dir","children":[{"name":"config.go","type":"file","fileId":64}]},{"name":"flags","type":"dir","children":[{"name":"flags.go","type":"file","fileId":65}]},{"name":"format","type":"dir","children":[{"name":"format.go","type":"file","fileId":66}]},{"name":"portforward","type":"dir","children":[{"name":"portforward.go","type":"file","fileId":67}]}]},{"name":"flags","type":"dir","children":[{"name":"flags.go","type":"file","fileId":68}]},{"name":"options","type":"dir","children":[{"name":"describe.go","type":"file","fileId":69},{"name":"list.go","type":"file","fileId":70},{"name":"logs.go","type":"file","fileId":71}]},{"name":"testutils","type":"dir","children":[{"name":"cobra.go","type":"file","fileId":72},{"name":"kubeconfig.go","type":"file","fileId":73},{"name":"mock_rest_client.go","type":"file","fileId":74},{"name":"params.go","type":"file","fileId":75},{"name":"utils.go","type":"file","fileId":76}]}]},{"name":"converter","type":"dir","children":[{"name":"convert.go","type":"file","fileId":77}]},{"name":"internal","type":"dir","children":[{"name":"jsonutil","type":"dir","children":[{"name":"jsonutil.go","type":"file","fileId":78}]},{"name":"protoutil","type":"dir","children":[{"name":"protoutil.go","type":"file","fileId":79}]}]},{"name":"logs","type":"dir","children":[{"name":"writer.go","type":"file","fileId":80}]},{"name":"metrics","type":"dir","children":[{"name":"metrics.go","type":"file","fileId":81}]},{"name":"pipelinerunmetrics","type":"dir","children":[{"name":"metrics.go","type":"file","fileId":82}]},{"name":"retention","type":"dir","children":[{"name":"config.go","type":"file","fileId":83},{"name":"job.go","type":"file","fileId":84}]},{"name":"taskrunmetrics","type":"dir","children":[{"name":"metrics.go","type":"file","fileId":85}]},{"name":"watcher","type":"dir","children":[{"name":"convert","type":"dir","children":[{"name":"convert.go","type":"file","fileId":86}]},{"name":"grpc","type":"dir","children":[{"name":"creds.go","type":"file","fileId":87}]},{"name":"logs","type":"dir","children":[{"name":"client.go","type":"file","fileId":88}]},{"name":"reconciler","type":"dir","children":[{"name":"annotation","type":"dir","children":[{"name":"annotation.go","type":"file","fileId":89}]},{"name":"client","type":"dir","children":[{"name":"client.go","type":"file","fileId":90}]},{"name":"dynamic","type":"dir","children":[{"name":"dynamic.go","type":"file","fileId":92}]},{"name":"leaderelection","type":"dir","children":[{"name":"leader_election.go","type":"file","fileId":93}]},{"name":"pipelinerun","type":"dir","children":[{"name":"controller.go","type":"file","fileId":94},{"name":"reconciler.go","type":"file","fileId":95}]},{"name":"taskrun","type":"dir","children":[{"name":"controller.go","type":"file","fileId":96},{"name":"reconciler.go","type":"file","fileId":97}]},{"name":"config.go","type":"file","fileId":91}]},{"name":"results","type":"dir","children":[{"name":"eventlist.go","type":"file","fileId":98},{"name":"logs.go","type":"file","fileId":99},{"name":"results.go","type":"file","fileId":100}]}]}]}]},"summary":{"totalLines":7483,"coveredLines":3693,"percent":49.35186422557798}};
    </script>
    <script>
      window.COVERAGE_CONFIG = {"syntaxEnabled":true};
    </script>
    <script>
      (function() {
  'use strict';

  const data = window.COVERAGE_DATA;
  const config = window.COVERAGE_CONFIG || { syntaxEnabled: true };

  // State
  let currentFileId = null;
  let searchQuery = '';
  let contentSearchQuery = '';
  let matches = [];
  let currentMatchIndex = -1;
  let expandedDirs = new Set();
  let syntaxHighlightEnabled = config.syntaxEnabled;
  let sortMode = 'name'; // 'name' or 'coverage'
  let anchorLine = null;        // First line clicked (anchor for shift-select)
  let selectedRange = null;     // { start: N, end: M } or null

  // DOM elements
  const fileTree = document.getElementById('file-tree');
  const viewport = document.getElementById('viewport');
  const filePath = document.getElementById('file-path');
  const summary = document.getElementById('summary');
  const searchInput = document.getElementById('search-input');
  const contentSearch = document.getElementById('content-search');
  const matchInfo = document.getElementById('match-info');
  const prevMatch = document.getElementById('prev-match');
  const nextMatch = document.getElementById('next-match');
  const themeToggle = document.getElementById('theme-toggle');
  const syntaxToggle = document.getElementById('syntax-toggle');
  const helpModal = document.getElementById('help-modal');
  const closeHelp = document.getElementById('close-help');
  const helpToggle = document.getElementById('help-toggle');

  // Coverage cache: fileId -> percentage
  let coverageCache = new Map();

  function initCoverageCache() {
    data.files.forEach((file, idx) => {
      coverageCache.set(idx, calculateFileCoverage(idx));
    });
  }

  function calculateFileCoverage(fileId) {
    const file = data.files[fileId];
    let totalStatements = 0;
    let coveredStatements = 0;

    file.coverage.forEach(cov => {
      if (cov > 0) totalStatements++;
      if (cov === 2) coveredStatements++;
    });

    return totalStatements === 0 ? 0 : (coveredStatements / totalStatements) * 100;
  }

  function calculateDirectoryCoverage(node) {
    if (node.type === 'file') {
      return coverageCache.get(node.fileId) || 0;
    }

    let totalCoverage = 0;
    let fileCount = 0;

    node.children?.forEach(child => {
      const childCov = calculateDirectoryCoverage(child);
      totalCoverage += childCov;
      fileCount++;
    });

    return fileCount === 0 ? 0 : totalCoverage / fileCount;
  }

  function sortTreeNodes(node, mode) {
    if (!node.children || node.children.length === 0) return node;

    // Deep copy to avoid mutating original
    const sorted = { ...node };
    sorted.children = [...node.children].map(child => sortTreeNodes(child, mode));

    // Sort children
    sorted.children.sort((a, b) => {
      // Directories always first
      if (a.type !== b.type) return a.type === 'dir' ? -1 : 1;

      if (mode === 'coverage') {
        const aCov = calculateDirectoryCoverage(a);
        const bCov = calculateDirectoryCoverage(b);
        console.log('Sorting:', a.name, '('+aCov.toFixed(1)+'%) vs', b.name, '('+bCov.toFixed(1)+'%)', '=', bCov - aCov);
        // Descending: high coverage first
        return aCov !== bCov ? bCov - aCov : a.name.localeCompare(b.name);
      }

      return a.name.localeCompare(b.name);
    });

    return sorted;
  }

  // Initialize
  function init() {
    initCoverageCache();
    loadSortPreference();
    renderSummary();
    renderTree();
    setupEventListeners();
    loadTheme();
    loadSyntaxPreference();

    // Check for deep link hash first, otherwise select first file
    if (!navigateToHash() && data.files.length > 0) {
      selectFile(0);
    }

    // Listen for hash changes (browser back/forward)
    window.addEventListener('hashchange', navigateToHash);
  }

  // Deep linking: parse URL hash
  function parseHash() {
    const hash = window.location.hash.slice(1);
    if (!hash) return null;

    const match = hash.match(/^file-(\d+)(?::line-(\d+)(?:-(\d+))?)?$/);
    if (!match) return null;

    return {
      fileId: parseInt(match[1], 10),
      lineStart: match[2] ? parseInt(match[2], 10) : null,
      lineEnd: match[3] ? parseInt(match[3], 10) : null
    };
  }

  // Deep linking: navigate to hash location
  function navigateToHash() {
    const target = parseHash();
    if (!target) return false;

    if (target.fileId < 0 || target.fileId >= data.files.length) return false;

    selectFile(target.fileId);

    if (target.lineStart) {
      requestAnimationFrame(() => {
        const lineEnd = target.lineEnd || target.lineStart;
        anchorLine = target.lineStart;
        selectedRange = { start: target.lineStart, end: lineEnd };
        selectLineRange(target.lineStart, lineEnd);
        scrollToLine(target.lineStart);
      });
    }

    return true;
  }

  // Deep linking: scroll to and highlight a line
  function scrollToLine(lineNum) {
    const lineEl = document.querySelector('.code-line[data-line="' + lineNum + '"]');
    if (!lineEl) return;

    lineEl.scrollIntoView({ behavior: 'smooth', block: 'center' });
  }

  // Clear all selected lines
  function clearLineSelection() {
    document.querySelectorAll('.code-line.selected-line').forEach(el => {
      el.classList.remove('selected-line');
    });
  }

  // Select a range of lines (inclusive)
  function selectLineRange(start, end) {
    clearLineSelection();
    const minLine = Math.min(start, end);
    const maxLine = Math.max(start, end);
    for (let i = minLine; i <= maxLine; i++) {
      const lineEl = document.querySelector('.code-line[data-line="' + i + '"]');
      if (lineEl) {
        lineEl.classList.add('selected-line');
      }
    }
  }

  // Deep linking: update URL hash
  function updateHash(fileId, lineStart, lineEnd) {
    let hash = 'file-' + fileId;
    if (lineStart) {
      hash += ':line-' + lineStart;
      if (lineEnd && lineEnd !== lineStart) {
        // Normalise so start < end
        const minLine = Math.min(lineStart, lineEnd);
        const maxLine = Math.max(lineStart, lineEnd);
        hash = 'file-' + fileId + ':line-' + minLine + '-' + maxLine;
      }
    }
    history.replaceState(null, '', '#' + hash);
  }

  function renderSummary() {
    // Build summary safely using DOM methods
    summary.textContent = '';
    const span = document.createElement('span');
    span.className = 'percent';
    span.textContent = data.summary.percent.toFixed(1) + '%';
    summary.appendChild(span);
    summary.appendChild(document.createTextNode(
      ' coverage (' + data.summary.coveredLines + '/' + data.summary.totalLines + ' lines)'
    ));
  }

  function renderTree() {
    fileTree.textContent = '';
    // Auto-expand all top-level directories
    if (data.tree.children && data.tree.children.length > 0) {
      data.tree.children.forEach(child => {
        if (child.type === 'dir') {
          expandedDirs.add(getNodePath(child, 0));
        }
      });
    }
    const sortedTree = sortTreeNodes(data.tree, sortMode);
    renderNode(sortedTree, fileTree, 0);
  }

  function renderNode(node, container, depth) {
    if (node.name === '.' && node.type === 'dir') {
      // Root node, render children directly
      node.children.forEach(child => renderNode(child, container, depth));
      return;
    }

    const nodeEl = document.createElement('div');
    nodeEl.className = 'tree-node';
    nodeEl.dataset.name = node.name.toLowerCase();

    const item = document.createElement('div');
    item.className = 'tree-item';
    item.style.setProperty('--depth', depth);

    const icon = document.createElement('span');
    icon.className = 'icon';

    const name = document.createElement('span');
    name.className = 'name';
    name.textContent = node.name;

    if (node.type === 'dir') {
      const dirPath = getNodePath(node, depth);
      icon.textContent = expandedDirs.has(dirPath) ? '\u25BC' : '\u25B6';
      if (expandedDirs.has(dirPath)) {
        nodeEl.classList.add('expanded');
      }

      item.addEventListener('click', (e) => {
        e.stopPropagation();
        toggleDir(nodeEl, dirPath, icon);
      });

      item.appendChild(icon);
      item.appendChild(name);

      // Add coverage badge to all directories
      const badge = document.createElement('span');
      badge.className = 'coverage-badge';
      badge.textContent = calculateDirectoryCoverage(node).toFixed(1) + '%';
      item.appendChild(badge);

      nodeEl.appendChild(item);

      if (node.children && node.children.length > 0) {
        const children = document.createElement('div');
        children.className = 'tree-children';
        node.children.forEach(child => renderNode(child, children, depth + 1));
        nodeEl.appendChild(children);
      }
    } else {
      icon.textContent = '\uD83D\uDCC4';
      nodeEl.dataset.fileId = node.fileId;

      item.addEventListener('click', (e) => {
        e.stopPropagation();
        selectFile(node.fileId);
      });

      item.appendChild(icon);
      item.appendChild(name);

      // Add coverage badge to files
      const badge = document.createElement('span');
      badge.className = 'coverage-badge';
      badge.textContent = calculateDirectoryCoverage(node).toFixed(1) + '%';
      item.appendChild(badge);

      nodeEl.appendChild(item);
    }

    container.appendChild(nodeEl);
  }

  function getNodePath(node, depth) {
    return node.name + '_' + depth;
  }

  function toggleDir(nodeEl, path, icon) {
    if (nodeEl.classList.contains('expanded')) {
      nodeEl.classList.remove('expanded');
      expandedDirs.delete(path);
      icon.textContent = '\u25B6';
    } else {
      nodeEl.classList.add('expanded');
      expandedDirs.add(path);
      icon.textContent = '\u25BC';
    }
  }

  function selectFile(fileId) {
    currentFileId = fileId;
    matches = [];
    currentMatchIndex = -1;
    matchInfo.textContent = '';
    contentSearch.value = '';
    contentSearchQuery = '';
    anchorLine = null;
    selectedRange = null;

    // Update selection in tree
    document.querySelectorAll('.tree-item.selected').forEach(el => {
      el.classList.remove('selected');
    });
    const selected = document.querySelector('[data-file-id="' + fileId + '"] .tree-item');
    if (selected) {
      selected.classList.add('selected');
    }

    const file = data.files[fileId];
    if (!file) return;

    filePath.textContent = file.path;
    renderCode(file);

    // Update URL hash for deep linking
    updateHash(fileId, null);
  }

  function renderCode(file) {
    viewport.textContent = '';

    if (!file.lines || file.lines.length === 0) {
      const empty = document.createElement('div');
      empty.className = 'empty-state';
      const iconDiv = document.createElement('div');
      iconDiv.className = 'icon';
      iconDiv.textContent = '\uD83D\uDCED';
      const textDiv = document.createElement('div');
      textDiv.textContent = 'No content';
      empty.appendChild(iconDiv);
      empty.appendChild(textDiv);
      viewport.appendChild(empty);
      return;
    }

    const container = document.createElement('div');
    container.className = 'code-container';

    file.lines.forEach((line, idx) => {
      const lineEl = document.createElement('div');
      lineEl.className = 'code-line';
      lineEl.dataset.line = idx + 1;

      const cov = file.coverage[idx];
      if (cov === 2) {
        lineEl.classList.add('covered');
      } else if (cov === 1) {
        lineEl.classList.add('uncovered');
      }

      const gutter = document.createElement('div');
      gutter.className = 'gutter';

      const lineNum = document.createElement('div');
      lineNum.className = 'line-number';
      lineNum.textContent = idx + 1;
      lineNum.title = 'Click to select line, Shift+Click for range';

      // Add click handler for line number deep linking
      const lineNumber = idx + 1;
      lineNum.addEventListener('click', (e) => {
        e.stopPropagation();

        if (e.shiftKey && anchorLine !== null) {
          // Shift-click: select range from anchor to clicked line
          const start = Math.min(anchorLine, lineNumber);
          const end = Math.max(anchorLine, lineNumber);
          selectedRange = { start: start, end: end };
          selectLineRange(start, end);
          updateHash(currentFileId, start, end);
        } else {
          // Regular click: set anchor and select single line
          anchorLine = lineNumber;
          selectedRange = { start: lineNumber, end: lineNumber };
          selectLineRange(lineNumber, lineNumber);
          updateHash(currentFileId, lineNumber, null);
        }
      });

      const content = document.createElement('div');
      content.className = 'line-content';
      content.textContent = line || ' ';

      lineEl.appendChild(gutter);
      lineEl.appendChild(lineNum);
      lineEl.appendChild(content);
      container.appendChild(lineEl);
    });

    viewport.appendChild(container);

    // Apply syntax highlighting after rendering if enabled
    if (syntaxHighlightEnabled) {
      applySyntaxHighlighting();
    }
  }

  function setupEventListeners() {
    // File search
    let searchTimeout;
    searchInput.addEventListener('input', (e) => {
      clearTimeout(searchTimeout);
      searchTimeout = setTimeout(() => {
        searchQuery = e.target.value.toLowerCase();
        filterTree();
      }, 300);
    });

    // Content search
    let contentTimeout;
    contentSearch.addEventListener('input', (e) => {
      clearTimeout(contentTimeout);
      contentTimeout = setTimeout(() => {
        contentSearchQuery = e.target.value;
        searchInFile();
      }, 300);
    });

    contentSearch.addEventListener('keydown', (e) => {
      if (e.key === 'Enter') {
        if (e.shiftKey) {
          goToPrevMatch();
        } else {
          goToNextMatch();
        }
      }
    });

    prevMatch.addEventListener('click', goToPrevMatch);
    nextMatch.addEventListener('click', goToNextMatch);

    // Theme toggle
    themeToggle.addEventListener('click', toggleTheme);

    // Syntax toggle
    syntaxToggle.addEventListener('click', toggleSyntax);

    // Sort controls
    const sortButtons = document.querySelectorAll('.sort-btn');
    console.log('Found', sortButtons.length, 'sort buttons');
    sortButtons.forEach(btn => {
      console.log('Attaching click handler to button:', btn.dataset.sort);
      btn.addEventListener('click', () => {
        console.log('Sort button clicked:', btn.dataset.sort);
        changeSortMode(btn.dataset.sort);
      });
    });

    // Keyboard shortcuts
    document.addEventListener('keydown', (e) => {
      if ((e.ctrlKey || e.metaKey) && e.key === 'f' && currentFileId !== null) {
        e.preventDefault();
        contentSearch.focus();
      }
      if ((e.ctrlKey || e.metaKey) && e.key === 'p') {
        e.preventDefault();
        searchInput.focus();
      }
      // Help modal
      if (e.key === '?' && !e.ctrlKey && !e.metaKey) {
        e.preventDefault();
        showHelp();
      }
      if (e.key === 'Escape') {
        // Exit search if focused
        if (document.activeElement === searchInput) {
          searchInput.value = '';
          searchQuery = '';
          filterTree();
          searchInput.blur();
          viewport.focus();
          return;
        }
        if (document.activeElement === contentSearch) {
          contentSearch.value = '';
          contentSearchQuery = '';
          matchInfo.textContent = '';
          matches = [];
          currentMatchIndex = -1;
          if (currentFileId !== null) {
            renderCode(data.files[currentFileId]);
          }
          contentSearch.blur();
          viewport.focus();
          return;
        }
        hideHelp();
      }
    });

    closeHelp.addEventListener('click', hideHelp);
    helpToggle.addEventListener('click', showHelp);
    helpModal.addEventListener('click', (e) => {
      if (e.target === helpModal) hideHelp();
    });
  }

  function filterTree() {
    const nodes = document.querySelectorAll('.tree-node');

    if (!searchQuery) {
      nodes.forEach(n => n.classList.remove('hidden'));
      return;
    }

    nodes.forEach(node => {
      const name = node.dataset.name || '';
      const fileId = node.dataset.fileId;

      if (fileId !== undefined) {
        const file = data.files[parseInt(fileId)];
        const matchesQuery = file && file.path.toLowerCase().includes(searchQuery);
        node.classList.toggle('hidden', !matchesQuery);
      } else {
        const hasVisibleChild = Array.from(node.querySelectorAll('[data-file-id]')).some(f => {
          const fid = parseInt(f.dataset.fileId);
          const file = data.files[fid];
          return file && file.path.toLowerCase().includes(searchQuery);
        });
        node.classList.toggle('hidden', !hasVisibleChild);
        if (hasVisibleChild && searchQuery) {
          node.classList.add('expanded');
          const icon = node.querySelector('.icon');
          if (icon && icon.textContent === '\u25B6') {
            icon.textContent = '\u25BC';
          }
        }
      }
    });
  }

  function searchInFile() {
    matches = [];
    currentMatchIndex = -1;

    // Re-render code to clear highlights
    if (currentFileId !== null) {
      const file = data.files[currentFileId];
      if (file) {
        renderCode(file);
      }
    }

    if (!contentSearchQuery || currentFileId === null) {
      matchInfo.textContent = '';
      return;
    }

    const file = data.files[currentFileId];
    if (!file) return;

    const query = contentSearchQuery.toLowerCase();

    file.lines.forEach((line, idx) => {
      const text = line || '';
      const lowerText = text.toLowerCase();
      let pos = 0;
      let matchIndex;

      while ((matchIndex = lowerText.indexOf(query, pos)) !== -1) {
        matches.push({ line: idx, start: matchIndex, length: query.length });
        pos = matchIndex + 1;
      }
    });

    if (matches.length > 0) {
      highlightMatches();
      currentMatchIndex = 0;
      scrollToMatch(0);
      updateMatchInfo();
    } else {
      matchInfo.textContent = 'No matches';
    }
  }

  function highlightMatches() {
    const file = data.files[currentFileId];
    if (!file) return;

    const lineEls = document.querySelectorAll('.code-line');

    // Group matches by line
    const matchesByLine = {};
    matches.forEach((m, idx) => {
      if (!matchesByLine[m.line]) matchesByLine[m.line] = [];
      matchesByLine[m.line].push({ ...m, idx });
    });

    Object.keys(matchesByLine).forEach(lineIdx => {
      const lineEl = lineEls[parseInt(lineIdx)];
      if (!lineEl) return;

      const content = lineEl.querySelector('.line-content');
      if (!content) return;

      const text = file.lines[parseInt(lineIdx)] || '';
      const lineMatches = matchesByLine[lineIdx].sort((a, b) => a.start - b.start);

      // Build content using DOM nodes for safety
      content.textContent = '';
      let lastEnd = 0;

      lineMatches.forEach(m => {
        // Text before match
        if (m.start > lastEnd) {
          content.appendChild(document.createTextNode(text.substring(lastEnd, m.start)));
        }
        // Match span
        const span = document.createElement('span');
        span.className = 'match-highlight';
        span.dataset.matchIdx = m.idx;
        span.textContent = text.substring(m.start, m.start + m.length);
        content.appendChild(span);
        lastEnd = m.start + m.length;
      });

      // Text after last match
      if (lastEnd < text.length) {
        content.appendChild(document.createTextNode(text.substring(lastEnd)));
      }

      // Handle empty line
      if (content.childNodes.length === 0) {
        content.textContent = ' ';
      }
    });
  }

  function scrollToMatch(idx) {
    document.querySelectorAll('.current-match').forEach(el => {
      el.classList.remove('current-match');
    });

    const matchEl = document.querySelector('[data-match-idx="' + idx + '"]');
    if (matchEl) {
      matchEl.classList.add('current-match');
      matchEl.scrollIntoView({ behavior: 'smooth', block: 'center' });
    }
  }

  function updateMatchInfo() {
    if (matches.length === 0) {
      matchInfo.textContent = 'No matches';
    } else {
      matchInfo.textContent = (currentMatchIndex + 1) + '/' + matches.length;
    }
  }

  function goToNextMatch() {
    if (matches.length === 0) return;
    currentMatchIndex = (currentMatchIndex + 1) % matches.length;
    scrollToMatch(currentMatchIndex);
    updateMatchInfo();
  }

  function goToPrevMatch() {
    if (matches.length === 0) return;
    currentMatchIndex = (currentMatchIndex - 1 + matches.length) % matches.length;
    scrollToMatch(currentMatchIndex);
    updateMatchInfo();
  }

  function toggleTheme() {
    const body = document.body;
    const current = body.dataset.theme;
    const next = current === 'dark' ? 'light' : 'dark';
    body.dataset.theme = next;
    localStorage.setItem('coverage-theme', next);
  }

  function loadTheme() {
    const saved = localStorage.getItem('coverage-theme');
    if (saved) {
      document.body.dataset.theme = saved;
    }
  }

  function applySyntaxHighlighting() {
    if (!syntaxHighlightEnabled || currentFileId === null) return;
    if (typeof hljs === 'undefined') return;

    const file = data.files[currentFileId];
    if (!file) return;

    const lineEls = document.querySelectorAll('.code-line');

    lineEls.forEach((lineEl, idx) => {
      const cov = file.coverage[idx];
      // Only highlight lines with no coverage info
      if (cov !== 0) return;

      const content = lineEl.querySelector('.line-content');
      if (!content || !content.textContent.trim()) return;

      const text = content.textContent;

      // Use hljs.highlight() which returns result object
      const result = hljs.highlight(text, { language: 'go' });

      // Parse the highlighted HTML safely using DOMParser
      const parser = new DOMParser();
      const doc = parser.parseFromString('<div>' + result.value + '</div>', 'text/html');
      const wrapper = doc.body.firstChild;

      // Clear and append parsed nodes
      content.textContent = '';
      while (wrapper.firstChild) {
        content.appendChild(wrapper.firstChild);
      }
    });
  }

  function toggleSyntax() {
    syntaxHighlightEnabled = !syntaxHighlightEnabled;
    syntaxToggle.classList.toggle('active', syntaxHighlightEnabled);
    localStorage.setItem('coverage-syntax', syntaxHighlightEnabled ? 'on' : 'off');

    // Re-render current file
    if (currentFileId !== null) {
      const file = data.files[currentFileId];
      if (file) {
        renderCode(file);
      }
    }
  }

  function loadSyntaxPreference() {
    const saved = localStorage.getItem('coverage-syntax');
    if (saved !== null) {
      // User preference overrides default
      syntaxHighlightEnabled = saved === 'on';
    }
    // Update button state
    syntaxToggle.classList.toggle('active', syntaxHighlightEnabled);
  }

  function changeSortMode(mode) {
    if (sortMode === mode) return;

    console.log('Changing sort mode from', sortMode, 'to', mode);
    sortMode = mode;
    localStorage.setItem('coverage-sort-mode', mode);

    // Update button states
    document.querySelectorAll('.sort-btn').forEach(btn => {
      btn.classList.toggle('active', btn.dataset.sort === mode);
    });

    // Re-render tree
    renderTree();
  }

  function loadSortPreference() {
    const saved = localStorage.getItem('coverage-sort-mode');
    if (saved && (saved === 'name' || saved === 'coverage')) {
      sortMode = saved;
    }

    // Update button states
    document.querySelectorAll('.sort-btn').forEach(btn => {
      btn.classList.toggle('active', btn.dataset.sort === sortMode);
    });
  }

  function showHelp() {
    helpModal.classList.remove('hidden');
  }

  function hideHelp() {
    helpModal.classList.add('hidden');
  }

  // Start the app
  init();
})();

    </script>
  </body>
</html>
