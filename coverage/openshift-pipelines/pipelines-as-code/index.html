<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Coverage Report</title>
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/go.min.js"></script>
    <style>
      :root {
  --sidebar-width: 280px;
  --topbar-height: 48px;
  --line-height: 20px;
  --font-mono: ui-monospace, SFMono-Regular, "SF Mono", Menlo, Consolas, "Liberation Mono", monospace;
}

[data-theme="dark"] {
  --bg: #1e1e1e;
  --bg-secondary: #252526;
  --bg-tertiary: #2d2d2d;
  --text: #d4d4d4;
  --text-muted: #808080;
  --border: #3c3c3c;
  --covered: rgba(35, 134, 54, 0.25);
  --covered-gutter: #238636;
  --uncovered: rgba(218, 54, 51, 0.25);
  --uncovered-gutter: #da3633;
  --highlight: #264f78;
  --highlight-match: #613214;
  --accent: #569cd6;
  --hover: #2a2d2e;
}

[data-theme="light"] {
  --bg: #ffffff;
  --bg-secondary: #f3f3f3;
  --bg-tertiary: #e8e8e8;
  --text: #24292f;
  --text-muted: #656d76;
  --border: #d0d7de;
  --covered: rgba(35, 134, 54, 0.15);
  --covered-gutter: #1a7f37;
  --uncovered: rgba(218, 54, 51, 0.15);
  --uncovered-gutter: #cf222e;
  --highlight: #ddf4ff;
  --highlight-match: #fff8c5;
  --accent: #0969da;
  --hover: #f6f8fa;
}

* {
  margin: 0;
  padding: 0;
  box-sizing: border-box;
}

html, body {
  height: 100%;
  overflow: hidden;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif;
  font-size: 14px;
  background: var(--bg);
  color: var(--text);
}

#app {
  display: grid;
  grid-template-columns: var(--sidebar-width) 1fr;
  height: 100%;
}

/* Sidebar */
#sidebar {
  display: flex;
  flex-direction: column;
  background: var(--bg-secondary);
  border-right: 1px solid var(--border);
  overflow: hidden;
}

#sidebar-header {
  padding: 16px;
  border-bottom: 1px solid var(--border);
}

/* Logo link */
#logo-link {
  text-decoration: none;
  color: inherit;
  display: block;
}

#logo-link:hover #logo-container {
  opacity: 0.8;
}

#logo-container {
  display: flex;
  align-items: center;
  gap: 10px;
  margin-bottom: 12px;
  transition: opacity 0.15s ease;
}

#logo-container .github-icon {
  flex-shrink: 0;
  color: var(--text-muted);
  transition: color 0.15s ease;
}

#logo-link:hover .github-icon {
  color: var(--accent);
}

#logo {
  flex-shrink: 0;
  width: 32px;
  height: 32px;
}

#logo-text {
  flex: 1;
  min-width: 0;
}

#sidebar-header h1 {
  font-size: 16px;
  font-weight: 600;
  margin: 0;
  line-height: 1.2;
}

#tagline {
  font-size: 12px;
  font-weight: 500;
  color: var(--text-muted);
  margin: 2px 0 0 0;
  line-height: 1.3;
}

#summary {
  font-size: 13px;
  color: var(--text-muted);
}

#summary .percent {
  font-weight: 600;
  color: var(--text);
}

#search-box {
  padding: 8px 16px;
  border-bottom: 1px solid var(--border);
}

#search-input {
  width: 100%;
  padding: 6px 10px;
  border: 1px solid var(--border);
  border-radius: 4px;
  background: var(--bg);
  color: var(--text);
  font-size: 13px;
}

#search-input:focus {
  outline: none;
  border-color: var(--accent);
}

/* Sort controls */
#sort-controls {
  display: flex;
  gap: 0;
  margin: 8px 12px;
  border: 1px solid var(--border);
  border-radius: 4px;
  overflow: hidden;
}

.sort-btn {
  flex: 1;
  display: flex;
  align-items: center;
  justify-content: center;
  gap: 4px;
  padding: 6px 8px;
  background: var(--bg-secondary);
  color: var(--text);
  border: none;
  cursor: pointer;
  font-size: 12px;
  transition: background-color 0.2s, color 0.2s;
}

.sort-btn:hover {
  background: var(--hover);
}

.sort-btn.active {
  background: var(--accent);
  color: #fff;
}

.sort-btn .icon {
  font-weight: 600;
}

.sort-btn .label {
  font-size: 11px;
}

/* Coverage badges for directories */
.coverage-badge {
  margin-left: auto;
  padding-left: 8px;
  font-size: 11px;
  color: var(--text-muted);
  font-weight: 500;
  font-family: var(--font-mono);
}

#file-tree {
  flex: 1;
  overflow-y: auto;
  padding: 8px 0;
}

/* Sidebar footer */
#sidebar-footer {
  padding: 12px 16px;
  border-top: 1px solid var(--border);
  font-size: 12px;
  text-align: center;
}

#sidebar-footer a {
  color: var(--text-muted);
  text-decoration: none;
  display: inline-flex;
  align-items: center;
  gap: 6px;
}

#sidebar-footer a:hover {
  color: var(--accent);
  text-decoration: underline;
}

#sidebar-footer .github-icon {
  flex-shrink: 0;
}

.tree-node {
  cursor: pointer;
  user-select: none;
}

.tree-item {
  display: flex;
  align-items: center;
  padding: 4px 16px;
  gap: 6px;
  white-space: nowrap;
}

.tree-item:hover {
  background: var(--hover);
}

.tree-item.selected {
  background: var(--highlight);
}

.tree-item .icon {
  width: 16px;
  text-align: center;
  font-size: 12px;
  color: var(--text-muted);
}

.tree-item .name {
  font-size: 13px;
  overflow: hidden;
  text-overflow: ellipsis;
  flex: 1;
  min-width: 0;
}

.tree-children {
  display: none;
}

.tree-node.expanded > .tree-children {
  display: block;
}

.tree-children .tree-item {
  padding-left: calc(16px + var(--depth, 0) * 16px);
}

.tree-node.hidden {
  display: none;
}

/* Canvas */
#canvas {
  display: flex;
  flex-direction: column;
  overflow: hidden;
}

#topbar {
  display: flex;
  align-items: center;
  justify-content: space-between;
  height: var(--topbar-height);
  padding: 0 16px;
  background: var(--bg-secondary);
  border-bottom: 1px solid var(--border);
  gap: 16px;
}

#file-path {
  font-size: 13px;
  font-family: var(--font-mono);
  color: var(--text-muted);
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

#topbar-actions {
  display: flex;
  align-items: center;
  gap: 12px;
}

#in-file-search {
  display: flex;
  align-items: center;
  gap: 8px;
}

#content-search {
  width: 180px;
  padding: 4px 8px;
  border: 1px solid var(--border);
  border-radius: 4px;
  background: var(--bg);
  color: var(--text);
  font-size: 12px;
}

#content-search:focus {
  outline: none;
  border-color: var(--accent);
}

#match-info {
  font-size: 12px;
  color: var(--text-muted);
  min-width: 60px;
}

#prev-match, #next-match {
  padding: 2px 8px;
  border: 1px solid var(--border);
  border-radius: 4px;
  background: var(--bg);
  color: var(--text);
  cursor: pointer;
  font-size: 10px;
}

#prev-match:hover, #next-match:hover {
  background: var(--hover);
}

#theme-toggle {
  padding: 6px 10px;
  border: 1px solid var(--border);
  border-radius: 4px;
  background: var(--bg);
  color: var(--text);
  cursor: pointer;
  font-size: 16px;
}

#theme-toggle:hover {
  background: var(--hover);
}

#syntax-toggle {
  padding: 6px 10px;
  border: 1px solid var(--border);
  border-radius: 4px;
  background: var(--bg);
  color: var(--text);
  cursor: pointer;
  font-size: 14px;
  font-family: var(--font-mono);
}

#syntax-toggle:hover {
  background: var(--hover);
}

#syntax-toggle.active {
  background: var(--accent);
  color: #fff;
  border-color: var(--accent);
}

#help-toggle {
  padding: 6px 10px;
  border: 1px solid var(--border);
  border-radius: 4px;
  background: var(--bg);
  color: var(--text);
  cursor: pointer;
  font-size: 14px;
  font-weight: 600;
}

#help-toggle:hover {
  background: var(--hover);
}

/* Override highlight.js to use theme-aware colors */
.hljs { background: transparent !important; }

[data-theme="dark"] .hljs-keyword { color: #569cd6; }
[data-theme="dark"] .hljs-type { color: #4ec9b0; }
[data-theme="dark"] .hljs-string { color: #ce9178; }
[data-theme="dark"] .hljs-number { color: #b5cea8; }
[data-theme="dark"] .hljs-comment { color: #6a9955; }
[data-theme="dark"] .hljs-built_in { color: #dcdcaa; }
[data-theme="dark"] .hljs-literal { color: #569cd6; }
[data-theme="dark"] .hljs-function { color: #dcdcaa; }

[data-theme="light"] .hljs-keyword { color: #0000ff; }
[data-theme="light"] .hljs-type { color: #267f99; }
[data-theme="light"] .hljs-string { color: #a31515; }
[data-theme="light"] .hljs-number { color: #098658; }
[data-theme="light"] .hljs-comment { color: #008000; }
[data-theme="light"] .hljs-built_in { color: #795e26; }
[data-theme="light"] .hljs-literal { color: #0000ff; }
[data-theme="light"] .hljs-function { color: #795e26; }

/* Viewport */
#viewport {
  flex: 1;
  overflow: auto;
  background: var(--bg);
  outline: none;
}

#viewport::-webkit-scrollbar {
  width: 14px;
  height: 14px;
}

#viewport::-webkit-scrollbar-track {
  background: var(--bg);
}

#viewport::-webkit-scrollbar-thumb {
  background: var(--border);
  border: 3px solid var(--bg);
  border-radius: 7px;
}

#viewport::-webkit-scrollbar-thumb:hover {
  background: var(--text-muted);
}

.code-container {
  display: table;
  min-width: 100%;
  font-family: var(--font-mono);
  font-size: 13px;
  line-height: var(--line-height);
}

.code-line {
  display: table-row;
}

.code-line:hover {
  background: var(--hover);
}

.code-line.covered {
  background: var(--covered);
}

.code-line.uncovered {
  background: var(--uncovered);
}

.code-line.covered:hover {
  background: var(--covered);
}

.code-line.uncovered:hover {
  background: var(--uncovered);
}

.gutter {
  display: table-cell;
  width: 4px;
  min-width: 4px;
}

.code-line.covered .gutter {
  background: var(--covered-gutter);
}

.code-line.uncovered .gutter {
  background: var(--uncovered-gutter);
}

.line-number {
  display: table-cell;
  width: 50px;
  min-width: 50px;
  padding: 0 12px 0 8px;
  text-align: right;
  color: var(--text-muted);
  user-select: none;
  vertical-align: top;
}

.line-content {
  display: table-cell;
  padding-right: 16px;
  white-space: pre;
  tab-size: 4;
}

.match-highlight {
  background: var(--highlight-match);
  border-radius: 2px;
}

.current-match {
  background: var(--accent);
  color: #fff;
}

/* Empty state */
.empty-state {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  height: 100%;
  color: var(--text-muted);
  gap: 8px;
}

.empty-state .icon {
  font-size: 48px;
  opacity: 0.5;
}

/* Scrollbar for file tree */
#file-tree::-webkit-scrollbar {
  width: 8px;
}

#file-tree::-webkit-scrollbar-track {
  background: transparent;
}

#file-tree::-webkit-scrollbar-thumb {
  background: var(--border);
  border-radius: 4px;
}

#file-tree::-webkit-scrollbar-thumb:hover {
  background: var(--text-muted);
}

/* Help modal */
.modal {
  position: fixed;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  background: rgba(0, 0, 0, 0.5);
  display: flex;
  align-items: center;
  justify-content: center;
  z-index: 1000;
}

.modal.hidden {
  display: none;
}

.modal-content {
  background: var(--bg-secondary);
  border: 1px solid var(--border);
  border-radius: 8px;
  padding: 24px;
  max-width: 400px;
  width: 90%;
}

.modal-content h2 {
  margin-bottom: 16px;
  font-size: 18px;
}

.modal-content dl {
  display: grid;
  grid-template-columns: auto 1fr;
  gap: 8px 16px;
}

.modal-content dt {
  font-family: var(--font-mono);
  background: var(--bg-tertiary);
  padding: 2px 6px;
  border-radius: 4px;
  font-size: 13px;
}

.modal-content dd {
  color: var(--text-muted);
}

.modal-content button {
  margin-top: 20px;
  padding: 8px 16px;
  border: 1px solid var(--border);
  border-radius: 4px;
  background: var(--accent);
  color: #fff;
  cursor: pointer;
  width: 100%;
}

/* Selected line range (multi-line selection) */
.code-line.selected-line {
  background-color: var(--highlight);
}

.code-line.selected-line.covered {
  background-color: var(--highlight);
}

.code-line.selected-line.uncovered {
  background-color: var(--highlight);
}

/* Line number click indicator */
.line-number {
  cursor: pointer;
}

.line-number:hover {
  color: var(--accent);
}

    </style>
  </head>
  <body data-theme="dark">
    <div id="app">
      <aside id="sidebar">
        <div id="sidebar-header">
          <a
            href="https://github.com/chmouel/go-better-html-coverage"
            id="logo-link"
            target="_blank"
            rel="noopener"
          >
            <div id="logo-container">
              <svg id="logo" viewBox="0 0 32 32" width="32" height="32">
                <defs>
                  <linearGradient
                    id="logoGradient"
                    x1="0%"
                    y1="0%"
                    x2="100%"
                    y2="100%"
                  >
                    <stop
                      offset="0%"
                      style="stop-color: var(--accent); stop-opacity: 1"
                    />
                    <stop
                      offset="100%"
                      style="stop-color: var(--accent); stop-opacity: 0.7"
                    />
                  </linearGradient>
                </defs>
                
                <circle
                  cx="16"
                  cy="16"
                  r="14"
                  fill="none"
                  stroke="url(#logoGradient)"
                  stroke-width="2"
                  opacity="0.3"
                />
                
                <path
                  d="M 10 17 L 14 21 L 22 11"
                  fill="none"
                  stroke="var(--accent)"
                  stroke-width="2.5"
                  stroke-linecap="round"
                  stroke-linejoin="round"
                />
                
                <circle
                  cx="24"
                  cy="10"
                  r="1.5"
                  fill="var(--accent)"
                  opacity="0.6"
                />
                <circle
                  cx="26"
                  cy="12"
                  r="1.5"
                  fill="var(--accent)"
                  opacity="0.6"
                />
              </svg>
              <div id="logo-text">
                <h1>GO Coverage</h1>
                <div id="tagline">A better HTML Go Coverage</div>
              </div>
            </div>
          </a>
          <div id="summary"></div>
        </div>
        <div id="search-box">
          <input type="text" id="search-input" placeholder="Search files..." />
        </div>
        <div id="sort-controls">
          <button
            class="sort-btn active"
            data-sort="name"
            title="Sort alphabetically"
          >
            <span class="icon">Aâ†’Z</span>
            <span class="label">Name</span>
          </button>
          <button
            class="sort-btn"
            data-sort="coverage"
            title="Sort by coverage percentage"
          >
            <span class="icon">%</span>
            <span class="label">Coverage</span>
          </button>
        </div>
        <div id="file-tree"></div>
        <footer id="sidebar-footer">
          <a
            href="https://github.com/chmouel/go-better-html-coverage"
            target="_blank"
            rel="noopener"
          >
            <svg
              class="github-icon"
              viewBox="0 0 16 16"
              width="14"
              height="14"
              fill="currentColor"
            >
              <path
                d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"
              />
            </svg>
            chmouel/go-better-html-coverage
          </a>
        </footer>
      </aside>
      <main id="canvas">
        <header id="topbar">
          <div id="file-path"></div>
          <div id="topbar-actions">
            <div id="in-file-search">
              <input
                type="text"
                id="content-search"
                placeholder="Search in file..."
              />
              <span id="match-info"></span>
              <button id="prev-match" title="Previous match">&#9650;</button>
              <button id="next-match" title="Next match">&#9660;</button>
            </div>
            <button id="syntax-toggle" title="Toggle syntax highlighting">
              &lt;/&gt;
            </button>
            <button id="theme-toggle" title="Toggle theme">&#9788;</button>
            <button id="help-toggle" title="Keyboard shortcuts">?</button>
          </div>
        </header>
        <div id="viewport" tabindex="-1"></div>
      </main>
      <div id="help-modal" class="modal hidden">
        <div class="modal-content">
          <h2>Keyboard Shortcuts</h2>
          <dl>
            <dt>Ctrl+P</dt>
            <dd>Focus file search</dd>
            <dt>Ctrl+F</dt>
            <dd>Search in file</dd>
            <dt>Enter</dt>
            <dd>Next match</dd>
            <dt>Shift+Enter</dt>
            <dd>Previous match</dd>
            <dt>?</dt>
            <dd>Show this help</dd>
            <dt>Esc</dt>
            <dd>Close modal</dd>
          </dl>
          <h2>Permalinks</h2>
          <dl>
            <dt>Click line</dt>
            <dd>Select line, update URL</dd>
            <dt>Shift+Click</dt>
            <dd>Select line range</dd>
          </dl>
          <button id="close-help">Close</button>
        </div>
      </div>
    </div>
    <script>
      window.COVERAGE_DATA = {"files":[{"id":0,"path":"pkg/acl/owners.go","lines":["package acl","","import (","\t\"fmt\"","","\t\"sigs.k8s.io/yaml\"",")","","type aliases = map[string][]string","","type simpleConfig struct {","\tApprovers []string `json:\"approvers,omitempty\"`","\tReviewers []string `json:\"reviewers,omitempty\"`","}","","type filtersConfig struct {","\tFilters map[string]simpleConfig `json:\"filters,omitempty\"`","}","","type aliasesConfig struct {","\tAliases aliases `json:\"aliases,omitempty\"`","}","","// UserInOwnerFile Parse OWNERS and OWNERS_ALIASES files and return true if the sender is in","// there. Support OWNERS simple configs (approvers, reviewers) and filters. When filters are used,","// only match against the \".*\" filter.","func UserInOwnerFile(ownersContent, ownersAliasesContent, sender string) (bool, error) {","\tsc := simpleConfig{}","\tfc := filtersConfig{}","\tac := aliasesConfig{}","\terr := yaml.Unmarshal([]byte(ownersContent), \u0026sc)","\tif err != nil {","\t\treturn false, fmt.Errorf(\"cannot parse OWNERS file Approvers and Reviewers: %w\", err)","\t}","\terr = yaml.Unmarshal([]byte(ownersContent), \u0026fc)","\tif err != nil {","\t\treturn false, fmt.Errorf(\"cannot parse OWNERS file Filters: %w\", err)","\t}","\terr = yaml.Unmarshal([]byte(ownersAliasesContent), \u0026ac)","\tif err != nil {","\t\treturn false, fmt.Errorf(\"cannot parse OWNERS_ALIASES: %w\", err)","\t}","","\tvar approvers, reviewers []string","\tif len(sc.Approvers) \u003e 0 || len(sc.Reviewers) \u003e 0 {","\t\tapprovers, reviewers = sc.Approvers, sc.Reviewers","\t\t// Simple config (approvers/reviewers) and filters can't exist together.","\t\t// We only check for the \".*\" filter (matching all files in the repo).","\t} else if filter, ok := fc.Filters[\".*\"]; ok {","\t\tif len(filter.Approvers) \u003e 0 || len(filter.Reviewers) \u003e 0 {","\t\t\tapprovers, reviewers = filter.Approvers, filter.Reviewers","\t\t}","\t}","\towners := expandAliases(append(approvers, reviewers...), ac.Aliases)","\tfor _, owner := range owners {","\t\tif owner == sender {","\t\t\treturn true, nil","\t\t}","\t}","\treturn false, nil","}","","// Expand aliases into the list of owners removing the duplicates.","// Due to the use of map for deduplication, the order is not guaranteed.","func expandAliases(owners []string, aliases aliases) []string {","\tdedups := make(map[string]bool)","\tfor _, owner := range owners {","\t\tif _, ok := dedups[owner]; !ok {","\t\t\t// check if owner is an alias","\t\t\tif alias, ok := aliases[owner]; ok {","\t\t\t\tfor _, name := range alias {","\t\t\t\t\tdedups[name] = true","\t\t\t\t}","\t\t\t} else {","\t\t\t\tdedups[owner] = true","\t\t\t}","\t\t}","\t}","\texpanded := make([]string, 0, len(dedups))","\tfor o := range dedups {","\t\texpanded = append(expanded, o)","\t}","\treturn expanded","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,2,0,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,0,2,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,0]},{"id":1,"path":"pkg/acl/regexp.go","lines":["package acl","","import (","\t\"regexp\"",")","","const OKToTestCommentRegexp = `(^|\\n)\\/ok-to-test(?:\\s+([a-fA-F0-9]{7,40}))?\\s*(\\r\\n|\\r|\\n|$)`","","// MatchRegexp Match a regexp to a string.","func MatchRegexp(reg, comment string) bool {","\tre := regexp.MustCompile(reg)","\treturn string(re.Find([]byte(comment))) != \"\"","}"],"coverage":[0,0,0,0,0,0,0,0,0,2,2,2,2]},{"id":2,"path":"pkg/action/patch.go","lines":["package action","","import (","\t\"context\"","\t\"encoding/json\"","\t\"fmt\"","","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"github.com/tektoncd/pipeline/pkg/client/clientset/versioned\"","\t\"go.uber.org/zap\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/apimachinery/pkg/types\"","\t\"k8s.io/client-go/util/retry\"",")","","// PatchPipelineRun patches a Tekton PipelineRun resource with the provided merge patch.","// It retries the patch operation on conflict, doubling the default retry parameters.","//","// Parameters:","// - ctx: The context for the patch operation.","// - logger: A SugaredLogger instance for logging information.","// - whatPatching: A string describing what is being patched, used for logging purposes.","// - tekton: A Tekton client interface for interacting with Tekton resources.","// - pr: The PipelineRun resource to be patched. If nil, the function returns nil.","// - mergePatch: A map representing the JSON merge patch to apply to the PipelineRun.","//","// Returns:","// - *tektonv1.PipelineRun: The patched PipelineRun resource, or the original PipelineRun if an error occurs.","// - error: An error if the patch operation fails after retries, or nil if successful.","//","// The function doubles the default retry parameters (steps, duration, factor, jitter) to handle conflicts more robustly.","// If the patch operation fails after retries, the original PipelineRun is returned along with the error.","func PatchPipelineRun(ctx context.Context, logger *zap.SugaredLogger, whatPatching string, tekton versioned.Interface, pr *tektonv1.PipelineRun, mergePatch map[string]any) (*tektonv1.PipelineRun, error) {","\tif pr == nil {","\t\treturn nil, nil","\t}","\tvar patchedPR *tektonv1.PipelineRun","\t// double the retry; see https://issues.redhat.com/browse/SRVKP-3134","\tdoubleRetry := retry.DefaultRetry","\tdoubleRetry.Steps *= 2","\tdoubleRetry.Duration *= 2","\tdoubleRetry.Factor *= 2","\tdoubleRetry.Jitter *= 2","\terr := retry.RetryOnConflict(doubleRetry, func() error {","\t\tpatch, err := json.Marshal(mergePatch)","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\tpatchedPR, err = tekton.TektonV1().PipelineRuns(pr.GetNamespace()).Patch(ctx, pr.GetName(), types.MergePatchType, patch, metav1.PatchOptions{})","\t\tif err != nil {","\t\t\tlogger.Infof(\"could not patch Pipelinerun with %v, retrying %v/%v: %v\", whatPatching, pr.GetNamespace(), pr.GetName(), err)","\t\t\treturn err","\t\t}","\t\tlogger.Infof(\"patched pipelinerun with %v: %v/%v\", whatPatching, patchedPR.Namespace, patchedPR.Name)","\t\treturn nil","\t})","\tif err != nil {","\t\t// return the original PipelineRun, let the caller decide what to do with it after the error is processed","\t\treturn pr, fmt.Errorf(\"failed to patch pipelinerun %v/%v with %v: %w\", pr.Namespace, whatPatching, pr.Name, err)","\t}","\treturn patchedPR, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,1,1,2,2,2,2,2,2,2,2,2,2,1,1,2,2,1,1,1,2,2,0,2,1,1,1,2,0]},{"id":3,"path":"pkg/adapter/adapter.go","lines":["package adapter","","import (","\t\"context\"","\t\"encoding/json\"","\t\"errors\"","\t\"fmt\"","\t\"io\"","\t\"net/http\"","\t\"os\"","\t\"strings\"","\t\"time\"","","\tcloudevents \"github.com/cloudevents/sdk-go/v2\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/versiondata\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/bitbucketcloud\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/bitbucketdatacenter\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/gitea\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/github\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/gitlab\"","\t\"go.uber.org/zap\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"knative.dev/eventing/pkg/adapter/v2\"","\t\"knative.dev/pkg/logging\"","\t\"knative.dev/pkg/system\"",")","","const globalAdapterPort = \"8082\"","","// For incoming webhook requests and GitHub Apps with many installations the handler takes long","// e.g GitHub App with ~400 installations, it takes ~180s. For OpenShift deployments this also","// requires matching timeout on the pipelines-as-code-controller route (default is 30s).","const httpTimeoutHandler = 600 * time.Second","","type envConfig struct {","\tadapter.EnvConfig","}","","func NewEnvConfig() adapter.EnvConfigAccessor {","\treturn \u0026envConfig{","\t\tadapter.EnvConfig{","\t\t\tNamespace: system.Namespace(),","\t\t},","\t}","}","","type listener struct {","\trun    *params.Run","\tkint   kubeinteraction.Interface","\tlogger *zap.SugaredLogger","\tevent  *info.Event","}","","type Response struct {","\tStatus  int    `json:\"status\"`","\tMessage string `json:\"message\"`","}","","var _ adapter.Adapter = (*listener)(nil)","","func New(run *params.Run, k *kubeinteraction.Interaction) adapter.AdapterConstructor {","\treturn func(ctx context.Context, _ adapter.EnvConfigAccessor, _ cloudevents.Client) adapter.Adapter {","\t\treturn \u0026listener{","\t\t\tlogger: logging.FromContext(ctx),","\t\t\trun:    run,","\t\t\tkint:   k,","\t\t}","\t}","}","","func (l *listener) Start(ctx context.Context) error {","\tadapterPort := globalAdapterPort","\tenvAdapterPort := os.Getenv(\"PAC_CONTROLLER_PORT\")","\tif envAdapterPort != \"\" {","\t\tadapterPort = envAdapterPort","\t}","","\t// Start pac config syncer","\tgo params.StartConfigSync(ctx, l.run)","","\tl.logger.Infof(\"Starting Pipelines as Code version: %s\", strings.TrimSpace(versiondata.Version))","\tmux := http.NewServeMux()","","\t// for handling probes","\tmux.HandleFunc(\"/live\", func(w http.ResponseWriter, _ *http.Request) {","\t\tw.WriteHeader(http.StatusOK)","\t\t_, _ = fmt.Fprint(w, \"ok\")","\t})","","\tmux.HandleFunc(\"/\", l.handleEvent(ctx))","","\tsrv := \u0026http.Server{","\t\tAddr: \":\" + adapterPort,","\t\tHandler: http.TimeoutHandler(mux,","\t\t\thttpTimeoutHandler, \"Listener Timeout!\\n\"),","\t\tReadHeaderTimeout: 5 * time.Second,","\t\tReadTimeout:       10 * time.Second,","\t\tIdleTimeout:       30 * time.Second,","\t}","","\tenabled, tlsCertFile, tlsKeyFile := l.isTLSEnabled()","\tif enabled {","\t\tif err := srv.ListenAndServeTLS(tlsCertFile, tlsKeyFile); err != nil {","\t\t\treturn err","\t\t}","\t} else {","\t\tif err := srv.ListenAndServe(); err != nil {","\t\t\treturn err","\t\t}","\t}","\treturn nil","}","","func (l listener) handleEvent(ctx context.Context) http.HandlerFunc {","\treturn func(response http.ResponseWriter, request *http.Request) {","\t\tif request.Method != http.MethodPost {","\t\t\tl.writeResponse(response, http.StatusOK, \"ok\")","\t\t\treturn","\t\t}","","\t\t// event body","\t\tpayload, err := io.ReadAll(request.Body)","\t\tif err != nil {","\t\t\tl.logger.Errorf(\"failed to read body : %v\", err)","\t\t\tresponse.WriteHeader(http.StatusInternalServerError)","\t\t\treturn","\t\t}","","\t\tvar event map[string]any","\t\tif string(payload) != \"\" {","\t\t\tif err := json.Unmarshal(payload, \u0026event); err != nil {","\t\t\t\tl.logger.Errorf(\"Invalid event body format format: %s\", err)","\t\t\t\tresponse.WriteHeader(http.StatusBadRequest)","\t\t\t\treturn","\t\t\t}","\t\t}","","\t\tvar gitProvider provider.Interface","\t\tvar logger *zap.SugaredLogger","","\t\tl.event = info.NewEvent()","\t\tpacInfo := l.run.Info.GetPacOpts()","","\t\tglobalRepo, err := l.run.Clients.PipelineAsCode.PipelinesascodeV1alpha1().Repositories(l.run.Info.Kube.Namespace).Get(","\t\t\tctx, l.run.Info.Controller.GlobalRepository, metav1.GetOptions{},","\t\t)","\t\tif err == nil \u0026\u0026 globalRepo != nil {","\t\t\tl.logger.Infof(\"detected global repository settings named %s in namespace %s\", l.run.Info.Controller.GlobalRepository, l.run.Info.Kube.Namespace)","\t\t} else {","\t\t\tglobalRepo = \u0026v1alpha1.Repository{}","\t\t}","","\t\tdetected, configuring, err := github.ConfigureRepository(ctx, l.run, request, string(payload), \u0026pacInfo, l.logger)","\t\tif detected {","\t\t\tif configuring \u0026\u0026 err == nil {","\t\t\t\tl.writeResponse(response, http.StatusCreated, \"configured\")","\t\t\t\treturn","\t\t\t}","\t\t\tif configuring \u0026\u0026 err != nil {","\t\t\t\tl.logger.Errorf(\"repository auto-configure has failed, err: %v\", err)","\t\t\t\tl.writeResponse(response, http.StatusOK, \"failed to configure\")","\t\t\t\treturn","\t\t\t}","\t\t\tl.writeResponse(response, http.StatusOK, \"skipped event\")","\t\t\treturn","\t\t}","","\t\tisIncoming, targettedRepo, err := l.detectIncoming(ctx, request, payload)","\t\tif err != nil {","\t\t\tif errors.Is(err, errMissingFields) {","\t\t\t\tl.writeResponse(response, http.StatusBadRequest, err.Error())","\t\t\t}","\t\t\tl.logger.Errorf(\"error processing incoming webhook: %v\", err)","\t\t\treturn","\t\t}","","\t\tif isIncoming {","\t\t\tgitProvider, logger, err = l.processIncoming(targettedRepo)","\t\t} else {","\t\t\tgitProvider, logger, err = l.detectProvider(request, string(payload))","\t\t}","","\t\t// figure out which provider request coming from","\t\tif err != nil || gitProvider == nil {","\t\t\tl.writeResponse(response, http.StatusOK, err.Error())","\t\t\treturn","\t\t}","\t\tgitProvider.SetPacInfo(\u0026pacInfo)","","\t\ts := sinker{","\t\t\trun:        l.run,","\t\t\tvcx:        gitProvider,","\t\t\tkint:       l.kint,","\t\t\tevent:      l.event,","\t\t\tlogger:     logger,","\t\t\tpayload:    payload,","\t\t\tpacInfo:    \u0026pacInfo,","\t\t\tglobalRepo: globalRepo,","\t\t}","","\t\t// clone the request to use it further","\t\tlocalRequest := request.Clone(request.Context())","","\t\tgo func() {","\t\t\terr := s.processEvent(ctx, localRequest)","\t\t\tif err != nil {","\t\t\t\tlogger.Errorf(\"an error occurred: %v\", err)","\t\t\t}","\t\t}()","","\t\tl.writeResponse(response, http.StatusAccepted, \"accepted\")","\t}","}","","func (l listener) processRes(processEvent bool, provider provider.Interface, logger *zap.SugaredLogger, skipReason string, err error) (provider.Interface, *zap.SugaredLogger, error) {","\tif processEvent {","\t\tprovider.SetLogger(logger)","\t\treturn provider, logger, nil","\t}","\tif err != nil {","\t\terrStr := fmt.Sprintf(\"got error while processing : %v\", err)","\t\tlogger.Error(errStr)","\t\treturn nil, logger, fmt.Errorf(\"%s\", errStr)","\t}","","\tif skipReason != \"\" {","\t\tlogger.Debugf(\"skipping non supported event: %s\", skipReason)","\t}","\treturn nil, logger, fmt.Errorf(\"skipping non supported event\")","}","","func (l listener) detectProvider(req *http.Request, reqBody string) (provider.Interface, *zap.SugaredLogger, error) {","\tlog := *l.logger","","\t// payload validation","\tvar event map[string]any","\tif err := json.Unmarshal([]byte(reqBody), \u0026event); err != nil {","\t\treturn nil, \u0026log, fmt.Errorf(\"invalid event body format: %w\", err)","\t}","","\tgitHub := github.New()","\tgitHub.Run = l.run","\tisGH, processReq, logger, reason, err := gitHub.Detect(req, reqBody, \u0026log)","\tif isGH {","\t\treturn l.processRes(processReq, gitHub, logger, reason, err)","\t}","","\tzegitea := \u0026gitea.Provider{}","\tisGitea, processReq, logger, reason, err := zegitea.Detect(req, reqBody, \u0026log)","\tif isGitea {","\t\treturn l.processRes(processReq, zegitea, logger, reason, err)","\t}","","\tbitServer := \u0026bitbucketdatacenter.Provider{}","\tisBitServer, processReq, logger, reason, err := bitServer.Detect(req, reqBody, \u0026log)","\tif isBitServer {","\t\treturn l.processRes(processReq, bitServer, logger, reason, err)","\t}","","\tgitLab := \u0026gitlab.Provider{}","\tisGitLab, processReq, logger, reason, err := gitLab.Detect(req, reqBody, \u0026log)","\tif isGitLab {","\t\treturn l.processRes(processReq, gitLab, logger, reason, err)","\t}","","\tbitCloud := \u0026bitbucketcloud.Provider{}","","\tisBitCloud, processReq, logger, reason, err := bitCloud.Detect(req, reqBody, \u0026log)","\tif isBitCloud {","\t\treturn l.processRes(processReq, bitCloud, logger, reason, err)","\t}","","\treturn l.processRes(false, nil, logger, \"\", fmt.Errorf(\"no supported Git provider has been detected\"))","}","","func (l listener) writeResponse(response http.ResponseWriter, statusCode int, message string) {","\tresponse.WriteHeader(statusCode)","\tresponse.Header().Set(\"Content-Type\", \"application/json\")","\tbody := Response{","\t\tStatus:  statusCode,","\t\tMessage: message,","\t}","\tif err := json.NewEncoder(response).Encode(body); err != nil {","\t\tl.logger.Errorf(\"failed to write back sink response: %v\", err)","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,2,2,2,2,2,2,0,0,2,2,1,1,1,1,0,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,2,1,1,1,1,1,1,1,1,1,1,1,0,0,2,2,1,1,1,1,1,0,0,2,1,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,0,0,0,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,0,0,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,0,2,2,2,1,1,0,2,2,2,1,1,0,2,2,2,1,1,0,2,2,2,2,1,1,0,2,0,0,2,2,2,2,2,2,2,2,1,1,0]},{"id":4,"path":"pkg/adapter/incoming.go","lines":["package adapter","","import (","\t\"context\"","\t\"crypto/subtle\"","\t\"encoding/json\"","\t\"errors\"","\t\"fmt\"","\t\"net/http\"","\t\"slices\"","","\tapincoming \"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/incoming\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/formatting\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/matcher\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/bitbucketcloud\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/bitbucketdatacenter\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/gitea\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/github\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/github/app\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/gitlab\"","\tktypes \"github.com/openshift-pipelines/pipelines-as-code/pkg/secrets/types\"","\t\"go.uber.org/zap\"",")","","const (","\tdefaultIncomingWebhookSecretKey = \"secret\"",")","","var errMissingFields = errors.New(\"missing required fields\")","","func errMissingSpecificFields(fields []string) error {","\treturn fmt.Errorf(\"%w: %s\", errMissingFields, fields)","}","","type incomingPayload struct {","\tlegacyMode bool // indicates the request was made using the deprecated queryparams method","","\tRepoName    string         `json:\"repository\"`","\tNamespace   string         `json:\"namespace,omitempty\"` // Optional unless Repository name is not unique","\tBranch      string         `json:\"branch\"`","\tPipelineRun string         `json:\"pipelinerun\"`","\tSecret      string         `json:\"secret\"`","\tParams      map[string]any `json:\"params\"`","}","","func (payload *incomingPayload) validate() error {","\tmissingFields := []string{}","","\tfor field, value := range map[string]string{","\t\t\"repository\":  payload.RepoName,","\t\t\"branch\":      payload.Branch,","\t\t\"pipelinerun\": payload.PipelineRun,","\t\t\"secret\":      payload.Secret,","\t} {","\t\tif value == \"\" {","\t\t\tmissingFields = append(missingFields, field)","\t\t}","\t}","","\tif len(missingFields) \u003e 0 {","\t\treturn errMissingSpecificFields(missingFields)","\t}","\treturn nil","}","","// parseIncomingPayload parses and validates the incoming payload.","func parseIncomingPayload(request *http.Request, payloadBody []byte) (incomingPayload, error) {","\tparsedPayload := incomingPayload{","\t\tRepoName:    request.URL.Query().Get(\"repository\"),","\t\tBranch:      request.URL.Query().Get(\"branch\"),","\t\tPipelineRun: request.URL.Query().Get(\"pipelinerun\"),","\t\tSecret:      request.URL.Query().Get(\"secret\"),","\t\tNamespace:   request.URL.Query().Get(\"namespace\"),","\t\tlegacyMode:  true,","\t}","","\tif parsedPayload.validate() != nil {","\t\tif request.Method == http.MethodPost \u0026\u0026 request.Header.Get(\"Content-Type\") == \"application/json\" \u0026\u0026 len(payloadBody) \u003e 0 {","\t\t\tparsedPayload = incomingPayload{legacyMode: false}","\t\t\tif err := json.Unmarshal(payloadBody, \u0026parsedPayload); err != nil {","\t\t\t\treturn parsedPayload, fmt.Errorf(\"invalid JSON body for incoming webhook: %w\", err)","\t\t\t}","\t\t}","\t}","","\treturn parsedPayload, parsedPayload.validate()","}","","func compareSecret(incomingSecret, secretValue string) bool {","\treturn subtle.ConstantTimeCompare([]byte(incomingSecret), []byte(secretValue)) != 0","}","","func applyIncomingParams(req *http.Request, payloadBody []byte, params []string) (apincoming.Payload, error) {","\tif req.Header.Get(\"Content-Type\") != \"application/json\" {","\t\treturn apincoming.Payload{}, fmt.Errorf(\"invalid content type, only application/json is accepted when posting a body\")","\t}","\tpayload, err := apincoming.ParseIncomingPayload(payloadBody)","\tif err != nil {","\t\treturn apincoming.Payload{}, fmt.Errorf(\"error parsing incoming payload, not the expected format?: %w\", err)","\t}","\tfor k := range payload.Params {","\t\tif !slices.Contains(params, k) {","\t\t\treturn apincoming.Payload{}, fmt.Errorf(\"param %s is not allowed in incoming webhook CR\", k)","\t\t}","\t}","\treturn payload, nil","}","","// detectIncoming checks if the request is for an \"incoming\" webhook request.","// If the request is for an \"incoming\" webhook request the request is parsed and matched to the expected","// repository.","func (l *listener) detectIncoming(ctx context.Context, req *http.Request, payloadBody []byte) (bool, *v1alpha1.Repository, error) {","\tif req.URL.Path != \"/incoming\" {","\t\treturn false, nil, nil","\t}","","\tl.logger.Infof(\"incoming request has been requested: %v\", req.URL)","\tpayload, err := parseIncomingPayload(req, payloadBody)","\tif payload.legacyMode {","\t\t// Log this, even if the request is invalid","\t\tl.logger.Warnf(\"[SECURITY] Incoming webhook used legacy URL-based secret passing. This is insecure and will be deprecated. Please use POST body instead.\")","\t}","\tif err != nil {","\t\treturn false, nil, err","\t}","","\trepo, err := matcher.GetRepoByName(ctx, l.run, payload.RepoName, payload.Namespace)","\tif err != nil {","\t\tif errors.Is(err, matcher.ErrRepositoryNameConflict) {","\t\t\treturn false, nil, fmt.Errorf(\"%w: %w\", err, errMissingSpecificFields([]string{\"namespace\"}))","\t\t}","\t\treturn false, nil, fmt.Errorf(\"error getting repo: %w\", err)","\t}","\tif repo == nil {","\t\treturn false, nil, fmt.Errorf(\"cannot find repository %s\", payload.RepoName)","\t}","","\tif repo.Spec.Incomings == nil {","\t\treturn false, nil, fmt.Errorf(\"you need to have incoming webhooks rules in your repo spec, repo: %s\", payload.RepoName)","\t}","","\thook := matcher.IncomingWebhookRule(payload.Branch, *repo.Spec.Incomings)","\tif hook == nil {","\t\treturn false, nil, fmt.Errorf(\"branch '%s' has not matched any rules in repo incoming webhooks spec: %+v\", payload.Branch, *repo.Spec.Incomings)","\t}","","\t// log incoming request","\tl.logger.Infof(\"incoming request targeting pipelinerun %s on branch %s for repository %s has been accepted\", payload.PipelineRun, payload.Branch, payload.RepoName)","","\tsecretOpts := ktypes.GetSecretOpt{","\t\tNamespace: repo.Namespace,","\t\tName:      hook.Secret.Name,","\t\tKey:       hook.Secret.Key,","\t}","","\tif secretOpts.Key == \"\" {","\t\tsecretOpts.Key = defaultIncomingWebhookSecretKey","\t}","","\tsecretValue, err := l.kint.GetSecret(ctx, secretOpts)","\tif err != nil {","\t\treturn false, nil, fmt.Errorf(\"error getting secret referenced in incoming-webhook: %w\", err)","\t}","\tif secretValue == \"\" {","\t\treturn false, nil, fmt.Errorf(\"secret referenced in incoming-webhook %s is empty or key %s is not existent\", hook.Secret.Name, hook.Secret.Key)","\t}","","\t// TODO: move to somewhere common to share between gitlab and here","\tif !compareSecret(payload.Secret, secretValue) {","\t\treturn false, nil, fmt.Errorf(\"secret passed to the webhook does not match the incoming webhook secret set on repository CR in secret %s\", hook.Secret.Name)","\t}","","\tif repo.Spec.GitProvider == nil || repo.Spec.GitProvider.Type == \"\" {","\t\tgh := github.New()","\t\tgh.Run = l.run","\t\tns := info.GetNS(ctx)","\t\tip := app.NewInstallation(req, l.run, repo, gh, ns)","\t\tenterpriseURL, token, installationID, err := ip.GetAndUpdateInstallationID(ctx)","\t\tif err != nil {","\t\t\treturn false, nil, err","\t\t}","\t\tl.event.Provider.URL = enterpriseURL","\t\tl.event.Provider.Token = token","\t\tl.event.InstallationID = installationID","\t\t// Github app is not installed for provided repository url","\t\tif l.event.InstallationID == 0 {","\t\t\treturn false, nil, fmt.Errorf(\"GithubApp is not installed for the provided repository url %s \", repo.Spec.URL)","\t\t}","\t}","","\t// make sure accepted is json","\tif string(payloadBody) != \"\" {","\t\tif l.event.Event, err = applyIncomingParams(req, payloadBody, hook.Params); err != nil {","\t\t\treturn false, nil, err","\t\t}","\t}","","\t// TODO: more than i think about it and more i think triggertarget should be","\t// eventType and vice versa, but keeping as is for now.","\tl.event.EventType = \"incoming\"","\tl.event.TriggerTarget = \"push\"","\tl.event.TargetPipelineRun = payload.PipelineRun","\tl.event.HeadBranch = payload.Branch","\tl.event.BaseBranch = payload.Branch","\tl.event.Request.Header = req.Header","\tl.event.Request.Payload = payloadBody","\tl.event.URL = repo.Spec.URL","\tl.event.Sender = \"incoming\"","","\treturn true, repo, err","}","","func (l *listener) processIncoming(targetRepo *v1alpha1.Repository) (provider.Interface, *zap.SugaredLogger, error) {","\t// can a git ssh URL be a Repo URL? I don't think this will even ever work","\torg, repo, err := formatting.GetRepoOwnerSplitted(targetRepo.Spec.URL)","\tif err != nil {","\t\treturn nil, nil, err","\t}","\tl.event.Organization = org","\tl.event.Repository = repo","","\tvar provider provider.Interface","\tif targetRepo.Spec.GitProvider == nil || targetRepo.Spec.GitProvider.Type == \"\" {","\t\tprovider = github.New()","\t} else {","\t\tswitch targetRepo.Spec.GitProvider.Type {","\t\tcase \"github\":","\t\t\tprovider = github.New()","\t\tcase \"gitlab\":","\t\t\tprovider = \u0026gitlab.Provider{}","\t\tcase \"gitea\", \"forgejo\":","\t\t\tprovider = \u0026gitea.Provider{}","\t\tcase \"bitbucket-cloud\":","\t\t\tprovider = \u0026bitbucketcloud.Provider{}","\t\tcase \"bitbucket-datacenter\":","\t\t\tprovider = \u0026bitbucketdatacenter.Provider{}","\t\tdefault:","\t\t\treturn l.processRes(false, nil, l.logger.With(\"namespace\", targetRepo.Namespace), \"\", fmt.Errorf(\"no supported Git provider has been detected\"))","\t\t}","\t}","","\treturn l.processRes(true, provider, l.logger.With(\"provider\", \"incoming\", \"namespace\", targetRepo.Namespace), \"\", nil)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,0,0,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,0,2,0,0,0,0,0,2,2,2,2,0,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,1,0,2,2,2,0,2,2,2,0,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,0,0,2,2,2,0,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,0,0,0,2,2,2,2,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,0]},{"id":5,"path":"pkg/adapter/sinker.go","lines":["package adapter","","import (","\t\"bytes\"","\t\"context\"","\t\"fmt\"","\t\"net/http\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/matcher\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/pipelineascode\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"go.uber.org/zap\"",")","","type sinker struct {","\trun        *params.Run","\tvcx        provider.Interface","\tkint       kubeinteraction.Interface","\tevent      *info.Event","\tlogger     *zap.SugaredLogger","\tpayload    []byte","\tpacInfo    *info.PacOpts","\tglobalRepo *v1alpha1.Repository","}","","func (s *sinker) processEventPayload(ctx context.Context, request *http.Request) error {","\tvar err error","\ts.event, err = s.vcx.ParsePayload(ctx, s.run, request, string(s.payload))","\tif err != nil {","\t\ts.logger.Errorf(\"failed to parse event: %v\", err)","\t\treturn err","\t}","\t// If ParsePayload returned nil event (intentional skip), exit early","\tif s.event == nil {","\t\treturn nil","\t}","","\t// Enhanced structured logging with source repository context for operators","\tlogFields := []interface{}{","\t\t\"event-sha\", s.event.SHA,","\t\t\"event-type\", s.event.EventType,","\t\t\"source-repo-url\", s.event.URL,","\t}","","\t// Add branch information if available","\tif s.event.BaseBranch != \"\" {","\t\tlogFields = append(logFields, \"target-branch\", s.event.BaseBranch)","\t}","\t// For PRs, also include source branch if different","\tif s.event.HeadBranch != \"\" \u0026\u0026 s.event.HeadBranch != s.event.BaseBranch {","\t\tlogFields = append(logFields, \"source-branch\", s.event.HeadBranch)","\t}","","\ts.logger = s.logger.With(logFields...)","\ts.vcx.SetLogger(s.logger)","","\ts.event.Request = \u0026info.Request{","\t\tHeader:  request.Header,","\t\tPayload: bytes.TrimSpace(s.payload),","\t}","\treturn nil","}","","func (s *sinker) processEvent(ctx context.Context, request *http.Request) error {","\tif s.event.EventType == \"incoming\" {","\t\tif request.Header.Get(\"X-GitHub-Enterprise-Host\") != \"\" {","\t\t\ts.event.Provider.URL = request.Header.Get(\"X-GitHub-Enterprise-Host\")","\t\t\ts.event.GHEURL = request.Header.Get(\"X-GitHub-Enterprise-Host\")","\t\t}","\t} else {","\t\tif err := s.processEventPayload(ctx, request); err != nil {","\t\t\treturn err","\t\t}","\t\tif s.event == nil {","\t\t\treturn nil","\t\t}","","\t\t// For ALL events: Setup authenticated client early (including token scoping)","\t\t// This centralizes client setup and token scoping in one place for all event types","\t\trepo, err := s.findMatchingRepository(ctx)","\t\tif err != nil {","\t\t\t// Continue with normal flow - repository matching will be handled in matchRepoPR","\t\t\ts.logger.Debugf(\"Could not find matching repository: %v\", err)","\t\t} else {","\t\t\t// We found the repository, now setup client with token scoping","\t\t\t// If setup fails here, it's a configuration error and we should fail fast","\t\t\tif err := s.setupClient(ctx, repo); err != nil {","\t\t\t\treturn fmt.Errorf(\"client setup failed: %w\", err)","\t\t\t}","\t\t\ts.logger.Debugf(\"Client setup completed for event type: %s\", s.event.EventType)","\t\t}","","\t\t// For PUSH events: commit message is already in event.SHATitle from the webhook payload","\t\t// We can check immediately without any API calls or repository lookups","\t\tif s.event.EventType == \"push\" \u0026\u0026 provider.SkipCI(s.event.SHATitle) {","\t\t\ts.logger.Infof(\"CI skipped for push event: commit %s contains skip command in message\", s.event.SHA)","\t\t\treturn s.createSkipCIStatus(ctx)","\t\t}","","\t\t// For PULL REQUEST events: commit message needs to be fetched via API","\t\t// Get commit info for skip-CI detection (only if we successfully set up client above)","\t\tif (s.event.EventType == \"pull_request\" || s.event.EventType == \"Merge Request\") \u0026\u0026 repo != nil {","\t\t\t// Get commit info (including commit message) via API","\t\t\tif err := s.vcx.GetCommitInfo(ctx, s.event); err != nil {","\t\t\t\treturn fmt.Errorf(\"could not get commit info: %w\", err)","\t\t\t}","\t\t\t// Check for skip-ci commands in pull request events","\t\t\tif s.event.HasSkipCommand {","\t\t\t\ts.logger.Infof(\"CI skipped for pull request event: commit %s contains skip command in message\", s.event.SHA)","\t\t\t\treturn s.createSkipCIStatus(ctx)","\t\t\t}","\t\t}","\t}","","\tp := pipelineascode.NewPacs(s.event, s.vcx, s.run, s.pacInfo, s.kint, s.logger, s.globalRepo)","\treturn p.Run(ctx)","}","","// findMatchingRepository finds the Repository CR that matches the event.","// This is a lightweight lookup to get credentials for early skip-ci checks.","// Uses the canonical matcher implementation to avoid code duplication.","func (s *sinker) findMatchingRepository(ctx context.Context) (*v1alpha1.Repository, error) {","\t// Use canonical matcher to find repository (empty string searches all namespaces)","\trepo, err := matcher.MatchEventURLRepo(ctx, s.run, s.event, \"\")","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"failed to match repository: %w\", err)","\t}","\tif repo == nil {","\t\treturn nil, fmt.Errorf(\"no repository found matching URL: %s\", s.event.URL)","\t}","","\treturn repo, nil","}","","// setupClient sets up the authenticated client with token scoping for ALL event types.","// This is the primary location where client setup and GitHub App token scoping happens.","// Centralizing this here ensures consistent behavior across all events and enables early","// optimizations like skip-CI detection before expensive processing.","func (s *sinker) setupClient(ctx context.Context, repo *v1alpha1.Repository) error {","\treturn pipelineascode.SetupAuthenticatedClient(","\t\tctx,","\t\ts.vcx,","\t\ts.kint,","\t\ts.run,","\t\ts.event,","\t\trepo,","\t\ts.globalRepo,","\t\ts.pacInfo,","\t\ts.logger,","\t)","}","","// createSkipCIStatus creates a neutral status check on the git provider when CI is skipped.","func (s *sinker) createSkipCIStatus(ctx context.Context) error {","\tstatusOpts := provider.StatusOpts{","\t\tStatus:     \"completed\",","\t\tConclusion: \"neutral\",","\t\tTitle:      \"CI Skipped\",","\t\tSummary:    fmt.Sprintf(\"%s - CI has been skipped\", s.pacInfo.ApplicationName),","\t\tText:       \"Commit contains a skip CI command. Use /test or /retest to manually trigger CI if needed.\",","\t\tDetailsURL: s.run.Clients.ConsoleUI().URL(),","\t}","","\tif err := s.vcx.CreateStatus(ctx, s.event, statusOpts); err != nil {","\t\ts.logger.Warnf(\"Failed to create skip-CI status: %v\", err)","\t\t// Don't return error - skip-CI should succeed even if status creation fails","\t\treturn nil","\t}","","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,0,1,1,1,0,1,1,1,1,1,1,1,1,0,0,2,2,1,1,1,1,2,2,2,2,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,0,0,0,1,1,1,1,1,0,1,1,1,1,0,0,0,1,1,0,0,0,0,0,2,2,2,2,1,1,2,2,2,0,2,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0]},{"id":6,"path":"pkg/adapter/tls.go","lines":["package adapter","","import (","\t\"context\"","\t\"os\"","\t\"path/filepath\"","","\tv1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"knative.dev/pkg/system\"",")","","const tlsMountPath = \"/etc/pipelines-as-code/tls\"","","// isTLSEnabled validates if tls secret exist and if the required fields are defined","// this is used to enable tls on the listener.","func (l listener) isTLSEnabled() (bool, string, string) {","\ttlsSecret := os.Getenv(\"TLS_SECRET_NAME\")","\ttlsKey := os.Getenv(\"TLS_KEY\")","\ttlsCert := os.Getenv(\"TLS_CERT\")","","\t// TODO: Should we make different TLS by controller?","\ttls, err := l.run.Clients.Kube.CoreV1().Secrets(system.Namespace()).","\t\tGet(context.Background(), tlsSecret, v1.GetOptions{})","\tif err != nil {","\t\treturn false, \"\", \"\"","\t}","\t_, ok := tls.Data[tlsKey]","\tif !ok {","\t\treturn false, \"\", \"\"","\t}","\t_, ok = tls.Data[tlsCert]","\tif !ok {","\t\treturn false, \"\", \"\"","\t}","","\treturn true,","\t\tfilepath.Join(tlsMountPath, tlsCert),","\t\tfilepath.Join(tlsMountPath, tlsKey)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,2,2,0]},{"id":7,"path":"pkg/apis/incoming/incoming.go","lines":["package incoming","","import \"encoding/json\"","","type (","\tParams  map[string]any","\tPayload struct {","\t\tParams Params `json:\"params\"`","\t}",")","","// ParseIncomingPayload parses the payload from the incoming webhook, in json format and has only one key params.","func ParseIncomingPayload(payload []byte) (Payload, error) {","\tvar incomingPayload Payload","\terr := json.Unmarshal(payload, \u0026incomingPayload)","\tif err != nil {","\t\treturn Payload{}, err","\t}","\treturn incomingPayload, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,0]},{"id":8,"path":"pkg/apis/pipelinesascode/v1alpha1/register.go","lines":["/*","Copyright 2021 Red Hat","","Licensed under the Apache License, Version 2.0 (the \"License\");","you may not use this file except in compliance with the License.","You may obtain a copy of the License at","","\thttp://www.apache.org/licenses/LICENSE-2.0","","Unless required by applicable law or agreed to in writing, software","distributed under the License is distributed on an \"AS IS\" BASIS,","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","See the License for the specific language governing permissions and","limitations under the License.","*/","","package v1alpha1","","import (","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/apimachinery/pkg/runtime\"","\t\"k8s.io/apimachinery/pkg/runtime/schema\"",")","","// SchemeGroupVersion is group version used to register these objects.","var SchemeGroupVersion = schema.GroupVersion{Group: pipelinesascode.GroupName, Version: \"v1alpha1\"}","","// Kind takes an unqualified kind and returns back a Group qualified GroupKind.","func Kind(kind string) schema.GroupKind {","\treturn SchemeGroupVersion.WithKind(kind).GroupKind()","}","","// Resource takes an unqualified resource and returns a Group qualified GroupResource.","func Resource(resource string) schema.GroupResource {","\treturn SchemeGroupVersion.WithResource(resource).GroupResource()","}","","var (","\t// SchemeBuilder initializes a scheme builder.","\tSchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes)","\t// AddToScheme is a global function that registers this API group \u0026 version to a scheme.","\tAddToScheme = SchemeBuilder.AddToScheme",")","","// Adds the list of known types to Scheme.","func addKnownTypes(scheme *runtime.Scheme) error {","\tscheme.AddKnownTypes(SchemeGroupVersion,","\t\t\u0026Repository{},","\t\t\u0026RepositoryList{},","\t)","\tmetav1.AddToGroupVersion(scheme, SchemeGroupVersion)","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1]},{"id":9,"path":"pkg/apis/pipelinesascode/v1alpha1/types.go","lines":["package v1alpha1","","import (","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\tduckv1 \"knative.dev/pkg/apis/duck/v1\"",")","","// +genclient","// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object","","// Repository is the representation of a Git repository from a Git provider platform.","// +kubebuilder:object:root=true","// +kubebuilder:subresource:status","// +kubebuilder:resource:scope=Namespaced,shortName=repo","// +kubebuilder:printcolumn:name=\"URL\",type=string,JSONPath=`.spec.url`","// +kubebuilder:printcolumn:name=\"Succeeded\",type=string,JSONPath=`.pipelinerun_status[-1].conditions[?(@.type==\"Succeeded\")].status`","// +kubebuilder:printcolumn:name=\"Reason\",type=string,JSONPath=`.pipelinerun_status[-1].conditions[?(@.type==\"Succeeded\")].reason`","// +kubebuilder:printcolumn:name=\"StartTime\",type=date,JSONPath=`.pipelinerun_status[-1].startTime`","// +kubebuilder:printcolumn:name=\"CompletionTime\",type=date,JSONPath=`.pipelinerun_status[-1].completionTime`","type Repository struct {","\tmetav1.TypeMeta   `json:\",inline\"`","\tmetav1.ObjectMeta `json:\"metadata\"`","","\tSpec   RepositorySpec        `json:\"spec\"`","\tStatus []RepositoryRunStatus `json:\"pipelinerun_status,omitempty\"`","}","","type RepositoryRunStatus struct {","\tduckv1.Status `json:\",inline\"`","","\t// PipelineRunName is the name of the PipelineRun","\t// +optional","\tPipelineRunName string `json:\"pipelineRunName,omitempty\"`","","\t// StartTime is the time the PipelineRun is actually started.","\t// +optional","\tStartTime *metav1.Time `json:\"startTime,omitempty\"`","","\t// CompletionTime is the time the PipelineRun completed.","\t// +optional","\tCompletionTime *metav1.Time `json:\"completionTime,omitempty\"`","","\t// SHA is the name of the SHA that has been tested","\t// +optional","\tSHA *string `json:\"sha,omitempty\"`","","\t// SHA the URL of the SHA to view it","\t// +optional","\tSHAURL *string `json:\"sha_url,omitempty\"`","","\t// Title is the title of the commit SHA that has been tested","\t// +optional","\tTitle *string `json:\"title,omitempty\"`","","\t// LogURL is the full URL to the log for this run.","\t// +optional","\tLogURL *string `json:\"logurl,omitempty\"`","","\t// TargetBranch is the target branch of that run","\t// +optional","\tTargetBranch *string `json:\"target_branch,omitempty\"`","","\t// EventType is the event type of that run","\t// +optional","\tEventType *string `json:\"event_type,omitempty\"`","","\t// CollectedTaskInfos is the information about tasks","\tCollectedTaskInfos *map[string]TaskInfos `json:\"failure_reason,omitempty\"`","}","","// TaskInfos contains information about a task.","type TaskInfos struct {","\tName           string       `json:\"name\"`","\tMessage        string       `json:\"message,omitempty\"`","\tLogSnippet     string       `json:\"log_snippet,omitempty\"`","\tReason         string       `json:\"reason,omitempty\"`","\tDisplayName    string       `json:\"display_name,omitempty\"`","\tCompletionTime *metav1.Time `json:\"completion_time,omitempty\"`","}","","// RepositorySpec defines the desired state of a Repository, including its URL,","// Git provider configuration, and operational settings.","type RepositorySpec struct {","\t// ConcurrencyLimit defines the maximum number of concurrent pipelineruns that can","\t// run for this repository. This helps prevent resource exhaustion when many events trigger","\t// pipelines simultaneously.","\t// +optional","\t// +kubebuilder:validation:Minimum=1","\tConcurrencyLimit *int `json:\"concurrency_limit,omitempty\"` // move it to settings in further version of the spec","","\t// URL of the repository we are building. Must be a valid HTTP/HTTPS Git repository URL","\t// that PAC will use to clone and fetch pipeline definitions from.","\t// +optional","\tURL string `json:\"url\"`","","\t// GitProvider details specific to a git provider configuration. Contains authentication,","\t// API endpoints, and provider type information needed to interact with the Git service.","\t// +optional","\tGitProvider *GitProvider `json:\"git_provider,omitempty\"`","","\t// Incomings defines incoming webhook configurations. Each configuration specifies how to","\t// handle external webhook requests that don't come directly from the primary Git provider.","\t// +optional","\tIncomings *[]Incoming `json:\"incoming,omitempty\"`","","\t// Params defines repository level parameters that can be referenced in PipelineRuns.","\t// These parameters can be used as default values or configured for specific events.","\t// +optional","\tParams *[]Params `json:\"params,omitempty\"`","","\t// Settings contains the configuration settings for the repository, including","\t// authorization policies, provider-specific configuration, and provenance settings.","\t// +optional","\tSettings *Settings `json:\"settings,omitempty\"`","}","","func (r *RepositorySpec) Merge(newRepo RepositorySpec) {","\tif newRepo.ConcurrencyLimit != nil \u0026\u0026 r.ConcurrencyLimit == nil {","\t\tr.ConcurrencyLimit = newRepo.ConcurrencyLimit","\t}","\tif newRepo.Settings != nil \u0026\u0026 r.Settings != nil {","\t\tr.Settings.Merge(newRepo.Settings)","\t}","\tif r.GitProvider != nil \u0026\u0026 newRepo.GitProvider != nil {","\t\tr.GitProvider.Merge(newRepo.GitProvider)","\t}","","\t// TODO(chmouel): maybe let it merges those between the user Repo Incomings and Params with the global ones?","\t// we need to gather feedback first with users to know what they want.","\tif newRepo.Incomings != nil \u0026\u0026 r.Incomings == nil {","\t\tr.Incomings = newRepo.Incomings","\t}","\tif newRepo.Params != nil \u0026\u0026 r.Params == nil {","\t\tr.Params = newRepo.Params","\t}","}","","type Settings struct {","\t// GithubAppTokenScopeRepos lists repositories that can access the GitHub App token when using the","\t// GitHub App authentication method. This allows specific repositories to use tokens generated for","\t// the GitHub App installation, useful for cross-repository access.","\t// +optional","\tGithubAppTokenScopeRepos []string `json:\"github_app_token_scope_repos,omitempty\"`","","\t// PipelineRunProvenance configures how PipelineRun definitions are fetched.","\t// Options:","\t// - 'source': Fetch definitions from the event source branch/SHA (default)","\t// - 'default_branch': Fetch definitions from the repository default branch","\t// +optional","\t// +kubebuilder:validation:Enum=source;default_branch","\tPipelineRunProvenance string `json:\"pipelinerun_provenance,omitempty\"`","","\t// Policy defines authorization policies for the repository, controlling who can","\t// trigger PipelineRuns under different conditions.","\t// +optional","\tPolicy *Policy `json:\"policy,omitempty\"`","","\t// Gitlab contains GitLab-specific settings for repositories hosted on GitLab.","\t// +optional","\tGitlab *GitlabSettings `json:\"gitlab,omitempty\"`","","\tGithub *GithubSettings `json:\"github,omitempty\"`","","\t// AIAnalysis contains AI/LLM analysis configuration for automated CI/CD pipeline analysis.","\t// +optional","\tAIAnalysis *AIAnalysisConfig `json:\"ai,omitempty\"`","}","","type GitlabSettings struct {","\t// CommentStrategy defines how GitLab comments are handled for pipeline results.","\t// Options:","\t// - 'disable_all': Disables all comments on merge requests","\t// +optional","\t// +kubebuilder:validation:Enum=\"\";disable_all","\tCommentStrategy string `json:\"comment_strategy,omitempty\"`","}","","type GithubSettings struct {","\t// CommentStrategy defines how GitLab comments are handled for pipeline results.","\t// Options:","\t// - 'disable_all': Disables all comments on merge requests","\t// +optional","\t// +kubebuilder:validation:Enum=\"\";disable_all","\tCommentStrategy string `json:\"comment_strategy,omitempty\"`","}","","func (s *Settings) Merge(newSettings *Settings) {","\tif newSettings.PipelineRunProvenance != \"\" \u0026\u0026 s.PipelineRunProvenance == \"\" {","\t\ts.PipelineRunProvenance = newSettings.PipelineRunProvenance","\t}","\tif newSettings.Policy != nil \u0026\u0026 s.Policy == nil {","\t\ts.Policy = newSettings.Policy","\t}","\tif newSettings.GithubAppTokenScopeRepos != nil \u0026\u0026 s.GithubAppTokenScopeRepos == nil {","\t\ts.GithubAppTokenScopeRepos = newSettings.GithubAppTokenScopeRepos","\t}","\tif newSettings.AIAnalysis != nil \u0026\u0026 s.AIAnalysis == nil {","\t\ts.AIAnalysis = newSettings.AIAnalysis","\t}","}","","type Policy struct {","\t// OkToTest defines a list of usernames that are allowed to trigger pipeline runs on pull requests","\t// from external contributors by commenting \"/ok-to-test\" on the PR. These users are typically","\t// repository maintainers or trusted contributors who can vouch for external contributions.","\t// +optional","\tOkToTest []string `json:\"ok_to_test,omitempty\"`","","\t// PullRequest defines a list of usernames that are explicitly allowed to execute","\t// pipelines on their pull requests, even if they wouldn't normally have permission.","\t// This is useful for allowing specific external contributors to trigger pipeline runs.","\t// +optional","\tPullRequest []string `json:\"pull_request,omitempty\"`","}","","type Params struct {","\t// Name of the parameter. This is the key that will be used to reference this parameter","\t// in PipelineRun definitions through via the {{ name }} syntax.","\t// +kubebuilder:validation:Required","\tName string `json:\"name\"`","","\t// Value of the parameter. The literal value to be provided to the PipelineRun.","\t// This field is mutually exclusive with SecretRef.","\t// +optional","\tValue string `json:\"value,omitempty\"`","","\t// SecretRef references a secret for the parameter value. Use this when the parameter","\t// contains sensitive information that should not be stored directly in the Repository CR.","\t// This field is mutually exclusive with Value.","\t// +optional","\tSecretRef *Secret `json:\"secret_ref,omitempty\"`","","\t// Filter defines when this parameter applies. It can be used to conditionally","\t// apply parameters based on the event type, branch name, or other attributes.","\t// +optional","\tFilter string `json:\"filter,omitempty\"`","}","","type Incoming struct {","\t// Type of the incoming webhook. Currently only 'webhook-url' is supported, which allows","\t// external systems to trigger PipelineRuns via generic webhook requests.","\t// +kubebuilder:validation:Required","\t// +kubebuilder:validation:Enum=webhook-url","\tType string `json:\"type\"`","","\t// Secret for the incoming webhook authentication. This secret is used to validate","\t// that webhook requests are coming from authorized sources.","\t// +kubebuilder:validation:Required","\tSecret Secret `json:\"secret\"`","","\t// Params defines parameter names to extract from the webhook payload. These parameters","\t// will be made available to the PipelineRuns triggered by this webhook.","\t// +optional","\tParams []string `json:\"params,omitempty\"`","","\t// Targets defines target branches for this webhook. When specified, only webhook","\t// events targeting these branches will trigger PipelineRuns.","\t// +optional","\tTargets []string `json:\"targets,omitempty\"`","}","","type GitProvider struct {","\t// URL of the git provider API endpoint. This is the base URL for API requests to the","\t// Git provider (e.g., 'https://api.github.com' for GitHub or a custom GitLab instance URL).","\t// +optional","\tURL string `json:\"url,omitempty\"`","","\t// User of the git provider. Username to use for authentication when using basic auth","\t// or token-based authentication methods. Not used for GitHub Apps authentication.","\t// +optional","\tUser string `json:\"user,omitempty\"`","","\t// Secret reference for authentication with the Git provider. Contains the token,","\t// password, or private key used to authenticate requests to the Git provider API.","\t// +optional","\tSecret *Secret `json:\"secret,omitempty\"`","","\t// WebhookSecret reference for webhook validation. Contains the shared secret used to","\t// validate that incoming webhooks are legitimate and coming from the Git provider.","\t// +optional","\tWebhookSecret *Secret `json:\"webhook_secret,omitempty\"`","","\t// Type of git provider. Determines which Git provider API and authentication flow to use.","\t// Supported values:","\t// - 'github': GitHub.com or GitHub Enterprise","\t// - 'gitlab': GitLab.com or self-hosted GitLab","\t// - 'bitbucket-datacenter': Bitbucket Data Center (self-hosted)","\t// - 'bitbucket-cloud': Bitbucket Cloud (bitbucket.org)","\t// - 'forgejo': Forgejo instances","\t// - 'gitea': Gitea instances (alias for forgejo, kept for backwards compatibility)","\t// +optional","\t// +kubebuilder:validation:Enum=github;gitlab;bitbucket-datacenter;bitbucket-cloud;gitea;forgejo","\tType string `json:\"type,omitempty\"`","}","","// areEquivalentProviderTypes checks if gitea/forgejo aliases refer to the same provider.","func areEquivalentProviderTypes(a, b string) bool {","\tif a == b {","\t\treturn true","\t}","\tgiteaTypes := map[string]bool{\"gitea\": true, \"forgejo\": true}","\treturn giteaTypes[a] \u0026\u0026 giteaTypes[b]","}","","func (g *GitProvider) Merge(newGitProvider *GitProvider) {","\t// only merge of the same type (gitea and forgejo are equivalent)","\tif newGitProvider.Type != \"\" \u0026\u0026 g.Type != \"\" \u0026\u0026 !areEquivalentProviderTypes(g.Type, newGitProvider.Type) {","\t\treturn","\t}","\tif newGitProvider.URL != \"\" \u0026\u0026 g.URL == \"\" {","\t\tg.URL = newGitProvider.URL","\t}","\tif newGitProvider.User != \"\" \u0026\u0026 g.User == \"\" {","\t\tg.User = newGitProvider.User","\t}","\tif newGitProvider.Type != \"\" \u0026\u0026 g.Type == \"\" {","\t\tg.Type = newGitProvider.Type","\t}","\tif newGitProvider.Secret != nil \u0026\u0026 g.Secret == nil {","\t\tg.Secret = newGitProvider.Secret","\t}","\tif newGitProvider.WebhookSecret != nil \u0026\u0026 g.WebhookSecret == nil {","\t\tg.WebhookSecret = newGitProvider.WebhookSecret","\t}","}","","type Secret struct {","\t// Name of the secret","\t// +kubebuilder:validation:Required","\tName string `json:\"name\"`","","\t// Key in the secret","\t// +optional","\tKey string `json:\"key\"`","}","","// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object","","// RepositoryList is the list of Repositories.","// +kubebuilder:object:root=true","type RepositoryList struct {","\tmetav1.TypeMeta `json:\",inline\"`","\tmetav1.ListMeta `json:\"metadata\"`","","\tItems []Repository `json:\"items\"`","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,1,1,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]},{"id":10,"path":"pkg/apis/pipelinesascode/v1alpha1/types_llm.go","lines":["package v1alpha1","","const (","\t// defaultContainerLogsMaxLines is the default maximum number of log lines to fetch per container.","\tdefaultContainerLogsMaxLines = 50","\tdefaultOpenAIURL             = \"https://api.openai.com/v1\"","\tdefaultGeminiURL             = \"https://generativelanguage.googleapis.com/v1beta\"",")","","// AIAnalysisConfig defines configuration for AI/LLM-powered analysis of CI/CD pipeline events.","type AIAnalysisConfig struct {","\t// Enabled controls whether AI analysis is active for this repository","\t// +kubebuilder:validation:Required","\tEnabled bool `json:\"enabled\"`","","\t// Provider specifies which LLM provider to use for analysis","\t// +kubebuilder:validation:Required","\t// +kubebuilder:validation:Enum=openai;gemini","\tProvider string `json:\"provider\"`","","\t// APIURL is an optional base URL to override the default API endpoint of the LLM provider.","\t// If not specified, provider-specific defaults are used:","\t// - OpenAI: https://api.openai.com/v1","\t// - Gemini: https://generativelanguage.googleapis.com/v1beta","\t// Use this to configure self-hosted LLM instances, proxy services, or alternative endpoints.","\t// +optional","\tAPIURL string `json:\"api_url,omitempty\"`","","\t// TokenSecretRef references the Kubernetes secret containing the LLM provider API token","\t// +kubebuilder:validation:Required","\tTokenSecretRef *Secret `json:\"secret_ref\"`","","\t// TimeoutSeconds sets the maximum time to wait for LLM analysis (default: 30)","\t// +optional","\t// +kubebuilder:validation:Minimum=1","\t// +kubebuilder:validation:Maximum=300","\tTimeoutSeconds int `json:\"timeout_seconds,omitempty\"`","","\t// MaxTokens limits the response length from the LLM (default: 1000)","\t// +optional","\t// +kubebuilder:validation:Minimum=1","\t// +kubebuilder:validation:Maximum=4000","\tMaxTokens int `json:\"max_tokens,omitempty\"`","","\t// Roles defines different analysis scenarios and their configurations","\t// +kubebuilder:validation:Required","\t// +kubebuilder:validation:MinItems=1","\t// +listType=map","\t// +listMapKey=name","\tRoles []AnalysisRole `json:\"roles\"`","}","","// AnalysisRole defines a specific analysis scenario with its prompt, conditions, and output configuration.","type AnalysisRole struct {","\t// Name is a unique identifier for this analysis role","\t// +kubebuilder:validation:Required","\tName string `json:\"name\"`","","\t// Prompt is the base prompt template sent to the LLM for analysis","\t// +kubebuilder:validation:Required","\tPrompt string `json:\"prompt\"`","","\t// Model specifies which LLM model to use for this role (optional).","\t// You can specify any model supported by your provider.","\t// If not specified, provider-specific defaults are used:","\t// - OpenAI: gpt-5-mini","\t// - Gemini: gemini-2.5-flash-lite","\t// +optional","\tModel string `json:\"model,omitempty\"`","","\t// OnCEL is a CEL expression that determines when this role should be triggered","\t// +optional","\tOnCEL string `json:\"on_cel,omitempty\"`","","\t// Output specifies where the analysis results should be sent (default: pr-comment)","\t// +optional","\t// +kubebuilder:default=pr-comment","\t// +kubebuilder:validation:Enum=pr-comment","\tOutput string `json:\"output,omitempty\"`","","\t// ContextItems defines what context data to include in the analysis","\t// +optional","\tContextItems *ContextConfig `json:\"context_items,omitempty\"`","}","","// ContextConfig defines what contextual information to include in LLM analysis.","type ContextConfig struct {","\t// CommitContent includes commit message and diff information","\t// +optional","\tCommitContent bool `json:\"commit_content,omitempty\"`","","\t// PRContent includes pull request title, description, and metadata","\t// +optional","\tPRContent bool `json:\"pr_content,omitempty\"`","","\t// ErrorContent includes error messages and failure summaries","\t// +optional","\tErrorContent bool `json:\"error_content,omitempty\"`","","\t// ContainerLogs configures inclusion of container/task logs","\t// +optional","\tContainerLogs *ContainerLogsConfig `json:\"container_logs,omitempty\"`","}","","// ContainerLogsConfig defines how container logs should be included in analysis.","type ContainerLogsConfig struct {","\t// Enabled controls whether container logs are included","\tEnabled bool `json:\"enabled\"`","","\t// MaxLines limits the number of log lines to include (default: 50)","\t// +optional","\t// +kubebuilder:validation:Minimum=1","\t// +kubebuilder:validation:Maximum=1000","\tMaxLines int `json:\"max_lines,omitempty\"`","}","","func (c *ContainerLogsConfig) GetMaxLines() int {","\tif c == nil || c.MaxLines == 0 {","\t\treturn defaultContainerLogsMaxLines","\t}","\treturn c.MaxLines","}","","// GetOutput returns the output destination with a default value if not specified.","func (r *AnalysisRole) GetOutput() string {","\tif r.Output == \"\" {","\t\treturn \"pr-comment\"","\t}","\treturn r.Output","}","","// GetModel returns the configured model or an empty string to use provider default.","func (r *AnalysisRole) GetModel() string {","\treturn r.Model","}","","// GetAPIURL returns the configured API URL, or the provider's default if not specified.","func (c *AIAnalysisConfig) GetAPIURL() string {","\tif c.APIURL != \"\" {","\t\treturn c.APIURL","\t}","\treturn GetProviderDefaultAPIURL(c.Provider)","}","","// GetProviderDefaultAPIURL returns the default API URL for a given LLM provider.","func GetProviderDefaultAPIURL(provider string) string {","\tswitch provider {","\tcase \"openai\":","\t\treturn defaultOpenAIURL","\tcase \"gemini\":","\t\treturn defaultGeminiURL","\tdefault:","\t\treturn \"\"","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,1,1,1,1,1,0,0,0,1,1,1,0,0,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,0,0]},{"id":11,"path":"pkg/cel/cel.go","lines":["package cel","","import (","\t\"bytes\"","\t\"encoding/json\"","\t\"fmt\"","","\t\"github.com/google/cel-go/cel\"","\t\"github.com/google/cel-go/common/decls\"","\t\"github.com/google/cel-go/common/types\"","\t\"github.com/google/cel-go/common/types/ref\"",")","","func evaluate(expr string, env *cel.Env, data map[string]any) (ref.Val, error) {","\tparsed, issues := env.Parse(expr)","\tif issues != nil \u0026\u0026 issues.Err() != nil {","\t\treturn nil, fmt.Errorf(\"failed to parse expression %#v: %w\", expr, issues.Err())","\t}","","\tchecked, issues := env.Check(parsed)","\tif issues != nil \u0026\u0026 issues.Err() != nil {","\t\treturn nil, fmt.Errorf(\"expression %#v check failed: %w\", expr, issues.Err())","\t}","","\tprg, err := env.Program(checked, cel.EvalOptions(cel.OptOptimize))","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"expression %#v failed to create a Program: %w\", expr, err)","\t}","","\tout, _, err := prg.Eval(data)","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"expression %#v failed to evaluate: %w\", expr, err)","\t}","","\treturn out, nil","}","","// Value evaluates a CEL expression with the given body, headers and","// / pacParams, it will output a Cel value or an error if selectedjm.","func Value(query string, body any, headers, pacParams map[string]string, changedFiles map[string]any) (ref.Val, error) {","\t// Marshal/Unmarshal the body to a map[string]any so we can access it from the CEL","\tvar jsonMap map[string]any","\tswitch b := body.(type) {","\tcase nil:","\t\tjsonMap = map[string]any{}","\tcase map[string]any:","\t\tjsonMap = b","\tdefault:","\t\tnbody, err := json.Marshal(body)","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\ttrimmed := bytes.TrimSpace(nbody)","\t\tif len(trimmed) == 0 || bytes.Equal(trimmed, []byte(\"null\")) {","\t\t\tjsonMap = map[string]any{}","\t\t} else {","\t\t\terr = json.Unmarshal(nbody, \u0026jsonMap)","\t\t\tif err != nil {","\t\t\t\treturn nil, err","\t\t\t}","\t\t\tif jsonMap == nil {","\t\t\t\tjsonMap = map[string]any{}","\t\t\t}","\t\t}","\t}","","\tmapStrDyn := types.NewMapType(types.StringType, types.DynType)","\tcelDec, _ := cel.NewEnv(","\t\tcel.VariableDecls(","\t\t\tdecls.NewVariable(\"body\", mapStrDyn),","\t\t\tdecls.NewVariable(\"headers\", mapStrDyn),","\t\t\tdecls.NewVariable(\"pac\", mapStrDyn),","\t\t\tdecls.NewVariable(\"files\", mapStrDyn),","\t\t\t// Direct variables as per documentation","\t\t\tdecls.NewVariable(\"event\", types.StringType),","\t\t\tdecls.NewVariable(\"event_type\", types.StringType),","\t\t\tdecls.NewVariable(\"target_branch\", types.StringType),","\t\t\tdecls.NewVariable(\"source_branch\", types.StringType),","\t\t\tdecls.NewVariable(\"target_url\", types.StringType),","\t\t\tdecls.NewVariable(\"source_url\", types.StringType),","\t\t\tdecls.NewVariable(\"event_title\", types.StringType),","\t\t\tdecls.NewVariable(\"revision\", types.StringType),","\t\t\tdecls.NewVariable(\"repo_owner\", types.StringType),","\t\t\tdecls.NewVariable(\"repo_name\", types.StringType),","\t\t\tdecls.NewVariable(\"sender\", types.StringType),","\t\t\tdecls.NewVariable(\"repo_url\", types.StringType),","\t\t\tdecls.NewVariable(\"git_tag\", types.StringType),","\t\t\tdecls.NewVariable(\"target_namespace\", types.StringType),","\t\t\tdecls.NewVariable(\"trigger_comment\", types.StringType),","\t\t\tdecls.NewVariable(\"pull_request_labels\", types.StringType),","\t\t\tdecls.NewVariable(\"pull_request_number\", types.StringType),","\t\t\tdecls.NewVariable(\"git_auth_secret\", types.StringType),","\t\t))","\tval, err := evaluate(query, celDec, map[string]any{","\t\t\"body\":    jsonMap,","\t\t\"pac\":     pacParams,","\t\t\"headers\": headers,","\t\t\"files\":   changedFiles,","\t\t// Direct variables - all from pacParams","\t\t\"event\":               pacParams[\"event\"],","\t\t\"event_type\":          pacParams[\"event_type\"],","\t\t\"target_branch\":       pacParams[\"target_branch\"],","\t\t\"source_branch\":       pacParams[\"source_branch\"],","\t\t\"target_url\":          pacParams[\"target_url\"],","\t\t\"source_url\":          pacParams[\"source_url\"],","\t\t\"event_title\":         pacParams[\"event_title\"],","\t\t\"revision\":            pacParams[\"revision\"],","\t\t\"repo_owner\":          pacParams[\"repo_owner\"],","\t\t\"repo_name\":           pacParams[\"repo_name\"],","\t\t\"sender\":              pacParams[\"sender\"],","\t\t\"repo_url\":            pacParams[\"repo_url\"],","\t\t\"git_tag\":             pacParams[\"git_tag\"],","\t\t\"target_namespace\":    pacParams[\"target_namespace\"],","\t\t\"trigger_comment\":     pacParams[\"trigger_comment\"],","\t\t\"pull_request_labels\": pacParams[\"pull_request_labels\"],","\t\t\"pull_request_number\": pacParams[\"pull_request_number\"],","\t\t\"git_auth_secret\":     pacParams[\"git_auth_secret\"],","\t})","\tif err != nil {","\t\treturn nil, err","\t}","\treturn val, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,2,2,1,1,0,2,2,1,1,0,2,2,1,1,0,2,0,0,0,0,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0]},{"id":12,"path":"pkg/changedfiles/changedfiles.go","lines":["package changedfiles","","type ChangedFiles struct {","\tAll      []string","\tAdded    []string","\tDeleted  []string","\tModified []string","\tRenamed  []string","}","","// removeDuplicates removes duplicates from a slice of strings.","func removeDuplicates(s []string) []string {","\tholdit := make(map[string]struct{})","\tresult := make([]string, 0, len(s))","\tfor _, str := range s {","\t\tif _, ok := holdit[str]; !ok {","\t\t\tholdit[str] = struct{}{}","\t\t\tresult = append(result, str)","\t\t}","\t}","\treturn result","}","","func (c *ChangedFiles) RemoveDuplicates() {","\tc.All = removeDuplicates(c.All)","\tc.Added = removeDuplicates(c.Added)","\tc.Deleted = removeDuplicates(c.Deleted)","\tc.Modified = removeDuplicates(c.Modified)","\tc.Renamed = removeDuplicates(c.Renamed)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,0,2,0,0,2,2,2,2,2,2,2]},{"id":13,"path":"pkg/cli/browser/browser.go","lines":["package browser","","import (","\t\"context\"","\t\"os/exec\"","\t\"runtime\"",")","","// OpenWebBrowser opens the specified URL in the default browser of the user.","func OpenWebBrowser(ctx context.Context, url string) error {","\tvar cmd string","","\targs := make([]string, 0, 1)","\tswitch runtime.GOOS {","\tcase \"windows\":","\t\tcmd = \"cmd\"","\t\targs = []string{\"/c\", \"start\"}","\tcase \"darwin\":","\t\tcmd = \"open\"","\tdefault: // \"linux\", \"freebsd\", \"openbsd\", \"netbsd\"","\t\tcmd = \"xdg-open\"","\t}","","\targs = append(args, url)","\treturn exec.CommandContext(ctx, cmd, args...).Start()","}"],"coverage":[0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,0]},{"id":14,"path":"pkg/cli/cli.go","lines":["package cli","","import (","\t\"os\"","","\t\"github.com/AlecAivazis/survey/v2\"","\t\"github.com/AlecAivazis/survey/v2/terminal\"",")","","// PacCliOpts is the struct that holds all the options for the CLI","// TODO: Pass this to a context.","type PacCliOpts struct {","\tNoColoring    bool","\tAllNameSpaces bool","\tNamespace     string","\tUseRealTime   bool","\tAskOpts       survey.AskOpt","\tNoHeaders     bool","}","","func NewAskopts(opt *survey.AskOptions) error {","\topt.Stdio = terminal.Stdio{","\t\tIn:  os.Stdin,","\t\tOut: os.Stdout,","\t\tErr: os.Stderr,","\t}","\treturn nil","}","","func NewCliOptions() *PacCliOpts {","\treturn \u0026PacCliOpts{","\t\tAskOpts: NewAskopts,","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1]},{"id":15,"path":"pkg/cli/color.go","lines":["package cli","","import (","\t\"fmt\"","\t\"os\"","\t\"strings\"","","\t\"github.com/mgutz/ansi\"",")","","var (","\tmagenta    = ansi.ColorFunc(\"magenta\")","\tcyan       = ansi.ColorFunc(\"cyan\")","\tred        = ansi.ColorFunc(\"red\")","\tredBold    = ansi.ColorFunc(\"red+b\")","\tyellow     = ansi.ColorFunc(\"yellow\")","\tblue       = ansi.ColorFunc(\"blue\")","\tblueBold   = ansi.ColorFunc(\"blue+b\")","\tgreen      = ansi.ColorFunc(\"green\")","\tgreenBold  = ansi.ColorFunc(\"green+b\")","\tgray       = ansi.ColorFunc(\"black+i\")","\tbold       = ansi.ColorFunc(\"default+b\")","\tdimmed     = ansi.ColorFunc(\"246\")","\tunderline  = ansi.ColorFunc(\"default+u\")","\tcyanBold   = ansi.ColorFunc(\"cyan+b\")","\torangeBold = ansi.ColorFunc(\"208\")","","\tgray256 = func(t string) string {","\t\treturn fmt.Sprintf(\"\\x1b[%d;5;%dm%s\\x1b[m\", 38, 242, t)","\t}","\thyperLink = func(title, href string) string {","\t\treturn fmt.Sprintf(\"\\x1b]8;;%s\\x1b\\\\%s\\x1b]8;;\\x1b\\\\\", href, title)","\t}",")","","func EnvColorDisabled() bool {","\treturn os.Getenv(\"NO_COLOR\") != \"\" || os.Getenv(\"CLICOLOR\") == \"0\"","}","","func EnvColorForced() bool {","\treturn os.Getenv(\"CLICOLOR_FORCE\") != \"\" \u0026\u0026 os.Getenv(\"CLICOLOR_FORCE\") != \"0\"","}","","func Is256ColorSupported() bool {","\tterm := os.Getenv(\"TERM\")","\tcolorterm := os.Getenv(\"COLORTERM\")","","\treturn strings.Contains(term, \"256\") ||","\t\tstrings.Contains(term, \"24bit\") ||","\t\tstrings.Contains(term, \"truecolor\") ||","\t\tstrings.Contains(colorterm, \"256\") ||","\t\tstrings.Contains(colorterm, \"24bit\") ||","\t\tstrings.Contains(colorterm, \"truecolor\")","}","","func NewColorScheme(enabled, is256enabled bool) *ColorScheme {","\treturn \u0026ColorScheme{","\t\tenabled:      enabled,","\t\tis256enabled: is256enabled,","\t}","}","","type ColorScheme struct {","\tenabled      bool","\tis256enabled bool","}","","func (c *ColorScheme) ColorStatus(status string) string {","\tswitch strings.ToLower(status) {","\tcase \"succeeded\":","\t\treturn c.Green(status)","\tcase \"failed\":","\t\treturn c.Red(status)","\tcase \"pipelineruntimeout\":","\t\treturn c.Yellow(\"Timeout\")","\tcase \"norun\":","\t\treturn c.Dimmed(status)","\tcase \"running\":","\t\treturn c.Blue(status)","\t}","\treturn status","}","","func (c *ColorScheme) Orange(t string) string {","\tif !c.enabled {","\t\treturn t","\t}","\treturn orangeBold(t)","}","","func (c *ColorScheme) Bold(t string) string {","\tif !c.enabled {","\t\treturn t","\t}","\treturn bold(t)","}","","func (c *ColorScheme) Dimmed(t string) string {","\tif !c.enabled {","\t\treturn t","\t}","\treturn dimmed(t)","}","","func (c *ColorScheme) Boldf(t string, args ...any) string {","\treturn c.Bold(fmt.Sprintf(t, args...))","}","","func (c *ColorScheme) Red(t string) string {","\tif !c.enabled {","\t\treturn t","\t}","\treturn red(t)","}","","func (c *ColorScheme) RedBold(t string) string {","\tif !c.enabled {","\t\treturn t","\t}","\treturn redBold(t)","}","","func (c *ColorScheme) Bullet() string {","\tif !c.enabled {","\t\treturn \"\"","\t}","","\treturn \"âˆ™ \"","}","","func (c *ColorScheme) BulletSpace() string {","\tif !c.enabled {","\t\treturn \"\"","\t}","","\treturn \"  \"","}","","func (c *ColorScheme) Redf(t string, args ...any) string {","\treturn c.Red(fmt.Sprintf(t, args...))","}","","func (c *ColorScheme) Yellow(t string) string {","\tif !c.enabled {","\t\treturn t","\t}","\treturn yellow(t)","}","","func (c *ColorScheme) Yellowf(t string, args ...any) string {","\treturn c.Yellow(fmt.Sprintf(t, args...))","}","","func (c *ColorScheme) Green(t string) string {","\tif !c.enabled {","\t\treturn t","\t}","\treturn green(t)","}","","func (c *ColorScheme) Underline(t string) string {","\tif !c.enabled {","\t\treturn t","\t}","\treturn underline(t)","}","","func (c *ColorScheme) Greenf(t string, args ...any) string {","\treturn c.Green(fmt.Sprintf(t, args...))","}","","func (c *ColorScheme) Gray(t string) string {","\tif !c.enabled {","\t\treturn t","\t}","\tif c.is256enabled {","\t\treturn gray256(t)","\t}","\treturn gray(t)","}","","func (c *ColorScheme) Grayf(t string, args ...any) string {","\treturn c.Gray(fmt.Sprintf(t, args...))","}","","func (c *ColorScheme) Magenta(t string) string {","\tif !c.enabled {","\t\treturn t","\t}","\treturn magenta(t)","}","","func (c *ColorScheme) Magentaf(t string, args ...any) string {","\treturn c.Magenta(fmt.Sprintf(t, args...))","}","","func (c *ColorScheme) Cyan(t string) string {","\tif !c.enabled {","\t\treturn t","\t}","\treturn cyan(t)","}","","func (c *ColorScheme) Cyanf(t string, args ...any) string {","\treturn c.Cyan(fmt.Sprintf(t, args...))","}","","func (c *ColorScheme) CyanBold(t string) string {","\tif !c.enabled {","\t\treturn t","\t}","\treturn cyanBold(t)","}","","func (c *ColorScheme) Blue(t string) string {","\tif !c.enabled {","\t\treturn t","\t}","\treturn blue(t)","}","","func (c *ColorScheme) BlueBold(t string) string {","\tif !c.enabled {","\t\treturn t","\t}","\treturn blueBold(t)","}","","func (c *ColorScheme) Bluef(t string, args ...any) string {","\treturn c.Blue(fmt.Sprintf(t, args...))","}","","func (c *ColorScheme) SuccessIcon() string {","\treturn c.SuccessIconWithColor(c.Green)","}","","func (c *ColorScheme) InfoIcon() string {","\treturn c.BlueBold(\"â„¹\")","}","","func (c *ColorScheme) SuccessIconWithColor(colo func(string) string) string {","\treturn colo(\"âœ“\")","}","","func (c *ColorScheme) WarningIcon() string {","\treturn c.Yellow(\"!\")","}","","func (c *ColorScheme) FailureIcon() string {","\treturn c.FailureIconWithColor(c.Red)","}","","func (c *ColorScheme) FailureIconWithColor(colo func(string) string) string {","\treturn colo(\"X\")","}","","func (c *ColorScheme) ColorFromString(s string) func(string) string {","\ts = strings.ToLower(s)","\tvar fn func(string) string","\tswitch s {","\tcase \"bold\":","\t\tfn = c.Bold","\tcase \"red\":","\t\tfn = c.Red","\tcase \"yellow\":","\t\tfn = c.Yellow","\tcase \"green\":","\t\tfn = c.Green","\tcase \"gray\":","\t\tfn = c.Gray","\tcase \"magenta\":","\t\tfn = c.Magenta","\tcase \"cyan\":","\t\tfn = c.Cyan","\tcase \"blue\":","\t\tfn = c.Blue","\tdefault:","\t\tfn = func(s string) string {","\t\t\treturn s","\t\t}","\t}","","\treturn fn","}","","func (c *ColorScheme) GreenBold(s string) string {","\tif !c.enabled {","\t\treturn s","\t}","\treturn greenBold(s)","}","","func (c *ColorScheme) HyperLink(title, href string) string {","\tif !c.enabled {","\t\treturn title","\t}","\treturn hyperLink(title, href)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,1,1,1,0,0,2,2,2,0,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,0,1,0,0,1,1,1,1,1,0,0,2,2,1,1,2,0,0,2,2,1,1,2,0,0,1,1,1,0,2,2,1,1,2,0,0,1,1,1,1,1,0,0,1,1,1,1,0,1,0,0,1,1,1,1,0,1,0,0,1,1,1,0,2,2,1,1,2,0,0,1,1,1,0,2,2,1,1,2,0,0,2,2,1,1,2,0,0,1,1,1,0,2,2,1,1,2,2,2,1,0,0,1,1,1,0,2,2,1,1,2,0,0,1,1,1,0,2,2,1,1,2,0,0,1,1,1,0,1,1,1,1,1,0,0,2,2,1,1,2,0,0,1,1,1,1,1,0,0,1,1,1,0,1,1,1,0,1,1,1,0,1,1,1,0,1,1,1,0,1,1,1,0,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0]},{"id":16,"path":"pkg/cli/info/configmap.go","lines":["package info","","import (","\t\"context\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"",")","","const infoConfigMap = \"pipelines-as-code-info\"","","type Options struct {","\tTargetNamespace string","\tControllerURL   string","\tProvider        string","}","","func IsGithubAppInstalled(ctx context.Context, run *params.Run, targetNamespace string) bool {","\tif _, err := run.Clients.Kube.CoreV1().Secrets(targetNamespace).Get(ctx, info.DefaultPipelinesAscodeSecretName, metav1.GetOptions{}); err != nil {","\t\treturn false","\t}","\treturn true","}","","func GetPACInfo(ctx context.Context, run *params.Run, targetNamespace string) (*Options, error) {","\tcm, err := run.Clients.Kube.CoreV1().ConfigMaps(targetNamespace).Get(ctx, infoConfigMap, metav1.GetOptions{})","\tif err != nil {","\t\treturn nil, err","\t}","","\treturn \u0026Options{","\t\tControllerURL: cm.Data[\"controller-url\"],","\t\tProvider:      cm.Data[\"provider\"],","\t}, nil","}","","func UpdateInfoConfigMap(ctx context.Context, run *params.Run, opts *Options) error {","\tcm, err := run.Clients.Kube.CoreV1().ConfigMaps(opts.TargetNamespace).Get(ctx, infoConfigMap, metav1.GetOptions{})","\tif err != nil {","\t\treturn err","\t}","","\tcm.Data[\"controller-url\"] = opts.ControllerURL","\tcm.Data[\"provider\"] = opts.Provider","","\t// the user will have read access to configmap","\t// but it might be the case, user is not admin and don't have access to update","\t// so don't error out, continue with printing a warning","\t_, err = run.Clients.Kube.CoreV1().ConfigMaps(opts.TargetNamespace).Update(ctx, cm, metav1.UpdateOptions{})","\tif err != nil {","\t\trun.Clients.Log.Warnf(\"failed to update pipelines-as-code-info configmap: %v\", err)","\t\treturn nil","\t}","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,0,2,2,2,2,2,0,2,2,2,2,0,0,2,2,2,2,2,0,2,2,2,2,2,2,2,2,1,1,1,2,0]},{"id":17,"path":"pkg/cli/iostreams.go","lines":["package cli","","import (","\t\"bytes\"","\t\"fmt\"","\t\"io\"","\t\"os\"","\t\"runtime\"","","\tsurveyCore \"github.com/AlecAivazis/survey/v2/core\"","\t\"github.com/mattn/go-colorable\"","\t\"github.com/mattn/go-isatty\"","\t\"github.com/mgutz/ansi\"",")","","type IOStreams struct {","\tIn     io.ReadCloser","\tOut    io.Writer","\tErrOut io.Writer","","\tcolorEnabled             bool","\tprogressIndicatorEnabled bool","\tstdoutTTYOverride        bool","\tstderrTTYOverride        bool","\tstderrIsTTY              bool","\tstdoutIsTTY              bool","\tis256enabled             bool","}","","func (s *IOStreams) ColorScheme() *ColorScheme {","\treturn NewColorScheme(s.ColorEnabled(), s.ColorSupport256())","}","","func (s *IOStreams) ColorEnabled() bool {","\treturn s.colorEnabled","}","","func (s *IOStreams) SetColorEnabled(colorEnabled bool) {","\ts.colorEnabled = colorEnabled","\ts.setSurveyColor()","\ts.progressIndicatorEnabled = colorEnabled","}","","func (s *IOStreams) setSurveyColor() {","\tif !s.colorEnabled {","\t\tsurveyCore.DisableColor = true","\t} else {","\t\t// override survey's poor choice of color","\t\tsurveyCore.TemplateFuncsWithColor[\"color\"] = func(style string) string {","\t\t\tswitch style {","\t\t\tcase \"white\":","\t\t\t\tif s.ColorSupport256() {","\t\t\t\t\treturn fmt.Sprintf(\"\\x1b[%d;5;%dm\", 38, 242)","\t\t\t\t}","\t\t\t\treturn ansi.ColorCode(\"default\")","\t\t\tdefault:","\t\t\t\treturn ansi.ColorCode(style)","\t\t\t}","\t\t}","\t}","}","","func (s *IOStreams) ColorSupport256() bool {","\treturn s.is256enabled","}","","func NewIOStreams() *IOStreams {","\tstdoutIsTTY := isTerminal(os.Stdout)","\tstderrIsTTY := isTerminal(os.Stderr)","","\tios := \u0026IOStreams{","\t\tIn:           os.Stdin,","\t\tOut:          colorable.NewColorable(os.Stdout),","\t\tErrOut:       colorable.NewColorable(os.Stderr),","\t\tcolorEnabled: EnvColorForced() || (!EnvColorDisabled() \u0026\u0026 stdoutIsTTY),","\t\tis256enabled: Is256ColorSupported(),","\t}","","\t// the colours are not working on windows, let's disable it","\tif runtime.GOOS == \"windows\" {","\t\tios.colorEnabled = false","\t}","","\tif stdoutIsTTY \u0026\u0026 stderrIsTTY {","\t\tios.progressIndicatorEnabled = true","\t}","","\tios.setSurveyColor()","","\t// prevent duplicate isTerminal queries now that we know the answer","\tios.SetStdoutTTY(stdoutIsTTY)","\tios.SetStderrTTY(stderrIsTTY)","\treturn ios","}","","func (s *IOStreams) SetStdoutTTY(isTTY bool) {","\ts.stdoutTTYOverride = true","\ts.stdoutIsTTY = isTTY","}","","func (s *IOStreams) SetStderrTTY(isTTY bool) {","\ts.stderrTTYOverride = true","\ts.stderrIsTTY = isTTY","}","","func (s *IOStreams) IsStdoutTTY() bool {","\tif s.stdoutTTYOverride {","\t\treturn s.stdoutIsTTY","\t}","\tif stdout, ok := s.Out.(*os.File); ok {","\t\treturn isTerminal(stdout)","\t}","\treturn false","}","","func isTerminal(f *os.File) bool {","\treturn isatty.IsTerminal(f.Fd()) || isatty.IsCygwinTerminal(f.Fd())","}","","func IOTest() (*IOStreams, *bytes.Buffer, *bytes.Buffer, *bytes.Buffer) {","\tin := \u0026bytes.Buffer{}","\tout := \u0026bytes.Buffer{}","\terrOut := \u0026bytes.Buffer{}","\treturn \u0026IOStreams{","\t\tIn:     io.NopCloser(in),","\t\tOut:    out,","\t\tErrOut: errOut,","\t}, in, out, errOut","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,2,2,2,0,2,2,2,2,2,0,2,2,2,2,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,1,1,0,2,2,2,2,2,2,0,0,2,2,2,2,0,2,2,2,2,0,2,2,2,2,2,2,2,1,0,0,2,2,2,0,2,2,2,2,2,2,2,2,2,2]},{"id":18,"path":"pkg/cli/prompt/select_repo.go","lines":["package prompt","","import (","\t\"context\"","\t\"fmt\"","\t\"strings\"","","\t\"github.com/AlecAivazis/survey/v2\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/formatting\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"",")","","func SelectRepo(ctx context.Context, cs *params.Run, namespace string) (*v1alpha1.Repository, error) {","\trepositories, err := cs.Clients.PipelineAsCode.PipelinesascodeV1alpha1().Repositories(namespace).List(ctx, metav1.ListOptions{})","\tif err != nil {","\t\treturn nil, err","\t}","","\tif len(repositories.Items) == 0 {","\t\treturn nil, fmt.Errorf(\"no repo found\")","\t}","\tif len(repositories.Items) == 1 {","\t\treturn \u0026repositories.Items[0], nil","\t}","","\tallRepositories := []string{}","\tfor _, repository := range repositories.Items {","\t\trepoOwner, err := formatting.GetRepoOwnerFromURL(repository.Spec.URL)","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\tallRepositories = append(allRepositories,","\t\t\tfmt.Sprintf(\"%s - %s\",","\t\t\t\trepository.GetName(),","\t\t\t\trepoOwner))","\t}","","\tvar replyString string","\tif err := SurveyAskOne(\u0026survey.Select{","\t\tMessage: \"Select a repository\",","\t\tOptions: allRepositories,","\t}, \u0026replyString); err != nil {","\t\treturn nil, err","\t}","","\tif replyString == \"\" {","\t\treturn nil, fmt.Errorf(\"you need to choose a repository\")","\t}","\treplyName := strings.Fields(replyString)[0]","","\tfor _, repository := range repositories.Items {","\t\tif repository.GetName() == replyName {","\t\t\treturn \u0026repository, nil","\t\t}","\t}","","\treturn nil, fmt.Errorf(\"cannot match repository\")","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,1,1,0,2,2,2,2,2,2,0,2,2,2,2,1,1,2,2,2,2,0,0,2,2,2,2,2,1,1,0,2,1,1,2,2,2,2,2,2,0,0,1,0]},{"id":19,"path":"pkg/cli/prompt/stubber.go","lines":["package prompt","","import (","\t\"fmt\"","\t\"reflect\"","","\t\"github.com/AlecAivazis/survey/v2\"","\t\"github.com/AlecAivazis/survey/v2/core\"",")","","type AskStubber struct {","\tAsks     [][]*survey.Question","\tAskOnes  []*survey.Prompt","\tCount    int","\tOneCount int","\tStubs    [][]*QuestionStub","\tStubOnes []*StubPrompt","}","","func InitAskStubber() (*AskStubber, func()) {","\torigSurveyAsk := SurveyAsk","\torigSurveyAskOne := SurveyAskOne","\tas := AskStubber{}","","\tSurveyAskOne = func(p survey.Prompt, response any, _ ...survey.AskOpt) error {","\t\tas.AskOnes = append(as.AskOnes, \u0026p)","\t\tcount := as.OneCount","\t\tas.OneCount++","\t\tif count \u003e= len(as.StubOnes) {","\t\t\tpanic(fmt.Sprintf(\"more asks than stubs. most recent call: %v\", p))","\t\t}","\t\tstubbedPrompt := as.StubOnes[count]","\t\tif stubbedPrompt.Default {","\t\t\t// TODO this is failing for basic AskOne invocations with a string result.","\t\t\tdefaultValue := reflect.ValueOf(p).Elem().FieldByName(\"Default\")","\t\t\t_ = core.WriteAnswer(response, \"\", defaultValue)","\t\t} else {","\t\t\t_ = core.WriteAnswer(response, \"\", stubbedPrompt.Value)","\t\t}","","\t\treturn nil","\t}","","\tteardown := func() {","\t\tSurveyAsk = origSurveyAsk","\t\tSurveyAskOne = origSurveyAskOne","\t}","\treturn \u0026as, teardown","}","","type StubPrompt struct {","\tValue   any","\tDefault bool","}","","type QuestionStub struct {","\tName    string","\tValue   any","\tDefault bool","}","","func (as *AskStubber) StubOne(value any) {","\tas.StubOnes = append(as.StubOnes, \u0026StubPrompt{","\t\tValue: value,","\t})","}","","func (as *AskStubber) StubOneDefault() {","\tas.StubOnes = append(as.StubOnes, \u0026StubPrompt{","\t\tDefault: true,","\t})","}","","func (as *AskStubber) Stub(stubbedQuestions []*QuestionStub) {","\t// A call to .Ask takes a list of questions; a stub is then a list of questions in the same order.","\tas.Stubs = append(as.Stubs, stubbedQuestions)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,1,0,2,2,1,1,1,2,2,2,0,2,0,0,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,1,1,1,1,1,0,1,1,1,1]},{"id":20,"path":"pkg/cli/status/status.go","lines":["package status","","import (","\t\"context\"","\t\"regexp\"","","\t\"github.com/google/go-github/v81/github\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\tpacv1alpha1 \"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction\"","\tkstatus \"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction/status\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\tsortrepostatus \"github.com/openshift-pipelines/pipelines-as-code/pkg/sort\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"",")","","// snatched from prow","// https://github.com/kubernetes/test-infra/blob/3c8cbed65c421670a7d37239b8ffceb91e0eb16b/prow/spyglass/lenses/buildlog/lens.go#L95","var (","\tErorrRE                                       = regexp.MustCompile(`timed out|ERROR:|(FAIL|Failure \\[)\\b|panic\\b|^E\\d{4} \\d\\d:\\d\\d:\\d\\d\\.\\d\\d\\d]`)","\tdefaultNumLinesOfLogsInContainersToGrabForErr = int64(10)",")","","// RepositoryRunStatusRemoveSameSHA remove an existing status with the same","// SHA. This would come from repo pipelinerun_status. We don't want the doublons","// and we rather use the ones from the live PR on cluster.","func RepositoryRunStatusRemoveSameSHA(rs []pacv1alpha1.RepositoryRunStatus, livePrSHA string) []pacv1alpha1.RepositoryRunStatus {","\tnewRepositoryStatus := []pacv1alpha1.RepositoryRunStatus{}","\tfor _, value := range rs {","\t\tif value.SHA != nil \u0026\u0026 *value.SHA == livePrSHA {","\t\t\tcontinue","\t\t}","\t\tnewRepositoryStatus = append(newRepositoryStatus, value)","\t}","\treturn newRepositoryStatus","}","","func convertPrStatusToRepositoryStatus(ctx context.Context, cs *params.Run, pr tektonv1.PipelineRun, logurl string) pacv1alpha1.RepositoryRunStatus {","\tkinteract, _ := kubeinteraction.NewKubernetesInteraction(cs)","\tfailurereasons := kstatus.CollectFailedTasksLogSnippet(ctx, cs, kinteract, \u0026pr, defaultNumLinesOfLogsInContainersToGrabForErr)","\tprSHA := pr.GetAnnotations()[keys.SHA]","\treturn pacv1alpha1.RepositoryRunStatus{","\t\tStatus:             pr.Status.Status,","\t\tLogURL:             \u0026logurl,","\t\tPipelineRunName:    pr.GetName(),","\t\tCollectedTaskInfos: \u0026failurereasons,","\t\tStartTime:          pr.Status.StartTime,","\t\tSHA:                github.Ptr(prSHA),","\t\tSHAURL:             github.Ptr(pr.GetAnnotations()[keys.ShaURL]),","\t\tTitle:              github.Ptr(pr.GetAnnotations()[keys.ShaTitle]),","\t\tTargetBranch:       github.Ptr(pr.GetAnnotations()[keys.Branch]),","\t\tEventType:          github.Ptr(pr.GetAnnotations()[keys.EventType]),","\t}","}","","func MixLivePRandRepoStatus(ctx context.Context, cs *params.Run, repository pacv1alpha1.Repository) []pacv1alpha1.RepositoryRunStatus {","\trepositorystatus := repository.Status","\tlabel := keys.Repository + \"=\" + repository.Name","\tprs, err := cs.Clients.Tekton.TektonV1().PipelineRuns(repository.Namespace).List(ctx, metav1.ListOptions{","\t\tLabelSelector: label,","\t})","\tif err != nil {","\t\treturn sortrepostatus.RepositorySortRunStatus(repositorystatus)","\t}","","\tfor i := range prs.Items {","\t\tpr := prs.Items[i]","\t\trepositorystatus = RepositoryRunStatusRemoveSameSHA(repositorystatus, pr.GetAnnotations()[keys.SHA])","\t\tlogurl := cs.Clients.ConsoleUI().DetailURL(\u0026pr)","\t\trepositorystatus = append(repositorystatus, convertPrStatusToRepositoryStatus(ctx, cs, pr, logurl))","\t}","\treturn sortrepostatus.RepositorySortRunStatus(repositorystatus)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,2,0,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,2,0]},{"id":21,"path":"pkg/cli/webhook/bitbucket_cloud.go","lines":["package webhook","","import (","\t\"context\"","\t\"fmt\"","\t\"net/url\"","\t\"strings\"","","\t\"github.com/AlecAivazis/survey/v2\"","\t\"github.com/ktrysmt/go-bitbucket\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/cli\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/cli/prompt\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/formatting\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/bitbucketcloud\"",")","","type bitbucketCloudConfig struct {","\tClient              *bitbucket.Client","\tIOStream            *cli.IOStreams","\tcontrollerURL       string","\trepoOwner           string","\trepoName            string","\tpersonalAccessToken string","\tusername            string","\tAPIURL              string","}","","func (bb *bitbucketCloudConfig) Run(_ context.Context, opts *Options) (*response, error) {","\terr := bb.askBBWebhookConfig(opts.RepositoryURL, opts.ControllerURL, opts.ProviderAPIURL, opts.PersonalAccessToken)","\tif err != nil {","\t\treturn nil, err","\t}","","\treturn \u0026response{","\t\tControllerURL:       bb.controllerURL,","\t\tPersonalAccessToken: bb.personalAccessToken,","\t\tWebhookSecret:       \"\",","\t\tAPIURL:              bb.APIURL,","\t\tUserName:            bb.username,","\t}, bb.create()","}","","func (bb *bitbucketCloudConfig) askBBWebhookConfig(repositoryURL, controllerURL, apiURL, personalAccessToken string) error {","\tif repositoryURL == \"\" {","\t\tmsg := \"Please enter the git repository url you want to be configured: \"","\t\tif err := prompt.SurveyAskOne(\u0026survey.Input{Message: msg}, \u0026repositoryURL,","\t\t\tsurvey.WithValidator(survey.Required)); err != nil {","\t\t\treturn err","\t\t}","\t} else {","\t\tfmt.Fprintf(bb.IOStream.Out, \"âœ“ Setting up Bitbucket Webhook for Repository %s\\n\", repositoryURL)","\t}","","\tdefaultRepo, err := formatting.GetRepoOwnerFromURL(repositoryURL)","\tif err != nil {","\t\treturn err","\t}","","\trepoArr := strings.Split(defaultRepo, \"/\")","\tif len(repoArr) != 2 {","\t\treturn fmt.Errorf(\"invalid repository, needs to be of format 'org-name/repo-name'\")","\t}","\tbb.repoOwner = repoArr[0]","\tbb.repoName = repoArr[1]","","\tif err := prompt.SurveyAskOne(\u0026survey.Input{","\t\tMessage: \"Please enter your bitbucket cloud username: \",","\t}, \u0026bb.username, survey.WithValidator(survey.Required)); err != nil {","\t\treturn err","\t}","","\tif personalAccessToken == \"\" {","\t\tfmt.Fprintln(bb.IOStream.Out, \"â„¹ ï¸You now need to create a Bitbucket Cloud app password, please checkout the docs at https://is.gd/fqMHiJ for the required permissions\")","\t\tif err := prompt.SurveyAskOne(\u0026survey.Password{","\t\t\tMessage: \"Please enter the Bitbucket Cloud app password: \",","\t\t}, \u0026bb.personalAccessToken, survey.WithValidator(survey.Required)); err != nil {","\t\t\treturn err","\t\t}","\t} else {","\t\tbb.personalAccessToken = personalAccessToken","\t}","","\tbb.controllerURL = controllerURL","","\t// confirm whether to use the detected url","\tif bb.controllerURL != \"\" {","\t\tvar answer bool","\t\tfmt.Fprintf(bb.IOStream.Out, \"ðŸ‘€ I have detected a controller url: %s\\n\", bb.controllerURL)","\t\terr := prompt.SurveyAskOne(\u0026survey.Confirm{","\t\t\tMessage: \"Do you want me to use it?\",","\t\t\tDefault: true,","\t\t}, \u0026answer)","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\tif !answer {","\t\t\tbb.controllerURL = \"\"","\t\t}","\t}","","\tif bb.controllerURL == \"\" {","\t\tif err := prompt.SurveyAskOne(\u0026survey.Input{","\t\t\tMessage: \"Please enter your controller public route URL: \",","\t\t}, \u0026bb.controllerURL, survey.WithValidator(survey.Required)); err != nil {","\t\t\treturn err","\t\t}","\t}","","\tif apiURL == \"\" \u0026\u0026 !strings.HasPrefix(repositoryURL, \"https://bitbucket.org\") {","\t\tif err := prompt.SurveyAskOne(\u0026survey.Input{","\t\t\tMessage: \"Please enter your Bitbucket enterprise API URL:: \",","\t\t}, \u0026bb.APIURL, survey.WithValidator(survey.Required)); err != nil {","\t\t\treturn err","\t\t}","\t} else {","\t\tbb.APIURL = apiURL","\t}","","\treturn nil","}","","func (bb *bitbucketCloudConfig) create() error {","\tif bb.Client == nil {","\t\tvar err error","\t\tbb.Client, err = bitbucket.NewBasicAuth(bb.repoOwner, bb.personalAccessToken)","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t}","\tif bb.APIURL != \"\" {","\t\tparsedURL, err := url.Parse(bb.APIURL)","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\tbb.Client.SetApiBaseURL(*parsedURL)","\t}","","\topts := \u0026bitbucket.WebhooksOptions{","\t\tOwner:    bb.repoOwner,","\t\tRepoSlug: bb.repoName,","\t\tUrl:      bb.controllerURL,","\t\tActive:   true,","\t\tEvents:   bitbucketcloud.PullRequestAllEvents,","\t}","\t_, err := bb.Client.Repositories.Webhooks.Create(opts)","\tif err != nil {","\t\treturn err","\t}","","\tfmt.Fprintf(bb.IOStream.Out, \"âœ“ Webhook has been created on repository %v/%v\\n\", bb.repoOwner, bb.repoName)","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,1,1,1,1,1,1,1,0,0,2,2,2,2,2,1,1,2,2,2,0,2,2,2,2,0,2,2,1,1,2,2,2,2,2,2,1,1,0,2,2,2,2,2,1,1,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,0,0,2,2,2,2,1,1,0,0,2,1,1,1,1,1,2,2,2,0,2,0,0,2,2,1,1,1,1,1,0,2,2,2,1,1,2,0,0,2,2,2,2,2,2,2,2,2,2,2,0,2,2,0]},{"id":22,"path":"pkg/cli/webhook/github.go","lines":["package webhook","","import (","\t\"context\"","\t\"fmt\"","\t\"io\"","\t\"net/http\"","\t\"strings\"","","\t\"github.com/AlecAivazis/survey/v2\"","\t\"github.com/google/go-github/v81/github\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/cli\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/cli/prompt\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/formatting\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/random\"","\t\"golang.org/x/oauth2\"",")","","type gitHubConfig struct {","\tClient              *github.Client","\tIOStream            *cli.IOStreams","\tcontrollerURL       string","\trepoOwner           string","\trepoName            string","\twebhookSecret       string","\tpersonalAccessToken string","\tAPIURL              string","}","","func (gh *gitHubConfig) Run(ctx context.Context, opts *Options) (*response, error) {","\terr := gh.askGHWebhookConfig(opts.RepositoryURL, opts.ControllerURL, opts.ProviderAPIURL, opts.PersonalAccessToken)","\tif err != nil {","\t\treturn nil, err","\t}","","\treturn \u0026response{","\t\tControllerURL:       gh.controllerURL,","\t\tPersonalAccessToken: gh.personalAccessToken,","\t\tWebhookSecret:       gh.webhookSecret,","\t\tAPIURL:              gh.APIURL,","\t}, gh.create(ctx)","}","","func (gh *gitHubConfig) askGHWebhookConfig(repoURL, controllerURL, apiURL, personalAccessToken string) error {","\tif repoURL == \"\" {","\t\tmsg := \"Please enter the git repository url you want to be configured: \"","\t\tif err := prompt.SurveyAskOne(\u0026survey.Input{Message: msg}, \u0026repoURL,","\t\t\tsurvey.WithValidator(survey.Required)); err != nil {","\t\t\treturn err","\t\t}","\t} else {","\t\tfmt.Fprintf(gh.IOStream.Out, \"âœ“ Setting up GitHub Webhook for Repository %s\\n\", repoURL)","\t}","","\tdefaultRepo, err := formatting.GetRepoOwnerFromURL(repoURL)","\tif err != nil {","\t\treturn err","\t}","","\tdefaultRepo = strings.TrimSuffix(defaultRepo, \"/\")","\trepoArr := strings.Split(defaultRepo, \"/\")","\tif len(repoArr) != 2 {","\t\treturn fmt.Errorf(\"invalid repository, needs to be of format 'org-name/repo-name'\")","\t}","","\tgh.repoOwner = repoArr[0]","\tgh.repoName = repoArr[1]","","\t// set controller url","\tgh.controllerURL = controllerURL","","\t// confirm whether to use the detected url","\tif gh.controllerURL != \"\" {","\t\tvar answer bool","\t\tfmt.Fprintf(gh.IOStream.Out, \"ðŸ‘€ I have detected a controller url: %s\\n\", gh.controllerURL)","\t\terr := prompt.SurveyAskOne(\u0026survey.Confirm{","\t\t\tMessage: \"Do you want me to use it?\",","\t\t\tDefault: true,","\t\t}, \u0026answer)","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\tif !answer {","\t\t\tgh.controllerURL = \"\"","\t\t}","\t}","","\tif gh.controllerURL == \"\" {","\t\tif err := prompt.SurveyAskOne(\u0026survey.Input{","\t\t\tMessage: \"Please enter your controller public route URL: \",","\t\t}, \u0026gh.controllerURL, survey.WithValidator(survey.Required)); err != nil {","\t\t\treturn err","\t\t}","\t}","","\tdata := random.AlphaString(12)","\tmsg := fmt.Sprintf(\"Please enter the secret to configure the webhook for payload validation (default: %s): \", data)","\tvar webhookSecret string","\tif err := prompt.SurveyAskOne(\u0026survey.Input{Message: msg, Default: data}, \u0026webhookSecret); err != nil {","\t\treturn err","\t}","","\tgh.webhookSecret = webhookSecret","","\tif personalAccessToken == \"\" {","\t\tfmt.Fprintln(gh.IOStream.Out, \"â„¹ ï¸You now need to create a GitHub personal access token, please checkout the docs at https://is.gd/KJ1dDH for the required scopes\")","\t\tif err := prompt.SurveyAskOne(\u0026survey.Password{","\t\t\tMessage: \"Please enter the GitHub access token: \",","\t\t}, \u0026gh.personalAccessToken, survey.WithValidator(survey.Required)); err != nil {","\t\t\treturn err","\t\t}","\t} else {","\t\tgh.personalAccessToken = personalAccessToken","\t}","","\tif apiURL == \"\" \u0026\u0026 !strings.HasPrefix(repoURL, \"https://github.com\") {","\t\tif err := prompt.SurveyAskOne(\u0026survey.Input{","\t\t\tMessage: \"Please enter your GitHub enterprise API URL: \",","\t\t}, \u0026gh.APIURL, survey.WithValidator(survey.Required)); err != nil {","\t\t\treturn err","\t\t}","\t} else {","\t\tgh.APIURL = apiURL","\t}","","\treturn nil","}","","func (gh *gitHubConfig) create(ctx context.Context) error {","\thook := \u0026github.Hook{","\t\tName:   github.Ptr(\"web\"),","\t\tActive: github.Ptr(true),","\t\tEvents: []string{","\t\t\t\"issue_comment\",","\t\t\ttriggertype.PullRequest.String(),","\t\t\t\"push\",","\t\t},","\t\tConfig: \u0026github.HookConfig{","\t\t\tURL:         github.Ptr(gh.controllerURL),","\t\t\tContentType: github.Ptr(\"json\"),","\t\t\tInsecureSSL: github.Ptr(\"0\"),","\t\t\tSecret:      github.Ptr(gh.webhookSecret),","\t\t},","\t}","","\tghClient, err := gh.newGHClientByToken(ctx)","\tif err != nil {","\t\treturn err","\t}","","\t_, res, err := ghClient.Repositories.CreateHook(ctx, gh.repoOwner, gh.repoName, hook)","\tif err != nil {","\t\treturn err","\t}","","\tif res.StatusCode != http.StatusCreated {","\t\tpayload, err := io.ReadAll(res.Body)","\t\tif err != nil {","\t\t\treturn fmt.Errorf(\"failed to read response body: %w\", err)","\t\t}","","\t\treturn fmt.Errorf(\"failed to create webhook on repository %v/%v, status code: %v, error : %v\",","\t\t\tgh.repoOwner, gh.repoName, res.StatusCode, payload)","\t}","","\tfmt.Fprintf(gh.IOStream.Out, \"âœ“ Webhook has been created on repository %v/%v\\n\", gh.repoOwner, gh.repoName)","\treturn nil","}","","func (gh *gitHubConfig) newGHClientByToken(ctx context.Context) (*github.Client, error) {","\tif gh.Client != nil {","\t\treturn gh.Client, nil","\t}","\tts := oauth2.StaticTokenSource(","\t\t\u0026oauth2.Token{AccessToken: gh.personalAccessToken},","\t)","","\tif gh.APIURL == \"\" || gh.APIURL == keys.PublicGithubAPIURL {","\t\treturn github.NewClient(oauth2.NewClient(ctx, ts)), nil","\t}","","\tgprovider, err := github.NewClient(oauth2.NewClient(ctx, ts)).WithEnterpriseURLs(gh.APIURL, gh.APIURL)","\tif err != nil {","\t\treturn nil, err","\t}","\treturn gprovider, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,1,1,1,1,1,1,0,0,2,2,2,2,2,1,1,2,2,2,0,2,2,2,2,0,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,2,1,1,0,0,2,2,2,2,1,1,0,0,2,2,2,2,1,1,0,2,2,2,2,2,2,2,1,1,2,2,2,0,2,1,1,1,1,1,2,2,2,0,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,2,2,2,0,2,1,1,1,1,0,1,1,0,0,2,2,0,0,2,2,2,2,1,1,1,1,1,1,1,0,1,1,1,1,1,0]},{"id":23,"path":"pkg/cli/webhook/gitlab.go","lines":["package webhook","","import (","\t\"context\"","\t\"fmt\"","\t\"io\"","\t\"net/http\"","","\t\"github.com/AlecAivazis/survey/v2\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/cli\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/cli/prompt\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/random\"","\tgitlab \"gitlab.com/gitlab-org/api/client-go\"",")","","type gitLabConfig struct {","\tClient              *gitlab.Client","\tIOStream            *cli.IOStreams","\tcontrollerURL       string","\tprojectID           string","\twebhookSecret       string","\tpersonalAccessToken string","\tAPIURL              string","}","","func (gl *gitLabConfig) Run(_ context.Context, opts *Options) (*response, error) {","\terr := gl.askGLWebhookConfig(opts.RepositoryURL, opts.ControllerURL, opts.ProviderAPIURL, opts.PersonalAccessToken)","\tif err != nil {","\t\treturn nil, err","\t}","","\treturn \u0026response{","\t\tControllerURL:       gl.controllerURL,","\t\tPersonalAccessToken: gl.personalAccessToken,","\t\tWebhookSecret:       gl.webhookSecret,","\t\tAPIURL:              gl.APIURL,","\t}, gl.create()","}","","// Changes in askGLWebhookConfig function.","func (gl *gitLabConfig) askGLWebhookConfig(repoURL, controllerURL, apiURL, personalAccessToken string) error {","\tif repoURL == \"\" {","\t\tmsg := \"Enter the GitLab repository URL to configure: \"","\t\tif err := prompt.SurveyAskOne(\u0026survey.Input{Message: msg}, \u0026repoURL,","\t\t\tsurvey.WithValidator(survey.Required)); err != nil {","\t\t\treturn err","\t\t}","\t} else {","\t\tfmt.Fprintf(gl.IOStream.Out, \"âœ“ Setting up GitLab Webhook for Repository %s\\n\", repoURL)","\t}","","\tmsg := \"Enter the project ID of your GitLab repository.\\nThe project ID is a unique number (e.g. 34405323) shown at the top of your GitLab project page: \"","\tif err := prompt.SurveyAskOne(\u0026survey.Input{Message: msg}, \u0026gl.projectID,","\t\tsurvey.WithValidator(survey.Required)); err != nil {","\t\treturn err","\t}","","\tgl.controllerURL = controllerURL","","\tif gl.controllerURL != \"\" {","\t\tvar answer bool","\t\tfmt.Fprintf(gl.IOStream.Out, \"ðŸ‘€ Controller URL detected: %s\\n\", gl.controllerURL)","\t\terr := prompt.SurveyAskOne(\u0026survey.Confirm{","\t\t\tMessage: \"Do you want me to use it?\",","\t\t\tDefault: true,","\t\t}, \u0026answer)","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\tif !answer {","\t\t\tgl.controllerURL = \"\"","\t\t}","\t}","","\tif gl.controllerURL == \"\" {","\t\tif err := prompt.SurveyAskOne(\u0026survey.Input{","\t\t\tMessage: \"Enter your controller's public route URL: \",","\t\t}, \u0026gl.controllerURL, survey.WithValidator(survey.Required)); err != nil {","\t\t\treturn err","\t\t}","\t}","","\tdata := random.AlphaString(12)","\tmsg = fmt.Sprintf(\"Enter a secret for webhook payload validation (default: %s): \", data)","\tvar webhookSecret string","\tif err := prompt.SurveyAskOne(\u0026survey.Input{Message: msg, Default: data}, \u0026webhookSecret); err != nil {","\t\treturn err","\t}","","\tgl.webhookSecret = webhookSecret","","\tif personalAccessToken == \"\" {","\t\tfmt.Fprintln(gl.IOStream.Out, \"â„¹ ï¸You need to create a GitLab personal access token with 'api' scope\")","\t\tfmt.Fprintln(gl.IOStream.Out, \"â„¹ ï¸Generate one at https://gitlab.com/-/profile/personal_access_tokens (see documentation: https://is.gd/rOEo9B)\")","\t\tif err := prompt.SurveyAskOne(\u0026survey.Password{","\t\t\tMessage: \"Enter your GitLab access token: \",","\t\t}, \u0026gl.personalAccessToken, survey.WithValidator(survey.Required)); err != nil {","\t\t\treturn err","\t\t}","\t} else {","\t\tgl.personalAccessToken = personalAccessToken","\t}","","\tif apiURL == \"\" {","\t\tif err := prompt.SurveyAskOne(\u0026survey.Input{","\t\t\tMessage: \"Enter your GitLab API URL: \",","\t\t}, \u0026gl.APIURL, survey.WithValidator(survey.Required)); err != nil {","\t\t\treturn err","\t\t}","\t} else {","\t\tgl.APIURL = apiURL","\t}","","\treturn nil","}","","// create function.","func (gl *gitLabConfig) create() error {","\tglClient, err := gl.newClient()","\tif err != nil {","\t\treturn err","\t}","","\thookOpts := \u0026gitlab.AddProjectHookOptions{","\t\tEnableSSLVerification: gitlab.Ptr(true),","\t\tMergeRequestsEvents:   gitlab.Ptr(true),","\t\tNoteEvents:            gitlab.Ptr(true),","\t\tPushEvents:            gitlab.Ptr(true),","\t\tTagPushEvents:         gitlab.Ptr(true),","\t\tToken:                 gitlab.Ptr(gl.webhookSecret),","\t\tURL:                   gitlab.Ptr(gl.controllerURL),","\t}","","\t_, resp, err := glClient.Projects.AddProjectHook(gl.projectID, hookOpts)","\tif err != nil {","\t\treturn err","\t}","","\tif resp.StatusCode != http.StatusCreated {","\t\tpayload, err := io.ReadAll(resp.Body)","\t\tif err != nil {","\t\t\treturn fmt.Errorf(\"failed to read response body: %w\", err)","\t\t}","\t\treturn fmt.Errorf(\"failed to create webhook, status code: %v, error : %v\",","\t\t\tresp.StatusCode, payload)","\t}","","\tfmt.Fprintln(gl.IOStream.Out, \"âœ“ Webhook successfully created on your repository\")","\treturn nil","}","","func (gl *gitLabConfig) newClient() (*gitlab.Client, error) {","\tif gl.Client != nil {","\t\treturn gl.Client, nil","\t}","","\tif gl.APIURL == \"\" {","\t\treturn gitlab.NewClient(gl.personalAccessToken)","\t}","\treturn gitlab.NewClient(gl.personalAccessToken, gitlab.WithBaseURL(gl.APIURL))","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,1,1,1,1,1,1,0,0,0,2,2,2,2,2,1,1,2,2,2,0,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,1,1,2,1,1,0,0,2,2,2,2,1,1,0,0,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,1,1,2,2,2,0,2,2,2,2,1,1,2,2,2,0,2,0,0,0,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,1,1,1,1,1,1,0,0,2,2,0,0,2,2,2,2,0,1,1,1,1,0]},{"id":24,"path":"pkg/cli/webhook/secret.go","lines":["package webhook","","import (","\t\"context\"","\t\"fmt\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/pipelineascode\"","\tcorev1 \"k8s.io/api/core/v1\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"",")","","func (w *Options) createWebhookSecret(ctx context.Context, response *response) error {","\t_, err := w.Run.Clients.Kube.CoreV1().Secrets(w.RepositoryNamespace).Create(ctx, \u0026corev1.Secret{","\t\tObjectMeta: metav1.ObjectMeta{","\t\t\tName: w.RepositoryName,","\t\t},","\t\tData: map[string][]byte{","\t\t\tpipelineascode.DefaultGitProviderSecretKey:        []byte(response.PersonalAccessToken),","\t\t\tpipelineascode.DefaultGitProviderWebhookSecretKey: []byte(response.WebhookSecret),","\t\t},","\t}, metav1.CreateOptions{})","\tif err != nil {","\t\treturn err","\t}","","\tfmt.Fprintf(w.IOStreams.Out, \"ðŸ”‘ Webhook Secret %s has been created in the %s namespace.\\n\", w.RepositoryName, w.RepositoryNamespace)","\treturn nil","}","","func (w *Options) updateWebhookSecret(ctx context.Context, response *response) error {","\tsecretInfo, err := w.Run.Clients.Kube.CoreV1().Secrets(w.RepositoryNamespace).Get(ctx, w.SecretName, metav1.GetOptions{})","\tif err != nil {","\t\treturn err","\t}","\tsecretInfo.Data[pipelineascode.DefaultGitProviderWebhookSecretKey] = []byte(response.WebhookSecret)","","\t_, err = w.Run.Clients.Kube.CoreV1().Secrets(w.RepositoryNamespace).Update(ctx, secretInfo, metav1.UpdateOptions{})","\tif err != nil {","\t\treturn err","\t}","","\tfmt.Fprintf(w.IOStreams.Out, \"ðŸ”‘ Secret %s has been updated with webhook secret in the %s namespace.\\n\", w.SecretName, w.RepositoryNamespace)","\treturn nil","}","","func (w *Options) updateRepositoryCR(ctx context.Context, res *response) error {","\trepo, err := w.Run.Clients.PipelineAsCode.PipelinesascodeV1alpha1().Repositories(w.RepositoryNamespace).","\t\tGet(ctx, w.RepositoryName, metav1.GetOptions{})","\tif err != nil {","\t\treturn err","\t}","","\tif repo.Spec.GitProvider == nil {","\t\trepo.Spec.GitProvider = \u0026v1alpha1.GitProvider{}","\t}","","\trepo.Spec.GitProvider.Secret = \u0026v1alpha1.Secret{","\t\tName: w.RepositoryName,","\t\tKey:  pipelineascode.DefaultGitProviderSecretKey,","\t}","\trepo.Spec.GitProvider.WebhookSecret = \u0026v1alpha1.Secret{","\t\tName: w.RepositoryName,","\t\tKey:  pipelineascode.DefaultGitProviderWebhookSecretKey,","\t}","","\tif res.UserName != \"\" {","\t\trepo.Spec.GitProvider.User = res.UserName","\t}","","\tif res.APIURL != \"\" {","\t\trepo.Spec.GitProvider.URL = res.APIURL","\t}","","\t_, err = w.Run.Clients.PipelineAsCode.PipelinesascodeV1alpha1().Repositories(w.RepositoryNamespace).","\t\tUpdate(ctx, repo, metav1.UpdateOptions{})","\tif err != nil {","\t\treturn err","\t}","","\tfmt.Fprintf(w.IOStreams.Out, \"ðŸ”‘ Repository CR %s has been updated with webhook secret in the %s namespace\\n\", w.RepositoryName, w.RepositoryNamespace)","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,2,0,0,2,2,2,1,1,2,2,2,2,1,1,0,2,2,0,0,2,2,2,2,1,1,0,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,0,2,2,2,1,1,0,2,2,0]},{"id":25,"path":"pkg/cli/webhook/webhook.go","lines":["package webhook","","import (","\t\"context\"","\t\"fmt\"","\t\"strings\"","","\t\"github.com/AlecAivazis/survey/v2\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/cli\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/cli/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/cli/prompt\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/cmd/tknpac/bootstrap\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"",")","","type Interface interface {","\tRun(context.Context, *Options) (*response, error)","}","","type Options struct {","\tRun                      *params.Run","\tIOStreams                *cli.IOStreams","\tPACNamespace             string","\tRepositoryURL            string","\tRepositoryName           string","\tRepositoryNamespace      string","\tProviderAPIURL           string","\tControllerURL            string","\tPersonalAccessToken      string","\tRepositoryCreateORUpdate bool","\tSecretName               string","\tProviderSecretKey        string","}","","type response struct {","\tUserName            string","\tControllerURL       string","\tWebhookSecret       string","\tPersonalAccessToken string","\tAPIURL              string","}","","func (w *Options) Install(ctx context.Context, providerType string) error {","\t// figure out pac installation namespace","\tinstalled, installationNS, err := bootstrap.DetectPacInstallation(ctx, w.PACNamespace, w.Run)","\tif !installed {","\t\treturn fmt.Errorf(\"pipelines as code not installed\")","\t}","\tif installed \u0026\u0026 err != nil {","\t\treturn err","\t}","","\t// fetch configmap to get controller url","\tpacInfo, err := info.GetPACInfo(ctx, w.Run, installationNS)","\tif err != nil {","\t\treturn err","\t}","","\t// check if info configmap has url then use that otherwise try to detect","\tif pacInfo.ControllerURL != \"\" \u0026\u0026 w.ControllerURL == \"\" {","\t\tw.ControllerURL = pacInfo.ControllerURL","\t} else {","\t\tw.ControllerURL, _ = bootstrap.DetectOpenShiftRoute(ctx, w.Run, w.PACNamespace)","\t}","","\tif w.RepositoryURL == \"\" {","\t\tq := \"Please enter the Git repository url: \"","\t\tif err := prompt.SurveyAskOne(\u0026survey.Input{Message: q}, \u0026w.RepositoryURL,","\t\t\tsurvey.WithValidator(survey.Required)); err != nil {","\t\t\treturn err","\t\t}","\t}","","\tvar webhookProvider Interface","\tswitch providerType {","\tcase \"github\":","\t\twebhookProvider = \u0026gitHubConfig{IOStream: w.IOStreams}","\tcase \"gitlab\":","\t\twebhookProvider = \u0026gitLabConfig{IOStream: w.IOStreams}","\tcase \"bitbucket-cloud\":","\t\twebhookProvider = \u0026bitbucketCloudConfig{IOStream: w.IOStreams}","\tdefault:","\t\treturn fmt.Errorf(\"invalid webhook provider\")","\t}","","\tresponse, err := webhookProvider.Run(ctx, w)","\tif err != nil {","\t\treturn err","\t}","","\t// RepositoryCreateORUpdate is false for tkn-pac webhook add command","\tif !w.RepositoryCreateORUpdate {","\t\treturn w.updateWebhookSecret(ctx, response)","\t}","","\t// create webhook secret in namespace where repository CR is created","\tif err := w.createWebhookSecret(ctx, response); err != nil {","\t\treturn err","\t}","","\t// update repo cr with webhook secret","\treturn w.updateRepositoryCR(ctx, response)","}","","func GetProviderName(url string) (string, error) {","\tvar (","\t\terr          error","\t\tproviderName string","\t)","\tswitch {","\tcase strings.Contains(url, \"github\"):","\t\tproviderName = \"github\"","\tcase strings.Contains(url, \"gitlab\"):","\t\tproviderName = \"gitlab\"","\tcase strings.Contains(url, \"bitbucket-cloud\"):","\t\tproviderName = \"bitbucket-cloud\"","\tdefault:","\t\tmsg := \"Please select the type of the git platform to setup webhook:\"","\t\tif err = prompt.SurveyAskOne(","\t\t\t\u0026survey.Select{","\t\t\t\tMessage: msg,","\t\t\t\tOptions: []string{\"github\", \"gitlab\", \"bitbucket-cloud\"},","\t\t\t\tDefault: 0,","\t\t\t}, \u0026providerName); err != nil {","\t\t\treturn \"\", err","\t\t}","\t}","\treturn providerName, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,1,1,0,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,0,0,1,1,1,0,0,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0]},{"id":26,"path":"pkg/configutil/config.go","lines":["package configutil","","import (","\t\"fmt\"","\t\"reflect\"","\t\"strconv\"","\t\"strings\"","","\t\"go.uber.org/zap\"",")","","func ValidateAndAssignValues(logger *zap.SugaredLogger, configData map[string]string, configStruct any, customValidations map[string]func(string) error, logUpdates bool) error {","\tstructValue := reflect.ValueOf(configStruct).Elem()","\tstructType := reflect.TypeOf(configStruct).Elem()","","\tvar errors []error","","\tfor i := 0; i \u003c structType.NumField(); i++ {","\t\tfield := structType.Field(i)","\t\tfieldName := field.Name","","\t\tjsonTag := field.Tag.Get(\"json\")","\t\t// Skip field which doesn't have json tag","\t\tif jsonTag == \"\" {","\t\t\tcontinue","\t\t}","","\t\t// Read value from ConfigMap","\t\tfieldValue := configData[strings.ToLower(jsonTag)]","","\t\t// If value is missing in ConfigMap, use default value from struct tag","\t\tif fieldValue == \"\" {","\t\t\tfieldValue = field.Tag.Get(\"default\")","\t\t}","","\t\tfieldValueKind := field.Type.Kind()","","\t\t//nolint","\t\tswitch fieldValueKind {","\t\tcase reflect.String:","\t\t\t// if fieldvalue is empty, skip validation and set the field as empty string","\t\t\tif validator, ok := customValidations[fieldName]; ok \u0026\u0026 fieldValue != \"\" {","\t\t\t\tif err := validator(fieldValue); err != nil {","\t\t\t\t\terrors = append(errors, fmt.Errorf(\"custom validation failed for field %s: %w\", fieldName, err))","\t\t\t\t\tcontinue","\t\t\t\t}","\t\t\t}","\t\t\toldValue := structValue.FieldByName(fieldName).String()","\t\t\tif oldValue != fieldValue \u0026\u0026 logUpdates {","\t\t\t\tlogger.Infof(\"updating value for field %s: from '%s' to '%s'\", fieldName, oldValue, fieldValue)","\t\t\t}","\t\t\tstructValue.FieldByName(fieldName).SetString(fieldValue)","","\t\tcase reflect.Bool:","\t\t\t// if fieldvalue is empty, set the field as false","\t\t\tif fieldValue == \"\" {","\t\t\t\tfieldValue = \"false\"","\t\t\t}","\t\t\tnewValue, err := strconv.ParseBool(fieldValue)","\t\t\tif err != nil {","\t\t\t\terrors = append(errors, fmt.Errorf(\"invalid value for bool field %s: %w\", fieldName, err))","\t\t\t\tcontinue","\t\t\t}","\t\t\toldValue := structValue.FieldByName(fieldName).Bool()","\t\t\tif oldValue != newValue \u0026\u0026 logUpdates {","\t\t\t\tlogger.Infof(\"updating value for field %s: from '%v' to '%v'\", fieldName, oldValue, newValue)","\t\t\t}","\t\t\tstructValue.FieldByName(fieldName).SetBool(newValue)","","\t\tcase reflect.Int:","\t\t\t// if fieldvalue is empty, skip validation and set the field as 0","\t\t\tif validator, ok := customValidations[fieldName]; ok \u0026\u0026 fieldValue != \"\" {","\t\t\t\tif err := validator(fieldValue); err != nil {","\t\t\t\t\terrors = append(errors, fmt.Errorf(\"custom validation failed for field %s: %w\", fieldName, err))","\t\t\t\t\tcontinue","\t\t\t\t}","\t\t\t}","\t\t\tif fieldValue == \"\" {","\t\t\t\tfieldValue = \"0\"","\t\t\t}","\t\t\tnewValue, err := strconv.ParseInt(fieldValue, 10, 64)","\t\t\tif err != nil {","\t\t\t\terrors = append(errors, fmt.Errorf(\"invalid value for int field %s: %w\", fieldName, err))","\t\t\t\tcontinue","\t\t\t}","\t\t\toldValue := structValue.FieldByName(fieldName).Int()","\t\t\tif oldValue != newValue \u0026\u0026 logUpdates {","\t\t\t\tlogger.Infof(\"updating value for field %s: from '%d' to '%d'\", fieldName, oldValue, newValue)","\t\t\t}","\t\t\tstructValue.FieldByName(fieldName).SetInt(newValue)","","\t\tdefault:","\t\t\t// Skip unsupported field types","\t\t\tcontinue","\t\t}","\t}","","\tif len(errors) \u003e 0 {","\t\treturn fmt.Errorf(\"validation errors: %v\", errors)","\t}","","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,0,2,2,2,1,1,2,2,2,2,0,2,2,2,2,2,0,2,2,2,1,1,1,0,0,2,2,2,2,2,2,2,0,2,2,2,2,2,0,1,1,1,0,0,0,2,2,2,0,2,0]},{"id":27,"path":"pkg/consoleui/custom.go","lines":["package consoleui","","import (","\t\"context\"","\t\"fmt\"","\t\"net/url\"","\t\"strings\"","\t\"sync\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/settings\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/templates\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"k8s.io/client-go/dynamic\"",")","","type CustomConsole struct {","\tpacInfo              *info.PacOpts","\tnamespace, pr, task  string","\tpod, firstFailedStep string","\textraParams          map[string]string","\tmu                   sync.RWMutex","}","","func NewCustomConsole(pacInfo *info.PacOpts) *CustomConsole {","\treturn \u0026CustomConsole{pacInfo: pacInfo}","}","","func (o *CustomConsole) GetName() string {","\tif o.pacInfo.CustomConsoleName == \"\" {","\t\treturn fmt.Sprintf(\"https://url.setting.%s.is.not.configured\", settings.CustomConsoleNameKey)","\t}","\treturn o.pacInfo.CustomConsoleName","}","","func (o *CustomConsole) URL() string {","\tif o.pacInfo.CustomConsoleURL == \"\" {","\t\treturn fmt.Sprintf(\"https://url.setting.%s.is.not.configured\", settings.CustomConsoleURLKey)","\t}","\treturn o.pacInfo.CustomConsoleURL","}","","func (o *CustomConsole) SetParams(mt map[string]string) {","\to.extraParams = mt","}","","// generateURL will generate a URL from a template, trim some of the spaces and","// \\n we get from yaml","// return the default URL if there it's not become a proper url or that it has","// some of the templates like {{}} left.","func (o *CustomConsole) generateURL(urlTmpl string) string {","\to.mu.RLock()","\tdefer o.mu.RUnlock()","\tdict := map[string]string{","\t\t\"namespace\":       o.namespace,","\t\t\"pr\":              o.pr,","\t\t\"task\":            o.task,","\t\t\"pod\":             o.pod,","\t\t\"firstFailedStep\": o.firstFailedStep,","\t}","\tfor k, v := range o.extraParams {","\t\tdict[k] = v","\t}","","\tnewurl := templates.ReplacePlaceHoldersVariables(urlTmpl, dict, nil, nil, nil)","\t// trim new line because yaml parser adds new line at the end of the string","\tnewurl = strings.TrimSpace(strings.TrimSuffix(newurl, \"\\n\"))","\tif _, err := url.ParseRequestURI(newurl); err != nil {","\t\treturn o.URL()","\t}","\t// detect if there is still some {{}} in the url","\tif keys.ParamsRe.MatchString(newurl) {","\t\treturn o.URL()","\t}","\treturn newurl","}","","func (o *CustomConsole) DetailURL(pr *tektonv1.PipelineRun) string {","\tif o.pacInfo.CustomConsolePRdetail == \"\" {","\t\treturn fmt.Sprintf(\"https://detailurl.setting.%s.is.not.configured\", settings.CustomConsolePRDetailKey)","\t}","\to.namespace = pr.GetNamespace()","\to.pr = pr.GetName()","\treturn o.generateURL(o.pacInfo.CustomConsolePRdetail)","}","","func (o *CustomConsole) NamespaceURL(pr *tektonv1.PipelineRun) string {","\tif o.pacInfo.CustomConsoleNamespaceURL == \"\" {","\t\treturn fmt.Sprintf(\"https://detailurl.setting.%s.is.not.configured\", settings.CustomConsoleNamespaceURLKey)","\t}","\to.namespace = pr.GetNamespace()","\treturn o.generateURL(o.pacInfo.CustomConsoleNamespaceURL)","}","","func (o *CustomConsole) TaskLogURL(pr *tektonv1.PipelineRun, taskRunStatus *tektonv1.PipelineRunTaskRunStatus) string {","\tif o.pacInfo.CustomConsolePRTaskLog == \"\" {","\t\treturn fmt.Sprintf(\"https://tasklogurl.setting.%s.is.not.configured\", settings.CustomConsolePRTaskLogKey)","\t}","\tfirstFailedStep := \"\"","\t// search for the first failed steps in taskrunstatus","\tfor _, step := range taskRunStatus.Status.Steps {","\t\tif step.Terminated != nil \u0026\u0026 step.Terminated.ExitCode != 0 {","\t\t\tfirstFailedStep = step.Name","\t\t\tbreak","\t\t}","\t}","","\to.namespace = pr.GetNamespace()","\to.pr = pr.GetName()","\to.task = taskRunStatus.PipelineTaskName","\to.pod = taskRunStatus.Status.PodName","\to.firstFailedStep = firstFailedStep","","\treturn o.generateURL(o.pacInfo.CustomConsolePRTaskLog)","}","","func (o *CustomConsole) UI(_ context.Context, _ dynamic.Interface) error {","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,2,2,2,2,2,0,0,2,2,2,2,2,0,0,2,2,2,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,0,2,1,1,2,0,0,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,2,0,0,1,1,1]},{"id":28,"path":"pkg/consoleui/interface.go","lines":["package consoleui","","import (","\t\"context\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"k8s.io/client-go/dynamic\"",")","","const consoleIsnotConfiguredURL = \"https://dashboard.is.not.configured\"","","type Interface interface {","\tDetailURL(pr *tektonv1.PipelineRun) string","\tTaskLogURL(pr *tektonv1.PipelineRun, taskRunStatusstatus *tektonv1.PipelineRunTaskRunStatus) string","\tNamespaceURL(pr *tektonv1.PipelineRun) string","\tUI(ctx context.Context, kdyn dynamic.Interface) error","\tURL() string","\tGetName() string","\tSetParams(mt map[string]string)","}","","type FallBackConsole struct{}","","func (f FallBackConsole) GetName() string {","\treturn \"Not configured\"","}","","func (f FallBackConsole) DetailURL(_ *tektonv1.PipelineRun) string {","\treturn consoleIsnotConfiguredURL","}","","func (f FallBackConsole) TaskLogURL(_ *tektonv1.PipelineRun, _ *tektonv1.PipelineRunTaskRunStatus) string {","\treturn consoleIsnotConfiguredURL","}","","func (f FallBackConsole) NamespaceURL(_ *tektonv1.PipelineRun) string {","\treturn consoleIsnotConfiguredURL","}","","func (f FallBackConsole) UI(_ context.Context, _ dynamic.Interface) error {","\treturn nil","}","","func (f FallBackConsole) URL() string {","\treturn consoleIsnotConfiguredURL","}","","func (f FallBackConsole) SetParams(_ map[string]string) {","}","","func New(ctx context.Context, kdyn dynamic.Interface, _ *info.Info) Interface {","\toc := \u0026OpenshiftConsole{}","\tif err := oc.UI(ctx, kdyn); err == nil {","\t\treturn oc","\t}","","\t// TODO: Try to detect TektonDashboard somehow by ingress?","\treturn FallBackConsole{}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,2,2,2,0,2,2,2,0,1,1,1,0,2,2,2,0,2,2,2,0,0,0,0,1,1,1,1,1,0,0,1,0]},{"id":29,"path":"pkg/consoleui/openshift.go","lines":["package consoleui","","import (","\t\"context\"","\t\"fmt\"","","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/apimachinery/pkg/runtime/schema\"","\t\"k8s.io/client-go/dynamic\"",")","","const (","\topenShiftConsoleNS                = \"openshift-console\"","\topenShiftConsoleRouteName         = \"console\"","\topenShiftPipelineNamespaceViewURL = \"%s/pipelines/ns/%s/pipeline-runs\"","\topenShiftPipelineDetailViewURL    = \"%s/k8s/ns/%s/tekton.dev~v1~PipelineRun/%s\"","\topenShiftPipelineTaskLogURL       = \"%s/logs/%s\"","\topenShiftRouteGroup               = \"route.openshift.io\"","\topenShiftRouteVersion             = \"v1\"","\topenShiftRouteResource            = \"routes\"","\topenshiftConsoleName              = \"OpenShift Console\"",")","","type OpenshiftConsole struct {","\thost string","}","","func (o *OpenshiftConsole) SetParams(_ map[string]string) {","}","","func (o *OpenshiftConsole) GetName() string {","\treturn openshiftConsoleName","}","","func (o *OpenshiftConsole) URL() string {","\tif o.host == \"\" {","\t\treturn \"https://openshift.url.is.not.configured\"","\t}","\treturn \"https://\" + o.host","}","","func (o *OpenshiftConsole) DetailURL(pr *tektonv1.PipelineRun) string {","\treturn fmt.Sprintf(openShiftPipelineDetailViewURL, o.URL(), pr.GetNamespace(), pr.GetName())","}","","func (o *OpenshiftConsole) TaskLogURL(pr *tektonv1.PipelineRun, taskRunStatus *tektonv1.PipelineRunTaskRunStatus) string {","\treturn fmt.Sprintf(openShiftPipelineTaskLogURL, o.DetailURL(pr), taskRunStatus.PipelineTaskName)","}","","func (o *OpenshiftConsole) NamespaceURL(pr *tektonv1.PipelineRun) string {","\treturn fmt.Sprintf(openShiftPipelineNamespaceViewURL, o.URL(), pr.GetNamespace())","}","","// UI use dynamic client to get the route of the openshift","// console where we can point to.","func (o *OpenshiftConsole) UI(ctx context.Context, kdyn dynamic.Interface) error {","\tgvr := schema.GroupVersionResource{","\t\tGroup: openShiftRouteGroup, Version: openShiftRouteVersion, Resource: openShiftRouteResource,","\t}","","\troute, err := kdyn.Resource(gvr).Namespace(openShiftConsoleNS).Get(ctx, openShiftConsoleRouteName,","\t\tmetav1.GetOptions{})","\tif err != nil {","\t\treturn err","\t}","","\tspec, ok := route.Object[\"spec\"].(map[string]any)","\tif !ok {","\t\t// this condition is satisfied if there's no metadata at all in the provided CR","\t\treturn fmt.Errorf(\"couldn't find spec in the OpenShift Console route\")","\t}","","\tif o.host, ok = spec[\"host\"].(string); !ok {","\t\t// this condition is satisfied if there's no metadata at all in the provided CR","\t\treturn fmt.Errorf(\"couldn't find spec.host in the OpenShift Console route\")","\t}","","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,2,2,2,2,2,0,0,2,2,2,0,2,2,2,0,2,2,2,0,0,0,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,0,2,2,2,2,0,2,0]},{"id":30,"path":"pkg/consoleui/tektondashboard.go","lines":["package consoleui","","import (","\t\"context\"","\t\"fmt\"","","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"k8s.io/client-go/dynamic\"",")","","type TektonDashboard struct {","\tBaseURL string","}","","const tektonDashboardName = \"Tekton Dashboard\"","","func (t *TektonDashboard) GetName() string {","\treturn tektonDashboardName","}","","func (t *TektonDashboard) DetailURL(pr *tektonv1.PipelineRun) string {","\treturn fmt.Sprintf(\"%s/#/namespaces/%s/pipelineruns/%s\", t.URL(), pr.GetNamespace(), pr.GetName())","}","","func (t *TektonDashboard) NamespaceURL(pr *tektonv1.PipelineRun) string {","\treturn fmt.Sprintf(\"%s/#/namespaces/%s/pipelineruns\", t.BaseURL, pr.GetNamespace())","}","","func (t *TektonDashboard) TaskLogURL(pr *tektonv1.PipelineRun, taskRunStatus *tektonv1.PipelineRunTaskRunStatus) string {","\treturn fmt.Sprintf(\"%s?pipelineTask=%s\", t.DetailURL(pr), taskRunStatus.PipelineTaskName)","}","","func (t *TektonDashboard) URL() string {","\t// if BaseURL is not provided, return fake URL","\tif t.BaseURL == \"\" || t.BaseURL == \"http://\" || t.BaseURL == \"https://\" {","\t\treturn \"https://dashboard.url.is.not.configured\"","\t}","\treturn t.BaseURL","}","","func (t *TektonDashboard) UI(_ context.Context, _ dynamic.Interface) error {","\treturn nil","}","","func (t *TektonDashboard) SetParams(_ map[string]string) {","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,2,2,2,0,2,2,2,0,2,2,2,0,2,2,2,1,1,2,0,0,2,2,2,0,0,0]},{"id":31,"path":"pkg/customparams/customparams.go","lines":["package customparams","","import (","\t\"context\"","\t\"fmt\"","","\tcelTypes \"github.com/google/cel-go/common/types\"","\t\"go.uber.org/zap\"","","\tapincoming \"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/incoming\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\tpacCel \"github.com/openshift-pipelines/pipelines-as-code/pkg/cel\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/events\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/opscomments\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\tsectypes \"github.com/openshift-pipelines/pipelines-as-code/pkg/secrets/types\"",")","","type CustomParams struct {","\tevent        *info.Event","\trun          *params.Run","\tk8int        kubeinteraction.Interface","\teventEmitter *events.EventEmitter","\trepo         *v1alpha1.Repository","\tvcx          provider.Interface","}","","func NewCustomParams(event *info.Event, repo *v1alpha1.Repository, run *params.Run, k8int kubeinteraction.Interface, eventEmitter *events.EventEmitter, prov provider.Interface) CustomParams {","\treturn CustomParams{","\t\tevent:        event,","\t\trepo:         repo,","\t\trun:          run,","\t\tk8int:        k8int,","\t\teventEmitter: eventEmitter,","\t\tvcx:          prov,","\t}","}","","// applyIncomingParams apply incoming params to an existing map (overwriting existing keys).","func (p *CustomParams) applyIncomingParams(ret map[string]string) map[string]string {","\tif p.event.Request == nil {","\t\treturn ret","\t}","\tif incomingParams, err := apincoming.ParseIncomingPayload(p.event.Request.Payload); err == nil {","\t\tfor k, v := range incomingParams.Params {","\t\t\tif vs, ok := v.(string); ok {","\t\t\t\tret[k] = vs","\t\t\t} else {","\t\t\t\tp.eventEmitter.EmitMessage(p.repo, zap.WarnLevel, \"IncomingParamsNotString\", fmt.Sprintf(\"cannot convert incoming param key: %s value: %v as string\", k, v))","\t\t\t}","\t\t}","\t}","\treturn ret","}","","// GetParams will process the parameters as set in the repo.Spec CR.","// value can come from a string or from a secretKeyRef or from a string value","// if both is set we pick the value and issue a warning in the user namespace","// we let the user specify a cel filter. If false then we skip the parameters.","// if multiple params name has a filter we pick up the first one that has","// matched true.","func (p *CustomParams) GetParams(ctx context.Context) (map[string]string, map[string]any, error) {","\tstdParams, changedFiles := p.makeStandardParamsFromEvent(ctx)","\tresolvedParams, mapFilters, parsedFromComment := map[string]string{}, map[string]string{}, map[string]string{}","\tif p.event.TriggerComment != \"\" {","\t\tparsedFromComment = opscomments.ParseKeyValueArgs(p.event.TriggerComment)","\t\tfor k, v := range parsedFromComment {","\t\t\tif _, ok := stdParams[k]; ok {","\t\t\t\tstdParams[k] = v","\t\t\t}","\t\t}","\t}","","\tif p.repo.Spec.Params == nil {","\t\treturn p.applyIncomingParams(stdParams), changedFiles, nil","\t}","","\tfor index, value := range *p.repo.Spec.Params {","\t\t// if the name is empty we skip it","\t\tif value.Name == \"\" {","\t\t\tp.eventEmitter.EmitMessage(p.repo, zap.ErrorLevel,","\t\t\t\t\"ParamsFilterSkipped\", fmt.Sprintf(\"no name has been set in params[%d] of repo %s\", index, p.repo.GetName()))","\t\t\tcontinue","\t\t}","\t\tif value.Filter != \"\" {","\t\t\t// if we already have a filter that has matched we skip it","\t\t\tif _, ok := mapFilters[value.Name]; ok {","\t\t\t\tp.eventEmitter.EmitMessage(p.repo, zap.WarnLevel,","\t\t\t\t\t\"ParamsFilterSkipped\", fmt.Sprintf(\"skipping params name %s, filter has already been matched previously\", value.Name))","\t\t\t\tcontinue","\t\t\t}","","\t\t\t// if the cel filter condition is false we skip it","\t\t\t// TODO: add headers to customparams?","\t\t\tcond, err := pacCel.Value(value.Filter, p.event.Event, nil, stdParams, changedFiles)","\t\t\tif err != nil {","\t\t\t\tp.eventEmitter.EmitMessage(p.repo, zap.ErrorLevel,","\t\t\t\t\t\"ParamsFilterError\", fmt.Sprintf(\"there is an error on the cel filter: %s: %s\", value.Name, err.Error()))","\t\t\t\treturn map[string]string{}, changedFiles, err","\t\t\t}","\t\t\tswitch cond.(type) {","\t\t\tcase celTypes.Bool:","\t\t\t\tif cond == celTypes.False {","\t\t\t\t\tp.eventEmitter.EmitMessage(p.repo, zap.InfoLevel,","\t\t\t\t\t\t\"ParamsFilterSkipped\", fmt.Sprintf(\"skipping params name %s, filter condition is false\", value.Name))","\t\t\t\t\tcontinue","\t\t\t\t}","\t\t\tdefault:","\t\t\t\tp.eventEmitter.EmitMessage(p.repo, zap.InfoLevel,","\t\t\t\t\t\"ParamsFilterSkipped\", fmt.Sprintf(\"skipping params name %s, filter condition is not a boolean reply: %s\", value.Name, cond.Type().TypeName()))","\t\t\t\tcontinue","\t\t\t}","\t\t\tmapFilters[value.Name] = value.Value","\t\t}","","\t\tif value.SecretRef != nil \u0026\u0026 value.Value != \"\" {","\t\t\tp.eventEmitter.EmitMessage(p.repo, zap.InfoLevel,","\t\t\t\t\"ParamsFilterUsedValue\",","\t\t\t\tfmt.Sprintf(\"repo %s, param name %s has a value and secretref, picking value\", p.repo.GetName(), value.Name))","\t\t}","","\t\t_, paramIsStd := stdParams[value.Name]","\t\t_, paramParsedFromContent := parsedFromComment[value.Name]","","\t\tswitch {","\t\tcase value.Value != \"\":","\t\t\tresolvedParams[value.Name] = value.Value","\t\tcase paramParsedFromContent \u0026\u0026 !paramIsStd:","\t\t\t// If the param is standard, it's initial value will be set later so we don't set it here.","\t\t\t// Setting to empty string allows the parsedFromComment overrides to set the overridden value below.","\t\t\tresolvedParams[value.Name] = \"\"","\t\tcase value.SecretRef != nil:","\t\t\tsecretValue, err := p.k8int.GetSecret(ctx, sectypes.GetSecretOpt{","\t\t\t\tNamespace: p.repo.GetNamespace(),","\t\t\t\tName:      value.SecretRef.Name,","\t\t\t\tKey:       value.SecretRef.Key,","\t\t\t})","\t\t\tif err != nil {","\t\t\t\treturn resolvedParams, changedFiles, err","\t\t\t}","\t\t\tresolvedParams[value.Name] = secretValue","\t\t}","\t}","","\t// TODO: Should we let the user override the standard params?","\t// we don't let them here","\tfor k, v := range stdParams {","\t\t// check if not already there","\t\tif _, ok := resolvedParams[k]; !ok \u0026\u0026 v != \"\" {","\t\t\tresolvedParams[k] = v","\t\t}","\t}","","\t// overwrite stdParams with parsed ones from the trigger comment","\tfor k, v := range parsedFromComment {","\t\tif _, ok := resolvedParams[k]; ok \u0026\u0026 v != \"\" {","\t\t\tresolvedParams[k] = v","\t\t}","\t}","","\treturn p.applyIncomingParams(resolvedParams), changedFiles, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,0,0,2,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,0,2,2,2,2,2,2,0,2,2,2,2,2,2,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,0,2,0,0,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,2,2,2,2,2,0,0,0,2,2,2,2,0,0,2,0]},{"id":32,"path":"pkg/customparams/standard.go","lines":["package customparams","","import (","\t\"context\"","\t\"fmt\"","\t\"strings\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/changedfiles\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/formatting\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/opscomments\"","\t\"go.uber.org/zap\"",")","","func (p *CustomParams) getChangedFiles(ctx context.Context) changedfiles.ChangedFiles {","\tif p.vcx == nil {","\t\treturn changedfiles.ChangedFiles{}","\t}","\tchangedFiles, err := p.vcx.GetFiles(ctx, p.event)","\tif err != nil {","\t\tp.eventEmitter.EmitMessage(p.repo, zap.ErrorLevel, \"ParamsError\", fmt.Sprintf(\"error getting changed files: %s\", err.Error()))","\t\treturn changedfiles.ChangedFiles{}","\t}","\tchangedFiles.RemoveDuplicates()","\treturn changedFiles","}","","// makeStandardParamsFromEvent will create a map of standard params out of the event.","func (p *CustomParams) makeStandardParamsFromEvent(ctx context.Context) (map[string]string, map[string]any) {","\trepoURL := p.event.URL","\t// On bitbucket data center you are have a special url for checking it out, they","\t// seemed to fix it in 2.0 but i guess we have to live with this until then.","\tif p.event.CloneURL != \"\" {","\t\trepoURL = p.event.CloneURL","\t}","\tchangedFiles := p.getChangedFiles(ctx)","\ttriggerCommentAsSingleLine := strings.ReplaceAll(strings.ReplaceAll(p.event.TriggerComment, \"\\r\\n\", \"\\\\n\"), \"\\n\", \"\\\\n\")","\tpullRequestLabels := strings.Join(p.event.PullRequestLabel, \"\\\\n\")","","\tgitTag := \"\"","\tif strings.HasPrefix(p.event.BaseBranch, \"refs/tags/\") {","\t\tgitTag = strings.TrimPrefix(p.event.BaseBranch, \"refs/tags/\")","\t}","","\treturn map[string]string{","\t\t\t\"revision\":            p.event.SHA,","\t\t\t\"repo_url\":            repoURL,","\t\t\t\"repo_owner\":          strings.ToLower(p.event.Organization),","\t\t\t\"repo_name\":           strings.ToLower(p.event.Repository),","\t\t\t\"target_branch\":       formatting.SanitizeBranch(p.event.BaseBranch),","\t\t\t\"source_branch\":       formatting.SanitizeBranch(p.event.HeadBranch),","\t\t\t\"git_tag\":             gitTag,","\t\t\t\"source_url\":          p.event.HeadURL,","\t\t\t\"sender\":              strings.ToLower(p.event.Sender),","\t\t\t\"target_namespace\":    p.repo.GetNamespace(),","\t\t\t\"event_type\":          opscomments.EventTypeBackwardCompat(p.eventEmitter, p.repo, p.event.EventType),","\t\t\t\"trigger_comment\":     triggerCommentAsSingleLine,","\t\t\t\"pull_request_labels\": pullRequestLabels,","\t\t}, map[string]any{","\t\t\t\"all\":      changedFiles.All,","\t\t\t\"added\":    changedFiles.Added,","\t\t\t\"deleted\":  changedFiles.Deleted,","\t\t\t\"modified\": changedFiles.Modified,","\t\t\t\"renamed\":  changedFiles.Renamed,","\t\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,1,1,2,2,1,1,1,2,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0]},{"id":33,"path":"pkg/events/emit.go","lines":["package events","","import (","\t\"context\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/formatting\"","\t\"go.uber.org/zap\"","\t\"go.uber.org/zap/zapcore\"","\tv1 \"k8s.io/api/core/v1\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/client-go/kubernetes\"",")","","func NewEventEmitter(client kubernetes.Interface, logger *zap.SugaredLogger) *EventEmitter {","\treturn \u0026EventEmitter{","\t\tclient: client,","\t\tlogger: logger,","\t}","}","","type EventEmitter struct {","\tclient kubernetes.Interface","\tlogger *zap.SugaredLogger","}","","func (e *EventEmitter) SetLogger(logger *zap.SugaredLogger) {","\te.logger = logger","}","","func (e *EventEmitter) EmitMessage(repo *v1alpha1.Repository, loggerLevel zapcore.Level, reason, message string) {","\tif repo != nil \u0026\u0026 e.client != nil {","\t\tevent := makeEvent(repo, loggerLevel, reason, message)","\t\tif _, err := e.client.CoreV1().Events(event.Namespace).Create(context.Background(), event, metav1.CreateOptions{}); err != nil {","\t\t\tif e.logger != nil {","\t\t\t\te.logger.Infof(\"Cannot create event: %s\", err.Error())","\t\t\t}","\t\t}","\t}","","\tif e.logger != nil {","\t\t//nolint","\t\tswitch loggerLevel {","\t\tcase zapcore.DebugLevel:","\t\t\te.logger.Debug(message)","\t\tcase zapcore.ErrorLevel:","\t\t\te.logger.Error(message)","\t\tcase zapcore.InfoLevel:","\t\t\te.logger.Info(message)","\t\tcase zapcore.WarnLevel:","\t\t\te.logger.Warn(message)","\t\t}","\t}","}","","func makeEvent(repo *v1alpha1.Repository, loggerLevel zapcore.Level, reason, message string) *v1.Event {","\tevent := \u0026v1.Event{","\t\tObjectMeta: metav1.ObjectMeta{","\t\t\tGenerateName: repo.Name + \"-\",","\t\t\tNamespace:    repo.Namespace,","\t\t\tLabels: map[string]string{","\t\t\t\tkeys.Repository: formatting.CleanValueKubernetes(repo.Name),","\t\t\t},","\t\t\tAnnotations: map[string]string{","\t\t\t\tkeys.Repository: repo.Name,","\t\t\t},","\t\t},","\t\tMessage: message,","\t\tReason:  reason,","\t\tType:    v1.EventTypeWarning,","\t\tInvolvedObject: v1.ObjectReference{","\t\t\tAPIVersion:      pipelinesascode.V1alpha1Version,","\t\t\tKind:            pipelinesascode.RepositoryKind,","\t\t\tNamespace:       repo.Namespace,","\t\t\tName:            repo.Name,","\t\t\tUID:             repo.UID,","\t\t\tResourceVersion: repo.ResourceVersion,","\t\t},","\t\tSource: v1.EventSource{","\t\t\tComponent: \"Pipelines As Code\",","\t\t},","\t}","\tif loggerLevel == zap.InfoLevel {","\t\tevent.Type = v1.EventTypeNormal","\t}","\treturn event","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,0,0,0,0,0,1,1,1,0,2,2,2,2,1,1,1,0,0,0,2,2,2,1,1,2,2,2,2,1,1,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0]},{"id":34,"path":"pkg/formatting/age.go","lines":["package formatting","","import (","\t\"github.com/hako/durafmt\"","\t\"github.com/jonboulle/clockwork\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"",")","","func Age(t *metav1.Time, c clockwork.Clock) string {","\tif t.IsZero() {","\t\treturn nonAttributedStr","\t}","\treturn durafmt.ParseShort(c.Since(t.Time)).String() + \" ago\"","}","","func Duration(t1, t2 *metav1.Time) string {","\tif t1.IsZero() || t2.IsZero() {","\t\treturn nonAttributedStr","\t}","\treturn durafmt.ParseShort(t2.Sub(t1.Time)).String()","}","","// PRDuration calculates the duration of a repository run, given its status.","// It takes a RepositoryRunStatus object as input.","// It returns a string with the duration of the run, or nonAttributedStr if the run has not started or completed.","func PRDuration(runStatus v1alpha1.RepositoryRunStatus) string {","\tif runStatus.StartTime == nil {","\t\treturn nonAttributedStr","\t}","","\tlasttime := runStatus.CompletionTime","\tif lasttime == nil {","\t\tif len(runStatus.Conditions) == 0 {","\t\t\treturn nonAttributedStr","\t\t}","\t\tlasttime = \u0026runStatus.Conditions[0].LastTransitionTime.Inner","\t}","","\treturn Duration(runStatus.StartTime, lasttime)","}","","func Timeout(t *metav1.Duration) string {","\tif t == nil {","\t\treturn nonAttributedStr","\t}","\treturn durafmt.Parse(t.Duration).String()","}"],"coverage":[0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,0,2,2,2,2,2,0,0,0,0,0,2,2,2,2,0,2,2,2,2,2,2,0,0,2,0,0,2,2,2,2,2,0]},{"id":35,"path":"pkg/formatting/array.go","lines":["package formatting","","import \"sort\"","","func UniqueStringArray(slice []string) []string {","\tkeys := make(map[string]bool)","\tlist := []string{}","\tfor _, entry := range slice {","\t\tif _, value := keys[entry]; !value {","\t\t\tkeys[entry] = true","\t\t\tlist = append(list, entry)","\t\t}","\t}","\tsort.Strings(list)","\treturn list","}"],"coverage":[0,0,0,0,2,2,2,2,2,2,2,2,0,2,2,0]},{"id":36,"path":"pkg/formatting/emoji.go","lines":["package formatting","","import (","\t\"fmt\"","","\tcorev1 \"k8s.io/api/core/v1\"","\tknative1 \"knative.dev/pkg/apis/duck/v1\"",")","","const nonAttributedStr = \"---\"","","// formatCondition knative formatcondition with emoji or not.","func formatCondition(c knative1.Conditions, skipemoji bool) string {","\tvar status, emoji string","\tif len(c) == 0 {","\t\treturn nonAttributedStr","\t}","","\tswitch c[0].Status {","\tcase corev1.ConditionFalse:","\t\temoji = \"ðŸ”´\"","\t\tstatus = \"Failed\"","\tcase corev1.ConditionTrue:","\t\temoji = \"ðŸŸ¢\"","\t\tstatus = \"Succeeded\"","\tcase corev1.ConditionUnknown:","\t\temoji = \"ðŸŸ¡\"","\t\tstatus = \"Running\"","\tdefault:","\t\temoji = \"ðŸ”„\"","\t\tstatus = \"Pending\"","\t}","","\tif !skipemoji {","\t\tstatus = fmt.Sprintf(\"%s %s\", emoji, status)","\t}","","\treturn status","}","","func ConditionEmoji(c knative1.Conditions) string {","\treturn formatCondition(c, false)","}","","func ConditionSad(c knative1.Conditions) string {","\treturn formatCondition(c, true)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,1,1,1,0,0,2,2,2,0,2,0,0,2,2,2,0,2,2,2]},{"id":37,"path":"pkg/formatting/k8labels.go","lines":["package formatting","","import (","\t\"strings\"",")","","// CleanValueKubernetes conform a string to kubernetes naming convention","// see https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names","// rules are:","// â€¢ contain at most 63 characters","// â€¢ contain only lowercase alphanumeric characters or '-'","// â€¢ start with an alphanumeric character","// â€¢ end with an alphanumeric character.","func CleanValueKubernetes(s string) string {","\tif len(s) \u003e= 63 {","\t\t// keep the last 62 characters","\t\ts = s[len(s)-62:]","\t}","","\treplasoeur := strings.NewReplacer(\":\", \"-\", \"/\", \"-\", \" \", \"_\", \"[\", \"__\", \"]\", \"__\")","\ts = strings.TrimRight(s, \" -_[]\")","\ts = strings.TrimLeft(s, \" -_[]\")","\treplaced := replasoeur.Replace(s)","\treturn strings.TrimSpace(replaced)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,2,2,2,2,2,0]},{"id":38,"path":"pkg/formatting/k8names.go","lines":["package formatting","","import (","\t\"regexp\"","\t\"strings\"",")","","// CleanKubernetesName takes a string and performs the following actions to make it a valid","// Kubernetes resource name:","//","// 1. Converts the string to lowercase.","// 2. Trims leading and trailing whitespace.","// 3. Replaces any characters that are not lowercase alphanumeric characters, '-', or '.' with '-'.","//","// The resulting string is a valid Kubernetes resource name.","// Reference https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names","","func CleanKubernetesName(s string) string {","\tregex := regexp.MustCompile(`[^a-z0-9\\.-]`)","\ts = strings.TrimSpace(strings.ToLower(s))","\treplaced := regex.ReplaceAllString(s, \"-\")","\treturn replaced","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2]},{"id":39,"path":"pkg/formatting/pipelinerun.go","lines":["package formatting","","import (","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\tcorev1 \"k8s.io/api/core/v1\"","\t\"knative.dev/pkg/apis\"",")","","// PipelineRunStatus return status of PR  success failed or skipped.","func PipelineRunStatus(pr *tektonv1.PipelineRun) string {","\tif len(pr.Status.Conditions) == 0 {","\t\treturn \"neutral\"","\t}","\tif pr.Status.GetCondition(apis.ConditionSucceeded).GetReason() == tektonv1.PipelineRunSpecStatusCancelled {","\t\treturn \"cancelled\"","\t}","\tif pr.Status.Conditions[0].Status == corev1.ConditionFalse {","\t\treturn \"failure\"","\t}","\treturn \"success\"","}"],"coverage":[0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,0]},{"id":40,"path":"pkg/formatting/repository.go","lines":["package formatting","","import (","\t\"github.com/jonboulle/clockwork\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/cli\"",")","","var shortShaLength = 7","","func ShowLastSHA(repository v1alpha1.Repository) string {","\tif len(repository.Status) == 0 {","\t\treturn nonAttributedStr","\t}","\treturn ShortSHA(*repository.Status[len(repository.Status)-1].SHA)","}","","func ShowStatus(repository v1alpha1.Repository, cs *cli.ColorScheme) string {","\tif len(repository.Status) == 0 {","\t\treturn cs.ColorStatus(\"NoRun\")","\t}","\tstatus := repository.Status[len(repository.Status)-1].Status.Conditions[0].GetReason()","\tlogurl := repository.Status[len(repository.Status)-1].LogURL","\treturn cs.HyperLink(cs.ColorStatus(status), *logurl)","}","","func ShowLastAge(repository v1alpha1.Repository, cw clockwork.Clock) string {","\tif len(repository.Status) == 0 {","\t\treturn nonAttributedStr","\t}","\treturn Age(repository.Status[len(repository.Status)-1].CompletionTime, cw)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,0,2,2,2,2,2,2,2,0,0,2,2,2,2,2,0]},{"id":41,"path":"pkg/formatting/starting.go","lines":["package formatting","","import (","\t\"bytes\"","\t_ \"embed\"","\t\"text/template\"",")","","//go:embed templates/starting.go.tmpl","var StartingPipelineRunHTML string","","//go:embed templates/starting.markdown.go.tmpl","var StartingPipelineRunMarkdown string","","//go:embed templates/queuing.go.tmpl","var QueuingPipelineRunHTML string","","//go:embed templates/queuing.markdown.go.tmpl","var QueuingPipelineRunMarkdown string","","//go:embed templates/pipelinerunstatus.tmpl","var PipelineRunStatusHTML string","","//go:embed templates/pipelinerunstatus_markdown.tmpl","var PipelineRunStatusMarkDown string","","type MessageTemplate struct {","\tPipelineRunName string","\tNamespace       string","\tNamespaceURL    string","\tConsoleName     string","\tConsoleURL      string","\tTknBinary       string","\tTknBinaryURL    string","\tTaskStatus      string","\tFailureSnippet  string","}","","func (mt MessageTemplate) MakeTemplate(tmpl string) (string, error) {","\toutputBuffer := bytes.Buffer{}","\tt := template.Must(template.New(\"Message\").Parse(tmpl))","\tdata := struct{ Mt MessageTemplate }{Mt: mt}","\tif err := t.Execute(\u0026outputBuffer, data); err != nil {","\t\treturn \"\", err","\t}","\treturn outputBuffer.String(), nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,0]},{"id":42,"path":"pkg/formatting/vcs.go","lines":["package formatting","","import (","\t\"fmt\"","\t\"net/url\"","\t\"path/filepath\"","\t\"strings\"","","\t\"golang.org/x/text/cases\"","\t\"golang.org/x/text/language\"",")","","// SanitizeBranch remove refs/heads from string, only removing the first prefix","// in case we have branch that are actually called refs-heads ðŸ™ƒ.","func SanitizeBranch(s string) string {","\tif strings.HasPrefix(s, \"refs/heads/\") {","\t\treturn strings.TrimPrefix(s, \"refs/heads/\")","\t}","\tif strings.HasPrefix(s, \"refs-heads-\") {","\t\treturn strings.TrimPrefix(s, \"refs-heads-\")","\t}","\treturn s","}","","// ShortSHA returns a shortsha.","func ShortSHA(sha string) string {","\tif sha == \"\" {","\t\treturn \"\"","\t}","\tif shortShaLength \u003e= len(sha)+1 {","\t\treturn sha","\t}","\treturn sha[0:shortShaLength]","}","","func GetRepoOwnerFromURL(ghURL string) (string, error) {","\torg, repo, err := GetRepoOwnerSplitted(ghURL)","\tif err != nil {","\t\treturn \"\", err","\t}","\trepo = strings.TrimSuffix(repo, \"/\")","\treturn strings.ToLower(fmt.Sprintf(\"%s/%s\", org, repo)), nil","}","","func GetRepoOwnerSplitted(u string) (string, string, error) {","\tuparse, err := url.Parse(u)","\tif err != nil {","\t\treturn \"\", \"\", err","\t}","\tparts := strings.Split(uparse.Path, \"/\")","\tif len(parts) \u003c 3 {","\t\treturn \"\", \"\", fmt.Errorf(\"invalid repo url at least a organization/project and a repo needs to be specified: %s\", u)","\t}","\torg := filepath.Join(parts[0 : len(parts)-1]...)","\trepo := parts[len(parts)-1]","\treturn org, repo, nil","}","","// CamelCasit pull_request \u003e PullRequest.","func CamelCasit(s string) string {","\tc := cases.Title(language.AmericanEnglish)","\treturn strings.ReplaceAll(c.String(strings.ReplaceAll(s, \"_\", \" \")), \" \", \"\")","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,0,0,2,2,2,1,1,2,2,2,2,2,2,2,0,0,0,2,2,2,2]},{"id":43,"path":"pkg/git/git.go","lines":["package git","","import (","\t\"bytes\"","\t\"context\"","\t\"fmt\"","\t\"os\"","\t\"os/exec\"","\t\"strings\"",")","","type Info struct {","\tURL          string","\tTopLevelPath string","\tSHA          string","\tBranch       string","}","","func RunGit(dir string, args ...string) (string, error) {","\tgitPath, err := exec.LookPath(\"git\")","\tif err != nil {","\t\t//nolint: nilerr","\t\treturn \"\", nil","\t}","\t// insert in args \"-c\", \"gitcommit.gpgsign=false\" at the beginning gpg sign when set in user","\targs = append([]string{\"-c\", \"commit.gpgsign=false\"}, args...)","","\tc := exec.CommandContext(context.Background(), gitPath, args...)","\tc.Env = []string{","\t\t\"PATH=\" + os.Getenv(\"PATH\"),","\t\t\"HOME=\" + os.Getenv(\"HOME\"),","\t\t\"LC_ALL=C\",","\t\t\"LANG=C\",","\t}","\tvar output bytes.Buffer","\tc.Stderr = \u0026output","\tc.Stdout = \u0026output","\t// This is the optional working directory. If not set, it defaults to the current","\t// working directory of the process.","\tif dir != \"\" {","\t\tc.Dir = dir","\t}","\tif err := c.Run(); err != nil {","\t\treturn \"\", fmt.Errorf(\"error running, %s, output: %s error: %w\", args, output.String(), err)","\t}","\treturn output.String(), nil","}","","// GetGitInfo try to detect the current remote for this URL return the origin url transformed and the topdir.","func GetGitInfo(dir string) *Info {","\tbrootdir, err := RunGit(dir, \"rev-parse\", \"--show-toplevel\")","\tif err != nil {","\t\treturn \u0026Info{}","\t}","","\tsha, err := RunGit(dir, \"rev-parse\", \"HEAD\")","\tif err != nil {","\t\treturn \u0026Info{}","\t}","","\theadbranch, err := RunGit(dir, \"rev-parse\", \"--abbrev-ref\", \"HEAD\")","\tif err != nil {","\t\treturn \u0026Info{}","\t}","","\tgitURL, err := RunGit(dir, \"remote\", \"get-url\", \"origin\")","\tif err != nil {","\t\tgitURL, err = RunGit(dir, \"remote\", \"get-url\", \"upstream\")","\t\tif err != nil {","\t\t\t// use top dir name as fallback","\t\t\tgitURL = brootdir","\t\t}","\t}","\tgitURL = strings.TrimSpace(gitURL)","\tgitURL = strings.TrimSuffix(gitURL, \".git\")","","\t// convert github and probably others ssh access format into https","\t// i think it only fails with bitbucket data center","\tif strings.HasPrefix(gitURL, \"git@\") {","\t\tsp := strings.Split(gitURL, \":\")","\t\tprefix := strings.ReplaceAll(sp[0], \"git@\", \"https://\")","\t\tgitURL = fmt.Sprintf(\"%s/%s\", prefix, strings.Join(sp[1:], \":\"))","\t}","","\treturn \u0026Info{","\t\tURL:          gitURL,","\t\tTopLevelPath: strings.TrimSpace(brootdir),","\t\tSHA:          strings.TrimSpace(sha),","\t\tBranch:       strings.TrimSpace(headbranch),","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,1,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,1,1,0,2,2,1,1,0,2,2,1,1,0,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,0]},{"id":44,"path":"pkg/hub/artifacthub.go","lines":["// Copyright Â© 2022 The Tekton Authors.","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//     http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package hub","","import (","\t\"context\"","\t\"encoding/json\"","\t\"fmt\"","\t\"strings\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"",")","","const (","\tartifactHubTaskType                   = \"tekton-task\"","\tartifactHubPipelineType               = \"tekton-pipeline\"","\tdefaultArtifactHubCatalogTaskName     = \"tekton-catalog-tasks\"","\tdefaultArtifactHubCatalogPipelineName = \"tekton-catalog-pipelines\"",")","","// artifactHubClient is a client for the Artifact Hub.","type artifactHubClient struct {","\tparams *params.Run","\turl    string","\tname   string","}","","// newArtifactHubClient returns a new Artifact Hub client.","func newArtifactHubClient(params *params.Run, url, name string) Client {","\turl = strings.TrimSuffix(url, \"/\") // Trim any trailing slash","\tif !strings.HasSuffix(url, \"/api/v1\") {","\t\turl = fmt.Sprintf(\"%s/api/v1\", url)","\t}","\treturn \u0026artifactHubClient{params: params, url: url, name: name}","}","","// GetResource gets a resource from the Artifact Hub.","func (a *artifactHubClient) GetResource(ctx context.Context, _, resource, kind string) (string, error) {","\tvar data string","\tvar err error","","\tif strings.Contains(resource, \":\") {","\t\tdata, err = a.getSpecificVersion(ctx, a.name, resource, kind)","\t} else {","\t\tdata, err = a.getLatestVersion(ctx, a.name, resource, kind)","\t}","\tif err != nil {","\t\treturn \"\", fmt.Errorf(\"could not fetch remote %s %s, artifacthub API returned: %w\", kind, resource, err)","\t}","","\treturn data, nil","}","","func getArtifactHubTypeByKind(catalogName, kind string) (string, string) {","\tvar pkgType string","\tswitch kind {","\tcase \"task\":","\t\tpkgType = artifactHubTaskType","\t\tif catalogName == \"default\" || catalogName == \"\" {","\t\t\tcatalogName = defaultArtifactHubCatalogTaskName","\t\t}","\tcase \"pipeline\":","\t\tpkgType = artifactHubPipelineType","\t\tif catalogName == \"default\" || catalogName == \"\" {","\t\t\tcatalogName = defaultArtifactHubCatalogPipelineName","\t\t}","\t\t// For other kinds, no changes are made.","\t}","","\treturn pkgType, catalogName","}","","// getLatestVersion gets the latest version of a resource from the Artifact Hub.","// url is like:","// https://artifacthub.io/api/v1/packages/tekton-task/tekton-catalog-tasks/git-clone","func (a *artifactHubClient) getLatestVersion(ctx context.Context, catalogName, resource, kind string) (string, error) {","\tpkgType, catalogName := getArtifactHubTypeByKind(catalogName, kind)","\turl := fmt.Sprintf(\"%s/packages/%s/%s/%s\", a.url, pkgType, catalogName, resource)","\tresp := new(artifactHubPkgResponse)","\tdata, err := a.params.Clients.GetURL(ctx, url)","\tif err != nil {","\t\treturn \"\", fmt.Errorf(\"could not fetch %s %s from hub, url: %s: %w\", kind, resource, url, err)","\t}","\terr = json.Unmarshal(data, \u0026resp)","\tif err != nil {","\t\treturn \"\", fmt.Errorf(\"could not unmarshal response from hub, url: %s: %w\", url, err)","\t}","\tif resp.Data.ManifestRaw == \"\" {","\t\treturn \"\", fmt.Errorf(\"manifest is empty in hub response for, url: %s %s\", url, resource)","\t}","\treturn resp.Data.ManifestRaw, nil","}","","// getSpecificVersion gets a specific version of a resource from the Artifact Hub.","// url is like:","// https://artifacthub.io/api/v1/packages/tekton-task/tekton-catalog-tasks/git-clone/0.9.0","func (a *artifactHubClient) getSpecificVersion(ctx context.Context, catalogName, resource, kind string) (string, error) {","\tpkgType, catalogName := getArtifactHubTypeByKind(catalogName, kind)","","\tsplit := strings.Split(resource, \":\")","\tversion := split[len(split)-1]","\tresourceName := split[0]","","\turl := fmt.Sprintf(\"%s/packages/%s/%s/%s/%s\", a.url, pkgType, catalogName, resourceName, version)","\tresp := new(artifactHubPkgResponse)","\tdata, err := a.params.Clients.GetURL(ctx, url)","\tif err != nil {","\t\treturn \"\", fmt.Errorf(\"could not fetch %s %s from hub, url: %s: %w\", kind, resource, url, err)","\t}","\terr = json.Unmarshal(data, \u0026resp)","\tif err != nil {","\t\treturn \"\", fmt.Errorf(\"could not unmarshal response from hub, url: %s: %w\", url, err)","\t}","\tif resp.Data.ManifestRaw == \"\" {","\t\treturn \"\", fmt.Errorf(\"manifest is empty in hub response for, url: %s %s\", url, resource)","\t}","\treturn resp.Data.ManifestRaw, nil","}","","// artifactHubPkgResponse is the response from the Artifact Hub API.","// It contains a `data` field, which holds the package data, including the raw manifest.","// The JSON structure is as follows:","//","//\t{","//\t  \"data\": {","//\t    \"manifestRaw\": \"\u003craw manifest content\u003e\"","//\t  }","//\t}","type artifactHubPkgResponse struct {","\tData artifactHubPkgData `json:\"data,omitempty\"`","}","","// artifactHubPkgData represents the data field in the response from the Artifact Hub API.","// It contains the raw manifest of a Tekton resource (e.g., task or pipeline) as a string.","// The JSON structure it maps to is:","//","//\t{","//\t  \"manifestRaw\": \"\u003craw manifest content\u003e\"","//\t}","type artifactHubPkgData struct {","\tManifestRaw string `json:\"manifestRaw\"`","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,0,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]},{"id":45,"path":"pkg/hub/get.go","lines":["// Copyright Â© 2022 The Tekton Authors.","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//     http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package hub","","import (","\t\"context\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"",")","","// GetResource returns a resource from the hub.","func GetResource(ctx context.Context, cs *params.Run, catalogName, resource, kind string) (string, error) {","\tclient, err := NewClient(ctx, cs, catalogName)","\tif err != nil {","\t\treturn \"\", err","\t}","","\treturn client.GetResource(ctx, catalogName, resource, kind)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,2,0]},{"id":46,"path":"pkg/hub/hub.go","lines":["// Copyright Â© 2022 The Tekton Authors.","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//     http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package hub","","import (","\t\"context\"","\t\"fmt\"","","\thubtypes \"github.com/openshift-pipelines/pipelines-as-code/pkg/hub/vars\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/settings\"",")","","// Client is an interface for a hub client.","type Client interface {","\tGetResource(ctx context.Context, catalogName, resource, kind string) (string, error)","}","","// NewClient returns a new hub client.","func NewClient(_ context.Context, cs *params.Run, catalogName string) (Client, error) {","\tvalue, ok := cs.Info.Pac.HubCatalogs.Load(catalogName)","\tif !ok {","\t\treturn nil, fmt.Errorf(\"could not get details for catalog name: %s\", catalogName)","\t}","\tcatalogValue, ok := value.(settings.HubCatalog)","\tif !ok {","\t\treturn nil, fmt.Errorf(\"could not get details for catalog name: %s\", catalogName)","\t}","","\tswitch catalogValue.Type {","\tcase hubtypes.TektonHubType:","\t\treturn newTektonHubClient(cs, catalogValue.URL, catalogValue.Name), nil","\tdefault:","\t\t// defaulting to Artifact Hub","\t\treturn newArtifactHubClient(cs, catalogValue.URL, catalogValue.Name), nil","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,0,0]},{"id":47,"path":"pkg/hub/tektonhub.go","lines":["// Copyright Â© 2022 The Tekton Authors.","//","// Licensed under the Apache License, Version 2.0 (the \"License\");","// you may not use this file except in compliance with the License.","// You may obtain a copy of the License at","//","//     http://www.apache.org/licenses/LICENSE-2.0","//","// Unless required by applicable law or agreed to in writing, software","// distributed under the License is distributed on an \"AS IS\" BASIS,","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","// See the License for the specific language governing permissions and","// limitations under the License.","","package hub","","import (","\t\"context\"","\t\"encoding/json\"","\t\"fmt\"","\t\"strings\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"",")","","// tektonHubClient is a client for the Tekton Hub.","type tektonHubClient struct {","\tparams *params.Run","\turl    string","\tname   string","}","","// newTektonHubClient returns a new Tekton Hub client.","func newTektonHubClient(params *params.Run, url, name string) Client {","\treturn \u0026tektonHubClient{params: params, url: url, name: name}","}","","type resourceVersionDataResponseBody struct {","\t// ID is the unique id of resource's version","\tID *uint `json:\"id,omitempty\"`","\t// Version of resource","\tVersion *string `json:\"version,omitempty\"`","\t// Display name of version","\tDisplayName *string `json:\"displayName,omitempty\"`","\t// Description of version","\tDescription *string `json:\"description,omitempty\"`","\t// Minimum pipelines version the resource's version is compatible with","\tMinPipelinesVersion *string `json:\"minPipelinesVersion,omitempty\"`","\t// Raw URL of resource's yaml file of the version","\tRawURL *string `json:\"rawURL,omitempty\"`","\t// Web URL of resource's yaml file of the version","\tWebURL *string `json:\"webURL,omitempty\"`","\t// Timestamp when version was last updated","\tUpdatedAt *string `json:\"updatedAt,omitempty\"`","}","","type hubResourceResponseBody struct {","\t// ID is the unique id of the resource","\tID *uint `json:\"id,omitempty\"`","\t// Name of resource","\tName *string `json:\"name,omitempty\"`","\t// Kind of resource","\tKind *string `json:\"kind,omitempty\"`","\t// Latest version of resource","\tLatestVersion *resourceVersionDataResponseBody `json:\"latestVersion,omitempty\"`","\t// List of all versions of a resource","\tVersions []*resourceVersionDataResponseBody `json:\"versions,omitempty\"`","}","","type hubResource struct {","\tData *hubResourceResponseBody `json:\"data,omitempty\"`","}","","type hubResourceVersion struct {","\tData *resourceVersionDataResponseBody `json:\"data,omitempty\"`","}","","// GetResource gets a resource from the Tekton Hub.","func (t *tektonHubClient) GetResource(ctx context.Context, _, resource, kind string) (string, error) {","\tvar rawURL string","\tvar err error","","\tif strings.Contains(resource, \":\") {","\t\trawURL, err = t.getSpecificVersion(ctx, t.name, resource, kind)","\t} else {","\t\trawURL, err = t.getLatestVersion(ctx, t.name, resource, kind)","\t}","\tif err != nil {","\t\treturn \"\", fmt.Errorf(\"could not fetch remote %s %s, hub API returned: %w\", kind, resource, err)","\t}","","\tdata, err := t.params.Clients.GetURL(ctx, rawURL)","\tif err != nil {","\t\treturn \"\", fmt.Errorf(\"could not fetch remote %s %s, hub API returned: %w\", kind, resource, err)","\t}","\treturn string(data), err","}","","func (t *tektonHubClient) getSpecificVersion(ctx context.Context, catalogName, resource, kind string) (string, error) {","\tsplit := strings.Split(resource, \":\")","\tversion := split[len(split)-1]","\tresourceName := split[0]","\turl := fmt.Sprintf(\"%s/resource/%s/%s/%s/%s\", t.url, catalogName, kind, resourceName, version)","\thr := hubResourceVersion{}","\tdata, err := t.params.Clients.GetURL(ctx, url)","\tif err != nil {","\t\treturn \"\", fmt.Errorf(\"could not fetch specific %s version from the hub %s:%s: %w\", kind, resource, version, err)","\t}","\terr = json.Unmarshal(data, \u0026hr)","\tif err != nil {","\t\treturn \"\", err","\t}","\treturn fmt.Sprintf(\"%s/raw\", url), nil","}","","func (t *tektonHubClient) getLatestVersion(ctx context.Context, catalogName, resource, kind string) (string, error) {","\turl := fmt.Sprintf(\"%s/resource/%s/%s/%s\", t.url, catalogName, kind, resource)","\thr := new(hubResource)","\tdata, err := t.params.Clients.GetURL(ctx, url)","\tif err != nil {","\t\treturn \"\", err","\t}","\terr = json.Unmarshal(data, \u0026hr)","\tif err != nil {","\t\treturn \"\", err","\t}","","\treturn fmt.Sprintf(\"%s/%s/raw\", url, *hr.Data.LatestVersion.Version), nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,0,1,0]},{"id":48,"path":"pkg/kubeinteraction/cleanups.go","lines":["package kubeinteraction","","import (","\t\"context\"","\t\"fmt\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/formatting\"","\tpsort \"github.com/openshift-pipelines/pipelines-as-code/pkg/sort\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"go.uber.org/zap\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"knative.dev/pkg/apis\"",")","","func (k Interaction) CleanupPipelines(ctx context.Context, logger *zap.SugaredLogger, repo *v1alpha1.Repository, pr *tektonv1.PipelineRun, maxKeep int) error {","\tif _, ok := pr.GetAnnotations()[keys.OriginalPRName]; !ok {","\t\treturn fmt.Errorf(\"generated pipelinerun should have had the %s label for selection set but we could not find it\", keys.OriginalPRName)","\t}","","\t// Select PR by repository and by its true pipelineRun name (not auto generated one)","\tlabelSelector := fmt.Sprintf(\"%s=%s,%s=%s,%s=%s\",","\t\tkeys.Repository, formatting.CleanValueKubernetes(repo.GetName()), keys.OriginalPRName,","\t\tformatting.CleanValueKubernetes(pr.GetLabels()[keys.OriginalPRName]),","\t\tkeys.State, StateCompleted)","\tlogger.Infof(\"selecting pipelineruns by labels \\\"%s\\\" for deletion\", labelSelector)","","\tpruns, err := k.Run.Clients.Tekton.TektonV1().PipelineRuns(repo.GetNamespace()).List(ctx,","\t\tmetav1.ListOptions{LabelSelector: labelSelector})","\tif err != nil {","\t\treturn err","\t}","","\tfor c, prun := range psort.PipelineRunSortByCompletionTime(pruns.Items) {","\t\tprReason := prun.GetStatusCondition().GetCondition(apis.ConditionSucceeded).GetReason()","\t\tif prReason == tektonv1.PipelineRunReasonRunning.String() || prReason == tektonv1.PipelineRunReasonPending.String() {","\t\t\tlogger.Infof(\"skipping cleaning PipelineRun %s since the conditions.reason is %s\", prun.GetName(), prReason)","\t\t\tcontinue","\t\t}","","\t\tif c \u003e= maxKeep {","\t\t\tlogger.Infof(\"cleaning old PipelineRun: %s\", prun.GetName())","\t\t\terr := k.Run.Clients.Tekton.TektonV1().PipelineRuns(repo.GetNamespace()).Delete(","\t\t\t\tctx, prun.GetName(), metav1.DeleteOptions{})","\t\t\tif err != nil {","\t\t\t\treturn err","\t\t\t}","","\t\t\t// Try to Delete the secret created for git-clone basic-auth, it should have been created with a ownerRef on the pipelinerun and due being deleted when the pipelinerun is deleted","\t\t\t// but in some cases of conflicts and the ownerRef not being set, the secret is not deleted, and we need to delete it manually.","\t\t\tif secretName, ok := prun.GetAnnotations()[keys.GitAuthSecret]; ok {","\t\t\t\terr = k.Run.Clients.Kube.CoreV1().Secrets(repo.GetNamespace()).Delete(ctx, secretName, metav1.DeleteOptions{})","\t\t\t\tif err == nil {","\t\t\t\t\tlogger.Infof(\"secret %s attached to pipelinerun %s has been deleted\", secretName, prun.GetName())","\t\t\t\t}","\t\t\t}","\t\t}","\t}","","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,1,1,0,0,2,2,2,2,2,2,2,2,2,1,1,0,2,2,2,2,2,0,0,2,2,2,2,2,1,1,0,0,0,2,2,2,2,2,0,0,0,0,2,0]},{"id":49,"path":"pkg/kubeinteraction/events.go","lines":["package kubeinteraction","","import (","\t\"context\"","","\t\"github.com/google/go-github/v81/github\"","\tcorev1 \"k8s.io/api/core/v1\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"",")","","func (k Interaction) GetEvents(ctx context.Context, ns, objtype, name string) (*corev1.EventList, error) {","\tkclient := k.Run.Clients.Kube.CoreV1()","\tselector := kclient.Events(ns).GetFieldSelector(github.Ptr(name), github.Ptr(ns), github.Ptr(objtype), nil)","\tevents, err := kclient.Events(ns).List(ctx, metav1.ListOptions{FieldSelector: selector.String()})","\tif err != nil {","\t\treturn nil, err","\t}","\treturn events, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0]},{"id":50,"path":"pkg/kubeinteraction/kubeinteraction.go","lines":["package kubeinteraction","","import (","\t\"context\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\tktypes \"github.com/openshift-pipelines/pipelines-as-code/pkg/secrets/types\"","\tpipelinev1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"go.uber.org/zap\"","\tcorev1 \"k8s.io/api/core/v1\"",")","","type Interface interface {","\tCleanupPipelines(context.Context, *zap.SugaredLogger, *v1alpha1.Repository, *pipelinev1.PipelineRun, int) error","\tCreateSecret(ctx context.Context, ns string, secret *corev1.Secret) error","\tDeleteSecret(context.Context, *zap.SugaredLogger, string, string) error","\tUpdateSecretWithOwnerRef(context.Context, *zap.SugaredLogger, string, string, *pipelinev1.PipelineRun) error","\tGetSecret(context.Context, ktypes.GetSecretOpt) (string, error)","\tGetPodLogs(context.Context, string, string, string, int64) (string, error)","}","","type Interaction struct {","\tRun *params.Run","}","","// validate the interface implementation.","var _ Interface = (*Interaction)(nil)","","func NewKubernetesInteraction(c *params.Run) (*Interaction, error) {","\treturn \u0026Interaction{","\t\tRun: c,","\t}, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2]},{"id":51,"path":"pkg/kubeinteraction/labels.go","lines":["package kubeinteraction","","import (","\t\"fmt\"","\t\"strconv\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\tapipac \"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/formatting\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/versiondata\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"",")","","const (","\tStateStarted   = \"started\"","\tStateQueued    = \"queued\"","\tStateCompleted = \"completed\"","\tStateFailed    = \"failed\"",")","","func AddLabelsAndAnnotations(event *info.Event, pipelineRun *tektonv1.PipelineRun, repo *apipac.Repository, providerConfig *info.ProviderConfig, paramsRun *params.Run) error {","\tif event == nil {","\t\treturn fmt.Errorf(\"event should not be nil\")","\t}","\tparamsinfo := paramsRun.Info","\t// Add labels on the soon-to-be created pipelinerun so UI/CLI can easily","\t// query them.","\tlabels := map[string]string{","\t\t// These keys are used in LabelSelector query, so we are keeping in Labels as it is.","\t\t// But adding same keys to Annotations so UI/CLI can fetch the actual value instead of modified value","\t\t\"app.kubernetes.io/managed-by\": pipelinesascode.GroupName,","\t\t\"app.kubernetes.io/version\":    formatting.CleanValueKubernetes(versiondata.Version),","\t\tkeys.URLOrg:                    formatting.CleanValueKubernetes(event.Organization),","\t\tkeys.URLRepository:             formatting.CleanValueKubernetes(event.Repository),","\t\tkeys.SHA:                       formatting.CleanValueKubernetes(event.SHA),","\t\tkeys.Repository:                formatting.CleanValueKubernetes(repo.GetName()),","\t\tkeys.State:                     StateStarted,","\t\tkeys.EventType:                 formatting.CleanValueKubernetes(event.EventType),","\t}","","\tannotations := map[string]string{","\t\tkeys.ShaTitle:      event.SHATitle,","\t\tkeys.ShaURL:        event.SHAURL,","\t\tkeys.RepoURL:       event.URL,","\t\tkeys.SourceRepoURL: event.HeadURL,","\t\tkeys.URLOrg:        event.Organization,","\t\tkeys.URLRepository: event.Repository,","\t\tkeys.SHA:           event.SHA,","\t\tkeys.Sender:        event.Sender,","\t\tkeys.EventType:     event.EventType,","\t\tkeys.Branch:        event.BaseBranch,","\t\tkeys.SourceBranch:  event.HeadBranch,","\t\tkeys.Repository:    repo.GetName(),","\t\tkeys.GitProvider:   providerConfig.Name,","\t\tkeys.ControllerInfo: fmt.Sprintf(`{\"name\":\"%s\",\"configmap\":\"%s\",\"secret\":\"%s\", \"gRepo\": \"%s\"}`,","\t\t\tparamsinfo.Controller.Name, paramsinfo.Controller.Configmap, paramsinfo.Controller.Secret, paramsinfo.Controller.GlobalRepository),","\t}","","\tif event.PullRequestNumber != 0 {","\t\tlabels[keys.PullRequest] = strconv.Itoa(event.PullRequestNumber)","\t\tannotations[keys.PullRequest] = strconv.Itoa(event.PullRequestNumber)","\t}","","\t// TODO: move to provider specific function","\tif providerConfig.Name == \"github\" || providerConfig.Name == \"github-enterprise\" {","\t\tif event.InstallationID != -1 {","\t\t\tannotations[keys.InstallationID] = strconv.FormatInt(event.InstallationID, 10)","\t\t}","\t\tif event.GHEURL != \"\" {","\t\t\tannotations[keys.GHEURL] = event.GHEURL","\t\t}","\t}","","\t// GitLab","\tif event.SourceProjectID != 0 {","\t\tannotations[keys.SourceProjectID] = strconv.Itoa(int(event.SourceProjectID))","\t}","\tif event.TargetProjectID != 0 {","\t\tannotations[keys.TargetProjectID] = strconv.Itoa(int(event.TargetProjectID))","\t}","","\tif value, ok := pipelineRun.GetObjectMeta().GetAnnotations()[keys.CancelInProgress]; ok {","\t\tlabels[keys.CancelInProgress] = value","\t}","","\tfor k, v := range labels {","\t\tpipelineRun.Labels[k] = v","\t}","\tfor k, v := range annotations {","\t\tpipelineRun.Annotations[k] = v","\t}","","\t// Add annotations to PipelineRuns to integrate with Tekton Results","\terr := AddResultsAnnotation(event, pipelineRun)","\tif err != nil {","\t\treturn fmt.Errorf(\"failed to add results annotations with error: %w\", err)","\t}","","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,0,0,2,1,1,1,1,1,1,0,0,0,2,1,1,2,1,1,0,2,2,2,0,2,2,2,2,2,2,0,0,2,2,1,1,0,2,0]},{"id":52,"path":"pkg/kubeinteraction/pod_logs.go","lines":["package kubeinteraction","","import (","\t\"context\"","\t\"io\"","","\t\"github.com/google/go-github/v81/github\"","\tcorev1 \"k8s.io/api/core/v1\"",")","","// GetPodLogs of a ns on a podname and container, tailLines is the number of","// line to tail -1 mean unlimited.","func (k Interaction) GetPodLogs(ctx context.Context, ns, podName, containerName string, tailLines int64) (string, error) {","\tkclient := k.Run.Clients.Kube.CoreV1()","\tpdOpts := \u0026corev1.PodLogOptions{","\t\tContainer: containerName,","\t}","\tif tailLines \u003e 0 {","\t\tpdOpts.TailLines = github.Ptr(tailLines)","\t}","\tios, err := kclient.Pods(ns).GetLogs(podName, pdOpts).Stream(ctx)","\tif err != nil {","\t\treturn \"\", err","\t}","\tlog, err := io.ReadAll(ios)","\treturn string(log), err","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0]},{"id":53,"path":"pkg/kubeinteraction/resultsannotation.go","lines":["package kubeinteraction","","import (","\t\"encoding/json\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"",")","","type ResultAnnotation struct {","\tRepo          string `json:\"repo\"`","\tCommit        string `json:\"commit\"`","\tEventType     string `json:\"eventType\"`","\tPullRequestID int    `json:\"pull_request-id,omitempty\"`","}","","// Add annotation to PipelineRuns produced by PaC for capturing additional","// data specific for TektonResults.","func AddResultsAnnotation(event *info.Event, pipelineRun *tektonv1.PipelineRun) error {","\tresultAnnotation := ResultAnnotation{","\t\tRepo:          event.Repository,","\t\tCommit:        event.SHA,","\t\tEventType:     event.EventType,","\t\tPullRequestID: event.PullRequestNumber,","\t}","","\tresAnnotationJSON, err := json.Marshal(resultAnnotation)","\tif err != nil {","\t\treturn err","\t}","","\t// append the result annotation","\tpipelineRun.Annotations[keys.ResultsRecordSummary] = string(resAnnotationJSON)","","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,1,1,0,0,2,2,2,0]},{"id":54,"path":"pkg/kubeinteraction/secrets.go","lines":["package kubeinteraction","","import (","\t\"context\"","\t\"fmt\"","","\tktypes \"github.com/openshift-pipelines/pipelines-as-code/pkg/secrets/types\"","\tpipelinev1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"go.uber.org/zap\"","\tcorev1 \"k8s.io/api/core/v1\"","\t\"k8s.io/apimachinery/pkg/api/errors\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/client-go/util/retry\"",")","","func (k Interaction) GetSecret(ctx context.Context, secretopt ktypes.GetSecretOpt) (string, error) {","\tsecret, err := k.Run.Clients.Kube.CoreV1().Secrets(secretopt.Namespace).Get(","\t\tctx, secretopt.Name, metav1.GetOptions{})","\tif err != nil {","\t\treturn \"\", err","\t}","\treturn string(secret.Data[secretopt.Key]), nil","}","","// DeleteSecret deletes the secret created for git-clone basic-auth.","func (k Interaction) DeleteSecret(ctx context.Context, _ *zap.SugaredLogger, targetNamespace, secretName string) error {","\terr := k.Run.Clients.Kube.CoreV1().Secrets(targetNamespace).Delete(ctx, secretName, metav1.DeleteOptions{})","\tif err != nil \u0026\u0026 !errors.IsNotFound(err) {","\t\treturn err","\t}","\treturn nil","}","","// UpdateSecretWithOwnerRef updates the secret with ownerReference.","func (k Interaction) UpdateSecretWithOwnerRef(ctx context.Context, logger *zap.SugaredLogger, targetNamespace, secretName string, pr *pipelinev1.PipelineRun) error {","\tcontrollerOwned := false","\townerRef := \u0026metav1.OwnerReference{","\t\tAPIVersion:         pr.GetGroupVersionKind().GroupVersion().String(),","\t\tKind:               pr.GetGroupVersionKind().Kind,","\t\tName:               pr.GetName(),","\t\tUID:                pr.GetUID(),","\t\tBlockOwnerDeletion: \u0026controllerOwned,","\t\tController:         \u0026controllerOwned,","\t}","\terr := retry.RetryOnConflict(retry.DefaultRetry, func() error {","\t\tsecret, err := k.Run.Clients.Kube.CoreV1().Secrets(targetNamespace).Get(ctx, secretName, metav1.GetOptions{})","\t\tif err != nil {","\t\t\treturn err","\t\t}","","\t\tsecret.OwnerReferences = []metav1.OwnerReference{*ownerRef}","","\t\t_, err = k.Run.Clients.Kube.CoreV1().Secrets(targetNamespace).Update(ctx, secret, metav1.UpdateOptions{})","\t\tif err != nil {","\t\t\tlogger.Infof(\"failed to update secret, retrying  %v/%v: %v\", targetNamespace, secretName, err)","\t\t\treturn err","\t\t}","\t\treturn nil","\t})","\tif err != nil {","\t\treturn fmt.Errorf(\"failed to update secret with ownerRef %v/%v: %w\", targetNamespace, secretName, err)","\t}","\treturn nil","}","","func (k Interaction) CreateSecret(ctx context.Context, ns string, secret *corev1.Secret) error {","\t_, err := k.Run.Clients.Kube.CoreV1().Secrets(ns).Create(ctx, secret, metav1.CreateOptions{})","\treturn err","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,2,2,2,1,1,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,2,2,2,1,1,1,2,0,2,1,1,2,0,0,1,1,1,1]},{"id":55,"path":"pkg/kubeinteraction/status/task_status.go","lines":["package status","","import (","\t\"context\"","\t\"fmt\"","\t\"regexp\"","\t\"strings\"","\t\"unicode/utf8\"","","\tpacv1alpha1 \"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"github.com/tektoncd/pipeline/pkg/client/clientset/versioned\"","\t\"k8s.io/apimachinery/pkg/api/errors\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"",")","","var reasonMessageReplacementRegexp = regexp.MustCompile(`\\(image: .*`)","","const maxErrorSnippetCharacterLimit = 65535 // This is the maximum size allowed by Github check run logs and may apply to all other providers","","// GetTaskRunStatusForPipelineTask takes a minimal embedded status child reference and returns the actual TaskRunStatus","// for the PipelineTask. It returns an error if the child reference's kind isn't TaskRun.","func GetTaskRunStatusForPipelineTask(ctx context.Context, client versioned.Interface, ns string, childRef tektonv1.ChildStatusReference) (*tektonv1.TaskRunStatus, error) {","\tif childRef.Kind != \"TaskRun\" {","\t\treturn nil, fmt.Errorf(\"could not fetch status for PipelineTask %s: should have kind TaskRun, but is %s\", childRef.PipelineTaskName, childRef.Kind)","\t}","","\ttr, err := client.TektonV1().TaskRuns(ns).Get(ctx, childRef.Name, metav1.GetOptions{})","\tif err != nil \u0026\u0026 !errors.IsNotFound(err) {","\t\treturn nil, err","\t}","\tif tr == nil {","\t\treturn nil, nil","\t}","","\treturn \u0026tr.Status, nil","}","","// GetStatusFromTaskStatusOrFromAsking will return the status of the taskruns,","// it would use the embedded one if it's available (pre tekton 0.44.0) or try","// to get it from the child references.","func GetStatusFromTaskStatusOrFromAsking(ctx context.Context, pr *tektonv1.PipelineRun, run *params.Run) map[string]*tektonv1.PipelineRunTaskRunStatus {","\ttrStatus := map[string]*tektonv1.PipelineRunTaskRunStatus{}","\tfor _, cr := range pr.Status.ChildReferences {","\t\tts, err := GetTaskRunStatusForPipelineTask(","\t\t\tctx, run.Clients.Tekton, pr.GetNamespace(), cr,","\t\t)","\t\tif err != nil {","\t\t\trun.Clients.Log.Warnf(\"cannot get taskrun status pr %s ns: %s err: %w\", pr.GetName(), pr.GetNamespace(), err)","\t\t\tcontinue","\t\t}","\t\tif ts == nil {","\t\t\trun.Clients.Log.Warnf(\"cannot get taskrun status pr %s ns: %s, ts come back nil?\", pr.GetName(), pr.GetNamespace(), err)","\t\t\tcontinue","\t\t}","\t\t// search in taskSpecs if there is a displayName for that status","\t\tif pr.Spec.PipelineSpec != nil \u0026\u0026 pr.Spec.PipelineSpec.Tasks != nil {","\t\t\tfor _, taskSpec := range pr.Spec.PipelineSpec.Tasks {","\t\t\t\tif ts.TaskSpec != nil \u0026\u0026 taskSpec.Name == cr.PipelineTaskName {","\t\t\t\t\tts.TaskSpec.DisplayName = taskSpec.DisplayName","\t\t\t\t}","\t\t\t}","\t\t}","\t\ttrStatus[cr.Name] = \u0026tektonv1.PipelineRunTaskRunStatus{","\t\t\tPipelineTaskName: cr.PipelineTaskName,","\t\t\tStatus:           ts,","\t\t}","\t}","\treturn trStatus","}","","// CollectFailedTasksLogSnippet collects all tasks information we are interested in.","// should really be in a tektoninteractions package but i lack imagination at the moment.","func CollectFailedTasksLogSnippet(ctx context.Context, cs *params.Run, kinteract kubeinteraction.Interface, pr *tektonv1.PipelineRun, numLines int64) map[string]pacv1alpha1.TaskInfos {","\tfailureReasons := map[string]pacv1alpha1.TaskInfos{}","\tif pr == nil {","\t\treturn failureReasons","\t}","","\ttrStatus := GetStatusFromTaskStatusOrFromAsking(ctx, pr, cs)","\tfor _, task := range trStatus {","\t\tif task.Status == nil {","\t\t\tcontinue","\t\t}","\t\tif len(task.Status.Conditions) == 0 {","\t\t\tcontinue","\t\t}","\t\tti := pacv1alpha1.TaskInfos{","\t\t\tName:           task.PipelineTaskName,","\t\t\tMessage:        reasonMessageReplacementRegexp.ReplaceAllString(task.Status.Conditions[0].Message, \"\"),","\t\t\tCompletionTime: task.Status.CompletionTime,","\t\t\tReason:         task.Status.Conditions[0].Reason,","\t\t}","\t\tif task.Status.TaskSpec != nil {","\t\t\tti.DisplayName = task.Status.TaskSpec.DisplayName","\t\t}","\t\t// don't check for pod logs into those","\t\tif ti.Reason == \"TaskRunValidationFailed\" || ti.Reason == tektonv1.TaskRunReasonCancelled.String() || ti.Reason == tektonv1.TaskRunReasonTimedOut.String() || ti.Reason == tektonv1.TaskRunReasonImagePullFailed.String() {","\t\t\tfailureReasons[task.PipelineTaskName] = ti","\t\t\tcontinue","\t\t} else if ti.Reason != tektonv1.PipelineRunReasonFailed.String() {","\t\t\tcontinue","\t\t}","","\t\tif kinteract != nil {","\t\t\tfor _, step := range task.Status.Steps {","\t\t\t\tif step.Terminated != nil \u0026\u0026 step.Terminated.ExitCode != 0 {","\t\t\t\t\tlog, err := kinteract.GetPodLogs(ctx, pr.GetNamespace(), task.Status.PodName, step.Container, numLines)","\t\t\t\t\tif err != nil {","\t\t\t\t\t\tcs.Clients.Log.Errorf(\"cannot get pod logs: %w\", err)","\t\t\t\t\t\tcontinue","\t\t\t\t\t}","\t\t\t\t\ttrimmed := strings.TrimSpace(log)","\t\t\t\t\tif strings.HasSuffix(trimmed, \" Skipping step because a previous step failed\") {","\t\t\t\t\t\tcontinue","\t\t\t\t\t}","\t\t\t\t\t// GitHub's character limit is actually in bytes, not unicode characters","\t\t\t\t\t// Truncate to maxErrorSnippetCharacterLimit bytes, then trim to last valid UTF-8 boundary","\t\t\t\t\tif len(trimmed) \u003e maxErrorSnippetCharacterLimit {","\t\t\t\t\t\ttrimmed = trimmed[:maxErrorSnippetCharacterLimit]","\t\t\t\t\t\t// Trim further to last valid rune boundary to ensure valid UTF-8","\t\t\t\t\t\tr, size := utf8.DecodeLastRuneInString(trimmed)","\t\t\t\t\t\tfor r == utf8.RuneError \u0026\u0026 size \u003e 0 {","\t\t\t\t\t\t\ttrimmed = trimmed[:len(trimmed)-size]","\t\t\t\t\t\t\tr, size = utf8.DecodeLastRuneInString(trimmed)","\t\t\t\t\t\t}","\t\t\t\t\t}","\t\t\t\t\tti.LogSnippet = trimmed","\t\t\t\t}","\t\t\t}","\t\t}","\t\tfailureReasons[task.PipelineTaskName] = ti","\t}","\treturn failureReasons","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,2,2,1,1,2,1,1,0,2,0,0,0,0,0,2,2,2,2,2,2,2,2,2,0,2,1,1,0,0,2,2,2,2,2,0,0,2,2,2,2,0,2,0,0,0,0,2,2,2,1,1,0,2,2,2,1,0,2,1,0,2,2,2,2,2,2,2,2,2,0,2,1,1,2,2,0,0,2,2,2,2,2,1,1,0,2,2,1,0,0,0,2,2,2,2,2,1,1,1,0,2,0,0,0,2,0,2,0]},{"id":56,"path":"pkg/kubeinteraction/wait.go","lines":["package kubeinteraction","","import (","\t\"context\"","\t\"fmt\"","\t\"time\"","","\t\"k8s.io/apimachinery/pkg/util/wait\"",")","","const (","\tinterval = 1 * time.Second",")","","func PollImmediateWithContext(ctx context.Context, pollTimeout time.Duration, fn func() (bool, error)) error {","\t//nolint: staticcheck","\treturn wait.PollImmediate(interval, pollTimeout, func() (bool, error) {","\t\tselect {","\t\tcase \u003c-ctx.Done():","\t\t\treturn true, fmt.Errorf(\"polling timed out, pipelinerun has exceeded its timeout: %v\", pollTimeout)","\t\tdefault:","\t\t}","\t\treturn fn()","\t})","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,1,1,0,0,2,0,0]},{"id":57,"path":"pkg/llm/analyzer.go","lines":["package llm","","import (","\t\"context\"","\t\"fmt\"","\t\"time\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/cel\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction\"","\tllmcontext \"github.com/openshift-pipelines/pipelines-as-code/pkg/llm/context\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/llm/ltypes\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"go.uber.org/zap\"",")","","// AnalysisResult represents the result of an LLM analysis.","type AnalysisResult struct {","\tRole     string","\tResponse *ltypes.AnalysisResponse","\tError    error","}","","// Analyzer coordinates the LLM analysis process.","type Analyzer struct {","\trun       *params.Run","\tkinteract kubeinteraction.Interface","\tfactory   *Factory","\tassembler *llmcontext.Assembler","\tlogger    *zap.SugaredLogger","}","","// NewAnalyzer creates a new LLM analyzer.","func NewAnalyzer(run *params.Run, kinteract kubeinteraction.Interface, logger *zap.SugaredLogger) *Analyzer {","\treturn \u0026Analyzer{","\t\trun:       run,","\t\tkinteract: kinteract,","\t\tfactory:   NewFactory(run, kinteract),","\t\tassembler: llmcontext.NewAssembler(run, kinteract, logger),","\t\tlogger:    logger,","\t}","}","","// AnalyzeRequest represents a request for LLM analysis.","type AnalyzeRequest struct {","\tPipelineRun *tektonv1.PipelineRun","\tEvent       *info.Event","\tRepository  *v1alpha1.Repository","\tProvider    provider.Interface","}","","// Analyze performs LLM analysis based on the repository configuration.","func (a *Analyzer) Analyze(ctx context.Context, request *AnalyzeRequest) ([]AnalysisResult, error) {","\tif request == nil {","\t\treturn nil, fmt.Errorf(\"analysis request is required\")","\t}","\tif request.Repository == nil {","\t\treturn nil, nil","\t}","","\tif request.Repository.Spec.Settings == nil || request.Repository.Spec.Settings.AIAnalysis == nil {","\t\ta.logger.With(","\t\t\t\"repository\", request.Repository.Name,","\t\t\t\"namespace\", request.Repository.Namespace,","\t\t).Debug(\"No AI analysis configuration found, skipping analysis\")","\t\treturn nil, nil","\t}","","\tconfig := request.Repository.Spec.Settings.AIAnalysis","\tif !config.Enabled {","\t\ta.logger.With(","\t\t\t\"repository\", request.Repository.Name,","\t\t\t\"namespace\", request.Repository.Namespace,","\t\t).Debug(\"AI analysis is disabled, skipping analysis\")","\t\treturn nil, nil","\t}","","\tanalysisLogger := a.logger.With(","\t\t\"provider\", config.Provider,","\t\t\"pipeline_run\", request.PipelineRun.Name,","\t\t\"namespace\", request.PipelineRun.Namespace,","\t\t\"repository\", request.Repository.Name,","\t\t\"roles_count\", len(config.Roles),","\t)","","\tanalysisLogger.Info(\"Starting LLM analysis\")","","\tif err := a.validateConfig(config); err != nil {","\t\tanalysisLogger.With(\"error\", err).Error(\"Invalid AI analysis configuration\")","\t\treturn nil, fmt.Errorf(\"invalid AI analysis configuration: %w\", err)","\t}","","\t// Secret must be in the same namespace as the Repository CR","\tnamespace := request.Repository.Namespace","","\t// Build CEL context for role filtering","\tcelContext, err := a.assembler.BuildCELContext(request.PipelineRun, request.Event, request.Repository)","\tif err != nil {","\t\tanalysisLogger.With(\"error\", err).Error(\"Failed to build CEL context\")","\t\treturn nil, fmt.Errorf(\"failed to build CEL context: %w\", err)","\t}","","\t// Process each role","\tresults := []AnalysisResult{}","\tcontextCache := make(map[string]map[string]any)","","\tfor _, role := range config.Roles {","\t\troleLogger := analysisLogger.With(\"role\", role.Name)","","\t\tshouldTrigger, err := a.shouldTriggerRole(role, celContext)","\t\tif err != nil {","\t\t\troleLogger.With(\"error\", err, \"cel_expression\", role.OnCEL).Warn(\"Failed to evaluate CEL expression\")","\t\t\tresults = append(results, AnalysisResult{","\t\t\t\tRole:  role.Name,","\t\t\t\tError: fmt.Errorf(\"CEL evaluation failed: %w\", err),","\t\t\t})","\t\t\tcontinue","\t\t}","","\t\tif !shouldTrigger {","\t\t\troleLogger.With(\"cel_expression\", role.OnCEL).Debug(\"Role did not match CEL condition, skipping\")","\t\t\tcontinue","\t\t}","","\t\troleLogger.Info(\"Executing analysis role\")","","\t\tcontextKey := getContextCacheKey(role.ContextItems)","\t\tvar roleContext map[string]any","\t\tvar cached bool","\t\tif roleContext, cached = contextCache[contextKey]; !cached {","\t\t\troleContext, err = a.assembler.BuildContext(","\t\t\t\tctx,","\t\t\t\trequest.PipelineRun,","\t\t\t\trequest.Event,","\t\t\t\trole.ContextItems,","\t\t\t\trequest.Provider,","\t\t\t)","\t\t\tif err != nil {","\t\t\t\troleLogger.With(\"error\", err).Warn(\"Failed to build context for role\")","\t\t\t\tresults = append(results, AnalysisResult{","\t\t\t\t\tRole:  role.Name,","\t\t\t\t\tError: fmt.Errorf(\"context build failed: %w\", err),","\t\t\t\t})","\t\t\t\tcontinue","\t\t\t}","\t\t\tcontextCache[contextKey] = roleContext","\t\t}","","\t\t// Create LLM client for this role","\t\tclient, err := a.createClient(ctx, config, namespace, \u0026role)","\t\tif err != nil {","\t\t\troleLogger.With(\"error\", err).Warn(\"Failed to create LLM client for role\")","\t\t\tresults = append(results, AnalysisResult{","\t\t\t\tRole:  role.Name,","\t\t\t\tError: fmt.Errorf(\"client creation failed: %w\", err),","\t\t\t})","\t\t\tcontinue","\t\t}","","\t\t// Create analysis request","\t\tanalysisRequest := \u0026ltypes.AnalysisRequest{","\t\t\tPrompt:         role.Prompt,","\t\t\tContext:        roleContext,","\t\t\tMaxTokens:      config.MaxTokens,","\t\t\tTimeoutSeconds: config.TimeoutSeconds,","\t\t}","","\t\t// Apply defaults","\t\tif analysisRequest.MaxTokens == 0 {","\t\t\tanalysisRequest.MaxTokens = ltypes.DefaultConfig.MaxTokens","\t\t}","\t\tif analysisRequest.TimeoutSeconds == 0 {","\t\t\tanalysisRequest.TimeoutSeconds = ltypes.DefaultConfig.TimeoutSeconds","\t\t}","","\t\troleLogger.With(","\t\t\t\"max_tokens\", analysisRequest.MaxTokens,","\t\t\t\"timeout_seconds\", analysisRequest.TimeoutSeconds,","\t\t\t\"context_items\", len(roleContext),","\t\t).Debug(\"Sending analysis request to LLM\")","","\t\t// Perform analysis","\t\tvar response *ltypes.AnalysisResponse","\t\tvar analysisErr error","\t\tanalysisStart := time.Now()","","\t\tconst maxRetries = 3","\t\tconst retryDelay = 2 * time.Second","","\t\tfor attempt := 1; attempt \u003c= maxRetries; attempt++ {","\t\t\tresponse, analysisErr = client.Analyze(ctx, analysisRequest)","\t\t\tif analysisErr == nil {","\t\t\t\tbreak // Success","\t\t\t}","","\t\t\troleLogger.With(","\t\t\t\t\"error\", analysisErr,","\t\t\t\t\"attempt\", attempt,","\t\t\t\t\"max_attempts\", maxRetries,","\t\t\t).Warn(\"LLM analysis attempt failed\")","","\t\t\tif attempt \u003c maxRetries {","\t\t\t\ttimer := time.NewTimer(retryDelay)","\t\t\t\tselect {","\t\t\t\tcase \u003c-timer.C:","\t\t\t\tcase \u003c-ctx.Done():","\t\t\t\t\troleLogger.With(\"context_error\", ctx.Err()).Warn(\"Context cancelled during retry backoff\")","\t\t\t\t\tanalysisErr = fmt.Errorf(\"context cancelled: %w\", ctx.Err())","\t\t\t\t\tattempt = maxRetries","\t\t\t\t}","\t\t\t\tif !timer.Stop() {","\t\t\t\t\tselect {","\t\t\t\t\tcase \u003c-timer.C:","\t\t\t\t\tdefault:","\t\t\t\t\t}","\t\t\t\t}","\t\t\t}","\t\t}","\t\tanalysisDuration := time.Since(analysisStart)","","\t\tif analysisErr != nil {","\t\t\troleLogger.With(","\t\t\t\t\"error\", analysisErr,","\t\t\t\t\"duration\", analysisDuration,","\t\t\t).Warn(\"LLM analysis failed for role after all retries\")","\t\t\tresults = append(results, AnalysisResult{","\t\t\t\tRole:  role.Name,","\t\t\t\tError: analysisErr,","\t\t\t})","\t\t\tcontinue","\t\t}","","\t\troleLogger.With(","\t\t\t\"tokens_used\", response.TokensUsed,","\t\t\t\"duration\", analysisDuration,","\t\t\t\"response_length\", len(response.Content),","\t\t).Info(\"LLM analysis completed successfully\")","","\t\tresults = append(results, AnalysisResult{","\t\t\tRole:     role.Name,","\t\t\tResponse: response,","\t\t})","\t}","","\tanalysisLogger.With(","\t\t\"total_results\", len(results),","\t\t\"successful_analyses\", countSuccessfulResults(results),","\t\t\"failed_analyses\", countFailedResults(results),","\t).Info(\"LLM analysis completed\")","","\treturn results, nil","}","","// getContextCacheKey generates a unique key for a context configuration.","func getContextCacheKey(config *v1alpha1.ContextConfig) string {","\tif config == nil {","\t\treturn \"default\"","\t}","\tmaxLines := 0","\tif config.ContainerLogs != nil {","\t\tmaxLines = config.ContainerLogs.GetMaxLines()","\t}","","\treturn fmt.Sprintf(\"commit:%t-pr:%t-error:%t-logs:%t-%d\",","\t\tconfig.CommitContent,","\t\tconfig.PRContent,","\t\tconfig.ErrorContent,","\t\tconfig.ContainerLogs != nil \u0026\u0026 config.ContainerLogs.Enabled,","\t\tmaxLines,","\t)","}","","// countSuccessfulResults counts the number of successful analysis results.","func countSuccessfulResults(results []AnalysisResult) int {","\tcount := 0","\tfor _, result := range results {","\t\tif result.Error == nil \u0026\u0026 result.Response != nil {","\t\t\tcount++","\t\t}","\t}","\treturn count","}","","// countFailedResults counts the number of failed analysis results.","func countFailedResults(results []AnalysisResult) int {","\tcount := 0","\tfor _, result := range results {","\t\tif result.Error != nil {","\t\t\tcount++","\t\t}","\t}","\treturn count","}","","// shouldTriggerRole evaluates the CEL expression to determine if a role should be triggered.","func (a *Analyzer) shouldTriggerRole(role v1alpha1.AnalysisRole, celContext map[string]any) (bool, error) {","\tif role.OnCEL == \"\" {","\t\treturn true, nil","\t}","","\tresult, err := cel.Value(role.OnCEL, celContext[\"body\"],","\t\tmake(map[string]string), // headers - empty for pipeline context","\t\tmake(map[string]string), // pac params - empty for now","\t\tmake(map[string]any))    // files - empty for pipeline context","\tif err != nil {","\t\treturn false, fmt.Errorf(\"failed to evaluate CEL expression '%s': %w\", role.OnCEL, err)","\t}","","\tif boolVal, ok := result.Value().(bool); ok {","\t\treturn boolVal, nil","\t}","","\treturn false, fmt.Errorf(\"CEL expression '%s' did not return boolean value\", role.OnCEL)","}","","// validateConfig validates the AI analysis configuration.","func (a *Analyzer) validateConfig(config *v1alpha1.AIAnalysisConfig) error {","\tif config.Provider == \"\" {","\t\treturn fmt.Errorf(\"provider is required\")","\t}","","\tif config.TokenSecretRef == nil {","\t\treturn fmt.Errorf(\"token secret reference is required\")","\t}","","\tif len(config.Roles) == 0 {","\t\treturn fmt.Errorf(\"at least one analysis role is required\")","\t}","","\tfor i, role := range config.Roles {","\t\tif role.Name == \"\" {","\t\t\treturn fmt.Errorf(\"role[%d]: name is required\", i)","\t\t}","","\t\tif role.Prompt == \"\" {","\t\t\treturn fmt.Errorf(\"role[%d]: prompt is required\", i)","\t\t}","","\t\toutput := role.GetOutput()","\t\tif output != \"pr-comment\" {","\t\t\treturn fmt.Errorf(\"role[%d]: invalid output destination '%s' (only 'pr-comment' is currently supported)\", i, output)","\t\t}","\t}","","\treturn nil","}","","// createClient creates an LLM client based on the configuration and role.","func (a *Analyzer) createClient(ctx context.Context, config *v1alpha1.AIAnalysisConfig, namespace string, role *v1alpha1.AnalysisRole) (ltypes.Client, error) {","\tclientConfig := \u0026ClientConfig{","\t\tProvider:       ltypes.AIProvider(config.Provider),","\t\tAPIURL:         config.GetAPIURL(),","\t\tModel:          role.GetModel(),","\t\tTokenSecretRef: config.TokenSecretRef,","\t\tTimeoutSeconds: config.TimeoutSeconds,","\t\tMaxTokens:      config.MaxTokens,","\t}","","\tif err := a.factory.ValidateConfig(clientConfig); err != nil {","\t\treturn nil, fmt.Errorf(\"invalid client configuration: %w\", err)","\t}","","\treturn a.factory.CreateClient(ctx, clientConfig, namespace)","}","","// GetSupportedProviders returns the list of supported LLM providers.","func (a *Analyzer) GetSupportedProviders() []ltypes.AIProvider {","\treturn a.factory.GetSupportedProviders()","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,1,1,0,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,0,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,0,0,0,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,0,0,0,1,1,1,1,1,1,0,1,0,0,0,1,1,1,1,1,1,0,1,0,0,0,2,2,2,2,0,2,2,2,2,2,2,2,0,2,2,2,0,1,0,0,0,2,2,2,2,0,2,2,2,0,2,2,2,0,2,2,2,2,0,2,2,2,0,2,2,2,2,0,0,2,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,0,1,1,1]},{"id":58,"path":"pkg/llm/context/assembler.go","lines":["package context","","import (","\t\"context\"","\t\"encoding/json\"","\t\"fmt\"","\t\"strings\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction\"","\tkstatus \"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction/status\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/sort\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"go.uber.org/zap\"",")","","const (","\t// DefaultMaxLogLines is the default maximum number of log lines to include in context.","\tDefaultMaxLogLines = 50",")","","// Assembler builds context data for LLM analysis from pipeline and event information.","type Assembler struct {","\trun       *params.Run","\tkinteract kubeinteraction.Interface","\tlogger    *zap.SugaredLogger","}","","// NewAssembler creates a new context assembler.","func NewAssembler(run *params.Run, kinteract kubeinteraction.Interface, logger *zap.SugaredLogger) *Assembler {","\treturn \u0026Assembler{","\t\trun:       run,","\t\tkinteract: kinteract,","\t\tlogger:    logger,","\t}","}","","// BuildContext assembles context data based on the provided configuration.","func (a *Assembler) BuildContext(","\tctx context.Context,","\tpipelineRun *tektonv1.PipelineRun,","\tevent *info.Event,","\tcontextConfig *v1alpha1.ContextConfig,","\tprovider provider.Interface,",") (map[string]any, error) {","\tcontextData := make(map[string]any)","","\tif contextConfig == nil {","\t\treturn a.buildBasicPipelineContext(pipelineRun, event), nil","\t}","","\tif contextConfig.CommitContent {","\t\tif commitData, err := a.buildCommitContent(ctx, event, provider); err != nil {","\t\t\ta.logger.Warnf(\"we couldn't retrieve the commit details. this may limit the analysis, but we'll proceed with the available information. (error: %v)\", err)","\t\t} else {","\t\t\tcontextData[\"commit\"] = commitData","\t\t}","\t}","","\tif contextConfig.PRContent {","\t\tif prData, err := a.buildPRContent(ctx, event, provider); err != nil {","\t\t\ta.logger.Warnf(\"Failed to build PR content: %v\", err)","\t\t} else {","\t\t\tcontextData[\"pull_request\"] = prData","\t\t}","\t}","","\tif contextConfig.ErrorContent {","\t\tif errorData := a.buildErrorContent(ctx, pipelineRun); errorData != nil {","\t\t\tcontextData[\"errors\"] = errorData","\t\t}","\t}","","\tif contextConfig.ContainerLogs != nil \u0026\u0026 contextConfig.ContainerLogs.Enabled {","\t\tmaxLines := contextConfig.ContainerLogs.MaxLines","\t\tif maxLines == 0 {","\t\t\tmaxLines = DefaultMaxLogLines","\t\t}","\t\tif logData := a.buildContainerLogs(ctx, pipelineRun, maxLines); logData != nil {","\t\t\tcontextData[\"logs\"] = logData","\t\t}","\t}","","\t// Always include basic pipeline information","\tcontextData[\"pipeline\"] = a.buildBasicPipelineContext(pipelineRun, event)","","\treturn contextData, nil","}","","// buildBasicPipelineContext creates basic pipeline context information.","func (a *Assembler) buildBasicPipelineContext(pipelineRun *tektonv1.PipelineRun, event *info.Event) map[string]any {","\tpipelineData := map[string]any{","\t\t\"name\":      pipelineRun.Name,","\t\t\"namespace\": pipelineRun.Namespace,","\t\t\"status\":    \"unknown\",","\t}","","\tif len(pipelineRun.Status.Conditions) \u003e 0 {","\t\tcondition := pipelineRun.Status.Conditions[0]","\t\tpipelineData[\"status\"] = condition.Status","\t\tpipelineData[\"reason\"] = condition.Reason","\t\tpipelineData[\"message\"] = condition.Message","\t}","","\tif pipelineRun.Status.StartTime != nil {","\t\tpipelineData[\"start_time\"] = pipelineRun.Status.StartTime.Time","\t}","\tif pipelineRun.Status.CompletionTime != nil {","\t\tpipelineData[\"completion_time\"] = pipelineRun.Status.CompletionTime.Time","\t}","","\tif event != nil {","\t\tpipelineData[\"event_type\"] = event.EventType","\t\tpipelineData[\"sha\"] = event.SHA","\t\tpipelineData[\"base_branch\"] = event.BaseBranch","\t\tpipelineData[\"head_branch\"] = event.HeadBranch","\t}","","\treturn pipelineData","}","","// buildCommitContent builds commit-related context information.","func (a *Assembler) buildCommitContent(ctx context.Context, event *info.Event, provider provider.Interface) (map[string]any, error) {","\tif event == nil {","\t\treturn nil, fmt.Errorf(\"event is nil\")","\t}","","\tcommitData := map[string]any{","\t\t\"sha\":     event.SHA,","\t\t\"message\": event.SHATitle,","\t}","","\t// Try to get additional commit information from the provider","\tif provider != nil {","\t\tif err := provider.GetCommitInfo(ctx, event); err != nil {","\t\t\ta.logger.Warnf(\"Failed to get additional commit info: %v\", err)","\t\t}","\t}","","\t// Add extended commit fields if available (after GetCommitInfo or if already populated)","\t// Add URL if available","\tif event.SHAURL != \"\" {","\t\tcommitData[\"url\"] = event.SHAURL","\t}","","\t// Add full commit message if available and different from title","\tif event.SHAMessage != \"\" \u0026\u0026 event.SHAMessage != event.SHATitle {","\t\tcommitData[\"full_message\"] = event.SHAMessage","\t}","","\t// Add author information if available","\t// Note: Email addresses are excluded for privacy/PII reasons","\tif event.SHAAuthorName != \"\" || !event.SHAAuthorDate.IsZero() {","\t\tauthor := map[string]any{}","\t\tif event.SHAAuthorName != \"\" {","\t\t\tauthor[\"name\"] = event.SHAAuthorName","\t\t}","\t\tif !event.SHAAuthorDate.IsZero() {","\t\t\tauthor[\"date\"] = event.SHAAuthorDate","\t\t}","\t\tcommitData[\"author\"] = author","\t}","","\t// Add committer information if available","\t// Note: Email addresses are excluded for privacy/PII reasons","\tif event.SHACommitterName != \"\" || !event.SHACommitterDate.IsZero() {","\t\tcommitter := map[string]any{}","\t\tif event.SHACommitterName != \"\" {","\t\t\tcommitter[\"name\"] = event.SHACommitterName","\t\t}","\t\tif !event.SHACommitterDate.IsZero() {","\t\t\tcommitter[\"date\"] = event.SHACommitterDate","\t\t}","\t\tcommitData[\"committer\"] = committer","\t}","","\treturn commitData, nil","}","","// buildPRContent builds pull request context information.","func (a *Assembler) buildPRContent(_ context.Context, event *info.Event, _ provider.Interface) (map[string]any, error) {","\tif event == nil || event.PullRequestNumber == 0 {","\t\treturn nil, fmt.Errorf(\"no pull request information available\")","\t}","","\tprData := map[string]any{","\t\t\"number\":      event.PullRequestNumber,","\t\t\"title\":       event.PullRequestTitle,","\t\t\"head_branch\": event.HeadBranch,","\t\t\"base_branch\": event.BaseBranch,","\t}","","\t// Note: PR body is not available in the Event struct","","\treturn prData, nil","}","","// buildErrorContent builds error and failure context information.","func (a *Assembler) buildErrorContent(ctx context.Context, pipelineRun *tektonv1.PipelineRun) map[string]any {","\tif len(pipelineRun.Status.Conditions) == 0 {","\t\treturn nil","\t}","","\tcondition := pipelineRun.Status.Conditions[0]","\tif condition.Status != \"False\" {","\t\treturn nil","\t}","","\terrorData := map[string]any{","\t\t\"condition_reason\":  condition.Reason,","\t\t\"condition_message\": condition.Message,","\t}","","\t// Get detailed task failure information","\ttaskInfos := kstatus.CollectFailedTasksLogSnippet(ctx, a.run, a.kinteract, pipelineRun, 3)","\tif len(taskInfos) \u003e 0 {","\t\tsortedTaskInfos := sort.TaskInfos(taskInfos)","","\t\tvar failedTasks []map[string]any","\t\tfor _, taskInfo := range sortedTaskInfos {","\t\t\tfailedTask := map[string]any{","\t\t\t\t\"name\":        taskInfo.Name,","\t\t\t\t\"reason\":      taskInfo.Reason,","\t\t\t\t\"message\":     taskInfo.Message,","\t\t\t\t\"log_snippet\": taskInfo.LogSnippet,","\t\t\t}","","\t\t\tif taskInfo.DisplayName != \"\" {","\t\t\t\tfailedTask[\"display_name\"] = taskInfo.DisplayName","\t\t\t}","","\t\t\tif taskInfo.CompletionTime != nil {","\t\t\t\tfailedTask[\"completion_time\"] = taskInfo.CompletionTime.Time","\t\t\t}","","\t\t\tfailedTasks = append(failedTasks, failedTask)","\t\t}","","\t\terrorData[\"failed_tasks\"] = failedTasks","\t}","","\treturn errorData","}","","// buildContainerLogs builds container logs context information.","func (a *Assembler) buildContainerLogs(ctx context.Context, pipelineRun *tektonv1.PipelineRun, maxLines int) map[string]any {","\t// Get detailed task information with logs","\ttaskInfos := kstatus.CollectFailedTasksLogSnippet(ctx, a.run, a.kinteract, pipelineRun, int64(maxLines))","\tif len(taskInfos) == 0 {","\t\treturn nil","\t}","","\tlogs := []map[string]any{}","\tfor _, taskInfo := range taskInfos {","\t\tlogEntry := map[string]any{","\t\t\t\"task_name\": taskInfo.Name,","\t\t\t\"log_lines\": strings.Split(taskInfo.LogSnippet, \"\\n\"),","\t\t}","","\t\tif taskInfo.DisplayName != \"\" {","\t\t\tlogEntry[\"display_name\"] = taskInfo.DisplayName","\t\t}","","\t\tlogs = append(logs, logEntry)","\t}","","\treturn map[string]any{","\t\t\"failed_tasks_logs\": logs,","\t\t\"max_lines\":         maxLines,","\t}","}","","// BuildCELContext builds context data for CEL expression evaluation.","//","// This function exposes a carefully curated subset of event data to CEL expressions.","// The following fields are EXCLUDED for security or internal implementation reasons:","//   - event.Provider (contains API tokens and webhook secrets)","//   - event.Request (contains raw HTTP headers and payload which may include secrets)","//   - event.InstallationID, AccountID, GHEURL, CloneURL (provider-specific internal IDs/URLs)","//   - event.SourceProjectID, TargetProjectID (GitLab-specific internal IDs)","//   - event.State (internal state management fields)","//   - event.Event (raw event object, already represented in structured fields)","func (a *Assembler) BuildCELContext(","\tpipelineRun *tektonv1.PipelineRun,","\tevent *info.Event,","\trepo *v1alpha1.Repository,",") (map[string]any, error) {","\t// Convert PipelineRun to map for CEL access","\tprMap, err := a.pipelineRunToMap(pipelineRun)","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"failed to convert PipelineRun to map: %w\", err)","\t}","","\t// Convert Repository to map for CEL access","\trepoMap, err := a.repositoryToMap(repo)","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"failed to convert Repository to map: %w\", err)","\t}","","\tcelData := map[string]any{","\t\t\"body\": map[string]any{","\t\t\t\"pipelineRun\": prMap,","\t\t\t\"repository\":  repoMap,","\t\t},","\t\t\"pac\": make(map[string]string), // PAC parameters will be populated by caller","\t}","","\t// Add event information to CEL context","\tif event != nil {","\t\teventMap := map[string]any{","\t\t\t// Event type and trigger information","\t\t\t\"event_type\":     event.EventType,","\t\t\t\"trigger_target\": event.TriggerTarget.String(),","","\t\t\t// Branch and commit information","\t\t\t\"sha\":            event.SHA,","\t\t\t\"sha_title\":      event.SHATitle,","\t\t\t\"base_branch\":    event.BaseBranch,","\t\t\t\"head_branch\":    event.HeadBranch,","\t\t\t\"default_branch\": event.DefaultBranch,","","\t\t\t// Repository information","\t\t\t\"organization\": event.Organization,","\t\t\t\"repository\":   event.Repository,","","\t\t\t// URLs (web URLs, not git URLs)","\t\t\t\"url\":      event.URL,","\t\t\t\"sha_url\":  event.SHAURL,","\t\t\t\"base_url\": event.BaseURL,","\t\t\t\"head_url\": event.HeadURL,","","\t\t\t// User information","\t\t\t\"sender\": event.Sender,","","\t\t\t// Webhook-specific","\t\t\t\"target_pipelinerun\": event.TargetPipelineRun,","\t\t}","","\t\t// Pull/Merge Request specific fields (only populated for PR events)","\t\tif event.PullRequestNumber \u003e 0 {","\t\t\teventMap[\"pull_request_number\"] = event.PullRequestNumber","\t\t\teventMap[\"pull_request_title\"] = event.PullRequestTitle","\t\t\teventMap[\"pull_request_labels\"] = event.PullRequestLabel","\t\t}","","\t\t// Comment trigger field (only populated when triggered by comment)","\t\tif event.TriggerComment != \"\" {","\t\t\teventMap[\"trigger_comment\"] = event.TriggerComment","\t\t}","","\t\tif bodyMap, ok := celData[\"body\"].(map[string]any); ok {","\t\t\tbodyMap[\"event\"] = eventMap","\t\t}","\t}","","\treturn celData, nil","}","","// pipelineRunToMap converts a PipelineRun to a map for CEL access.","func (a *Assembler) pipelineRunToMap(pr *tektonv1.PipelineRun) (map[string]any, error) {","\t// Marshal to JSON and back to get a clean map representation","\tjsonData, err := json.Marshal(pr)","\tif err != nil {","\t\treturn nil, err","\t}","","\tvar prMap map[string]any","\tif err := json.Unmarshal(jsonData, \u0026prMap); err != nil {","\t\treturn nil, err","\t}","","\treturn prMap, nil","}","","// repositoryToMap converts a Repository to a map for CEL access.","func (a *Assembler) repositoryToMap(repo *v1alpha1.Repository) (map[string]any, error) {","\t// Marshal to JSON and back to get a clean map representation","\tjsonData, err := json.Marshal(repo)","\tif err != nil {","\t\treturn nil, err","\t}","","\tvar repoMap map[string]any","\tif err := json.Unmarshal(jsonData, \u0026repoMap); err != nil {","\t\treturn nil, err","\t}","","\treturn repoMap, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,1,1,1,1,1,1,0,0,1,1,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1,0,0,0,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,0,1,1,1,1,1,1,0,1,0,0,0,2,2,2,2,0,2,2,2,2,2,2,2,1,1,1,0,0,0,0,2,2,2,0,0,2,2,2,0,0,0,2,2,2,2,2,2,2,2,2,0,0,0,0,2,2,2,2,2,2,2,2,2,0,0,2,0,0,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,0,1,0,0,1,0,0,1,0,0,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,1,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,1,1,0,0,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,0,2,2,2,0,0,2,0,0,0,2,2,2,2,1,1,0,2,2,1,1,0,2,0,0,0,2,2,2,2,1,1,0,2,2,1,1,0,2,0]},{"id":59,"path":"pkg/llm/factory.go","lines":["package llm","","import (","\t\"context\"","\t\"fmt\"","\t\"net/url\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction\"","\tllmtypes \"github.com/openshift-pipelines/pipelines-as-code/pkg/llm/ltypes\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/llm/providers/gemini\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/llm/providers/openai\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/secrets/types\"",")","","// ClientConfig holds the configuration needed to create LLM clients.","type ClientConfig struct {","\tProvider       llmtypes.AIProvider","\tAPIURL         string","\tModel          string // Model name to use (empty string uses provider default)","\tTokenSecretRef *v1alpha1.Secret","\tTimeoutSeconds int","\tMaxTokens      int","}","","// Factory creates LLM clients based on provider configuration.","type Factory struct {","\trun       *params.Run","\tkinteract kubeinteraction.Interface","}","","// NewFactory creates a new LLM client factory.","func NewFactory(run *params.Run, kinteract kubeinteraction.Interface) *Factory {","\treturn \u0026Factory{","\t\trun:       run,","\t\tkinteract: kinteract,","\t}","}","","// CreateClient creates an LLM client based on the provided configuration.","func (f *Factory) CreateClient(ctx context.Context, config *ClientConfig, namespace string) (llmtypes.Client, error) {","\tif config == nil {","\t\treturn nil, fmt.Errorf(\"client configuration is required\")","\t}","","\tif err := f.ValidateConfig(config); err != nil {","\t\treturn nil, fmt.Errorf(\"invalid client configuration: %w\", err)","\t}","","\ttoken, err := f.getTokenFromSecret(ctx, config.TokenSecretRef, namespace)","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"failed to retrieve LLM token: %w\", err)","\t}","","\ttimeoutSeconds, maxTokens := f.applyDefaults(config.TimeoutSeconds, config.MaxTokens)","\tbaseURL := config.APIURL","\tmodel := config.Model","\tif model == \"\" {","\t\tmodel = getDefaultModel(config.Provider)","\t}","","\tbaseClient, err := f.createProviderClient(config.Provider, token, baseURL, model, timeoutSeconds, maxTokens)","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"failed to create %s client: %w\", config.Provider, err)","\t}","","\treturn baseClient, nil","}","","// ValidateConfig validates the client configuration.","func (f *Factory) ValidateConfig(config *ClientConfig) error {","\tif config == nil {","\t\treturn fmt.Errorf(\"client configuration is required\")","\t}","","\tif config.Provider == \"\" {","\t\treturn fmt.Errorf(\"LLM provider is required\")","\t}","","\tif config.TokenSecretRef == nil {","\t\treturn fmt.Errorf(\"token secret reference is required\")","\t}","\tif config.TokenSecretRef.Name == \"\" {","\t\treturn fmt.Errorf(\"token secret name is required\")","\t}","","\tif config.APIURL != \"\" {","\t\tif err := f.validateURL(config.APIURL); err != nil {","\t\t\treturn fmt.Errorf(\"invalid api_url: %w\", err)","\t\t}","\t}","","\tif !f.isProviderSupported(config.Provider) {","\t\treturn fmt.Errorf(\"unsupported LLM provider: %s\", config.Provider)","\t}","","\tif config.TimeoutSeconds \u003c 0 {","\t\treturn fmt.Errorf(\"timeout seconds must be non-negative\")","\t}","","\tif config.MaxTokens \u003c 0 {","\t\treturn fmt.Errorf(\"max tokens must be non-negative\")","\t}","","\treturn nil","}","","// GetSupportedProviders returns a list of supported LLM providers.","func (f *Factory) GetSupportedProviders() []llmtypes.AIProvider {","\treturn []llmtypes.AIProvider{","\t\tllmtypes.LLMProviderOpenAI,","\t\tllmtypes.LLMProviderGemini,","\t}","}","","// isProviderSupported checks if the given provider is supported.","func (f *Factory) isProviderSupported(provider llmtypes.AIProvider) bool {","\tfor _, supported := range f.GetSupportedProviders() {","\t\tif provider == supported {","\t\t\treturn true","\t\t}","\t}","\treturn false","}","","// validateURL validates that the URL is properly formatted.","func (f *Factory) validateURL(urlStr string) error {","\tif urlStr == \"\" {","\t\treturn nil // Empty is valid (optional)","\t}","","\tparsedURL, err := url.Parse(urlStr)","\tif err != nil {","\t\treturn fmt.Errorf(\"failed to parse URL '%s': %w\", urlStr, err)","\t}","","\tif parsedURL.Scheme != \"http\" \u0026\u0026 parsedURL.Scheme != \"https\" {","\t\treturn fmt.Errorf(\"URL scheme must be 'http' or 'https', got '%s'\", parsedURL.Scheme)","\t}","","\tif parsedURL.Host == \"\" {","\t\treturn fmt.Errorf(\"URL must contain a host\")","\t}","","\treturn nil","}","","// getTokenFromSecret retrieves the API token from a Kubernetes secret.","func (f *Factory) getTokenFromSecret(ctx context.Context, secretRef *v1alpha1.Secret, namespace string) (string, error) {","\tif secretRef == nil {","\t\treturn \"\", fmt.Errorf(\"secret reference is nil\")","\t}","","\tkey := secretRef.Key","\tif key == \"\" {","\t\tkey = \"token\"","\t}","","\topt := types.GetSecretOpt{","\t\tNamespace: namespace,","\t\tName:      secretRef.Name,","\t\tKey:       key,","\t}","","\tsecretValue, err := f.kinteract.GetSecret(ctx, opt)","\tif err != nil {","\t\treturn \"\", fmt.Errorf(\"failed to get secret %s/%s: %w\", namespace, secretRef.Name, err)","\t}","","\tif secretValue == \"\" {","\t\treturn \"\", fmt.Errorf(\"secret %s/%s key %s is empty\", namespace, secretRef.Name, key)","\t}","","\treturn secretValue, nil","}","","// applyDefaults applies default values for timeout and max tokens if not specified.","func (f *Factory) applyDefaults(timeoutSeconds, maxTokens int) (int, int) {","\tif timeoutSeconds == 0 {","\t\ttimeoutSeconds = llmtypes.DefaultConfig.TimeoutSeconds","\t}","\tif maxTokens == 0 {","\t\tmaxTokens = llmtypes.DefaultConfig.MaxTokens","\t}","\treturn timeoutSeconds, maxTokens","}","","// createProviderClient creates a provider-specific client.","func (f *Factory) createProviderClient(provider llmtypes.AIProvider, token, baseURL, model string, timeoutSeconds, maxTokens int) (llmtypes.Client, error) {","\tswitch provider {","\tcase llmtypes.LLMProviderOpenAI:","\t\tconfig := \u0026openai.Config{","\t\t\tAPIKey:         token,","\t\t\tModel:          model,","\t\t\tTimeoutSeconds: timeoutSeconds,","\t\t\tMaxTokens:      maxTokens,","\t\t}","\t\tif baseURL != \"\" {","\t\t\tconfig.BaseURL = baseURL","\t\t}","\t\treturn openai.NewClient(config)","\tcase llmtypes.LLMProviderGemini:","\t\tconfig := \u0026gemini.Config{","\t\t\tAPIKey:         token,","\t\t\tModel:          model,","\t\t\tTimeoutSeconds: timeoutSeconds,","\t\t\tMaxTokens:      maxTokens,","\t\t}","\t\tif baseURL != \"\" {","\t\t\tconfig.BaseURL = baseURL","\t\t}","\t\treturn gemini.NewClient(config)","\tdefault:","\t\treturn nil, fmt.Errorf(\"unsupported LLM provider: %s\", provider)","\t}","}","","// CreateClientFromProvider creates a client directly from provider string and secret info.","func (f *Factory) CreateClientFromProvider(ctx context.Context, provider, secretName, secretKey, namespace string, timeoutSeconds, maxTokens int) (llmtypes.Client, error) {","\tconfig := \u0026ClientConfig{","\t\tProvider: llmtypes.AIProvider(provider),","\t\tAPIURL:   \"\", // Use provider default","\t\tTokenSecretRef: \u0026v1alpha1.Secret{","\t\t\tName: secretName,","\t\t\tKey:  secretKey,","\t\t},","\t\tTimeoutSeconds: timeoutSeconds,","\t\tMaxTokens:      maxTokens,","\t}","","\tif err := f.ValidateConfig(config); err != nil {","\t\treturn nil, fmt.Errorf(\"invalid client configuration: %w\", err)","\t}","","\treturn f.CreateClient(ctx, config, namespace)","}","","// getDefaultModel returns the default model for a provider.","func getDefaultModel(provider llmtypes.AIProvider) string {","\tswitch provider {","\tcase llmtypes.LLMProviderOpenAI:","\t\treturn \"gpt-5-mini\"","\tcase llmtypes.LLMProviderGemini:","\t\treturn \"gemini-2.5-flash-lite\"","\tdefault:","\t\treturn \"\"","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,0,2,2,1,1,0,2,2,2,0,2,2,2,2,0,2,2,2,2,2,2,0,2,2,1,1,0,2,0,0,0,2,2,2,2,0,2,2,2,0,2,2,2,2,1,1,0,2,2,2,2,0,0,2,2,2,0,2,2,2,0,2,2,2,0,2,0,0,0,2,2,2,2,2,2,0,0,2,2,2,2,2,0,2,0,0,0,2,2,1,1,0,2,2,2,2,0,2,2,2,0,2,1,1,0,2,0,0,0,2,2,1,1,0,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,0,2,1,1,0,2,0,0,0,2,2,1,1,2,1,1,2,0,0,0,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,2,2,2,2,2,2,1,1,2,1,1,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,0,0,0,2,2,2,2,2,2,2,2,0,0]},{"id":60,"path":"pkg/llm/ltypes/types.go","lines":["package ltypes","","import (","\t\"context\"","\t\"time\"",")","","// Client defines the interface for LLM providers.","type Client interface {","\tAnalyze(ctx context.Context, request *AnalysisRequest) (*AnalysisResponse, error)","\tGetProviderName() string","\tValidateConfig() error","}","","// AnalysisRequest represents a request to analyze CI/CD pipeline data.","type AnalysisRequest struct {","\tPrompt         string                 `json:\"prompt\"`","\tContext        map[string]interface{} `json:\"context\"`","\tMaxTokens      int                    `json:\"max_tokens\"`","\tTimeoutSeconds int                    `json:\"timeout_seconds\"`","}","","// AnalysisResponse represents the response from an LLM analysis.","type AnalysisResponse struct {","\tContent    string        `json:\"content\"`","\tTokensUsed int           `json:\"tokens_used\"`","\tProvider   string        `json:\"provider\"`","\tTimestamp  time.Time     `json:\"timestamp\"`","\tDuration   time.Duration `json:\"duration\"`","}","","// AnalysisError represents an error from LLM analysis.","type AnalysisError struct {","\tProvider  string `json:\"provider\"`","\tType      string `json:\"type\"`","\tMessage   string `json:\"message\"`","\tRetryable bool   `json:\"retryable\"`","}","","func (e *AnalysisError) Error() string {","\treturn e.Message","}","","// AIProvider represents a supported LLM provider.","type AIProvider string","","const (","\tLLMProviderOpenAI AIProvider = \"openai\"","\tLLMProviderGemini AIProvider = \"gemini\"",")","","// Config holds the default configuration values.","type Config struct {","\tTimeoutSeconds int","\tMaxTokens      int","}","","// DefaultConfig contains the default configuration values.","var DefaultConfig = Config{","\tTimeoutSeconds: 30,","\tMaxTokens:      1000,","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]},{"id":61,"path":"pkg/llm/orchestrator.go","lines":["package llm","","import (","\t\"context\"","\t\"fmt\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"go.uber.org/zap\"",")","","// Orchestrator coordinates the complete LLM analysis workflow.","type Orchestrator struct {","\trun           *params.Run","\tkinteract     kubeinteraction.Interface","\tlogger        *zap.SugaredLogger","\toutputHandler *OutputHandler","}","","// NewOrchestrator creates a new LLM analysis orchestrator.","func NewOrchestrator(run *params.Run, kinteract kubeinteraction.Interface, logger *zap.SugaredLogger) *Orchestrator {","\treturn \u0026Orchestrator{","\t\trun:           run,","\t\tkinteract:     kinteract,","\t\tlogger:        logger,","\t\toutputHandler: NewOutputHandler(run, logger),","\t}","}","","// ExecuteAnalysis performs the complete LLM analysis workflow.","func (o *Orchestrator) ExecuteAnalysis(","\tctx context.Context,","\trepo *v1alpha1.Repository,","\tpr *tektonv1.PipelineRun,","\tevent *info.Event,","\tprov provider.Interface,",") error {","\tif repo.Spec.Settings == nil || repo.Spec.Settings.AIAnalysis == nil || !repo.Spec.Settings.AIAnalysis.Enabled {","\t\to.logger.Debug(\"AI analysis not configured or disabled, skipping\")","\t\treturn nil","\t}","","\to.logger.Infof(\"Starting LLM analysis for pipeline %s/%s\", pr.Namespace, pr.Name)","","\t// Create LLM analyzer","\tanalyzer := NewAnalyzer(o.run, o.kinteract, o.logger)","","\t// Create analysis request","\trequest := \u0026AnalyzeRequest{","\t\tPipelineRun: pr,","\t\tEvent:       event,","\t\tRepository:  repo,","\t\tProvider:    prov,","\t}","","\t// Perform analysis","\tresults, err := analyzer.Analyze(ctx, request)","\tif err != nil {","\t\treturn fmt.Errorf(\"LLM analysis failed: %w\", err)","\t}","","\tif len(results) == 0 {","\t\to.logger.Debug(\"No analysis results generated\")","\t\treturn nil","\t}","","\t// Process analysis results","\tfor _, result := range results {","\t\tif result.Error != nil {","\t\t\to.logger.Warnf(\"Analysis failed for role %s: %v\", result.Role, result.Error)","\t\t\tcontinue","\t\t}","","\t\tif result.Response == nil {","\t\t\to.logger.Warnf(\"No response for role %s\", result.Role)","\t\t\tcontinue","\t\t}","","\t\to.logger.Infof(\"Processing LLM analysis result for role %s, tokens used: %d\", result.Role, result.Response.TokensUsed)","","\t\tif err := o.outputHandler.HandleOutput(ctx, repo, pr, result, event, prov); err != nil {","\t\t\to.logger.Warnf(\"Failed to handle output for role %s: %v\", result.Role, err)","\t\t\t// Continue processing other results even if one fails","\t\t}","\t}","","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,0,0,1,1,1,1,1,1,0,0,1,0]},{"id":62,"path":"pkg/llm/output_handler.go","lines":["package llm","","import (","\t\"context\"","\t\"fmt\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"go.uber.org/zap\"",")","","// OutputHandler handles the output of LLM analysis results to various destinations.","type OutputHandler struct {","\trun    *params.Run","\tlogger *zap.SugaredLogger","}","","// NewOutputHandler creates a new output handler.","func NewOutputHandler(run *params.Run, logger *zap.SugaredLogger) *OutputHandler {","\treturn \u0026OutputHandler{","\t\trun:    run,","\t\tlogger: logger,","\t}","}","","// HandleOutput processes the LLM analysis output according to the role configuration.","func (h *OutputHandler) HandleOutput(ctx context.Context, repo *v1alpha1.Repository, _ *tektonv1.PipelineRun, result AnalysisResult, event *info.Event, prov provider.Interface) error {","\tif repo.Spec.Settings == nil || repo.Spec.Settings.AIAnalysis == nil {","\t\treturn fmt.Errorf(\"AI analysis configuration is nil\")","\t}","","\t// Find the role configuration","\tvar roleConfig *v1alpha1.AnalysisRole","\tfor _, role := range repo.Spec.Settings.AIAnalysis.Roles {","\t\tif role.Name == result.Role {","\t\t\troleConfig = \u0026role","\t\t\tbreak","\t\t}","\t}","","\tif roleConfig == nil {","\t\treturn fmt.Errorf(\"role configuration not found for %s\", result.Role)","\t}","","\toutput := roleConfig.GetOutput()","\tif output != \"pr-comment\" {","\t\treturn fmt.Errorf(\"unsupported output destination: %s (only 'pr-comment' is currently supported)\", output)","\t}","","\treturn h.postPRComment(ctx, result, event, prov)","}","","// postPRComment posts LLM analysis as a PR comment.","func (h *OutputHandler) postPRComment(ctx context.Context, result AnalysisResult, event *info.Event, prov provider.Interface) error {","\tif event.PullRequestNumber == 0 {","\t\th.logger.Debug(\"No pull request associated with this event, skipping PR comment\")","\t\treturn nil","\t}","","\t// Format the comment with LLM analysis","\tcomment := fmt.Sprintf(\"## ðŸ¤– AI Analysis - %s\\n\\n%s\\n\\n---\\n*Generated by Pipelines-as-Code LLM Analysis*\",","\t\tresult.Role, result.Response.Content)","","\t// Create a unique marker for this analysis role to allow updates","\tupdateMarker := fmt.Sprintf(\"llm-analysis-%s\", result.Role)","","\tif err := prov.CreateComment(ctx, event, comment, updateMarker); err != nil {","\t\treturn fmt.Errorf(\"failed to create PR comment: %w\", err)","\t}","","\th.logger.Infof(\"Posted LLM analysis as PR comment for role %s\", result.Role)","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,1,1,0,0,0,1,1,1,0,1,1,1,1,0,1,0,0,0,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,0,1,1,0]},{"id":63,"path":"pkg/llm/providers/common.go","lines":["// Package providers contains shared functionality for LLM provider clients.","package providers","","import (","\t\"encoding/json\"","\t\"fmt\"","\t\"net/url\"","\t\"strings\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/llm/ltypes\"",")","","// CommonConfig represents the common configuration fields across all LLM providers.","type CommonConfig struct {","\tAPIKey         string","\tTimeoutSeconds int","\tMaxTokens      int","}","","// ValidateCommonConfig validates common configuration fields.","// It checks that the API key is provided and that numeric fields are non-negative.","func ValidateCommonConfig(config any) error {","\tif config == nil {","\t\treturn fmt.Errorf(\"config is required\")","\t}","","\tcommonCfg, ok := config.(*CommonConfig)","\tif !ok {","\t\treturn fmt.Errorf(\"invalid config type: expected *CommonConfig, got %T\", config)","\t}","","\tif commonCfg.APIKey == \"\" {","\t\treturn fmt.Errorf(\"API key is required\")","\t}","","\tif commonCfg.TimeoutSeconds \u003c 0 {","\t\treturn fmt.Errorf(\"timeout seconds must be non-negative\")","\t}","","\tif commonCfg.MaxTokens \u003c 0 {","\t\treturn fmt.Errorf(\"max tokens must be non-negative\")","\t}","","\treturn nil","}","","// ApplyDefaults applies default values to common configuration fields.","// It sets TimeoutSeconds to 30 if it's zero, and MaxTokens to 1000 if it's zero.","func ApplyDefaults(config any) error {","\tif config == nil {","\t\treturn fmt.Errorf(\"config is required\")","\t}","","\tcommonCfg, ok := config.(*CommonConfig)","\tif !ok {","\t\treturn fmt.Errorf(\"invalid config type: expected *CommonConfig, got %T\", config)","\t}","","\tif commonCfg.TimeoutSeconds == 0 {","\t\tcommonCfg.TimeoutSeconds = 30","\t}","","\tif commonCfg.MaxTokens == 0 {","\t\tcommonCfg.MaxTokens = 1000","\t}","","\treturn nil","}","","// ValidateBaseURL validates that the provided baseURL is a valid HTTP/HTTPS URL.","func ValidateBaseURL(baseURL string) error {","\tif baseURL == \"\" {","\t\treturn fmt.Errorf(\"base URL is required\")","\t}","","\tparsedURL, err := url.Parse(baseURL)","\tif err != nil {","\t\treturn fmt.Errorf(\"invalid base URL: %w\", err)","\t}","","\tif parsedURL.Scheme != \"http\" \u0026\u0026 parsedURL.Scheme != \"https\" {","\t\treturn fmt.Errorf(\"base URL must use http or https scheme, got: %s\", parsedURL.Scheme)","\t}","","\tif parsedURL.Host == \"\" {","\t\treturn fmt.Errorf(\"base URL must have a valid host\")","\t}","","\tif strings.ContainsAny(baseURL, \" \\t\\n\\r\") {","\t\treturn fmt.Errorf(\"base URL contains invalid whitespace characters\")","\t}","","\treturn nil","}","","// BuildPrompt combines the base prompt with context data.","// This function is shared across all LLM providers to ensure consistent prompt formatting.","func BuildPrompt(request *ltypes.AnalysisRequest) (string, error) {","\tvar promptBuilder strings.Builder","","\t// Start with the base prompt","\tpromptBuilder.WriteString(request.Prompt)","\tpromptBuilder.WriteString(\"\\n\\n\")","","\tif len(request.Context) \u003e 0 {","\t\tpromptBuilder.WriteString(\"Context Information:\\n\")","","\t\tfor key, value := range request.Context {","\t\t\tfmt.Fprintf(\u0026promptBuilder, \"=== %s ===\\n\", strings.ToUpper(key))","","\t\t\tswitch v := value.(type) {","\t\t\tcase string:","\t\t\t\tpromptBuilder.WriteString(v)","\t\t\tcase map[string]any, []any:","\t\t\t\tjsonData, err := json.MarshalIndent(v, \"\", \"  \")","\t\t\t\tif err != nil {","\t\t\t\t\treturn \"\", fmt.Errorf(\"failed to marshal context %s: %w\", key, err)","\t\t\t\t}","\t\t\t\tpromptBuilder.Write(jsonData)","\t\t\tdefault:","\t\t\t\tfmt.Fprintf(\u0026promptBuilder, \"%v\", v)","\t\t\t}","","\t\t\tpromptBuilder.WriteString(\"\\n\\n\")","\t\t}","\t}","","\treturn promptBuilder.String(), nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,2,2,2,2,0,2,2,2,0,2,2,2,0,2,2,2,0,2,0,0,0,0,2,2,2,2,0,2,2,2,2,0,2,2,2,0,2,2,2,0,2,0,0,0,2,2,2,2,0,2,2,2,2,0,2,2,2,0,2,2,2,0,2,1,1,0,2,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,0,0,0,2,0]},{"id":64,"path":"pkg/llm/providers/gemini/client.go","lines":["// Package gemini is the Client implementation for Google Gemini LLM integration.","package gemini","","import (","\t\"bytes\"","\t\"context\"","\t\"encoding/json\"","\t\"fmt\"","\t\"net/http\"","\t\"strings\"","\t\"time\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/llm/ltypes\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/llm/providers\"",")","","const (","\tdefaultBaseURL = \"https://generativelanguage.googleapis.com/v1beta\"","\tdefaultModel   = \"gemini-2.5-flash-lite\"","","\t// Default configuration values.","\tdefaultTimeoutSeconds = 30","\tdefaultMaxTokens      = 1000",")","","// Config holds the configuration for Gemini client.","type Config struct {","\tAPIKey         string","\tBaseURL        string","\tModel          string","\tTimeoutSeconds int","\tMaxTokens      int","}","","// Client implements the LLM interface for Google Gemini.","type Client struct {","\tconfig     *Config","\thttpClient *http.Client","}","","// NewClient creates a new Gemini client.","func NewClient(config *Config) (*Client, error) {","\tif config == nil {","\t\treturn nil, fmt.Errorf(\"config is required\")","\t}","","\tif config.APIKey == \"\" {","\t\treturn nil, fmt.Errorf(\"API key is required\")","\t}","","\tcommonCfg := \u0026providers.CommonConfig{","\t\tAPIKey:         config.APIKey,","\t\tTimeoutSeconds: config.TimeoutSeconds,","\t\tMaxTokens:      config.MaxTokens,","\t}","\tif err := providers.ApplyDefaults(commonCfg); err != nil {","\t\treturn nil, err","\t}","","\tconfig.TimeoutSeconds = commonCfg.TimeoutSeconds","\tconfig.MaxTokens = commonCfg.MaxTokens","","\tif config.BaseURL == \"\" {","\t\tconfig.BaseURL = defaultBaseURL","\t}","\tif config.Model == \"\" {","\t\tconfig.Model = defaultModel","\t}","","\tclient := \u0026Client{","\t\tconfig: config,","\t\thttpClient: \u0026http.Client{","\t\t\tTimeout: time.Duration(config.TimeoutSeconds) * time.Second,","\t\t},","\t}","","\treturn client, nil","}","","// Analyze sends an analysis request to Gemini and returns the response.","func (c *Client) Analyze(ctx context.Context, request *ltypes.AnalysisRequest) (*ltypes.AnalysisResponse, error) {","\tstartTime := time.Now()","","\tfullPrompt, err := providers.BuildPrompt(request)","\tif err != nil {","\t\treturn nil, \u0026ltypes.AnalysisError{","\t\t\tProvider:  c.GetProviderName(),","\t\t\tType:      \"prompt_build_error\",","\t\t\tMessage:   fmt.Sprintf(\"failed to build prompt: %v\", err),","\t\t\tRetryable: false,","\t\t}","\t}","","\tapiRequest := \u0026geminiRequest{","\t\tContents: []geminiContent{","\t\t\t{","\t\t\t\tParts: []geminiPart{","\t\t\t\t\t{","\t\t\t\t\t\tText: fullPrompt,","\t\t\t\t\t},","\t\t\t\t},","\t\t\t},","\t\t},","\t\tGenerationConfig: \u0026geminiGenerationConfig{","\t\t\tMaxOutputTokens: request.MaxTokens,","\t\t},","\t}","","\trequestBody, err := json.Marshal(apiRequest)","\tif err != nil {","\t\treturn nil, \u0026ltypes.AnalysisError{","\t\t\tProvider:  c.GetProviderName(),","\t\t\tType:      \"request_marshal_error\",","\t\t\tMessage:   fmt.Sprintf(\"failed to marshal request: %v\", err),","\t\t\tRetryable: false,","\t\t}","\t}","","\turl := fmt.Sprintf(\"%s/models/%s:generateContent?key=%s\", c.config.BaseURL, c.config.Model, c.config.APIKey)","\thttpReq, err := http.NewRequestWithContext(ctx, http.MethodPost, url, bytes.NewBuffer(requestBody))","\tif err != nil {","\t\treturn nil, \u0026ltypes.AnalysisError{","\t\t\tProvider:  c.GetProviderName(),","\t\t\tType:      \"http_request_error\",","\t\t\tMessage:   fmt.Sprintf(\"failed to create HTTP request: %v\", err),","\t\t\tRetryable: false,","\t\t}","\t}","","\thttpReq.Header.Set(\"Content-Type\", \"application/json\")","","\tresp, err := c.httpClient.Do(httpReq)","\tif err != nil {","\t\treturn nil, \u0026ltypes.AnalysisError{","\t\t\tProvider:  c.GetProviderName(),","\t\t\tType:      \"http_error\",","\t\t\tMessage:   fmt.Sprintf(\"HTTP request failed: %v\", err),","\t\t\tRetryable: true,","\t\t}","\t}","\tdefer resp.Body.Close()","","\tvar apiResponse geminiResponse","\tif err := json.NewDecoder(resp.Body).Decode(\u0026apiResponse); err != nil {","\t\treturn nil, \u0026ltypes.AnalysisError{","\t\t\tProvider:  c.GetProviderName(),","\t\t\tType:      \"response_parse_error\",","\t\t\tMessage:   fmt.Sprintf(\"failed to parse response: %v\", err),","\t\t\tRetryable: false,","\t\t}","\t}","","\tif resp.StatusCode != http.StatusOK {","\t\terrorType := \"api_error\"","\t\tretryable := false","","\t\tswitch {","\t\tcase resp.StatusCode == http.StatusTooManyRequests:","\t\t\terrorType = \"rate_limit_exceeded\"","\t\t\tretryable = true","\t\tcase resp.StatusCode == http.StatusUnauthorized || resp.StatusCode == http.StatusForbidden:","\t\t\terrorType = \"invalid_api_key\"","\t\t\tretryable = false","\t\tcase resp.StatusCode \u003e= 500:","\t\t\terrorType = \"server_error\"","\t\t\tretryable = true","\t\t}","","\t\terrorMsg := fmt.Sprintf(\"Gemini API error (status %d)\", resp.StatusCode)","\t\tif apiResponse.Error != nil {","\t\t\terrorMsg = fmt.Sprintf(\"Gemini API error: %s\", apiResponse.Error.Message)","\t\t}","","\t\treturn nil, \u0026ltypes.AnalysisError{","\t\t\tProvider:  c.GetProviderName(),","\t\t\tType:      errorType,","\t\t\tMessage:   errorMsg,","\t\t\tRetryable: retryable,","\t\t}","\t}","","\tif len(apiResponse.Candidates) == 0 {","\t\treturn nil, \u0026ltypes.AnalysisError{","\t\t\tProvider:  c.GetProviderName(),","\t\t\tType:      \"empty_response\",","\t\t\tMessage:   \"no candidates in API response\",","\t\t\tRetryable: false,","\t\t}","\t}","","\tcandidate := apiResponse.Candidates[0]","\tif len(candidate.Content.Parts) == 0 {","\t\treturn nil, \u0026ltypes.AnalysisError{","\t\t\tProvider:  c.GetProviderName(),","\t\t\tType:      \"empty_response\",","\t\t\tMessage:   \"no content parts in API response\",","\t\t\tRetryable: false,","\t\t}","\t}","","\tcontent := candidate.Content.Parts[0].Text","","\ttokensUsed := len(strings.Fields(content + fullPrompt))","","\tresponse := \u0026ltypes.AnalysisResponse{","\t\tContent:    content,","\t\tTokensUsed: tokensUsed,","\t\tProvider:   c.GetProviderName(),","\t\tTimestamp:  time.Now(),","\t\tDuration:   time.Since(startTime),","\t}","","\treturn response, nil","}","","// GetProviderName returns the provider name.","func (c *Client) GetProviderName() string {","\treturn string(ltypes.LLMProviderGemini)","}","","// ValidateConfig validates the client configuration.","func (c *Client) ValidateConfig() error {","\tcommonCfg := \u0026providers.CommonConfig{","\t\tAPIKey:         c.config.APIKey,","\t\tTimeoutSeconds: c.config.TimeoutSeconds,","\t\tMaxTokens:      c.config.MaxTokens,","\t}","\tif err := providers.ValidateCommonConfig(commonCfg); err != nil {","\t\treturn err","\t}","","\tif err := providers.ValidateBaseURL(c.config.BaseURL); err != nil {","\t\treturn err","\t}","","\treturn nil","}","","// Gemini API request/response structures","","type geminiRequest struct {","\tContents         []geminiContent         `json:\"contents\"`","\tGenerationConfig *geminiGenerationConfig `json:\"generationConfig,omitempty\"`","}","","type geminiContent struct {","\tParts []geminiPart `json:\"parts\"`","}","","type geminiPart struct {","\tText string `json:\"text\"`","}","","type geminiGenerationConfig struct {","\tMaxOutputTokens int `json:\"maxOutputTokens,omitempty\"`","}","","type geminiResponse struct {","\tCandidates []geminiCandidate `json:\"candidates\"`","\tError      *geminiError      `json:\"error,omitempty\"`","}","","type geminiCandidate struct {","\tContent       geminiContent `json:\"content\"`","\tFinishReason  string        `json:\"finishReason\"`","\tSafetyRatings []any         `json:\"safetyRatings\"`","}","","type geminiError struct {","\tCode    int    `json:\"code\"`","\tMessage string `json:\"message\"`","\tStatus  string `json:\"status\"`","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,2,2,2,0,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,0,2,2,2,1,1,1,1,1,1,1,0,2,2,2,2,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,0,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,0,0,2,2,2,2,2,2,2,2,2,0,2,2,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]},{"id":65,"path":"pkg/llm/providers/openai/client.go","lines":["// Package openai is the Client implementation for OpenAI LLM integration.","package openai","","import (","\t\"bytes\"","\t\"context\"","\t\"encoding/json\"","\t\"fmt\"","\t\"net/http\"","\t\"time\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/llm/ltypes\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/llm/providers\"",")","","const (","\tdefaultBaseURL = \"https://api.openai.com/v1\"","\tdefaultModel   = \"gpt-4\"","","\t// Default configuration values.","\tdefaultTimeoutSeconds = 30","\tdefaultMaxTokens      = 1000",")","","// Config holds the configuration for OpenAI client.","type Config struct {","\tAPIKey         string","\tBaseURL        string","\tModel          string","\tTimeoutSeconds int","\tMaxTokens      int","\tHTTPClient     *http.Client","}","","// Client implements the LLM interface for OpenAI.","type Client struct {","\tconfig     *Config","\thttpClient *http.Client","}","","// NewClient creates a new OpenAI client.","func NewClient(config *Config) (*Client, error) {","\tif config == nil {","\t\treturn nil, fmt.Errorf(\"config is required\")","\t}","","\tif config.APIKey == \"\" {","\t\treturn nil, fmt.Errorf(\"API key is required\")","\t}","","\tcommonCfg := \u0026providers.CommonConfig{","\t\tAPIKey:         config.APIKey,","\t\tTimeoutSeconds: config.TimeoutSeconds,","\t\tMaxTokens:      config.MaxTokens,","\t}","\tif err := providers.ApplyDefaults(commonCfg); err != nil {","\t\treturn nil, err","\t}","","\tconfig.TimeoutSeconds = commonCfg.TimeoutSeconds","\tconfig.MaxTokens = commonCfg.MaxTokens","","\tif config.BaseURL == \"\" {","\t\tconfig.BaseURL = defaultBaseURL","\t}","\tif config.Model == \"\" {","\t\tconfig.Model = defaultModel","\t}","","\thttpClient := config.HTTPClient","\tif httpClient == nil {","\t\thttpClient = \u0026http.Client{}","\t}","","\tif config.TimeoutSeconds \u003e 0 \u0026\u0026 httpClient.Timeout == 0 {","\t\thttpClient.Timeout = time.Duration(config.TimeoutSeconds) * time.Second","\t}","","\tclient := \u0026Client{","\t\tconfig:     config,","\t\thttpClient: httpClient,","\t}","","\treturn client, nil","}","","// Analyze sends an analysis request to OpenAI and returns the response.","func (c *Client) Analyze(ctx context.Context, request *ltypes.AnalysisRequest) (*ltypes.AnalysisResponse, error) {","\tstartTime := time.Now()","","\tfullPrompt, err := providers.BuildPrompt(request)","\tif err != nil {","\t\treturn nil, \u0026ltypes.AnalysisError{","\t\t\tProvider:  c.GetProviderName(),","\t\t\tType:      \"prompt_build_error\",","\t\t\tMessage:   fmt.Sprintf(\"failed to build prompt: %v\", err),","\t\t\tRetryable: false,","\t\t}","\t}","","\tapiRequest := \u0026openaiRequest{","\t\tModel:     c.config.Model,","\t\tMaxTokens: request.MaxTokens,","\t\tMessages: []openaiMessage{","\t\t\t{","\t\t\t\tRole:    \"user\",","\t\t\t\tContent: fullPrompt,","\t\t\t},","\t\t},","\t}","","\trequestBody, err := json.Marshal(apiRequest)","\tif err != nil {","\t\treturn nil, \u0026ltypes.AnalysisError{","\t\t\tProvider:  c.GetProviderName(),","\t\t\tType:      \"request_marshal_error\",","\t\t\tMessage:   fmt.Sprintf(\"failed to marshal request: %v\", err),","\t\t\tRetryable: false,","\t\t}","\t}","","\thttpReq, err := http.NewRequestWithContext(ctx, http.MethodPost, c.config.BaseURL+\"/chat/completions\", bytes.NewBuffer(requestBody))","\tif err != nil {","\t\treturn nil, \u0026ltypes.AnalysisError{","\t\t\tProvider:  c.GetProviderName(),","\t\t\tType:      \"http_request_error\",","\t\t\tMessage:   fmt.Sprintf(\"failed to create HTTP request: %v\", err),","\t\t\tRetryable: false,","\t\t}","\t}","","\thttpReq.Header.Set(\"Content-Type\", \"application/json\")","\thttpReq.Header.Set(\"Authorization\", \"Bearer \"+c.config.APIKey)","","\tresp, err := c.httpClient.Do(httpReq)","\tif err != nil {","\t\treturn nil, \u0026ltypes.AnalysisError{","\t\t\tProvider:  c.GetProviderName(),","\t\t\tType:      \"http_error\",","\t\t\tMessage:   fmt.Sprintf(\"HTTP request failed: %v\", err),","\t\t\tRetryable: true,","\t\t}","\t}","\tdefer resp.Body.Close()","","\tvar apiResponse openaiResponse","\tif err := json.NewDecoder(resp.Body).Decode(\u0026apiResponse); err != nil {","\t\treturn nil, \u0026ltypes.AnalysisError{","\t\t\tProvider:  c.GetProviderName(),","\t\t\tType:      \"response_parse_error\",","\t\t\tMessage:   fmt.Sprintf(\"failed to parse response: %v\", err),","\t\t\tRetryable: false,","\t\t}","\t}","","\tif resp.StatusCode != http.StatusOK {","\t\terrorType := \"api_error\"","\t\tretryable := false","","\t\tswitch {","\t\tcase resp.StatusCode == http.StatusTooManyRequests:","\t\t\terrorType = \"rate_limit_exceeded\"","\t\t\tretryable = true","\t\tcase resp.StatusCode == http.StatusUnauthorized:","\t\t\terrorType = \"invalid_api_key\"","\t\t\tretryable = false","\t\tcase resp.StatusCode \u003e= 500:","\t\t\terrorType = \"server_error\"","\t\t\tretryable = true","\t\t}","","\t\terrorMsg := fmt.Sprintf(\"OpenAI API error (status %d)\", resp.StatusCode)","\t\tif apiResponse.Error != nil {","\t\t\terrorMsg = fmt.Sprintf(\"OpenAI API error: %s\", apiResponse.Error.Message)","\t\t}","","\t\treturn nil, \u0026ltypes.AnalysisError{","\t\t\tProvider:  c.GetProviderName(),","\t\t\tType:      errorType,","\t\t\tMessage:   errorMsg,","\t\t\tRetryable: retryable,","\t\t}","\t}","","\tif len(apiResponse.Choices) == 0 {","\t\treturn nil, \u0026ltypes.AnalysisError{","\t\t\tProvider:  c.GetProviderName(),","\t\t\tType:      \"empty_response\",","\t\t\tMessage:   \"no choices in API response\",","\t\t\tRetryable: false,","\t\t}","\t}","","\tcontent := apiResponse.Choices[0].Message.Content","\ttokensUsed := apiResponse.Usage.TotalTokens","","\tresponse := \u0026ltypes.AnalysisResponse{","\t\tContent:    content,","\t\tTokensUsed: tokensUsed,","\t\tProvider:   c.GetProviderName(),","\t\tTimestamp:  time.Now(),","\t\tDuration:   time.Since(startTime),","\t}","","\treturn response, nil","}","","// GetProviderName returns the provider name.","func (c *Client) GetProviderName() string {","\treturn string(ltypes.LLMProviderOpenAI)","}","","// ValidateConfig validates the client configuration.","func (c *Client) ValidateConfig() error {","\tcommonCfg := \u0026providers.CommonConfig{","\t\tAPIKey:         c.config.APIKey,","\t\tTimeoutSeconds: c.config.TimeoutSeconds,","\t\tMaxTokens:      c.config.MaxTokens,","\t}","\tif err := providers.ValidateCommonConfig(commonCfg); err != nil {","\t\treturn err","\t}","","\tif err := providers.ValidateBaseURL(c.config.BaseURL); err != nil {","\t\treturn err","\t}","","\treturn nil","}","","// OpenAI API request/response structures","","type openaiRequest struct {","\tModel     string          `json:\"model\"`","\tMessages  []openaiMessage `json:\"messages\"`","\tMaxTokens int             `json:\"max_tokens,omitempty\"`","}","","type openaiMessage struct {","\tRole    string `json:\"role\"`","\tContent string `json:\"content\"`","}","","type openaiResponse struct {","\tID      string         `json:\"id\"`","\tObject  string         `json:\"object\"`","\tCreated int64          `json:\"created\"`","\tModel   string         `json:\"model\"`","\tChoices []openaiChoice `json:\"choices\"`","\tUsage   openaiUsage    `json:\"usage\"`","\tError   *openaiError   `json:\"error,omitempty\"`","}","","type openaiChoice struct {","\tIndex        int           `json:\"index\"`","\tMessage      openaiMessage `json:\"message\"`","\tFinishReason string        `json:\"finish_reason\"`","}","","type openaiUsage struct {","\tPromptTokens     int `json:\"prompt_tokens\"`","\tCompletionTokens int `json:\"completion_tokens\"`","\tTotalTokens      int `json:\"total_tokens\"`","}","","type openaiError struct {","\tMessage string `json:\"message\"`","\tType    string `json:\"type\"`","\tCode    string `json:\"code\"`","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,2,2,2,0,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,0,2,2,2,2,0,2,2,2,0,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,0,2,2,1,1,1,1,1,1,1,0,2,2,2,2,2,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,0,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,0,0,2,2,2,2,2,2,2,2,2,0,2,2,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]},{"id":66,"path":"pkg/matcher/annotation_matcher.go","lines":["package matcher","","import (","\t\"context\"","\t\"fmt\"","\t\"regexp\"","\t\"strings\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\tapipac \"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/customparams\"","\tpacerrors \"github.com/openshift-pipelines/pipelines-as-code/pkg/errors\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/events\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/formatting\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/opscomments\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","","\t\"github.com/gobwas/glob\"","\t\"github.com/google/cel-go/common/types\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"go.uber.org/zap\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"knative.dev/pkg/apis\"",")","","const (","\t// regex allows array of string or a single string","\t// eg. [\"foo\", \"bar\"], [\"foo\"] or \"foo\".","\treValidateTag = `^\\[(.*)\\]$|^[^[\\]\\s]*$`","\t// maximum number of characters to display in logs for gitops comments.","\tmaxCommentLogLength = 160",")","","// prunBranch is value from annotations and baseBranch is event.Base value from event.","func branchMatch(prunBranch, baseBranch string) bool {","\t// Helper function to match glob pattern","\tmatchGlob := func(pattern, branch string) bool {","\t\tg := glob.MustCompile(pattern)","\t\treturn g.Match(branch)","\t}","","\t// Case: target is refs/heads/..","\tif strings.HasPrefix(prunBranch, \"refs/heads/\") {","\t\tref := baseBranch","\t\tif !strings.HasPrefix(baseBranch, \"refs/heads/\") \u0026\u0026 !strings.HasPrefix(baseBranch, \"refs/tags/\") {","\t\t\t// If base is without refs/heads/.. and not refs/tags/.. prefix, add it","\t\t\tref = \"refs/heads/\" + baseBranch","\t\t}","\t\treturn matchGlob(prunBranch, ref)","\t}","","\t// Case: target is not refs/heads/.. and not refs/tags/..","\tif !strings.HasPrefix(prunBranch, \"refs/heads/\") \u0026\u0026 !strings.HasPrefix(prunBranch, \"refs/tags/\") {","\t\tprunRef := \"refs/heads/\" + prunBranch","\t\tref := baseBranch","\t\tif !strings.HasPrefix(baseBranch, \"refs/heads/\") \u0026\u0026 !strings.HasPrefix(baseBranch, \"refs/tags/\") {","\t\t\t// If base is without refs/heads/.. and not refs/tags/.. prefix, add it","\t\t\tref = \"refs/heads/\" + baseBranch","\t\t}","\t\treturn matchGlob(prunRef, ref)","\t}","","\t// Match the prunRef pattern with the baseBranch","\t// this will cover the scenarios of match globs like refs/tags/0.* and any other if any","\treturn matchGlob(prunBranch, baseBranch)","}","","// TODO: move to another file since it's common to all annotations_* files.","func getAnnotationValues(annotation string) ([]string, error) {","\tre := regexp.MustCompile(reValidateTag)","\tannotation = strings.TrimSpace(annotation)","\tmatch := re.MatchString(annotation)","\tif !match {","\t\treturn nil, fmt.Errorf(\"annotations in pipeline are in wrong format: %s\", annotation)","\t}","","\t// if it's not an array then it would be a single string","\tif !strings.HasPrefix(annotation, \"[\") {","\t\t// replace \u0026#44; with comma so users can have comma in the annotation","\t\tannot := strings.ReplaceAll(annotation, \"\u0026#44;\", \",\")","\t\treturn []string{annot}, nil","\t}","","\t// Split all tasks by comma and make sure to trim spaces in there","\tsplit := strings.Split(re.FindStringSubmatch(annotation)[1], \",\")","\tfor i := range split {","\t\tsplit[i] = strings.TrimSpace(strings.ReplaceAll(split[i], \"\u0026#44;\", \",\"))","\t}","","\tif split[0] == \"\" {","\t\treturn nil, fmt.Errorf(\"annotation \\\"%s\\\" has empty values\", annotation)","\t}","","\treturn split, nil","}","","func getTargetBranch(prun *tektonv1.PipelineRun, event *info.Event) (bool, string, string, error) {","\tvar targetEvent, targetBranch string","\tif key, ok := prun.GetObjectMeta().GetAnnotations()[keys.OnEvent]; ok {","\t\tif key == \"[]\" {","\t\t\treturn false, \"\", \"\", fmt.Errorf(\"annotation %s is empty\", keys.OnEvent)","\t\t}","\t\ttargetEvents := []string{event.TriggerTarget.String()}","\t\tif event.EventType == triggertype.Incoming.String() {","\t\t\t// if we have a incoming event, we want to match pipelineruns on both incoming and push","\t\t\ttargetEvents = []string{triggertype.Incoming.String(), triggertype.Push.String()}","\t\t}","\t\tmatched, err := matchOnAnnotation(key, targetEvents, false)","\t\ttargetEvent = key","\t\tif err != nil {","\t\t\treturn false, \"\", \"\", err","\t\t}","\t\tif !matched {","\t\t\treturn false, \"\", \"\", nil","\t\t}","\t}","\tif key, ok := prun.GetObjectMeta().GetAnnotations()[keys.OnTargetBranch]; ok {","\t\tif key == \"[]\" {","\t\t\treturn false, \"\", \"\", fmt.Errorf(\"annotation %s is empty\", keys.OnTargetBranch)","\t\t}","\t\ttargetEvents := []string{event.BaseBranch}","\t\tmatched, err := matchOnAnnotation(key, targetEvents, true)","\t\ttargetBranch = key","\t\tif err != nil {","\t\t\treturn false, \"\", \"\", err","\t\t}","\t\tif !matched {","\t\t\treturn false, \"\", \"\", nil","\t\t}","\t}","","\tif targetEvent == \"\" || targetBranch == \"\" {","\t\treturn false, \"\", \"\", nil","\t}","\treturn true, targetEvent, targetBranch, nil","}","","type Match struct {","\tPipelineRun *tektonv1.PipelineRun","\tRepo        *apipac.Repository","\tConfig      map[string]string","}","","// getName returns the name of the PipelineRun, if GenerateName is not set, it","// returns the name generateName takes precedence over name since it will be","// generated when applying the PipelineRun by the tekton controller.","func getName(prun *tektonv1.PipelineRun) string {","\tname := prun.GetGenerateName()","\tif name == \"\" {","\t\tname = prun.GetName()","\t}","\treturn name","}","","// checkPipelineRunAnnotation checks if the Pipelinerun has","// `on-event`/`on-target-branch annotations` with `on-cel-expression`","// and if present then warns the user that `on-cel-expression` will take precedence.","func checkPipelineRunAnnotation(prun *tektonv1.PipelineRun, eventEmitter *events.EventEmitter, repo *apipac.Repository) {","\t// Define the annotations to check in a slice for easy iteration","\tchecks := []struct {","\t\tkey   string","\t\tvalue string","\t}{","\t\t{\"on-event\", prun.GetObjectMeta().GetAnnotations()[keys.OnEvent]},","\t\t{\"on-target-branch\", prun.GetObjectMeta().GetAnnotations()[keys.OnTargetBranch]},","\t}","","\t// Preallocate the annotations slice with the exact capacity needed","\tannotations := make([]string, 0, len(checks))","","\t// Iterate through each check and append the key if the value is non-empty","\tfor _, check := range checks {","\t\tif check.value != \"\" {","\t\t\tannotations = append(annotations, check.key)","\t\t}","\t}","","\tprName := getName(prun)","\tif len(annotations) \u003e 0 {","\t\tignoredAnnotations := strings.Join(annotations, \", \")","\t\tmsg := fmt.Sprintf(","\t\t\t\"Warning: The PipelineRun '%s' has 'on-cel-expression' defined along with [%s] annotation(s). The 'on-cel-expression' will take precedence and these annotations will be ignored\",","\t\t\tprName,","\t\t\tignoredAnnotations,","\t\t)","\t\teventEmitter.EmitMessage(repo, zap.WarnLevel, \"RepositoryTakesOnCelExpressionPrecedence\", msg)","\t}","}","","func MatchPipelinerunByAnnotation(ctx context.Context, logger *zap.SugaredLogger, pruns []*tektonv1.PipelineRun, cs *params.Run, event *info.Event, vcx provider.Interface, eventEmitter *events.EventEmitter, repo *apipac.Repository, reportErrors bool) ([]Match, error) {","\tmatchedPRs := []Match{}","\tlogger.Debugf(\"MatchPipelinerunByAnnotation: pipelineruns=%d event_type=%s trigger_target=%s report_errors=%t\", len(pruns), event.EventType, event.TriggerTarget, reportErrors)","\tinfomsg := fmt.Sprintf(\"matching pipelineruns to event: URL=%s, target-branch=%s, source-branch=%s, target-event=%s\",","\t\tevent.URL,","\t\tevent.BaseBranch,","\t\tevent.HeadBranch,","\t\tevent.TriggerTarget,","\t)","","\tif len(event.PullRequestLabel) \u003e 0 {","\t\tinfomsg += fmt.Sprintf(\", labels=%s\", strings.Join(event.PullRequestLabel, \"|\"))","\t}","","\tif event.EventType == triggertype.Incoming.String() {","\t\tinfomsg = fmt.Sprintf(\"%s, target-pipelinerun=%s\", infomsg, event.TargetPipelineRun)","\t} else if event.EventType == triggertype.PullRequest.String() {","\t\tinfomsg = fmt.Sprintf(\"%s, pull-request=%d\", infomsg, event.PullRequestNumber)","\t}","\tlogger.Info(infomsg)","","\t// Resolve custom params once for all PipelineRuns (for use in CEL expressions)","\t// Track original event type to detect changes (when on-comment matches)","\toriginalEventType := event.EventType","\tcustomParams := resolveCustomParamsForCEL(ctx, repo, event, cs, vcx, eventEmitter, logger)","\tif len(customParams) \u003e 0 {","\t\tlogger.Debugf(\"resolved %d custom params from repo for CEL\", len(customParams))","\t}","","\tcelValidationErrors := []*pacerrors.PacYamlValidations{}","\tfor _, prun := range pruns {","\t\tlogger.Debugf(\"MatchPipelinerunByAnnotation: evaluating pipelinerun=%s annotations=%d\", getName(prun), len(prun.GetObjectMeta().GetAnnotations()))","\t\tprMatch := Match{","\t\t\tPipelineRun: prun,","\t\t\tConfig:      map[string]string{},","\t\t}","","\t\tprName := getName(prun)","\t\tif event.TargetPipelineRun != \"\" \u0026\u0026 event.TargetPipelineRun == strings.TrimSuffix(prName, \"-\") {","\t\t\tlogger.Infof(\"matched target pipelinerun with name: %s, target pipelinerun: %s\", prName, event.TargetPipelineRun)","\t\t\tmatchedPRs = append(matchedPRs, prMatch)","\t\t\tcontinue","\t\t}","","\t\tif prun.GetObjectMeta().GetAnnotations() == nil {","\t\t\tlogger.Debugf(\"PipelineRun %s does not have any annotations\", prName)","\t\t\tcontinue","\t\t}","","\t\tif maxPrNumber, ok := prun.GetObjectMeta().GetAnnotations()[keys.MaxKeepRuns]; ok {","\t\t\tprMatch.Config[\"max-keep-runs\"] = maxPrNumber","\t\t\tlogger.Debugf(\"PipelineRun %s: max-keep-runs=%s\", prName, maxPrNumber)","\t\t}","","\t\tif targetNS, ok := prun.GetObjectMeta().GetAnnotations()[keys.TargetNamespace]; ok {","\t\t\tprMatch.Config[\"target-namespace\"] = targetNS","\t\t\tprMatch.Repo, _ = MatchEventURLRepo(ctx, cs, event, targetNS)","\t\t\tif prMatch.Repo == nil {","\t\t\t\tlogger.Warnf(\"could not find Repository CRD in branch %s, the pipelineRun %s has a label that explicitly targets it\", targetNS, prName)","\t\t\t\tcontinue","\t\t\t}","\t\t\tlogger.Debugf(\"PipelineRun %s: matched target namespace repo=%s/%s\", prName, prMatch.Repo.GetNamespace(), prMatch.Repo.GetName())","\t\t}","","\t\tif targetComment, ok := prun.GetObjectMeta().GetAnnotations()[keys.OnComment]; ok {","\t\t\tre, err := regexp.Compile(targetComment)","\t\t\tif err != nil {","\t\t\t\tlogger.Warnf(\"could not compile regexp %s from pipelineRun %s\", targetComment, prName)","\t\t\t\tcontinue","\t\t\t}","","\t\t\tstrippedComment := strings.TrimSpace(","\t\t\t\tstrings.TrimPrefix(strings.TrimSuffix(event.TriggerComment, \"\\r\\n\"), \"\\r\\n\"))","\t\t\tif re.MatchString(strippedComment) {","\t\t\t\tevent.EventType = opscomments.OnCommentEventType.String()","","\t\t\t\t// Re-resolve custom params if event type changed, so filters based on event_type stay accurate","\t\t\t\tif event.EventType != originalEventType {","\t\t\t\t\tcustomParams = resolveCustomParamsForCEL(ctx, repo, event, cs, vcx, eventEmitter, logger)","\t\t\t\t\toriginalEventType = event.EventType","\t\t\t\t\tlogger.Debugf(\"PipelineRun %s: event_type switched to %s due to comment match\", prName, event.EventType)","\t\t\t\t}","","\t\t\t\tcomment := event.TriggerComment","\t\t\t\tif len(comment) \u003e maxCommentLogLength {","\t\t\t\t\tcomment = comment[:maxCommentLogLength] + \"...\"","\t\t\t\t}","\t\t\t\tlogger.Infof(\"matched pipelinerun with name: %s on gitops comment: %q\", prName, comment)","","\t\t\t\tmatchedPRs = append(matchedPRs, prMatch)","\t\t\t\tcontinue","\t\t\t}","\t\t}","\t\t// if the event is a comment event, but we don't have any match from the keys.OnComment then skip the other evaluations","\t\tif event.EventType == opscomments.NoOpsCommentEventType.String() || event.EventType == opscomments.OnCommentEventType.String() {","\t\t\tcontinue","\t\t}","","\t\tif celExpr, ok := prun.GetObjectMeta().GetAnnotations()[keys.OnCelExpression]; ok {","\t\t\tcheckPipelineRunAnnotation(prun, eventEmitter, repo)","","\t\t\tlogger.Debugf(\"PipelineRun %s: evaluating CEL expression\", prName)","\t\t\tout, err := celEvaluate(ctx, celExpr, event, vcx, customParams, eventEmitter, repo)","\t\t\tif err != nil {","\t\t\t\tlogger.Errorf(\"there was an error evaluating the CEL expression, skipping: %v\", err)","\t\t\t\tif checkIfCELEvaluateError(err) {","\t\t\t\t\tcelValidationErrors = append(celValidationErrors, \u0026pacerrors.PacYamlValidations{","\t\t\t\t\t\tName: prName,","\t\t\t\t\t\tErr:  fmt.Errorf(\"CEL expression evaluation error: %s\", sanitizeErrorAsMarkdown(err)),","\t\t\t\t\t})","\t\t\t\t}","\t\t\t\tcontinue","\t\t\t}","\t\t\tlogger.Debugf(\"PipelineRun %s: CEL result=%v\", prName, out)","\t\t\tif out != types.True {","\t\t\t\tlogger.Infof(\"CEL expression for PipelineRun %s is not matching, skipping\", prName)","\t\t\t\tcontinue","\t\t\t}","\t\t\tlogger.Infof(\"CEL expression has been evaluated and matched\")","\t\t} else {","\t\t\t// If the event is a pull_request and the event type is label_update, but the PipelineRun","\t\t\t// does not contain an 'on-label' annotation, do not match this PipelineRun, as it is not intended for this event.","\t\t\t// Check this early to avoid any unnecessary processing.","\t\t\t_, hasOnLabel := prun.GetObjectMeta().GetAnnotations()[keys.OnLabel]","\t\t\tif event.TriggerTarget == triggertype.PullRequest \u0026\u0026 event.EventType == string(triggertype.PullRequestLabeled) \u0026\u0026 !hasOnLabel {","\t\t\t\tlogger.Infof(\"label update event, PipelineRun %s does not have a on-label for any of those labels: %s\", prName, strings.Join(event.PullRequestLabel, \"|\"))","\t\t\t\tcontinue","\t\t\t}","","\t\t\tmatched, targetEvent, targetBranch, err := getTargetBranch(prun, event)","\t\t\tif err != nil {","\t\t\t\treturn matchedPRs, err","\t\t\t}","\t\t\tif !matched {","\t\t\t\tlogger.Debugf(\"PipelineRun %s: target branch/event did not match\", prName)","\t\t\t\tcontinue","\t\t\t}","\t\t\tprMatch.Config[\"target-branch\"] = targetBranch","\t\t\tprMatch.Config[\"target-event\"] = targetEvent","\t\t\tlogger.Debugf(\"PipelineRun %s: matched target event=%s target branch=%s\", prName, targetEvent, targetBranch)","","\t\t\tif key, ok := prun.GetObjectMeta().GetAnnotations()[keys.OnPathChange]; ok {","\t\t\t\tchangedFiles, err := vcx.GetFiles(ctx, event)","\t\t\t\tif err != nil {","\t\t\t\t\tlogger.Errorf(\"error getting changed files: %v\", err)","\t\t\t\t\tcontinue","\t\t\t\t}","\t\t\t\t// // TODO(chmou): we use the matchOnAnnotation function, it's","\t\t\t\t// really made to match git branches but we can still use it for","\t\t\t\t// our own path changes. we may split up if needed to refine.","\t\t\t\tmatched, err := matchOnAnnotation(key, changedFiles.All, true)","\t\t\t\tif err != nil {","\t\t\t\t\treturn matchedPRs, err","\t\t\t\t}","\t\t\t\tif !matched {","\t\t\t\t\tlogger.Debugf(\"PipelineRun %s: path-change annotation did not match\", prName)","\t\t\t\t\tcontinue","\t\t\t\t}","\t\t\t\tlogger.Infof(\"matched PipelineRun with name: %s, annotation PathChange: %q\", prName, key)","\t\t\t\tprMatch.Config[\"path-change\"] = key","\t\t\t}","","\t\t\tif key, ok := prun.GetObjectMeta().GetAnnotations()[keys.OnLabel]; ok {","\t\t\t\tmatched, err := matchOnAnnotation(key, event.PullRequestLabel, false)","\t\t\t\tif err != nil {","\t\t\t\t\treturn matchedPRs, err","\t\t\t\t}","\t\t\t\tif !matched {","\t\t\t\t\tlogger.Debugf(\"PipelineRun %s: label annotation did not match\", prName)","\t\t\t\t\tcontinue","\t\t\t\t}","\t\t\t\tlogger.Infof(\"matched PipelineRun with name: %s, annotation Label: %q\", prName, key)","\t\t\t\tprMatch.Config[\"label\"] = key","\t\t\t}","","\t\t\tif key, ok := prun.GetObjectMeta().GetAnnotations()[keys.OnPathChangeIgnore]; ok {","\t\t\t\tchangedFiles, err := vcx.GetFiles(ctx, event)","\t\t\t\tif err != nil {","\t\t\t\t\tlogger.Errorf(\"error getting changed files: %v\", err)","\t\t\t\t\tcontinue","\t\t\t\t}","\t\t\t\t// // TODO(chmou): we use the matchOnAnnotation function, it's","\t\t\t\t// really made to match git branches but we can still use it for","\t\t\t\t// our own path changes. we may split up if needed to refine.","\t\t\t\tmatched, err := matchOnAnnotation(key, changedFiles.All, true)","\t\t\t\tif err != nil {","\t\t\t\t\treturn matchedPRs, err","\t\t\t\t}","\t\t\t\tif matched {","\t\t\t\t\tlogger.Infof(\"Skipping pipelinerun with name: %s, annotation PathChangeIgnore: %q\", prName, key)","\t\t\t\t\tcontinue","\t\t\t\t}","\t\t\t\tprMatch.Config[\"path-change-ignore\"] = key","\t\t\t\tlogger.Debugf(\"PipelineRun %s: path-change-ignore did not match, continuing\", prName)","\t\t\t}","\t\t}","","\t\tlogger.Infof(\"matched pipelinerun with name: %s, annotation Config: %q\", prName, prMatch.Config)","\t\tmatchedPRs = append(matchedPRs, prMatch)","\t}","","\tif len(celValidationErrors) \u003e 0 \u0026\u0026 reportErrors {","\t\tlogger.Debugf(\"MatchPipelinerunByAnnotation: reporting %d CEL validation errors\", len(celValidationErrors))","\t\treportCELValidationErrors(ctx, repo, celValidationErrors, eventEmitter, vcx, event)","\t}","","\tif len(matchedPRs) \u003e 0 {","\t\t// Filter out templates that already have successful PipelineRuns for /retest and /ok-to-test","\t\tif event.EventType == opscomments.RetestAllCommentEventType.String() ||","\t\t\tevent.EventType == opscomments.OkToTestCommentEventType.String() {","\t\t\tlogger.Debugf(\"MatchPipelinerunByAnnotation: filtering successful templates for event_type=%s\", event.EventType)","\t\t\treturn filterSuccessfulTemplates(ctx, logger, cs, event, repo, matchedPRs), nil","\t\t}","\t\treturn matchedPRs, nil","\t}","","\treturn nil, fmt.Errorf(\"%s\", buildAvailableMatchingAnnotationErr(event, pruns))","}","","// filterSuccessfulTemplates filters out templates that already have successful PipelineRuns","// when executing /ok-to-test or /retest gitops commands, implementing per-template checking.","func filterSuccessfulTemplates(ctx context.Context, logger *zap.SugaredLogger, cs *params.Run, event *info.Event, repo *apipac.Repository, matchedPRs []Match) []Match {","\tif event.SHA == \"\" {","\t\treturn matchedPRs","\t}","","\t// Get all existing PipelineRuns for this SHA","\tlabelSelector := fmt.Sprintf(\"%s=%s\", keys.SHA, formatting.CleanValueKubernetes(event.SHA))","\texistingPRs, err := cs.Clients.Tekton.TektonV1().PipelineRuns(repo.GetNamespace()).List(ctx, metav1.ListOptions{","\t\tLabelSelector: labelSelector,","\t})","\tif err != nil {","\t\tlogger.Errorf(\"failed to list existing PipelineRuns for SHA %s: %v\", event.SHA, err)","\t\treturn matchedPRs // Return all templates if we can't check","\t}","\tlogger.Debugf(\"filterSuccessfulTemplates: existing pipelineruns=%d for sha=%s\", len(existingPRs.Items), event.SHA)","","\t// Create a map of template names to their most recent successful run","\tsuccessfulTemplates := make(map[string]*tektonv1.PipelineRun)","","\tfor i := range existingPRs.Items {","\t\tpr := \u0026existingPRs.Items[i]","","\t\t// Get the original template name this PipelineRun came from","\t\toriginalPRName, ok := pr.GetAnnotations()[keys.OriginalPRName]","\t\tif !ok {","\t\t\toriginalPRName, ok = pr.GetLabels()[keys.OriginalPRName]","\t\t}","\t\tif !ok {","\t\t\tcontinue // Skip PipelineRuns without template identification","\t\t}","","\t\t// Check if this PipelineRun succeeded","\t\tif pr.Status.GetCondition(apis.ConditionSucceeded).IsTrue() {","\t\t\t// Keep the most recent successful run for each template","\t\t\tif existing, exists := successfulTemplates[originalPRName]; !exists ||","\t\t\t\tpr.CreationTimestamp.After(existing.CreationTimestamp.Time) {","\t\t\t\tsuccessfulTemplates[originalPRName] = pr","\t\t\t}","\t\t}","\t}","","\t// Filter out templates that have successful runs","\tvar filteredPRs []Match","","\tfor _, match := range matchedPRs {","\t\ttemplateName := getName(match.PipelineRun)","","\t\tif successfulPR, hasSuccessfulRun := successfulTemplates[templateName]; hasSuccessfulRun {","\t\t\tlogger.Infof(\"skipping template '%s' for sha %s as it already has a successful pipelinerun '%s'\",","\t\t\t\ttemplateName, event.SHA, successfulPR.Name)","\t\t} else {","\t\t\tfilteredPRs = append(filteredPRs, match)","\t\t}","\t}","","\t// Return the filtered list (which may be empty if all templates were skipped)","\treturn filteredPRs","}","","func buildAvailableMatchingAnnotationErr(event *info.Event, pruns []*tektonv1.PipelineRun) string {","\terrmsg := \"available annotations of the PipelineRuns annotations in .tekton/ dir:\"","\tfor _, prun := range pruns {","\t\tname := getName(prun)","\t\terrmsg += fmt.Sprintf(\" [PipelineRun: %s, annotations:\", name)","\t\tfor annotation, value := range prun.GetAnnotations() {","\t\t\tif !strings.HasPrefix(annotation, pipelinesascode.GroupName+\"/on-\") {","\t\t\t\tcontinue","\t\t\t}","\t\t\terrmsg += fmt.Sprintf(\" %s: \", strings.Replace(annotation, pipelinesascode.GroupName+\"/\", \"\", 1))","\t\t\tif annotation == keys.OnCelExpression {","\t\t\t\terrmsg += \"celexpression\"","\t\t\t} else {","\t\t\t\terrmsg += value","\t\t\t}","\t\t\terrmsg += \", \"","\t\t}","\t\terrmsg = strings.TrimSuffix(errmsg, \", \")","\t\terrmsg += \"],\"","\t}","\terrmsg = strings.TrimSpace(errmsg)","\terrmsg = strings.TrimSuffix(errmsg, \",\")","\tnopsevent := \"\"","\tif event.EventType != opscomments.NoOpsCommentEventType.String() {","\t\tnopsevent = fmt.Sprintf(\" payload target event is %s with\", event.EventType)","\t}","\terrmsg = fmt.Sprintf(\"cannot match the event to any pipelineruns in the .tekton/ directory,%s source branch %s and target branch %s. %s\", nopsevent, event.HeadBranch, event.BaseBranch, errmsg)","\treturn errmsg","}","","func matchOnAnnotation(annotations string, eventType []string, branchMatching bool) (bool, error) {","\ttargets, err := getAnnotationValues(annotations)","\tif err != nil {","\t\treturn false, err","\t}","","\tvar gotit string","\tfor _, v := range targets {","\t\tfor _, e := range eventType {","\t\t\tif v == e {","\t\t\t\tgotit = v","\t\t\t}","","\t\t\tif branchMatching \u0026\u0026 branchMatch(v, e) {","\t\t\t\tgotit = v","\t\t\t}","\t\t}","\t}","\tif gotit == \"\" {","\t\treturn false, nil","\t}","\treturn true, nil","}","","func MatchRunningPipelineRunForIncomingWebhook(eventType, incomingPipelineRun string, prs []*tektonv1.PipelineRun) []*tektonv1.PipelineRun {","\t// return all pipelineruns if EventType is not incoming or TargetPipelineRun is \"\"","\tif eventType != \"incoming\" || incomingPipelineRun == \"\" {","\t\treturn prs","\t}","","\tfor _, pr := range prs {","\t\t// check incomingPipelineRun with pr name or generateName","\t\tif incomingPipelineRun == pr.GetName() || incomingPipelineRun == pr.GetGenerateName() {","\t\t\treturn []*tektonv1.PipelineRun{pr}","\t\t}","\t}","\treturn nil","}","","// resolveCustomParamsForCEL resolves custom parameters from the Repository CR for use in CEL expressions.","// It returns a map of parameter names to values, excluding reserved keywords.","// All parameters are returned as strings, including those from secret_ref.","func resolveCustomParamsForCEL(ctx context.Context, repo *apipac.Repository, event *info.Event, cs *params.Run, vcx provider.Interface, eventEmitter *events.EventEmitter, logger *zap.SugaredLogger) map[string]string {","\tif repo == nil || repo.Spec.Params == nil {","\t\treturn map[string]string{}","\t}","\tlogger.Debugf(\"resolveCustomParamsForCEL: repo=%s/%s params=%d\", repo.GetNamespace(), repo.GetName(), len(*repo.Spec.Params))","","\t// Create kubeinteraction interface","\tkinteract, err := kubeinteraction.NewKubernetesInteraction(cs)","\tif err != nil {","\t\tlogger.Warnf(\"failed to create kubernetes interaction for custom params: %s\", err.Error())","\t\treturn map[string]string{}","\t}","","\t// Use existing customparams package to resolve all params","\tcp := customparams.NewCustomParams(event, repo, cs, kinteract, eventEmitter, vcx)","\tallParams, _, err := cp.GetParams(ctx)","\tif err != nil {","\t\teventEmitter.EmitMessage(repo, zap.WarnLevel, \"CustomParamsCELError\",","\t\t\tfmt.Sprintf(\"failed to resolve custom params for CEL: %s\", err.Error()))","\t\treturn map[string]string{}","\t}","\tlogger.Debugf(\"resolveCustomParamsForCEL: resolved %d total params\", len(allParams))","","\t// Filter to only include params defined in repo.Spec.Params (not standard PAC params)","\tresult := make(map[string]string)","\tfor _, param := range *repo.Spec.Params {","\t\tif value, ok := allParams[param.Name]; ok {","\t\t\tresult[param.Name] = value","\t\t}","\t}","\tlogger.Debugf(\"resolveCustomParamsForCEL: returned %d custom params\", len(result))","","\treturn result","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,2,2,0,0,0,0,2,0,0,0,2,2,2,2,2,2,2,0,0,2,2,2,2,2,0,0,2,2,2,2,0,2,2,2,0,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,0,2,2,2,2,2,2,2,2,1,1,2,2,2,0,0,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,0,0,2,2,2,2,0,2,2,2,2,2,2,0,2,0,0,2,2,2,1,1,0,0,2,2,2,2,2,2,2,1,1,1,1,0,2,2,1,1,2,2,2,2,0,0,0,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,0,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,1,1,0,0,0,0,2,2,1,1,2,2,2,0,2,2,0,0,2,2,2,1,1,2,2,2,0,2,2,0,0,2,2,2,1,1,0,0,0,0,2,2,1,1,2,2,2,0,2,2,0,0,0,2,2,0,0,2,2,2,2,0,2,2,2,2,1,1,1,2,0,0,2,0,0,0,0,2,2,2,2,0,0,2,2,2,2,2,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,0,0,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,0,2,2,0,2,2,2,2,2,2,2,2,0,0,2,2,2,1,1,0,2,2,2,2,2,2,0,2,2,2,0,0,2,2,2,2,0,0,2,2,2,2,2,0,2,2,2,2,2,0,2,0,0,0,0,0,2,2,2,2,2,2,2,2,2,1,1,1,0,0,2,2,2,1,1,1,1,2,2,2,2,2,2,2,2,0,2,2,2,0]},{"id":67,"path":"pkg/matcher/annotation_tasks_install.go","lines":["package matcher","","import (","\t\"context\"","\t\"errors\"","\t\"fmt\"","\t\"os\"","\t\"regexp\"","\t\"strings\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/hub\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/settings\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\ttektonv1beta1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1beta1\"","\t\"go.uber.org/zap\"","\tk8scheme \"k8s.io/client-go/kubernetes/scheme\"",")","","const (","\ttaskAnnotationsRegexp     = `task(-[0-9]+)?$`","\tpipelineAnnotationsRegexp = `pipeline$`",")","","type RemoteTasks struct {","\tRun               *params.Run","\tProviderInterface provider.Interface","\tEvent             *info.Event","\tLogger            *zap.SugaredLogger","}","","// nolint: dupl","func (rt RemoteTasks) convertToPipeline(ctx context.Context, uri, data string) (*tektonv1.Pipeline, error) {","\tdecoder := k8scheme.Codecs.UniversalDeserializer()","\tobj, _, err := decoder.Decode([]byte(data), nil, nil)","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"remote pipeline from URI %s cannot be parsed as a Kubernetes resource: %w\", uri, err)","\t}","","\tvar pipeline *tektonv1.Pipeline","\tswitch o := obj.(type) {","\tcase *tektonv1.Pipeline:","\t\tpipeline = o","\tcase *tektonv1beta1.Pipeline: //nolint: staticcheck","\t\tc := \u0026tektonv1.Pipeline{}","\t\t// TODO: figure ou the issue we have with setdefault setting defaults SA","\t\t// and then don't let pipeline do its job to automatically set a","\t\t// pipeline on configuration","\t\t// o.SetDefaults(ctx)","\t\t// ctx2 := features.SetFeatureFlag(context.Background())","\t\t// if err := o.Validate(ctx2); err != nil {","\t\t// return nil, fmt.Errorf(\"remote pipeline from uri: %s with name %s cannot be validated: %w\", uri, o.GetName(), err)","\t\t// }","\t\tif err := o.ConvertTo(ctx, c); err != nil {","\t\t\treturn nil, fmt.Errorf(\"remote pipeline from URI %s with name %s cannot be converted to v1beta1: %w\", uri, o.GetName(), err)","\t\t}","\t\tpipeline = c","\tdefault:","\t\treturn nil, fmt.Errorf(\"remote pipeline from URI %s has not been recognized as a Tekton pipeline: %v\", uri, o)","\t}","","\treturn pipeline, nil","}","","// nolint: dupl","// golint has decided that this is a duplication with convertToPipeline but I swear it isn't - these two are different functions","// and not even sure this is possible to do with generic complexity.","func (rt RemoteTasks) convertTotask(ctx context.Context, uri, data string) (*tektonv1.Task, error) {","\tdecoder := k8scheme.Codecs.UniversalDeserializer()","\tobj, _, err := decoder.Decode([]byte(data), nil, nil)","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"remote task from URI %s cannot be parsed as a Kubernetes resource: %w\", uri, err)","\t}","","\tvar task *tektonv1.Task","\tswitch o := obj.(type) {","\tcase *tektonv1.Task:","\t\ttask = o","\tcase *tektonv1beta1.Task: //nolint: staticcheck // we need to support v1beta1","\t\tc := \u0026tektonv1.Task{}","\t\t// o.SetDefaults(ctx)","\t\t// if err := o.Validate(ctx); err != nil {","\t\t// \treturn nil, fmt.Errorf(\"remote task %s cannot be validated properly: err: %w\", o.GetName(), err)","\t\t// return nil, fmt.Errorf(\"remote task from uri: %s with name %s cannot be validated: %w\", uri, o.GetName(), err)","\t\t// }","\t\tif err := o.ConvertTo(ctx, c); err != nil {","\t\t\treturn nil, fmt.Errorf(\"remote task from URI %s with name %s cannot be converted to v1beta1: %w\", uri, o.GetName(), err)","\t\t}","\t\ttask = c","\tdefault:","\t\treturn nil, fmt.Errorf(\"remote task from URI %s has not been recognized as a Tekton task: %v\", uri, o)","\t}","","\treturn task, nil","}","","func (rt RemoteTasks) getRemote(ctx context.Context, uri string, fromHub bool, kind string) (string, error) {","\trt.Logger.Debugf(\"getRemote: uri=%s kind=%s fromHub=%t\", uri, kind, fromHub)","\tif fetchedFromURIFromProvider, task, err := rt.ProviderInterface.GetTaskURI(ctx, rt.Event, uri); fetchedFromURIFromProvider {","\t\trt.Logger.Debugf(\"getRemote: fetched %s via provider hook for uri=%s\", kind, uri)","\t\treturn task, err","\t}","","\tswitch {","\tcase strings.HasPrefix(uri, \"https://\"), strings.HasPrefix(uri, \"http://\"): // if it starts with http(s)://, it is a remote resource","\t\trt.Logger.Debugf(\"getRemote: fetching %s from http(s) url\", kind)","\t\tdata, err := rt.Run.Clients.GetURL(ctx, uri)","\t\tif err != nil {","\t\t\treturn \"\", err","\t\t}","\t\trt.Logger.Infof(\"successfully fetched %s from remote HTTPS URL\", uri)","\t\treturn string(data), nil","\tcase fromHub \u0026\u0026 strings.Contains(uri, \"://\"): // if it contains ://, it is a remote custom catalog","\t\tsplit := strings.Split(uri, \"://\")","\t\tcatalogID := split[0]","\t\trt.Logger.Debugf(\"getRemote: fetching %s from custom hub catalog=%s\", kind, catalogID)","\t\tvalue, _ := rt.Run.Info.Pac.HubCatalogs.Load(catalogID)","\t\tif _, ok := rt.Run.Info.Pac.HubCatalogs.Load(catalogID); !ok {","\t\t\trt.Logger.Infof(\"custom catalog %s is not found, skipping\", catalogID)","\t\t\treturn \"\", nil","\t\t}","\t\turi = strings.TrimPrefix(uri, fmt.Sprintf(\"%s://\", catalogID))","\t\tdata, err := hub.GetResource(ctx, rt.Run, catalogID, uri, kind)","\t\tif err != nil {","\t\t\treturn \"\", err","\t\t}","\t\tcatalogValue, ok := value.(settings.HubCatalog)","\t\tif !ok {","\t\t\treturn \"\", fmt.Errorf(\"could not get details for catalog name: %s\", catalogID)","\t\t}","\t\trt.Logger.Infof(\"successfully fetched %s %s from custom catalog Hub %s on URL %s\", kind, uri, catalogID, catalogValue.URL)","\t\treturn data, nil","\tcase strings.Contains(uri, \"/\"): // if it contains a slash, it is a file inside a repository","\t\trt.Logger.Debugf(\"getRemote: fetching %s from repository path\", kind)","\t\tvar data string","\t\tvar err error","\t\tif rt.Event.SHA != \"\" {","\t\t\tdata, err = rt.ProviderInterface.GetFileInsideRepo(ctx, rt.Event, uri, \"\")","\t\t\tif err != nil {","\t\t\t\treturn \"\", err","\t\t\t}","\t\t} else {","\t\t\tdata, err = getFileFromLocalFS(uri, rt.Logger)","\t\t\tif err != nil {","\t\t\t\treturn \"\", err","\t\t\t}","\t\t\tif data == \"\" {","\t\t\t\treturn \"\", nil","\t\t\t}","\t\t}","","\t\trt.Logger.Infof(\"successfully fetched %s inside repository\", uri)","\t\treturn data, nil","\tcase fromHub: // finally a simple word will fetch from the default catalog (if enabled)","\t\trt.Logger.Debugf(\"getRemote: fetching %s from default hub catalog\", kind)","\t\tdata, err := hub.GetResource(ctx, rt.Run, \"default\", uri, kind)","\t\tif err != nil {","\t\t\treturn \"\", err","\t\t}","\t\tvalue, _ := rt.Run.Info.Pac.HubCatalogs.Load(\"default\")","\t\tcatalogValue, ok := value.(settings.HubCatalog)","\t\tif !ok {","\t\t\treturn \"\", fmt.Errorf(\"could not get details for catalog name: %s\", \"default\")","\t\t}","\t\trt.Logger.Infof(\"successfully fetched %s %s from default configured catalog Hub on URL %s\", uri, kind, catalogValue.URL)","\t\treturn data, nil","\t}","\treturn \"\", fmt.Errorf(`cannot find \"%s\" anywhere`, uri)","}","","func grabValuesFromAnnotations(annotations map[string]string, annotationReg string) ([]string, error) {","\trtareg := regexp.MustCompile(fmt.Sprintf(\"%s/%s\", pipelinesascode.GroupName, annotationReg))","\tvar ret []string","\tfor annotationK, annotationV := range annotations {","\t\tif !rtareg.MatchString(annotationK) {","\t\t\tcontinue","\t\t}","\t\titems, err := getAnnotationValues(annotationV)","\t\tif err != nil {","\t\t\treturn ret, err","\t\t}","\t\tret = append(items, ret...)","\t}","\treturn ret, nil","}","","func GrabTasksFromAnnotations(annotations map[string]string) ([]string, error) {","\treturn grabValuesFromAnnotations(annotations, taskAnnotationsRegexp)","}","","func GrabPipelineFromAnnotations(annotations map[string]string) (string, error) {","\tpipelinesAnnotation, err := grabValuesFromAnnotations(annotations, pipelineAnnotationsRegexp)","\tif err != nil {","\t\treturn \"\", err","\t}","\tif len(pipelinesAnnotation) \u003e 1 {","\t\treturn \"\", fmt.Errorf(\"only one pipeline is allowed on remote resolution, we have received multiple of them: %+v\", pipelinesAnnotation)","\t}","\tif len(pipelinesAnnotation) == 0 {","\t\treturn \"\", nil","\t}","\treturn pipelinesAnnotation[0], nil","}","","func (rt RemoteTasks) GetTaskFromAnnotationName(ctx context.Context, name string) (*tektonv1.Task, error) {","\trt.Logger.Debugf(\"GetTaskFromAnnotationName: name=%s\", name)","\tdata, err := rt.getRemote(ctx, name, true, \"task\")","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"error getting remote task \\\"%s\\\": %w\", name, err)","\t}","\tif data == \"\" {","\t\treturn nil, fmt.Errorf(\"remote task \\\"%s\\\" not found\", name)","\t}","","\ttask, err := rt.convertTotask(ctx, name, data)","\tif err != nil {","\t\treturn nil, err","\t}","\treturn task, nil","}","","func (rt RemoteTasks) GetPipelineFromAnnotationName(ctx context.Context, name string) (*tektonv1.Pipeline, error) {","\trt.Logger.Debugf(\"GetPipelineFromAnnotationName: name=%s\", name)","\tdata, err := rt.getRemote(ctx, name, true, \"pipeline\")","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"error getting remote pipeline \\\"%s\\\": %w\", name, err)","\t}","\tif data == \"\" {","\t\treturn nil, fmt.Errorf(\"remote pipeline \\\"%s\\\" not found\", name)","\t}","","\tpipeline, err := rt.convertToPipeline(ctx, name, data)","\tif err != nil {","\t\treturn nil, err","\t}","\treturn pipeline, nil","}","","// getFileFromLocalFS get task locally if file exist","// TODO: may want to try chroot to the git root dir first as well if we are able so.","func getFileFromLocalFS(fileName string, logger *zap.SugaredLogger) (string, error) {","\tvar data string","\t// We are most probably running with tkn pac resolve -f here, so","\t// let's try by any chance to check locally if the task is here on","\t// the filesystem","\tif _, err := os.Stat(fileName); errors.Is(err, os.ErrNotExist) {","\t\tlogger.Warnf(\"could not find remote file %s inside Repo\", fileName)","\t\treturn \"\", nil","\t}","","\tb, err := os.ReadFile(fileName)","\tdata = string(b)","\tif err != nil {","\t\treturn \"\", err","\t}","\treturn data, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,1,1,0,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,0,0,2,0,0,0,0,0,2,2,2,2,1,1,0,2,2,1,1,2,2,2,2,2,2,2,2,1,1,2,2,2,0,0,2,0,0,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,0,0,2,2,2,2,2,2,1,1,2,2,2,1,1,2,2,0,1,0,0,2,2,2,2,2,2,0,2,2,2,2,2,0,2,0,0,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,0,0,0,0,2,2,2,2,2,2,2,2,2,0,2,2,2,1,1,2,0]},{"id":68,"path":"pkg/matcher/cel.go","lines":["package matcher","","import (","\t\"context\"","\t\"encoding/json\"","\t\"fmt\"","\t\"strings\"","","\t\"github.com/gobwas/glob\"","\t\"github.com/google/cel-go/cel\"","\t\"github.com/google/cel-go/common/decls\"","\t\"github.com/google/cel-go/common/types\"","\t\"github.com/google/cel-go/common/types/ref\"","\tapipac \"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/changedfiles\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/events\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"go.uber.org/zap\"",")","","func celEvaluate(ctx context.Context, expr string, event *info.Event, vcx provider.Interface, customParams map[string]string, eventEmitter *events.EventEmitter, repo *apipac.Repository) (ref.Val, error) {","\teventTitle := event.PullRequestTitle","\tif event.TriggerTarget == triggertype.Push {","\t\teventTitle = event.SHATitle","\t}","","\tnbody, err := json.Marshal(event.Event)","\tif err != nil {","\t\treturn nil, err","\t}","\tvar jsonMap map[string]any","\terr = json.Unmarshal(nbody, \u0026jsonMap)","\tif err != nil {","\t\treturn nil, err","\t}","\theaderMap := make(map[string]string)","\tfor k, v := range event.Request.Header {","\t\theaderMap[strings.ToLower(k)] = v[0]","\t}","","\tstandardParams := map[string]bool{","\t\t\"event\": true, \"event_type\": true, \"headers\": true, \"body\": true,","\t\t\"event_title\": true, \"target_branch\": true, \"source_branch\": true,","\t\t\"target_url\": true, \"source_url\": true, \"files\": true,","\t}","","\tvarDecls := []cel.EnvOption{","\t\tcel.Lib(celPac{vcx, ctx, event}),","\t\tcel.VariableDecls(","\t\t\tdecls.NewVariable(\"event\", types.StringType),","\t\t\tdecls.NewVariable(\"event_type\", types.StringType),","\t\t\tdecls.NewVariable(\"headers\", types.NewMapType(types.StringType, types.DynType)),","\t\t\tdecls.NewVariable(\"body\", types.NewMapType(types.StringType, types.DynType)),","\t\t\tdecls.NewVariable(\"event_title\", types.StringType),","\t\t\tdecls.NewVariable(\"target_branch\", types.StringType),","\t\t\tdecls.NewVariable(\"source_branch\", types.StringType),","\t\t\tdecls.NewVariable(\"target_url\", types.StringType),","\t\t\tdecls.NewVariable(\"source_url\", types.StringType),","\t\t\tdecls.NewVariable(\"files\", types.NewMapType(types.StringType, types.DynType)),","\t\t),","\t}","","\tfor k := range customParams {","\t\tif !standardParams[k] {","\t\t\tvarDecls = append(varDecls, cel.VariableDecls(decls.NewVariable(k, types.StringType)))","\t\t}","\t}","","\tenv, err := cel.NewEnv(varDecls...)","\tif err != nil {","\t\treturn nil, err","\t}","","\tparsed, issues := env.Parse(expr)","\tif issues != nil \u0026\u0026 issues.Err() != nil {","\t\treturn nil, fmt.Errorf(\"failed to parse expression %#v: %w\", expr, issues.Err())","\t}","","\tchecked, issues := env.Check(parsed)","\tif issues != nil \u0026\u0026 issues.Err() != nil {","\t\treturn nil, fmt.Errorf(\"expression %#v check failed: %w\", expr, issues.Err())","\t}","","\t// Convert AST for inspection","\tcheckedExpr, err := cel.AstToCheckedExpr(checked)","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"failed to convert AST: %w\", err)","\t}","\tastRoot := checkedExpr.GetExpr()","","\t// For push events, handle refs/heads/ prefix stripping for target_branch and source_branch.","\t// We use AST inspection to detect if these variables are referenced, rather than string parsing.","\tif event.TriggerTarget == triggertype.Push {","\t\t// Check if expression uses target_branch and doesn't contain refs/heads/ literal","\t\tif walkExprAST(astRoot, matchIdentifier(\"target_branch\")) {","\t\t\tif !containsRefsHeadsLiteral(astRoot) {","\t\t\t\tevent.BaseBranch = strings.TrimPrefix(event.BaseBranch, \"refs/heads/\")","\t\t\t}","\t\t}","\t\t// Check if expression uses source_branch and doesn't contain refs/heads/ literal","\t\tif walkExprAST(astRoot, matchIdentifier(\"source_branch\")) {","\t\t\tif !containsRefsHeadsLiteral(astRoot) {","\t\t\t\tevent.HeadBranch = strings.TrimPrefix(event.HeadBranch, \"refs/heads/\")","\t\t\t}","\t\t}","\t}","","\t// Fetch changed files only if the expression references the \"files\" variable.","\t// This avoids unnecessary API calls when files aren't used in the expression.","\tchangedFiles := changedfiles.ChangedFiles{}","\tif walkExprAST(astRoot, matchIdentifier(\"files\")) {","\t\tchangedFiles, err = vcx.GetFiles(ctx, event)","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t}","","\t// For label events, check if the expression references labels or event_type.","\t// If not, return False to skip matching - this prevents generic \"event == pull_request\"","\t// expressions from unintentionally matching on label add/remove events.","\tif event.TriggerTarget == triggertype.PullRequest \u0026\u0026 event.EventType == string(triggertype.PullRequestLabeled) {","\t\tlabelMatcher := combinedMatcher(","\t\t\tmatchIdentifier(\"event_type\"),","\t\t\tmatchFieldAccess(\"labels\", \"pull_request_labels\"),","\t\t\tmatchBracketAccess(\"labels\", \"pull_request_labels\"),","\t\t)","\t\tif !walkExprAST(astRoot, labelMatcher) {","\t\t\treturn types.False, nil","\t\t}","\t}","","\tdata := map[string]any{","\t\t\"event\":         event.TriggerTarget.String(),","\t\t\"event_type\":    event.EventType,","\t\t\"event_title\":   eventTitle,","\t\t\"target_branch\": event.BaseBranch,","\t\t\"source_branch\": event.HeadBranch,","\t\t\"target_url\":    event.BaseURL,","\t\t\"source_url\":    event.HeadURL,","\t\t\"body\":          jsonMap,","\t\t\"headers\":       headerMap,","\t\t\"files\": map[string]any{","\t\t\t\"all\":      changedFiles.All,","\t\t\t\"added\":    changedFiles.Added,","\t\t\t\"deleted\":  changedFiles.Deleted,","\t\t\t\"modified\": changedFiles.Modified,","\t\t\t\"renamed\":  changedFiles.Renamed,","\t\t},","\t}","","\tfor k, v := range customParams {","\t\tif !standardParams[k] {","\t\t\tdata[k] = v","\t\t} else if eventEmitter != nil \u0026\u0026 repo != nil {","\t\t\teventEmitter.EmitMessage(repo, zap.WarnLevel, \"CELParamConflict\",","\t\t\t\tfmt.Sprintf(\"custom parameter '%s' conflicts with standard CEL variable and was ignored\", k))","\t\t}","\t}","","\tprg, err := env.Program(checked)","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"expression %#v failed to create a Program: %w\", expr, err)","\t}","","\tout, _, err := prg.Eval(data)","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"expression %#v failed to evaluate: %w\", expr, err)","\t}","\treturn out, nil","}","","type celPac struct {","\tvcx   provider.Interface","\tctx   context.Context","\tevent *info.Event","}","","func (t celPac) ProgramOptions() []cel.ProgramOption {","\treturn []cel.ProgramOption{}","}","","func (t celPac) pathChanged(vals ref.Val) ref.Val {","\tvar match types.Bool","\tchangedFiles, err := t.vcx.GetFiles(t.ctx, t.event)","\tif err != nil {","\t\treturn types.Bool(false)","\t}","\tfor i := range changedFiles.All {","\t\tif v, ok := vals.Value().(string); ok {","\t\t\tg := glob.MustCompile(v)","\t\t\tif g.Match(changedFiles.All[i]) {","\t\t\t\treturn types.Bool(true)","\t\t\t}","\t\t}","\t\tmatch = types.Bool(false)","\t}","","\treturn match","}","","func (t celPac) CompileOptions() []cel.EnvOption {","\treturn []cel.EnvOption{","\t\tcel.Function(\"pathChanged\",","\t\t\tcel.MemberOverload(\"pathChanged\", []*cel.Type{cel.StringType}, cel.BoolType,","\t\t\t\tcel.UnaryBinding(t.pathChanged))),","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,2,2,1,1,2,2,2,1,1,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,1,1,0,2,2,2,2,0,2,2,1,1,0,0,2,2,1,1,2,2,2,2,2,2,2,2,2,2,0,0,2,1,1,1,0,0,0,0,0,2,2,2,2,1,1,0,0,0,0,0,2,1,1,1,1,1,1,1,1,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,1,1,0,2,2,1,1,2,0,0,0,0,0,0,0,0,2,2,2,0,2,2,2,2,1,1,2,2,2,2,2,2,0,2,0,0,2,0,0,2,2,2,2,2,2,2]},{"id":69,"path":"pkg/matcher/cel_ast.go","lines":["package matcher","","import (","\t\"strings\"","","\texprpb \"google.golang.org/genproto/googleapis/api/expr/v1alpha1\"",")","","// NodeMatcher is a predicate function that inspects a CEL AST node.","// It returns true if the node matches the criteria being searched for.","type NodeMatcher func(expr *exprpb.Expr) bool","","// walkExprAST recursively walks a CEL expression AST and returns true","// if any node matches the provided matcher function.","//","// CEL AST Node Types:","//   - ConstExpr: Literal constant values (strings, numbers, bools, null, bytes)","//   - IdentExpr: Variable references (e.g., \"event_type\", \"body\")","//   - SelectExpr: Field access with dot notation (e.g., body.labels)","//   - CallExpr: Function calls and operators (e.g., size(), _[_] for bracket notation)","//   - ListExpr: List literals (e.g., [1, 2, 3])","//   - StructExpr: Map/struct literals (e.g., {\"key\": \"value\"})","//   - ComprehensionExpr: List operations (e.g., list.exists(x, condition))","func walkExprAST(expr *exprpb.Expr, matcher NodeMatcher) bool {","\tif expr == nil {","\t\treturn false","\t}","","\t// Check current node first","\tif matcher(expr) {","\t\treturn true","\t}","","\t// Recurse into children based on node type","\tswitch e := expr.GetExprKind().(type) {","\tcase *exprpb.Expr_ConstExpr:","\t\t// Constants have no children","\t\treturn false","","\tcase *exprpb.Expr_IdentExpr:","\t\t// Identifiers have no children","\t\treturn false","","\tcase *exprpb.Expr_SelectExpr:","\t\treturn walkExprAST(e.SelectExpr.GetOperand(), matcher)","","\tcase *exprpb.Expr_CallExpr:","\t\tif walkExprAST(e.CallExpr.GetTarget(), matcher) {","\t\t\treturn true","\t\t}","\t\tfor _, arg := range e.CallExpr.GetArgs() {","\t\t\tif walkExprAST(arg, matcher) {","\t\t\t\treturn true","\t\t\t}","\t\t}","","\tcase *exprpb.Expr_ListExpr:","\t\tfor _, elem := range e.ListExpr.GetElements() {","\t\t\tif walkExprAST(elem, matcher) {","\t\t\t\treturn true","\t\t\t}","\t\t}","","\tcase *exprpb.Expr_StructExpr:","\t\tfor _, entry := range e.StructExpr.GetEntries() {","\t\t\tif walkExprAST(entry.GetMapKey(), matcher) {","\t\t\t\treturn true","\t\t\t}","\t\t\tif walkExprAST(entry.GetValue(), matcher) {","\t\t\t\treturn true","\t\t\t}","\t\t}","","\tcase *exprpb.Expr_ComprehensionExpr:","\t\tcomp := e.ComprehensionExpr","\t\tif walkExprAST(comp.GetIterRange(), matcher) {","\t\t\treturn true","\t\t}","\t\tif walkExprAST(comp.GetAccuInit(), matcher) {","\t\t\treturn true","\t\t}","\t\tif walkExprAST(comp.GetLoopCondition(), matcher) {","\t\t\treturn true","\t\t}","\t\tif walkExprAST(comp.GetLoopStep(), matcher) {","\t\t\treturn true","\t\t}","\t\tif walkExprAST(comp.GetResult(), matcher) {","\t\t\treturn true","\t\t}","\t}","","\treturn false","}","","// matchIdentifier returns a NodeMatcher that matches IdentExpr nodes","// with the specified variable name.","func matchIdentifier(name string) NodeMatcher {","\treturn func(expr *exprpb.Expr) bool {","\t\tif ident := expr.GetIdentExpr(); ident != nil {","\t\t\treturn ident.GetName() == name","\t\t}","\t\treturn false","\t}","}","","// matchFieldAccess returns a NodeMatcher that matches SelectExpr nodes","// (dot notation field access) with any of the specified field names.","// Example: body.labels matches field \"labels\".","func matchFieldAccess(fieldNames ...string) NodeMatcher {","\tfieldSet := make(map[string]bool, len(fieldNames))","\tfor _, f := range fieldNames {","\t\tfieldSet[f] = true","\t}","\treturn func(expr *exprpb.Expr) bool {","\t\tif sel := expr.GetSelectExpr(); sel != nil {","\t\t\treturn fieldSet[sel.GetField()]","\t\t}","\t\treturn false","\t}","}","","// matchBracketAccess returns a NodeMatcher that matches bracket notation","// field access (body[\"labels\"]) with any of the specified key names.","// In CEL AST, bracket notation is represented as CallExpr with function \"_[_]\".","func matchBracketAccess(keyNames ...string) NodeMatcher {","\tkeySet := make(map[string]bool, len(keyNames))","\tfor _, k := range keyNames {","\t\tkeySet[k] = true","\t}","\treturn func(expr *exprpb.Expr) bool {","\t\tcall := expr.GetCallExpr()","\t\tif call == nil || call.GetFunction() != \"_[_]\" || len(call.GetArgs()) != 2 {","\t\t\treturn false","\t\t}","\t\t// Check if the key (second argument) is a string literal matching our keys","\t\tif keyExpr := call.GetArgs()[1]; keyExpr != nil {","\t\t\tif constExpr := keyExpr.GetConstExpr(); constExpr != nil {","\t\t\t\treturn keySet[constExpr.GetStringValue()]","\t\t\t}","\t\t}","\t\treturn false","\t}","}","","// combinedMatcher returns a NodeMatcher that returns true if any of","// the provided matchers match.","func combinedMatcher(matchers ...NodeMatcher) NodeMatcher {","\treturn func(expr *exprpb.Expr) bool {","\t\tfor _, m := range matchers {","\t\t\tif m(expr) {","\t\t\t\treturn true","\t\t\t}","\t\t}","\t\treturn false","\t}","}","","// containsRefsHeadsLiteral checks if the AST contains a string literal \"refs/heads/\".","// This is used to determine if the user explicitly included the refs/heads/ prefix","// in their CEL expression when comparing branches.","func containsRefsHeadsLiteral(expr *exprpb.Expr) bool {","\treturn walkExprAST(expr, func(e *exprpb.Expr) bool {","\t\tif constExpr := e.GetConstExpr(); constExpr != nil {","\t\t\treturn strings.Contains(constExpr.GetStringValue(), \"refs/heads/\")","\t\t}","\t\treturn false","\t})","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,0,2,2,2,0,0,2,2,2,2,0,2,2,2,0,2,2,0,2,2,2,2,2,2,2,2,0,0,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,0,0,2,2,2,2,2,2,1,1,2,1,1,2,1,1,2,1,1,0,0,2,0,0,0,0,2,2,2,2,2,2,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,0,1,0,0,0,0,0,2,2,2,2,2,2,0,2,0,0,0,0,0,0,2,2,2,2,2,2,0,0]},{"id":70,"path":"pkg/matcher/errors.go","lines":["package matcher","","import (","\t\"context\"","\t\"fmt\"","\t\"strings\"","","\tapipac \"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\tpacerrors \"github.com/openshift-pipelines/pipelines-as-code/pkg/errors\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/events\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","","\t\"go.uber.org/zap\"",")","","// checkCELEvaluateError checks if error is from CEL evaluation stages.","func checkIfCELEvaluateError(err error) bool {","\tif err == nil {","\t\treturn false","\t}","","\terrMsg := err.Error()","","\tpatterns := []string{","\t\t`failed to parse expression`,","\t\t`check failed`,","\t\t`failed to create a Program`,","\t\t`failed to evaluate`,","\t}","","\tfor _, pattern := range patterns {","\t\tif strings.Contains(errMsg, pattern) {","\t\t\treturn true","\t\t}","\t}","","\treturn false","}","","func reportCELValidationErrors(ctx context.Context, repo *apipac.Repository, validationErrors []*pacerrors.PacYamlValidations, eventEmitter *events.EventEmitter, vcx provider.Interface, event *info.Event) {","\terrorRows := make([]string, 0, len(validationErrors))","\tfor _, err := range validationErrors {","\t\terrorRows = append(errorRows, fmt.Sprintf(\"| %s | `%s` |\", err.Name, err.Err.Error()))","\t}","\tif len(errorRows) == 0 {","\t\treturn","\t}","\tmarkdownErrMessage := fmt.Sprintf(`%s","%s`, provider.ValidationErrorTemplate, strings.Join(errorRows, \"\\n\"))","\tif err := vcx.CreateComment(ctx, event, markdownErrMessage, provider.ValidationErrorTemplate); err != nil {","\t\teventEmitter.EmitMessage(repo, zap.ErrorLevel, \"PipelineRunCommentCreationError\",","\t\t\tfmt.Sprintf(\"failed to create comment: %s\", err.Error()))","\t}","}","","// sanitizeErrorAsMarkdown prepares a CEL evaluation error string to be rendered","// inside a GitHub / GitLab markdown table without breaking its layout.","//","// Markdown tables use the vertical bar character (`|`) as a column delimiter. If","// the original error message contains an un-escaped pipe the markdown renderer","// interprets it as the start of a new column or row which distorts the table","// produced by Pipelines-as-Code when reporting validation errors.","//","// To avoid this we escape every pipe with a backslash (\\|). We also replace any","// newline or carriage-return characters with a single space so that the whole","// error is kept on one row, preserving readability in the rendered comment.","func sanitizeErrorAsMarkdown(err error) string {","\terrStr := err.Error()","\terrStr = strings.ReplaceAll(errStr, \"|\", \"\\\\|\")","\terrStr = strings.ReplaceAll(errStr, \"\\n\", \" \")","\terrStr = strings.ReplaceAll(errStr, \"\\r\", \" \")","\treturn errStr","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,1,0,0,2,2,2,2,2,2,1,1,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2]},{"id":71,"path":"pkg/matcher/repo_runinfo_matcher.go","lines":["package matcher","","import (","\t\"context\"","\t\"errors\"","\t\"fmt\"","\t\"strings\"","","\t\"github.com/gobwas/glob\"","\tapipac \"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/sort\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"",")","","var ErrRepositoryNameConflict = errors.New(\"multiple repositories exist with the given name\")","","func MatchEventURLRepo(ctx context.Context, cs *params.Run, event *info.Event, ns string) (*apipac.Repository, error) {","\trepositories, err := cs.Clients.PipelineAsCode.PipelinesascodeV1alpha1().Repositories(ns).List(","\t\tctx, metav1.ListOptions{})","\tsort.RepositorySortByCreationOldestTime(repositories.Items)","\tif err != nil {","\t\treturn nil, err","\t}","\tfor _, repo := range repositories.Items {","\t\trepo.Spec.URL = strings.TrimSuffix(repo.Spec.URL, \"/\")","\t\tif repo.Spec.URL == event.URL {","\t\t\treturn \u0026repo, nil","\t\t}","\t}","","\treturn nil, nil","}","","// GetRepoByName get a repo by name anywhere on a cluster.","// Parameter 'ns' may optionally be supplied in case of a naming conflict.","func GetRepoByName(ctx context.Context, cs *params.Run, repoName, ns string) (*apipac.Repository, error) {","\trepositories, err := cs.Clients.PipelineAsCode.PipelinesascodeV1alpha1().Repositories(ns).List(","\t\tctx, metav1.ListOptions{","\t\t\tFieldSelector: \"metadata.name==\" + repoName,","\t\t})","\tif err != nil {","\t\treturn nil, err","\t}","","\tswitch len(repositories.Items) {","\tcase 0:","\t\treturn nil, nil","\tcase 1:","\t\treturn \u0026repositories.Items[0], nil","\tdefault:","\t\treturn nil, ErrRepositoryNameConflict","\t}","}","","// IncomingWebhookRule will match a rule to an incoming rule, currently a rule is a target branch.","// Supports both exact string matching and glob patterns.","// Uses first-match-wins strategy: returns the first webhook with a matching target.","func IncomingWebhookRule(branch string, incomingWebhooks []apipac.Incoming) *apipac.Incoming {","\t// TODO: one day we will match the hook.Type here when we get something else than the dumb one (ie: slack)","\tfor i := range incomingWebhooks {","\t\thook := \u0026incomingWebhooks[i]","","\t\t// Check each target in this webhook","\t\tfor _, target := range hook.Targets {","\t\t\tmatched, err := matchTarget(branch, target)","\t\t\tif err != nil {","\t\t\t\t// Skip invalid glob patterns and continue to next target","\t\t\t\tcontinue","\t\t\t}","","\t\t\tif matched {","\t\t\t\t// First match wins - return immediately","\t\t\t\treturn hook","\t\t\t}","\t\t}","\t}","\treturn nil","}","","// matchTarget checks if a branch matches a target pattern using glob matching.","// Supports both exact string matching and glob patterns.","func matchTarget(branch, target string) (bool, error) {","\tg, err := glob.Compile(target)","\tif err != nil {","\t\treturn false, fmt.Errorf(\"invalid glob pattern %q: %w\", target, err)","\t}","","\treturn g.Match(branch), nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,1,1,2,2,2,2,2,0,0,2,0,0,0,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,0,0,2,0,0,0,0,2,2,2,2,2,0,2,0]},{"id":72,"path":"pkg/opscomments/args.go","lines":["package opscomments","","import (","\t\"regexp\"","\t\"strings\"",")","","// ParseKeyValueArgs will parse things like key=value key=\"value\" key=\"value1 value2\"","// key=\"value1 \\\"value2\\\"\" key=value1=value2.","func ParseKeyValueArgs(input string) map[string]string {","\tif !strings.HasPrefix(input, \"/\") {","\t\treturn nil","\t}","\tkeyValueRegex := regexp.MustCompile(`(\\w+)=(?:\"([^\"\\\\]*(?:\\\\.[^\"\\\\]*)*)\"|([^\"'\\s]+))`)","\tmatches := keyValueRegex.FindAllStringSubmatch(input, -1)","\tkeyValuePairs := make(map[string]string)","","\tfor _, match := range matches {","\t\tkey := match[1]","\t\tvar value string","\t\tif match[2] != \"\" {","\t\t\tvalue = strings.ReplaceAll(match[2], `\\\"`, `\"`)","\t\t} else {","\t\t\tvalue = match[3]","\t\t}","\t\tkeyValuePairs[key] = value","\t}","","\treturn keyValuePairs","}"],"coverage":[0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,0]},{"id":73,"path":"pkg/opscomments/comments.go","lines":["package opscomments","","import (","\t\"fmt\"","\t\"regexp\"","\t\"strings\"","","\t\"go.uber.org/zap\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/acl\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/events\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"",")","","var (","\ttestAllRegex      = regexp.MustCompile(`(?m)^/test\\s*$`)","\tretestAllRegex    = regexp.MustCompile(`(?m)^/retest\\s*$`)","\ttestSingleRegex   = regexp.MustCompile(`(?m)^/test[ \\t]+\\S+`)","\tretestSingleRegex = regexp.MustCompile(`(?m)^/retest[ \\t]+\\S+`)","\toktotestRegex     = regexp.MustCompile(acl.OKToTestCommentRegexp)","\tcancelAllRegex    = regexp.MustCompile(`(?m)^(/cancel)\\s*$`)","\tcancelSingleRegex = regexp.MustCompile(`(?m)^(/cancel)[ \\t]+\\S+`)",")","","type EventType string","","func (e EventType) String() string {","\treturn string(e)","}","","var (","\tNoOpsCommentEventType        = EventType(\"no-ops-comment\")","\tTestAllCommentEventType      = EventType(\"test-all-comment\")","\tTestSingleCommentEventType   = EventType(\"test-comment\")","\tRetestSingleCommentEventType = EventType(\"retest-comment\")","\tRetestAllCommentEventType    = EventType(\"retest-all-comment\")","\tOnCommentEventType           = EventType(\"on-comment\")","\tCancelCommentSingleEventType = EventType(\"cancel-comment\")","\tCancelCommentAllEventType    = EventType(\"cancel-all-comment\")","\tOkToTestCommentEventType     = EventType(\"ok-to-test-comment\")",")","","const (","\ttestComment   = \"/test\"","\tretestComment = \"/retest\"","\tcancelComment = \"/cancel\"",")","","func CommentEventType(comment string) EventType {","\tswitch {","\tcase retestAllRegex.MatchString(comment):","\t\treturn RetestAllCommentEventType","\tcase retestSingleRegex.MatchString(comment):","\t\treturn RetestSingleCommentEventType","\tcase testAllRegex.MatchString(comment):","\t\treturn TestAllCommentEventType","\tcase testSingleRegex.MatchString(comment):","\t\treturn TestSingleCommentEventType","\tcase oktotestRegex.MatchString(comment):","\t\treturn OkToTestCommentEventType","\tcase cancelAllRegex.MatchString(comment):","\t\treturn CancelCommentAllEventType","\tcase cancelSingleRegex.MatchString(comment):","\t\treturn CancelCommentSingleEventType","\tdefault:","\t\treturn NoOpsCommentEventType","\t}","}","","// SetEventTypeAndTargetPR function will set the event type and target test pipeline run in an event.","func SetEventTypeAndTargetPR(event *info.Event, comment string) {","\tcommentType := CommentEventType(comment)","\tif commentType == RetestSingleCommentEventType || commentType == TestSingleCommentEventType {","\t\tevent.TargetTestPipelineRun = GetPipelineRunFromTestComment(comment)","\t}","\tif commentType == CancelCommentAllEventType || commentType == CancelCommentSingleEventType {","\t\tevent.CancelPipelineRuns = true","\t}","\tif commentType == CancelCommentSingleEventType {","\t\tevent.TargetCancelPipelineRun = GetPipelineRunFromCancelComment(comment)","\t}","\tevent.EventType = commentType.String()","\tevent.TriggerComment = comment","}","","func IsOkToTestComment(comment string) bool {","\treturn oktotestRegex.MatchString(comment)","}","","// GetSHAFromOkToTestComment extracts the optional SHA from an /ok-to-test comment.","func GetSHAFromOkToTestComment(comment string) string {","\tmatches := oktotestRegex.FindStringSubmatch(comment)","\tif len(matches) \u003e 2 {","\t\treturn strings.TrimSpace(matches[2])","\t}","\treturn \"\"","}","","// EventTypeBackwardCompat handle the backward compatibility we need to keep until","// we have done the deprecated notice","//","// 2024-07-01 chmouel","//","//\tset anyOpsComments to pull_request see https://issues.redhat.com/browse/SRVKP-5775","//\twe keep on-comment to the \"on-comment\" type","func EventTypeBackwardCompat(eventEmitter *events.EventEmitter, repo *v1alpha1.Repository, label string) string {","\tif label == OnCommentEventType.String() {","\t\treturn label","\t}","\tif IsAnyOpsEventType(label) {","\t\teventEmitter.EmitMessage(repo, zap.WarnLevel, \"DeprecatedOpsComment\",","\t\t\tfmt.Sprintf(\"the %s event type is deprecated, this will be changed to %s in the future\",","\t\t\t\tlabel, triggertype.PullRequest.String()))","\t\treturn triggertype.PullRequest.String()","\t}","\treturn label","}","","func IsAnyOpsEventType(eventType string) bool {","\treturn eventType == TestSingleCommentEventType.String() ||","\t\teventType == TestAllCommentEventType.String() ||","\t\teventType == RetestAllCommentEventType.String() ||","\t\teventType == RetestSingleCommentEventType.String() ||","\t\teventType == CancelCommentSingleEventType.String() ||","\t\teventType == CancelCommentAllEventType.String() ||","\t\teventType == OkToTestCommentEventType.String() ||","\t\teventType == OnCommentEventType.String()","}","","// AnyOpsKubeLabelInSelector will output a Kubernetes label out of all possible","// CommentEvent Type for selection.","func AnyOpsKubeLabelInSelector() string {","\treturn fmt.Sprintf(\"%s,%s,%s,%s,%s,%s,%s,%s\",","\t\tTestSingleCommentEventType.String(),","\t\tTestAllCommentEventType.String(),","\t\tRetestAllCommentEventType.String(),","\t\tRetestSingleCommentEventType.String(),","\t\tCancelCommentSingleEventType.String(),","\t\tCancelCommentAllEventType.String(),","\t\tOkToTestCommentEventType.String(),","\t\tOnCommentEventType.String())","}","","func GetPipelineRunFromTestComment(comment string) string {","\tif strings.Contains(comment, testComment) {","\t\treturn getNameFromComment(testComment, comment)","\t}","\treturn getNameFromComment(retestComment, comment)","}","","func GetPipelineRunFromCancelComment(comment string) string {","\treturn getNameFromComment(cancelComment, comment)","}","","func getNameFromComment(typeOfComment, comment string) string {","\tsplitTest := strings.Split(strings.TrimSpace(comment), typeOfComment)","\tif len(splitTest) \u003c 2 {","\t\treturn \"\"","\t}","\t// now get the first line","\tgetFirstLine := strings.Split(splitTest[1], \"\\n\")","","\t// and the first argument","\tfirstArg := strings.Split(getFirstLine[0], \" \")","\tif len(firstArg) \u003c 2 {","\t\treturn \"\"","\t}","","\t// trim spaces","\treturn strings.TrimSpace(firstArg[1])","}","","func GetPipelineRunAndBranchNameFromTestComment(comment string) (string, string, error) {","\tif strings.Contains(comment, testComment) {","\t\treturn getPipelineRunAndBranchNameFromComment(testComment, comment)","\t}","\treturn getPipelineRunAndBranchNameFromComment(retestComment, comment)","}","","func GetPipelineRunAndBranchNameFromCancelComment(comment string) (string, string, error) {","\treturn getPipelineRunAndBranchNameFromComment(cancelComment, comment)","}","","// getPipelineRunAndBranchNameFromComment function will take GitOps comment and split the comment","// by /test, /retest or /cancel to return branch name and pipelinerun name.","func getPipelineRunAndBranchNameFromComment(typeOfComment, comment string) (string, string, error) {","\tvar prName, branchName string","\tsplitTest := strings.Split(comment, typeOfComment)","","\t// after the split get the second part of the typeOfComment (/test, /retest or /cancel)","\t// as second part can be branch name or pipelinerun name and branch name","\t// ex: /test branch:nightly, /test prname branch:nightly","\tif splitTest[1] != \"\" \u0026\u0026 strings.Contains(splitTest[1], \":\") {","\t\tbranchData := strings.Split(splitTest[1], \":\")","","\t\t// make sure no other word is supported other than branch word","\t\tif !strings.Contains(branchData[0], \"branch\") {","\t\t\treturn prName, branchName, fmt.Errorf(\"the GitOps comment%s does not contain a branch word\", branchData[0])","\t\t}","\t\tbranchName = strings.Split(strings.TrimSpace(branchData[1]), \" \")[0]","","\t\t// if data after the split contains prname then fetch that","\t\tprData := strings.Split(strings.TrimSpace(branchData[0]), \" \")","\t\tif len(prData) \u003e 1 {","\t\t\tprName = strings.TrimSpace(prData[0])","\t\t}","\t} else {","\t\t// get the second part of the typeOfComment (/test, /retest or /cancel)","\t\t// as second part contains pipelinerun name","\t\t// ex: /test prname","\t\tgetFirstLine := strings.Split(splitTest[1], \"\\n\")","\t\t// trim spaces","\t\tprName = strings.TrimSpace(getFirstLine[0])","\t}","\treturn prName, branchName, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,0,0,2,2,2,2,2,1,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,0,0,2,2,2,0,2,2,2,1,1,0,2,2,2,2,2,2,2,0,0,2,0,0,2,2,2,2,2,0,0,2,2,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0]},{"id":74,"path":"pkg/params/cli.go","lines":["package params","","import (","\t\"os\"","","\t\"github.com/AlecAivazis/survey/v2\"","\t\"github.com/AlecAivazis/survey/v2/terminal\"",")","","type PacCliOpts struct {","\tNoColoring    bool","\tAllNameSpaces bool","\tNamespace     string","\tAskOpts       survey.AskOpt","}","","func NewCliOptions() *PacCliOpts {","\treturn \u0026PacCliOpts{","\t\tAskOpts: func(opt *survey.AskOptions) error {","\t\t\topt.Stdio = terminal.Stdio{","\t\t\t\tIn:  os.Stdin,","\t\t\t\tOut: os.Stdout,","\t\t\t\tErr: os.Stderr,","\t\t\t}","\t\t\treturn nil","\t\t},","\t}","}","","func (c *PacCliOpts) Ask(qss []*survey.Question, answer any) error {","\treturn survey.Ask(qss, answer, c.AskOpts)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1]},{"id":75,"path":"pkg/params/clients/clients.go","lines":["package clients","","import (","\t\"context\"","\t\"fmt\"","\t\"io\"","\t\"net\"","\t\"net/http\"","\t\"sync\"","\t\"time\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/consoleui\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/generated/clientset/versioned\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/pkg/errors\"","\tversioned2 \"github.com/tektoncd/pipeline/pkg/client/clientset/versioned\"","\t\"go.uber.org/zap\"","\t\"k8s.io/client-go/dynamic\"","\t\"k8s.io/client-go/kubernetes\"","\t\"k8s.io/client-go/rest\"","\t\"k8s.io/client-go/tools/clientcmd\"",")","","const (","\t// most programming languages  do not have a timeout, but c# does a default","\t// of 100 seconds so using that value.","\tConnectMaxWaitTime = 100 * time.Second","\tRequestMaxWaitTime = 100 * time.Second",")","","type Clients struct {","\tClientInitialized bool","\tPipelineAsCode    versioned.Interface","\tTekton            versioned2.Interface","\tKube              kubernetes.Interface","\tHTTP              http.Client","\tLog               *zap.SugaredLogger","\tDynamic           dynamic.Interface","\tconsoleUIMutex    *sync.Mutex","\tconsoleUI         consoleui.Interface","}","","func (c *Clients) InitClients() {","\tc.consoleUIMutex = \u0026sync.Mutex{}","}","","func (c *Clients) GetURL(ctx context.Context, url string) ([]byte, error) {","\tnctx, cancel := context.WithTimeout(ctx, RequestMaxWaitTime)","\tdefer cancel()","","\treq, err := http.NewRequestWithContext(nctx, http.MethodGet, url, nil)","\tif err != nil {","\t\treturn []byte{}, err","\t}","\tres, err := c.HTTP.Do(req)","\tif err != nil {","\t\treturn []byte{}, err","\t}","\tdefer res.Body.Close()","\tstatusOK := res.StatusCode \u003e= 200 \u0026\u0026 res.StatusCode \u003c 300","\tif !statusOK {","\t\treturn nil, fmt.Errorf(\"Non-OK HTTP status: %d\", res.StatusCode)","\t}","","\tdata, err := io.ReadAll(res.Body)","\tif err != nil {","\t\treturn []byte{}, err","\t}","\treturn data, nil","}","","// Set kube client based on config.","func (c *Clients) kubeClient(config *rest.Config) (kubernetes.Interface, error) {","\tk8scs, err := kubernetes.NewForConfig(config)","\tif err != nil {","\t\treturn nil, errors.Wrapf(err, \"failed to create k8s client from config\")","\t}","","\treturn k8scs, nil","}","","func (c *Clients) dynamicClient(config *rest.Config) (dynamic.Interface, error) {","\tdynamicClient, err := dynamic.NewForConfig(config)","\tif err != nil {","\t\treturn nil, errors.Wrapf(err, \"failed to create dynamic client from config\")","\t}","\treturn dynamicClient, err","}","","func (c *Clients) kubeConfig(info *info.Info) (*rest.Config, error) {","\tloadingRules := clientcmd.NewDefaultClientConfigLoadingRules()","\tif info.Kube.ConfigPath != \"\" {","\t\tloadingRules.ExplicitPath = info.Kube.ConfigPath","\t}","\tconfigOverrides := \u0026clientcmd.ConfigOverrides{}","\tif info.Kube.Context != \"\" {","\t\tconfigOverrides.CurrentContext = info.Kube.Context","\t}","\tkubeConfig := clientcmd.NewNonInteractiveDeferredLoadingClientConfig(loadingRules, configOverrides)","\tif info.Kube.Namespace == \"\" {","\t\tnamespace, _, err := kubeConfig.Namespace()","\t\tif err != nil {","\t\t\treturn nil, errors.Wrap(err, \"Couldn't get kubeConfiguration namespace\")","\t\t}","\t\tinfo.Kube.Namespace = namespace","\t}","\tconfig, err := kubeConfig.ClientConfig()","\tif err != nil {","\t\treturn nil, errors.Wrap(err, \"Parsing kubeconfig failed\")","\t}","\treturn config, nil","}","","func (c *Clients) tektonClient(config *rest.Config) (versioned2.Interface, error) {","\tcs, err := versioned2.NewForConfig(config)","\tif err != nil {","\t\treturn nil, err","\t}","\treturn cs, nil","}","","func (c *Clients) pacClient(config *rest.Config) (versioned.Interface, error) {","\tcs, err := versioned.NewForConfig(config)","\tif err != nil {","\t\treturn nil, err","\t}","","\treturn cs, nil","}","","func (c *Clients) consoleUIClient(ctx context.Context, dynamic dynamic.Interface, info *info.Info) consoleui.Interface {","\treturn consoleui.New(ctx, dynamic, info)","}","","func (c *Clients) NewClients(ctx context.Context, info *info.Info) error {","\tif c.consoleUIMutex == nil {","\t\tc.consoleUIMutex = \u0026sync.Mutex{}","\t}","\tif c.ClientInitialized {","\t\treturn nil","\t}","\tprod, _ := zap.NewProduction()","\tlogger := prod.Sugar()","\tdefer func() {","\t\t_ = logger.Sync() // flushes buffer, if any","\t}()","\tc.Log = logger","","\tc.HTTP = http.Client{","\t\tTimeout: RequestMaxWaitTime,","\t\tTransport: \u0026http.Transport{","\t\t\tDialContext: (\u0026net.Dialer{","\t\t\t\tTimeout: ConnectMaxWaitTime,","\t\t\t}).DialContext,","\t\t},","\t}","\tconfig, err := c.kubeConfig(info)","\tif err != nil {","\t\treturn err","\t}","\tconfig.QPS = 50","\tconfig.Burst = 50","","\tc.Kube, err = c.kubeClient(config)","\tif err != nil {","\t\treturn err","\t}","\tc.Tekton, err = c.tektonClient(config)","\tif err != nil {","\t\treturn err","\t}","","\tc.PipelineAsCode, err = c.pacClient(config)","\tif err != nil {","\t\treturn err","\t}","","\tc.Dynamic, err = c.dynamicClient(config)","\tif err != nil {","\t\treturn err","\t}","","\tc.SetConsoleUI(c.consoleUIClient(ctx, c.Dynamic, info))","\tc.ClientInitialized = true","","\treturn nil","}","","func (c *Clients) ConsoleUI() consoleui.Interface {","\tc.consoleUIMutex.Lock()","\tdefer c.consoleUIMutex.Unlock()","\treturn c.consoleUI","}","","func (c *Clients) SetConsoleUI(consoleUI consoleui.Interface) {","\tif c.consoleUIMutex == nil {","\t\tc.consoleUIMutex = \u0026sync.Mutex{}","\t}","\tc.consoleUIMutex.Lock()","\tdefer c.consoleUIMutex.Unlock()","\tc.consoleUI = consoleUI","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,2,2,2,2,2,2,1,1,2,2,1,1,2,2,2,2,2,0,2,2,1,1,2,0,0,0,1,1,1,1,1,0,1,0,0,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,0,0,1,1,1,1,1,1,0,0,1,1,1,1,1,0,1,0,0,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,0,1,1,1,1,0,0,1,1,1,1,1,0,1,1,1,1,1,1,1,0]},{"id":76,"path":"pkg/params/config_sync.go","lines":["package params","","import (","\t\"context\"","\t\"fmt\"","\t\"os\"","\t\"os/signal\"","\t\"syscall\"","","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/client-go/informers\"","\t\"k8s.io/client-go/tools/cache\"","\t\"knative.dev/pkg/system\"",")","","func StartConfigSync(ctx context.Context, run *Run) {","\t// init pac config","\t_ = run.UpdatePacConfig(ctx)","","\tinformerFactory := informers.NewSharedInformerFactoryWithOptions(run.Clients.Kube, 0,","\t\tinformers.WithNamespace(system.Namespace()),","\t\tinformers.WithTweakListOptions(func(opts *metav1.ListOptions) {","\t\t\topts.FieldSelector = fmt.Sprintf(\"metadata.name=%s\", run.Info.Controller.Configmap)","\t\t}))","\tinformer := informerFactory.Core().V1().ConfigMaps().Informer()","\t_, _ = informer.AddEventHandler(cache.ResourceEventHandlerFuncs{","\t\tAddFunc: func(_ any) {","\t\t\t// nothing to do","\t\t},","\t\tUpdateFunc: func(_, _ any) {","\t\t\t_ = run.UpdatePacConfig(ctx)","\t\t},","\t\tDeleteFunc: func(_ any) {","\t\t\t// nothing to do","\t\t},","\t})","","\tstopCh := make(chan struct{})","\tdefer close(stopCh)","","\t// start the informer","\tinformer.Run(stopCh)","","\t// Wait for termination signal to stop the informer","\tsig := make(chan os.Signal, 1)","\tsignal.Notify(sig, syscall.SIGINT, syscall.SIGTERM)","\t\u003c-sig","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0]},{"id":77,"path":"pkg/params/info/controller_info.go","lines":["package info","","import (","\t\"context\"","\t\"os\"",")","","var currentControllerName = contextKey(\"current-controller-name\")","","const (","\tDefaultPipelinesAscodeSecretName = \"pipelines-as-code-secret\"","","\tDefaultPipelinesAscodeConfigmapName = \"pipelines-as-code\"","\tDefaultGlobalRepoName               = \"pipelines-as-code\"","\tdefaultControllerLabel              = \"default\"",")","","var InstallNamespaces = []string{\"openshift-pipelines\", \"pipelines-as-code\"}","","type ControllerInfo struct {","\tName             string `json:\"name\"`","\tConfigmap        string `json:\"configmap\"`","\tSecret           string `json:\"secret\"`","\tGlobalRepository string `json:\"gRepo\"`","}","","// GetControllerInfoFromEnvOrDefault retrieves controller info from the env or use the defaults","// TODO: handles doublons when fallbacking in case there is multiple","// controllers but no env variable.","func GetControllerInfoFromEnvOrDefault() *ControllerInfo {","\tcontrollerlabel, ok := os.LookupEnv(\"PAC_CONTROLLER_LABEL\")","\tif !ok {","\t\tcontrollerlabel = defaultControllerLabel","\t}","\tcontrollerSecret, ok := os.LookupEnv(\"PAC_CONTROLLER_SECRET\")","\tif !ok {","\t\tcontrollerSecret = DefaultPipelinesAscodeSecretName","\t}","\tcontrollerConfigMap, ok := os.LookupEnv(\"PAC_CONTROLLER_CONFIGMAP\")","\tif !ok {","\t\tcontrollerConfigMap = DefaultPipelinesAscodeConfigmapName","\t}","\tglobalRepo, ok := os.LookupEnv(\"PAC_CONTROLLER_GLOBAL_REPOSITORY\")","\tif !ok {","\t\tglobalRepo = DefaultGlobalRepoName","\t}","\treturn \u0026ControllerInfo{","\t\tName:             controllerlabel,","\t\tSecret:           controllerSecret,","\t\tConfigmap:        controllerConfigMap,","\t\tGlobalRepository: globalRepo,","\t}","}","","// StoreCurrentControllerName stores current controller name in the context.","func StoreCurrentControllerName(ctx context.Context, name string) context.Context {","\treturn context.WithValue(ctx, currentControllerName, name)","}","","// GetCurrentControllerName retrieves current controller name from the context.","func GetCurrentControllerName(ctx context.Context) string {","\tif val := ctx.Value(currentControllerName); val != nil {","\t\tif name, ok := val.(string); ok {","\t\t\treturn name","\t\t}","\t}","\treturn \"\"","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,0,0,2,2,2,2,2,0,2,0]},{"id":78,"path":"pkg/params/info/events.go","lines":["package info","","import (","\t\"net/http\"","\t\"time\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"",")","","type Event struct {","\tState","\tEvent any","","\t// EventType is what coming from the provider header, i.e:","\t// GitHub -\u003e pull_request","\t// GitLab -\u003e Merge Request Hook","\t// Incoming Webhook  -\u003e incoming (always a push)","\t// Usually used for payload filtering passed from trigger directly","\tEventType string","","\t// Full request","\tRequest *Request","","\t// TriggerTarget stable field across providers, ie: on GitLab, Github and","\t// others it would be always be pull_request we can rely on to know if it's","\t// a push or a pull_request","\tTriggerTarget triggertype.Trigger","","\t// Target PipelineRun, the target PipelineRun user request. Used in incoming webhook","\tTargetPipelineRun string","","\tBaseBranch    string // branch against where we are making the PR","\tDefaultBranch string // master/main branches to know where things like the OWNERS file is located.","\tHeadBranch    string // branch from where our SHA get tested","\tBaseURL       string // url against where we are making the PR","\tHeadURL       string // url from where our SHA get tested","\tSHA           string","\tSender        string","\tURL           string // WEB url not the git URL, which would match to the repo.spec","\tSHAURL        string // pretty URL for web browsing for UIs (cli/web)","\tSHATitle      string // commit title for UIs","","\t// Full commit information populated by provider.GetCommitInfo()","\tSHAMessage        string    // full commit message (not just title)","\tSHAAuthorName     string    // commit author name","\tSHAAuthorEmail    string    // commit author email","\tSHAAuthorDate     time.Time // when the commit was authored","\tSHACommitterName  string    // committer name (may differ from author)","\tSHACommitterEmail string    // committer email","\tSHACommitterDate  time.Time // when the commit was committed","","\tPullRequestNumber int      // Pull or Merge Request number","\tPullRequestTitle  string   // Title of the pull Request","\tPullRequestLabel  []string // Labels of the pull Request","\tTriggerComment    string   // The comment triggering the pipelinerun when using on-comment annotation","","\t// HasSkipCommand indicates whether the commit message contains a skip CI command","\t// (e.g., [skip ci], [ci skip], [skip tkn], [tkn skip]). When true, PipelineRun","\t// execution will be skipped unless overridden by a GitOps command (e.g., /test, /retest).","\t// This allows users to bypass CI for documentation changes or minor fixes while still","\t// maintaining the ability to manually trigger builds when needed.","\tHasSkipCommand bool","","\t// TODO: move forge specifics to each driver","\t// Github","\tOrganization   string","\tRepository     string","\tInstallationID int64","\tGHEURL         string","","\t// TODO: move out inside the provider","\t// Bitbucket Cloud","\tAccountID string","","\t// TODO: move out inside the provider","\t// Bitbucket Data Center","\tCloneURL string // bitbucket data center has a different url for cloning the repo than normal public html url","\tProvider *Provider","","\t// GitLab","\tSourceProjectID int64","\tTargetProjectID int64","}","","type State struct {","\tTargetTestPipelineRun   string","\tCancelPipelineRuns      bool","\tTargetCancelPipelineRun string","}","","type Provider struct {","\tToken                 string","\tURL                   string","\tUser                  string","\tWebhookSecret         string","\tWebhookSecretFromRepo bool","}","","type Request struct {","\tHeader  http.Header","\tPayload []byte","}","","// DeepCopyInto deep copy runinfo in another instance.","func (r *Event) DeepCopyInto(out *Event) {","\t*out = *r","}","","// NewEvent returns a new Event.","func NewEvent() *Event {","\treturn \u0026Event{","\t\tProvider: \u0026Provider{},","\t\tRequest:  \u0026Request{},","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,2,2,2,2,2,2]},{"id":79,"path":"pkg/params/info/info.go","lines":["package info","","import (","\t\"net/http\"","\t\"sync\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/settings\"","\t\"go.uber.org/zap\"",")","","type Info struct {","\tpacMutex   *sync.Mutex","\tPac        *PacOpts","\tKube       *KubeOpts","\tController *ControllerInfo","}","","func NewInfo() Info {","\treturn Info{","\t\tpacMutex:   \u0026sync.Mutex{},","\t\tPac:        NewPacOpts(),","\t\tKube:       \u0026KubeOpts{},","\t\tController: GetControllerInfoFromEnvOrDefault(),","\t}","}","","func (i *Info) InitInfo() {","\ti.pacMutex = \u0026sync.Mutex{}","}","","func (i *Info) GetPacOpts() PacOpts {","\tif i.pacMutex == nil {","\t\ti.pacMutex = \u0026sync.Mutex{}","\t}","\ti.pacMutex.Lock()","\tdefer i.pacMutex.Unlock()","\treturn *i.Pac","}","","func (i *Info) UpdatePacOpts(logger *zap.SugaredLogger, configData map[string]string, httpClient *http.Client) (*settings.Settings, error) {","\tif i.pacMutex == nil {","\t\ti.pacMutex = \u0026sync.Mutex{}","\t}","\ti.pacMutex.Lock()","\tdefer i.pacMutex.Unlock()","","\tif err := settings.SyncConfig(logger, \u0026i.Pac.Settings, configData, settings.DefaultValidators(), httpClient); err != nil {","\t\treturn nil, err","\t}","\treturn \u0026i.Pac.Settings, nil","}","","func (i *Info) DeepCopy(out *Info) {","\t*out = *i","}","","type (","\tcontextKey string",")"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,0,1,1,1,0,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,0,0,0,0]},{"id":80,"path":"pkg/params/info/kube.go","lines":["package info","","import (","\t\"fmt\"","\t\"os\"","\t\"runtime\"","","\t\"github.com/spf13/cobra\"",")","","type KubeOpts struct {","\tConfigPath string","\tContext    string","\tNamespace  string","}","","func userHomeDir() string {","\tif runtime.GOOS == \"windows\" {","\t\thome := os.Getenv(\"HOMEDRIVE\") + os.Getenv(\"HOMEPATH\")","\t\tif home == \"\" {","\t\t\thome = os.Getenv(\"USERPROFILE\")","\t\t}","\t\treturn home","\t}","\treturn os.Getenv(\"HOME\")","}","","func (k *KubeOpts) AddFlags(cmd *cobra.Command) {","\tenvkconfig := os.Getenv(\"KUBECONFIG\")","\tif envkconfig == \"\" {","\t\tenvkconfig = fmt.Sprintf(\"%s/.kube/config\", userHomeDir())","\t}","\tcmd.PersistentFlags().StringVarP(","\t\t\u0026k.ConfigPath,","\t\t\"kubeconfig\", \"k\", envkconfig,","\t\tfmt.Sprintf(\"Path to the kubeconfig file to use for CLI requests (default: %s)\", envkconfig))","","\tcmd.PersistentFlags().StringVarP(","\t\t\u0026k.Namespace,","\t\t\"namespace\", \"n\", \"\",","\t\t\"If present, the namespace scope for this CLI request\")","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,1,1,1,1,1,0,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0]},{"id":81,"path":"pkg/params/info/ns.go","lines":["package info","","import \"context\"","","var nsContextKey = contextKey(\"namespace\")","","// StoreNS stores namespace in context.","func StoreNS(ctx context.Context, ns string) context.Context {","\treturn context.WithValue(ctx, nsContextKey, ns)","}","","// GetNS gets namespace from context.","func GetNS(ctx context.Context) string {","\tif val := ctx.Value(nsContextKey); val != nil {","\t\tif ns, ok := val.(string); ok {","\t\t\treturn ns","\t\t}","\t}","\treturn \"\"","}"],"coverage":[0,0,0,0,0,0,0,1,1,1,0,0,1,1,1,1,1,0,1,0]},{"id":82,"path":"pkg/params/info/pac.go","lines":["package info","","import (","\t\"os\"","\t\"strings\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/settings\"","\t\"github.com/spf13/cobra\"",")","","type PacOpts struct {","\tsettings.Settings","\tWebhookType        string","\tPayloadFile        string","\tTektonDashboardURL string","}","","func NewPacOpts() *PacOpts {","\treturn \u0026PacOpts{","\t\tSettings: settings.DefaultSettings(),","\t}","}","","func (p *PacOpts) DeepCopy(out *PacOpts) {","\t*out = *p","}","","func (p *PacOpts) AddFlags(cmd *cobra.Command) error {","\tcmd.PersistentFlags().StringVarP(\u0026p.WebhookType, \"git-provider-type\", \"\",","\t\tos.Getenv(\"PAC_GIT_PROVIDER_TYPE\"),","\t\t\"Webhook type\")","","\tcmd.PersistentFlags().StringVarP(\u0026p.PayloadFile,","\t\t\"payload-file\", \"\", os.Getenv(\"PAC_PAYLOAD_FILE\"), \"A file containing the webhook payload\")","","\tapplicationName := os.Getenv(\"PAC_APPLICATION_NAME\")","\tcmd.Flags().StringVar(\u0026p.ApplicationName,","\t\t\"application-name\", applicationName,","\t\t\"The name of the application.\")","","\tsecretAutoCreation := false","\tsecretAutoCreationEnv := os.Getenv(\"PAC_SECRET_AUTO_CREATE\")","\tif strings.ToLower(secretAutoCreationEnv) == \"true\" ||","\t\tstrings.ToLower(secretAutoCreationEnv) == \"yes\" || secretAutoCreationEnv == \"1\" {","\t\tsecretAutoCreation = true","\t}","\tcmd.Flags().BoolVar(\u0026p.SecretAutoCreation,","\t\t\"secret-auto-creation\",","\t\tsecretAutoCreation,","\t\t\"Whether to create automatically secrets.\")","","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0]},{"id":83,"path":"pkg/params/install.go","lines":["package params","","import (","\t\"context\"","\t\"fmt\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"",")","","const pipelineAsCodeControllerName = \"pipelines-as-code-controller\"","","func GetInstallLocation(ctx context.Context, run *Run) (string, string, error) {","\tfor _, ns := range info.InstallNamespaces {","\t\tversion := \"unknown\"","\t\tdeployment, err := run.Clients.Kube.AppsV1().Deployments(ns).Get(ctx, pipelineAsCodeControllerName, metav1.GetOptions{})","\t\tif err != nil {","\t\t\tcontinue","\t\t}","\t\tif val, ok := deployment.GetLabels()[\"app.kubernetes.io/version\"]; ok {","\t\t\tversion = val","\t\t}","\t\treturn ns, version, nil","\t}","\treturn \"\", \"\", fmt.Errorf(\"cannot find your pipelines-as-code installation, check that it is installed and you have access\")","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,2,2,2,2,0,2,0]},{"id":84,"path":"pkg/params/run.go","lines":["package params","","import (","\t\"context\"","\t\"fmt\"","\t\"os\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/consoleui\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/clients\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"",")","","type Run struct {","\tClients clients.Clients","\tInfo    info.Info","}","","func (r *Run) UpdatePacConfig(ctx context.Context) error {","\tns := info.GetNS(ctx)","\tif ns == \"\" {","\t\treturn fmt.Errorf(\"failed to find namespace\")","\t}","","\t// TODO: move this to kubeinteractions class so we can add unittests.","\tcfg, err := r.Clients.Kube.CoreV1().ConfigMaps(ns).Get(ctx, r.Info.Controller.Configmap, metav1.GetOptions{})","\tif err != nil {","\t\treturn err","\t}","","\tupdatedPacInfo, err := r.Info.UpdatePacOpts(r.Clients.Log, cfg.Data, \u0026r.Clients.HTTP)","\tif err != nil {","\t\treturn err","\t}","","\tif updatedPacInfo.TektonDashboardURL != \"\" \u0026\u0026 updatedPacInfo.TektonDashboardURL != r.Clients.ConsoleUI().URL() {","\t\tr.Clients.Log.Infof(\"updating console url to: %s\", updatedPacInfo.TektonDashboardURL)","\t\tr.Clients.SetConsoleUI(\u0026consoleui.TektonDashboard{BaseURL: updatedPacInfo.TektonDashboardURL})","\t}","\tif os.Getenv(\"PAC_TEKTON_DASHBOARD_URL\") != \"\" {","\t\tr.Clients.Log.Infof(\"using tekton dashboard url on: %s\", os.Getenv(\"PAC_TEKTON_DASHBOARD_URL\"))","\t\tr.Clients.SetConsoleUI(\u0026consoleui.TektonDashboard{BaseURL: os.Getenv(\"PAC_TEKTON_DASHBOARD_URL\")})","\t}","\tif updatedPacInfo.CustomConsoleURL != \"\" {","\t\tr.Clients.Log.Infof(\"updating console url to: %s\", updatedPacInfo.CustomConsoleURL)","\t\tpacInfo := r.Info.GetPacOpts()","\t\tr.Clients.SetConsoleUI(consoleui.NewCustomConsole(\u0026pacInfo))","\t}","","\t// This is the case when reverted settings for CustomConsole and TektonDashboard then URL should point to OpenshiftConsole for Openshift platform","\tif updatedPacInfo.CustomConsoleURL == \"\" \u0026\u0026","\t\t(updatedPacInfo.TektonDashboardURL == \"\" \u0026\u0026 os.Getenv(\"PAC_TEKTON_DASHBOARD_URL\") == \"\") {","\t\tr.Clients.SetConsoleUI(\u0026consoleui.OpenshiftConsole{})","\t\t_ = r.Clients.ConsoleUI().UI(ctx, r.Clients.Dynamic)","\t}","","\treturn nil","}","","func New() *Run {","\treturn \u0026Run{","\t\tInfo: info.NewInfo(),","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,0,1,0,0,1,1,1,1,1]},{"id":85,"path":"pkg/params/settings/config.go","lines":["package settings","","import (","\t\"fmt\"","\t\"net/http\"","\t\"net/url\"","\t\"regexp\"","\t\"strings\"","\t\"sync\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/configutil\"","\thubType \"github.com/openshift-pipelines/pipelines-as-code/pkg/hub/vars\"","\t\"go.uber.org/zap\"",")","","const (","\tPACApplicationNameDefaultValue = \"Pipelines as Code CI\"","","\tHubURLKey                          = \"hub-url\"","\tHubCatalogNameKey                  = \"hub-catalog-name\"","\tHubCatalogTypeKey                  = \"hub-catalog-type\"","\tArtifactHubCatalogNameDefaultValue = \"artifacthub\"","\tArtifactHubURLDefaultValue         = \"https://artifacthub.io/api/v1\"","","\tCustomConsoleNameKey         = \"custom-console-name\"","\tCustomConsoleURLKey          = \"custom-console-url\"","\tCustomConsolePRDetailKey     = \"custom-console-url-pr-details\"","\tCustomConsolePRTaskLogKey    = \"custom-console-url-pr-tasklog\"","\tCustomConsoleNamespaceURLKey = \"custom-console-url-namespace\"","","\tSecretGhAppTokenRepoScopedKey = \"secret-github-app-token-scoped\" //nolint: gosec",")","","var (","\tTknBinaryName       = `tkn`","\tTknBinaryURL        = `https://tekton.dev/docs/cli/#installation`","\thubCatalogNameRegex = regexp.MustCompile(`^catalog-(\\d+)-`)",")","","type HubCatalog struct {","\tIndex string","\tName  string","\tURL   string","\tType  string","}","","// if there is a change performed on the default value,","// update the same on \"config/302-pac-configmap.yaml\".","type Settings struct {","\tApplicationName                     string `default:\"Pipelines as Code CI\" json:\"application-name\"`","\tHubCatalogs                         *sync.Map","\tRemoteTasks                         bool   `default:\"true\"                                 json:\"remote-tasks\"`","\tMaxKeepRunsUpperLimit               int    `json:\"max-keep-run-upper-limit\"`","\tDefaultMaxKeepRuns                  int    `json:\"default-max-keep-runs\"`","\tBitbucketCloudCheckSourceIP         bool   `default:\"true\"                                 json:\"bitbucket-cloud-check-source-ip\"`","\tBitbucketCloudAdditionalSourceIP    string `json:\"bitbucket-cloud-additional-source-ip\"`","\tTektonDashboardURL                  string `json:\"tekton-dashboard-url\"`","\tAutoConfigureNewGitHubRepo          bool   `default:\"false\"                                json:\"auto-configure-new-github-repo\"`","\tAutoConfigureRepoNamespaceTemplate  string `json:\"auto-configure-repo-namespace-template\"`","\tAutoConfigureRepoRepositoryTemplate string `json:\"auto-configure-repo-repository-template\"`","","\tSecretAutoCreation               bool   `default:\"true\"                             json:\"secret-auto-create\"`","\tSecretGHAppRepoScoped            bool   `default:\"true\"                             json:\"secret-github-app-token-scoped\"`","\tSecretGhAppTokenScopedExtraRepos string `json:\"secret-github-app-scope-extra-repos\"`","","\tErrorLogSnippet              bool   `default:\"true\"                                                                          json:\"error-log-snippet\"`","\tErrorLogSnippetNumberOfLines int    `default:\"3\"                                                                             json:\"error-log-snippet-number-of-lines\"`","\tErrorDetection               bool   `default:\"true\"                                                                          json:\"error-detection-from-container-logs\"`","\tErrorDetectionNumberOfLines  int    `default:\"50\"                                                                            json:\"error-detection-max-number-of-lines\"`","\tErrorDetectionSimpleRegexp   string `default:\"^(?P\u003cfilename\u003e[^:]*):(?P\u003cline\u003e[0-9]+):(?P\u003ccolumn\u003e[0-9]+)?([ ]*)?(?P\u003cerror\u003e.*)\" json:\"error-detection-simple-regexp\"`","","\tEnableCancelInProgressOnPullRequests bool `json:\"enable-cancel-in-progress-on-pull-requests\"`","\tEnableCancelInProgressOnPush         bool `json:\"enable-cancel-in-progress-on-push\"`","","\tSkipPushEventForPRCommits bool `json:\"skip-push-event-for-pr-commits\" default:\"true\"` // nolint:tagalign","","\tCustomConsoleName         string `json:\"custom-console-name\"`","\tCustomConsoleURL          string `json:\"custom-console-url\"`","\tCustomConsolePRdetail     string `json:\"custom-console-url-pr-details\"`","\tCustomConsolePRTaskLog    string `json:\"custom-console-url-pr-tasklog\"`","\tCustomConsoleNamespaceURL string `json:\"custom-console-url-namespace\"`","","\tRememberOKToTest   bool `json:\"remember-ok-to-test\"`","\tRequireOkToTestSHA bool `json:\"require-ok-to-test-sha\"`","}","","func (s *Settings) DeepCopy(out *Settings) {","\t*out = *s","}","","func DefaultSettings() Settings {","\tnewSettings := \u0026Settings{}","\thubCatalog := \u0026sync.Map{}","\thubCatalog.Store(\"default\", HubCatalog{","\t\tIndex: \"default\",","\t\tURL:   ArtifactHubURLDefaultValue,","\t\tType:  hubType.ArtifactHubType,","\t})","\tnewSettings.HubCatalogs = hubCatalog","","\t_ = configutil.ValidateAndAssignValues(nil, map[string]string{}, newSettings, map[string]func(string) error{}, false)","","\treturn *newSettings","}","","func DefaultValidators() map[string]func(string) error {","\treturn map[string]func(string) error{","\t\t\"ErrorDetectionSimpleRegexp\": isValidRegex,","\t\t\"TektonDashboardURL\":         isValidURL,","\t\t\"CustomConsoleURL\":           isValidURL,","\t\t\"CustomConsolePRTaskLog\":     startWithHTTPorHTTPS,","\t\t\"CustomConsolePRDetail\":      startWithHTTPorHTTPS,","\t}","}","","func SyncConfig(logger *zap.SugaredLogger, setting *Settings, config map[string]string, validators map[string]func(string) error, httpClient *http.Client) error {","\tsetting.HubCatalogs = getHubCatalogs(logger, setting.HubCatalogs, config, httpClient)","","\terr := configutil.ValidateAndAssignValues(logger, config, setting, validators, true)","\tif err != nil {","\t\treturn fmt.Errorf(\"failed to validate and assign values: %w\", err)","\t}","","\tvalue, _ := setting.HubCatalogs.Load(\"default\")","\tcatalogDefault, ok := value.(HubCatalog)","\tif ok {","\t\tif catalogDefault.URL != config[HubURLKey] {","\t\t\tlogger.Infof(\"CONFIG: hub URL set to %v\", config[HubURLKey])","\t\t\tcatalogDefault.URL = config[HubURLKey]","\t\t}","\t\tif catalogDefault.Name != config[HubCatalogNameKey] {","\t\t\tlogger.Infof(\"CONFIG: hub catalog name set to %v\", config[HubCatalogNameKey])","\t\t\tcatalogDefault.Name = config[HubCatalogNameKey]","\t\t}","\t}","\tsetting.HubCatalogs.Store(\"default\", catalogDefault)","\t// TODO: detect changes in extra hub catalogs","","\treturn nil","}","","func isValidURL(rawURL string) error {","\tif _, err := url.ParseRequestURI(rawURL); err != nil {","\t\treturn fmt.Errorf(\"invalid value for URL, error: %w\", err)","\t}","\treturn nil","}","","func isValidRegex(regex string) error {","\tif _, err := regexp.Compile(regex); err != nil {","\t\treturn fmt.Errorf(\"invalid regex: %w\", err)","\t}","\treturn nil","}","","func startWithHTTPorHTTPS(url string) error {","\tif !strings.HasPrefix(url, \"http://\") \u0026\u0026 !strings.HasPrefix(url, \"https://\") {","\t\treturn fmt.Errorf(\"invalid value, must start with http:// or https://\")","\t}","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,0,2,2,2,2,1,1,1,2,1,1,1,0,2,2,2,2,0,0,2,2,2,2,2,0,0,2,2,2,2,2,0,0,2,2,2,2,2,0]},{"id":86,"path":"pkg/params/settings/default.go","lines":["package settings","","import (","\t\"context\"","\t\"fmt\"","\t\"net/http\"","\t\"net/url\"","\t\"strings\"","\t\"sync\"","\t\"time\"","","\thubtypes \"github.com/openshift-pipelines/pipelines-as-code/pkg/hub/vars\"","\t\"go.uber.org/zap\"",")","","func getHubCatalogs(logger *zap.SugaredLogger, catalogs *sync.Map, config map[string]string, httpClient *http.Client) *sync.Map {","\tif catalogs == nil {","\t\tcatalogs = \u0026sync.Map{}","\t}","\tif hubURL, ok := config[HubURLKey]; !ok || hubURL == \"\" {","\t\tconfig[HubURLKey] = ArtifactHubURLDefaultValue","\t\tlogger.Infof(\"CONFIG: using default hub url %s\", ArtifactHubURLDefaultValue)","\t}","","\tif hubType, ok := config[HubCatalogTypeKey]; !ok || hubType == \"\" {","\t\tconfig[HubCatalogTypeKey] = hubtypes.ArtifactHubType","\t\tif config[HubURLKey] != \"\" {","\t\t\tconfig[HubCatalogTypeKey] = getHubCatalogTypeViaAPI(config[HubURLKey], httpClient)","\t\t}","\t} else if hubType != hubtypes.ArtifactHubType \u0026\u0026 hubType != hubtypes.TektonHubType {","\t\tlogger.Warnf(\"CONFIG: invalid hub type %s, defaulting to %s\", hubType, hubtypes.ArtifactHubType)","\t\tconfig[HubCatalogTypeKey] = hubtypes.ArtifactHubType","\t}","\thc := HubCatalog{","\t\tIndex: \"default\",","\t\tName:  config[HubCatalogNameKey],","\t\tURL:   config[HubURLKey],","\t\tType:  config[HubCatalogTypeKey],","\t}","\tcatalogs.Store(\"default\", hc)","","\tfor k := range config {","\t\tm := hubCatalogNameRegex.FindStringSubmatch(k)","\t\tif len(m) \u003e 0 {","\t\t\tindex := m[1]","\t\t\tcPrefix := fmt.Sprintf(\"catalog-%s\", index)","\t\t\tskip := false","\t\t\tfor _, kk := range []string{\"id\", \"name\", \"url\"} {","\t\t\t\tcKey := fmt.Sprintf(\"%s-%s\", cPrefix, kk)","\t\t\t\t// check if key exist in config","\t\t\t\tif _, ok := config[cKey]; !ok {","\t\t\t\t\tlogger.Warnf(\"CONFIG: hub %v should have the key %s, skipping catalog configuration\", index, cKey)","\t\t\t\t\tskip = true","\t\t\t\t\tbreak","\t\t\t\t} else if config[cKey] == \"\" {","\t\t\t\t\tlogger.Warnf(\"CONFIG: hub %v catalog configuration have empty value for key %s, skipping catalog configuration\", index, cKey)","\t\t\t\t\tskip = true","\t\t\t\t\tbreak","\t\t\t\t}","\t\t\t}","\t\t\tif !skip {","\t\t\t\tcatalogID := config[fmt.Sprintf(\"%s-id\", cPrefix)]","\t\t\t\tif catalogID == \"http\" || catalogID == \"https\" {","\t\t\t\t\tlogger.Warnf(\"CONFIG: custom hub catalog name cannot be %s, skipping catalog configuration\", catalogID)","\t\t\t\t\tbreak","\t\t\t\t}","\t\t\t\tcatalogURL := config[fmt.Sprintf(\"%s-url\", cPrefix)]","\t\t\t\tu, err := url.Parse(catalogURL)","\t\t\t\tif err != nil || u.Scheme == \"\" || u.Host == \"\" {","\t\t\t\t\tlogger.Warnf(\"CONFIG: custom hub %s, catalog url %s is not valid, skipping catalog configuration\", catalogID, catalogURL)","\t\t\t\t\tbreak","\t\t\t\t}","\t\t\t\tcatalogName := config[fmt.Sprintf(\"%s-name\", cPrefix)]","\t\t\t\tcatalogType := config[fmt.Sprintf(\"%s-type\", cPrefix)]","\t\t\t\tif catalogType == \"\" {","\t\t\t\t\tcatalogType = getHubCatalogTypeViaAPI(config[fmt.Sprintf(\"%s-url\", cPrefix)], httpClient)","\t\t\t\t}","","\t\t\t\tvalue, ok := catalogs.Load(catalogID)","\t\t\t\tif ok {","\t\t\t\t\tcatalogValues, ok := value.(HubCatalog)","\t\t\t\t\tif ok \u0026\u0026 (catalogValues.Name == catalogName) \u0026\u0026 (catalogValues.URL == catalogURL) \u0026\u0026 (catalogValues.Index == index) \u0026\u0026 (catalogValues.Type == catalogType) {","\t\t\t\t\t\tcontinue","\t\t\t\t\t}","\t\t\t\t}","\t\t\t\tlogger.Infof(\"CONFIG: setting custom hub %s, catalog %s\", catalogID, catalogURL)","\t\t\t\tcatalogs.Store(catalogID, HubCatalog{","\t\t\t\t\tIndex: index,","\t\t\t\t\tName:  catalogName,","\t\t\t\t\tURL:   catalogURL,","\t\t\t\t\tType:  catalogType,","\t\t\t\t})","\t\t\t}","\t\t}","\t}","\treturn catalogs","}","","func getHubCatalogTypeViaAPI(hubURL string, httpClient *http.Client) string {","\tstatsURL := fmt.Sprintf(\"%s/api/v1/stats\", strings.TrimSuffix(hubURL, \"/\"))","","\tctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)","\tdefer cancel()","","\treq, err := http.NewRequestWithContext(ctx, http.MethodGet, statsURL, nil)","\tif err != nil {","\t\treturn hubtypes.TektonHubType","\t}","","\tresp, err := httpClient.Do(req)","\tif err != nil {","\t\treturn hubtypes.TektonHubType","\t}","\tdefer resp.Body.Close()","","\tif resp.StatusCode == http.StatusOK {","\t\treturn hubtypes.ArtifactHubType","\t}","","\t// if the API call fails, return Tekton Hub type","\treturn hubtypes.TektonHubType","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,0,2,2,2,2,2,0,2,2,2,2,2,0,2,2,2,2,2,0,0,2,2,2,2,2,2,2,0,0,0,2,0,0,2,2,2,2,2,2,2,2,1,1,0,2,2,1,1,2,2,2,2,2,0,0,2,0]},{"id":87,"path":"pkg/params/triggertype/types.go","lines":["package triggertype","","type (","\tTrigger string",")","","// IsPullRequestType all Triggertype that are actually a pull request.","func IsPullRequestType(s string) Trigger {","\teventType := s","\tswitch s {","\tcase PullRequest.String(), OkToTest.String(), Retest.String(), Cancel.String(), PullRequestLabeled.String():","\t\teventType = PullRequest.String()","\t}","\treturn Trigger(eventType)","}","","func (t Trigger) String() string {","\treturn string(t)","}","","func StringToType(s string) Trigger {","\tswitch s {","\tcase OkToTest.String():","\t\treturn OkToTest","\tcase Retest.String():","\t\treturn Retest","\tcase Push.String():","\t\treturn Push","\tcase PullRequest.String():","\t\treturn PullRequest","\tcase Cancel.String():","\t\treturn Cancel","\tcase CheckSuiteRerequested.String():","\t\treturn CheckSuiteRerequested","\tcase CheckRunRerequested.String():","\t\treturn CheckRunRerequested","\tcase Incoming.String():","\t\treturn Incoming","\tcase Comment.String():","\t\treturn Comment","\tcase PullRequestLabeled.String():","\t\treturn PullRequestLabeled","\t}","\treturn \"\"","}","","const (","\tCancel                Trigger = \"cancel\"","\tCheckRunRerequested   Trigger = \"check-run-rerequested\"","\tCheckSuiteRerequested Trigger = \"check-suite-rerequested\"","\tComment               Trigger = \"comment\"","\tIncoming              Trigger = \"incoming\"","\tPullRequestLabeled    Trigger = \"pull_request_labeled\"","\tOkToTest              Trigger = \"ok-to-test\"","\tPullRequestClosed     Trigger = \"pull_request_closed\"","\tPullRequest           Trigger = \"pull_request\" // it's should be \"pull_request_opened_updated\" but let's keep it simple.","\tPush                  Trigger = \"push\"","\tRetest                Trigger = \"retest\"",")"],"coverage":[0,0,0,0,0,0,0,1,1,1,1,1,0,1,0,0,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]},{"id":88,"path":"pkg/pipelineascode/cancel_pipelineruns.go","lines":["package pipelineascode","","import (","\t\"context\"","\t\"fmt\"","\t\"strconv\"","\t\"strings\"","\t\"sync\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/resolve\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"go.uber.org/zap\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/apimachinery/pkg/labels\"","\t\"k8s.io/apimachinery/pkg/selection\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/action\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/formatting\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/opscomments\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"",")","","type matchingCond func(pr tektonv1.PipelineRun) bool","","var cancelMergePatch = map[string]any{","\t\"spec\": map[string]any{","\t\t\"status\": tektonv1.PipelineRunSpecStatusCancelledRunFinally,","\t},","}","","// cancelAllInProgressBelongingToClosedPullRequest cancels all in-progress PipelineRuns","// that belong to a specific pull request in the given repository.","func (p *PacRun) cancelAllInProgressBelongingToClosedPullRequest(ctx context.Context, repo *v1alpha1.Repository) error {","\tlabelsMap := map[string]string{","\t\tkeys.URLRepository: formatting.CleanValueKubernetes(p.event.Repository),","\t\tkeys.PullRequest:   strconv.Itoa(p.event.PullRequestNumber),","\t}","\toperator := selection.Equals","\tcancelInProgress := fmt.Sprintf(\"%t\", p.pacInfo.EnableCancelInProgressOnPullRequests)","\tp.debugf(\"cancelAllInProgress: repo=%s/%s pr=%d cancel_in_progress=%s\", repo.GetNamespace(), repo.GetName(), p.event.PullRequestNumber, cancelInProgress)","","\t// First, build the label selector based on the URLRepository and PullRequest fields,","\t// followed by applying filtering logic for the 'cancel-in-progress' annotation.","\tlabelSelector := getLabelSelector(labelsMap, operator)","\tlabelSelector += fmt.Sprintf(\",%s in (pull_request, Merge_Request, %s)\", keys.EventType, opscomments.AnyOpsKubeLabelInSelector())","","\tif cancelInProgress == \"true\" {","\t\t// When the 'cancel-in-progress' setting is enabled globally via the Pipelines-as-Code ConfigMap,","\t\t// then exclude those PipelineRuns that explicitly override this setting by having the","\t\t// 'cancel-in-progress' annotation set to 'false' and continue cancelling others which are having","\t\t// 'cancel-in-progress' annotation set to 'true' or with no annotation specified at all and are","\t\t// subjected to cancellation under the global setting.","\t\t//","\t\t// Note: The 'selection.NotIn' operator is used to exclude PipelineRuns that have the","\t\t// 'cancel-in-progress' annotation explicitly set to 'false', effectively opting them out of cancellation.","\t\tlabelsMap = map[string]string{keys.CancelInProgress: \"false\"}","\t\toperator = selection.NotIn //codespell:ignore 'NotIn'","\t} else {","\t\t// When the 'cancel-in-progress' setting is disabled globally via the Pipelines-as-Code ConfigMap,","\t\t// filter and list only those PipelineRuns that explicitly override the global setting by having the","\t\t// 'cancel-in-progress' annotation set to 'true'.","\t\tlabelsMap = map[string]string{keys.CancelInProgress: \"true\"}","\t}","\t// Append the label selector filter for the 'cancel-in-progress' annotation.","\tlabelSelector += fmt.Sprintf(\",%s\", getLabelSelector(labelsMap, operator))","\tp.debugf(\"cancelAllInProgress: labelSelector=%s\", labelSelector)","","\tprs, err := p.run.Clients.Tekton.TektonV1().PipelineRuns(repo.Namespace).List(ctx, metav1.ListOptions{","\t\tLabelSelector: labelSelector,","\t})","\tif err != nil {","\t\treturn fmt.Errorf(\"failed to list pipelineRuns : %w\", err)","\t}","","\tif len(prs.Items) == 0 {","\t\tmsg := fmt.Sprintf(\"no pipelinerun found for repository: %v and pullRequest %v\",","\t\t\tp.event.Repository, p.event.PullRequestNumber)","\t\tp.eventEmitter.EmitMessage(repo, zap.InfoLevel, \"CancelInProgress\", msg)","\t\treturn nil","\t}","\tp.debugf(\"cancelAllInProgress: found %d pipelineruns to consider\", len(prs.Items))","","\tp.cancelPipelineRuns(ctx, prs, repo, func(_ tektonv1.PipelineRun) bool {","\t\treturn true","\t})","","\treturn nil","}","","// cancelInProgressMatchingPR cancels all PipelineRuns associated with a given repository and pull request,","// except for the one that triggered the cancellation. It first checks if the cancellation is in progress","// and if the repository has a concurrency limit. If a concurrency limit is set, it returns an error as","// cancellation is not supported with concurrency limits. It then retrieves the original pull request name","// from the annotations and lists all PipelineRuns with matching labels. For each PipelineRun that is not","// already done, cancelled, or gracefully stopped, it patches the PipelineRun to cancel it.","func (p *PacRun) cancelInProgressMatchingPipelineRun(ctx context.Context, matchPR *tektonv1.PipelineRun, repo *v1alpha1.Repository) error {","\tif matchPR == nil {","\t\treturn nil","\t}","","\tcancelInProgress := \"\"","\tcancellingVia := \"globally via Pipelines-as-Code ConfigMap\"","\t// First, check value from Pipelines-as-Code ConfigMap for the event type","\tif p.event.TriggerTarget == triggertype.PullRequest { //nolint: staticcheck","\t\tcancelInProgress = fmt.Sprintf(\"%t\", p.pacInfo.EnableCancelInProgressOnPullRequests)","\t} else if p.event.TriggerTarget == triggertype.Push {","\t\tcancelInProgress = fmt.Sprintf(\"%t\", p.pacInfo.EnableCancelInProgressOnPush)","\t}","","\t// As per feature behavior, PipelineRun annotation should override setting of Pipelines-as-Code ConfigMap","\tif value, ok := matchPR.GetAnnotations()[keys.CancelInProgress]; ok {","\t\tcancelInProgress = value","\t\tcancellingVia = \"via PipelineRun annotation\"","\t}","","\tif cancelInProgress != \"true\" {","\t\tp.debugf(\"cancelInProgress: disabled for pipelinerun=%s via=%s\", matchPR.GetName(), cancellingVia)","\t\treturn nil","\t}","","\tp.run.Clients.Log.Infof(\"cancel-in-progress for event %s is enabled %s\", string(p.event.TriggerTarget), cancellingVia)","\tp.debugf(\"cancelInProgress: enabled for pipelinerun=%s\", matchPR.GetName())","","\t// As PipelineRuns are filtered by name, OriginalPRName should be taken from","\t// labels instead of annotations because of constraints imposed by kube API.","\tprName, ok := matchPR.GetLabels()[keys.OriginalPRName]","\tif !ok {","\t\tp.debugf(\"cancelInProgress: missing original PR name label for pipelinerun=%s\", matchPR.GetName())","\t\treturn nil","\t}","","\tif repo.Spec.ConcurrencyLimit != nil \u0026\u0026 *repo.Spec.ConcurrencyLimit \u003e 0 {","\t\treturn fmt.Errorf(\"cancel in progress is not supported with concurrency limit\")","\t}","","\tlabelMap := map[string]string{","\t\tkeys.URLRepository:  formatting.CleanValueKubernetes(p.event.Repository),","\t\tkeys.OriginalPRName: prName,","\t}","\tif p.event.TriggerTarget == triggertype.PullRequest {","\t\tlabelMap[keys.PullRequest] = strconv.Itoa(p.event.PullRequestNumber)","\t}","\tlabelSelector := getLabelSelector(labelMap, selection.Equals)","\tif p.event.TriggerTarget == triggertype.PullRequest {","\t\t// \"Merge_Request\" included since EventType is not normalized to \"Pull Request\" like TriggerTarget","\t\tlabelSelector += fmt.Sprintf(\",%s in (pull_request, Merge_Request, %s)\", keys.EventType, opscomments.AnyOpsKubeLabelInSelector())","\t}","\tp.run.Clients.Log.Infof(\"cancel-in-progress: selecting pipelineRuns to cancel with labels: %v\", labelSelector)","\tp.debugf(\"cancelInProgress: labelSelector=%s\", labelSelector)","\tprs, err := p.run.Clients.Tekton.TektonV1().PipelineRuns(matchPR.GetNamespace()).List(ctx, metav1.ListOptions{","\t\tLabelSelector: labelSelector,","\t})","\tif err != nil {","\t\treturn fmt.Errorf(\"failed to list pipelineRuns : %w\", err)","\t}","","\tp.cancelPipelineRuns(ctx, prs, repo, func(pr tektonv1.PipelineRun) bool {","\t\t// skip our own for cancellation","\t\tif sourceBranch, ok := pr.GetAnnotations()[keys.SourceBranch]; ok {","\t\t\t// NOTE(chmouel): Every PR has their own branch and so is every push to different branch","\t\t\t// it means we only cancel pipelinerun of the same name that runs to","\t\t\t// the unique branch. Note: HeadBranch is the branch from where the PR","\t\t\t// comes from in git jargon.","\t\t\tif strings.TrimPrefix(sourceBranch, \"refs/heads/\") != strings.TrimPrefix(p.event.HeadBranch, \"refs/heads/\") {","\t\t\t\tp.logger.Infof(\"cancel-in-progress: skipping pipelinerun %v/%v as it is not from the same branch, annotation source-branch: %s event headbranch: %s\", pr.GetNamespace(), pr.GetName(), sourceBranch, p.event.HeadBranch)","\t\t\t\treturn false","\t\t\t}","\t\t}","","\t\treturn pr.GetName() != matchPR.GetName()","\t})","\treturn nil","}","","// cancelPipelineRunsOpsComment cancels all PipelineRuns associated with a given repository and pull request.","// when the user issue a cancel comment.","func (p *PacRun) cancelPipelineRunsOpsComment(ctx context.Context, repo *v1alpha1.Repository) error {","\trepo = p.resolveRepoForTargetCancelPipelineRun(ctx, repo)","","\tlabelSelector := getLabelSelector(map[string]string{","\t\tkeys.URLRepository: formatting.CleanValueKubernetes(p.event.Repository),","\t\tkeys.SHA:           formatting.CleanValueKubernetes(p.event.SHA),","\t}, selection.Equals)","","\tif p.event.TriggerTarget == triggertype.PullRequest {","\t\tlabelSelector = getLabelSelector(map[string]string{","\t\t\tkeys.URLRepository: formatting.CleanValueKubernetes(p.event.Repository),","\t\t\tkeys.PullRequest:   strconv.Itoa(p.event.PullRequestNumber),","\t\t}, selection.Equals)","\t}","","\tprs, err := p.run.Clients.Tekton.TektonV1().PipelineRuns(repo.Namespace).List(ctx, metav1.ListOptions{","\t\tLabelSelector: labelSelector,","\t})","\tif err != nil {","\t\treturn fmt.Errorf(\"failed to list pipelineRuns : %w\", err)","\t}","","\tif len(prs.Items) == 0 {","\t\tmsg := fmt.Sprintf(\"no pipelinerun found for repository: %v , sha: %v and pulRequest %v\",","\t\t\tp.event.Repository, p.event.SHA, p.event.PullRequestNumber)","\t\tp.eventEmitter.EmitMessage(repo, zap.InfoLevel, \"CancelInProgress\", msg)","\t\treturn nil","\t}","\tp.debugf(\"cancelPipelineRunsOpsComment: found %d pipelineruns to consider\", len(prs.Items))","","\tp.cancelPipelineRuns(ctx, prs, repo, func(pr tektonv1.PipelineRun) bool {","\t\tif p.event.TargetCancelPipelineRun != \"\" {","\t\t\tif prName, ok := pr.GetAnnotations()[keys.OriginalPRName]; !ok || prName != p.event.TargetCancelPipelineRun {","\t\t\t\treturn false","\t\t\t}","\t\t}","\t\treturn true","\t})","","\treturn nil","}","","func (p *PacRun) resolveRepoForTargetCancelPipelineRun(ctx context.Context, repo *v1alpha1.Repository) *v1alpha1.Repository {","\tif repo == nil || p.event.TargetCancelPipelineRun == \"\" || p.vcx == nil {","\t\treturn repo","\t}","","\tprovenance := \"source\"","\tif repo.Spec.Settings != nil \u0026\u0026 repo.Spec.Settings.PipelineRunProvenance != \"\" {","\t\tprovenance = repo.Spec.Settings.PipelineRunProvenance","\t}","","\trawTemplates, err := p.vcx.GetTektonDir(ctx, p.event, tektonDir, provenance)","\tif err != nil || rawTemplates == \"\" {","\t\treturn repo","\t}","","\tallTemplates := p.makeTemplate(ctx, repo, rawTemplates)","\ttypes, err := resolve.ReadTektonTypes(ctx, p.logger, allTemplates)","\tif err != nil {","\t\treturn repo","\t}","","\tpipelineRuns, err := resolve.MetadataResolve(types.PipelineRuns)","\tif err != nil || len(pipelineRuns) == 0 {","\t\treturn repo","\t}","","\ttargetPR := filterRunningPipelineRunOnTargetTest(p.event.TargetCancelPipelineRun, pipelineRuns)","\tif targetPR == nil {","\t\tfor _, pr := range pipelineRuns {","\t\t\tprName := strings.TrimSuffix(pipelineRunIdentifier(pr), \"-\")","\t\t\tif prName == p.event.TargetCancelPipelineRun {","\t\t\t\ttargetPR = pr","\t\t\t\tbreak","\t\t\t}","\t\t}","\t}","\tif targetPR == nil {","\t\treturn repo","\t}","","\ttargetRepo := p.resolveTargetNamespaceRepo(ctx, repo, targetPR)","\tif targetRepo == nil {","\t\tp.logger.Warnf(","\t\t\t\"resolveRepoForTargetCancelPipelineRun: target repo not found for pipelinerun=%s, falling back to repo=%s/%s\",","\t\t\tpipelineRunIdentifier(targetPR),","\t\t\trepo.GetNamespace(),","\t\t\trepo.GetName(),","\t\t)","\t\treturn repo","\t}","\treturn targetRepo","}","","func (p *PacRun) cancelPipelineRuns(ctx context.Context, prs *tektonv1.PipelineRunList, repo *v1alpha1.Repository, condition matchingCond) {","\tvar wg sync.WaitGroup","\tp.debugf(\"cancelPipelineRuns: evaluating %d pipelineruns\", len(prs.Items))","\tfor _, pr := range prs.Items {","\t\tif !condition(pr) {","\t\t\tcontinue","\t\t}","","\t\tif pr.IsCancelled() || pr.IsGracefullyCancelled() || pr.IsGracefullyStopped() {","\t\t\tp.logger.Infof(\"cancel-in-progress: skipping cancelling pipelinerun %v/%v, already in %v state\", pr.GetNamespace(), pr.GetName(), pr.Spec.Status)","\t\t\tcontinue","\t\t}","","\t\tif pr.IsDone() {","\t\t\tp.logger.Infof(\"cancel-in-progress: skipping cancelling pipelinerun %v/%v, already done\", pr.GetNamespace(), pr.GetName())","\t\t\tcontinue","\t\t}","","\t\tp.logger.Infof(\"cancel-in-progress: cancelling pipelinerun %v/%v\", pr.GetNamespace(), pr.GetName())","\t\twg.Add(1)","\t\tgo func(ctx context.Context, pr tektonv1.PipelineRun) {","\t\t\tdefer wg.Done()","\t\t\tif _, err := action.PatchPipelineRun(ctx, p.logger, \"cancel patch\", p.run.Clients.Tekton, \u0026pr, cancelMergePatch); err != nil {","\t\t\t\terrMsg := fmt.Sprintf(\"failed to cancel pipelineRun %s/%s: %s\", pr.GetNamespace(), pr.GetName(), err.Error())","\t\t\t\tp.eventEmitter.EmitMessage(repo, zap.ErrorLevel, \"CancelInProgress\", errMsg)","\t\t\t}","\t\t}(ctx, pr)","\t}","\twg.Wait()","}","","func getLabelSelector(labelsMap map[string]string, operator selection.Operator) string {","\tlabelSelector := labels.NewSelector()","\tfor k, v := range labelsMap {","\t\treq, _ := labels.NewRequirement(k, operator, []string{v})","\t\tif req != nil {","\t\t\tlabelSelector = labelSelector.Add(*req)","\t\t}","\t}","\treturn labelSelector.String()","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,0,2,0,0,0,0,0,0,0,0,2,2,2,2,0,2,2,2,2,2,2,2,2,0,0,2,2,2,2,0,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,0,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,0,0,2,0,2,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,0,0,2,0,0,2,2,2,2,0,2,2,1,1,0,2,2,1,1,0,2,2,2,1,1,0,2,2,1,1,0,2,2,1,1,1,1,1,0,0,0,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,0,0,2,2,2,0,0,2,2,2,0,0,2,2,2,2,2,1,1,1,0,0,2,0,0,2,2,2,2,2,2,2,0,2,0]},{"id":89,"path":"pkg/pipelineascode/client_setup.go","lines":["package pipelineascode","","import (","\t\"context\"","\t\"fmt\"","\t\"strings\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/events\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/github\"","\t\"go.uber.org/zap\"",")","","// SetupAuthenticatedClient sets up the authenticated VCS client with proper token scoping.","// This is the centralized place for all client authentication and token scoping logic.","//","// This function is idempotent and safe to call multiple times.","func SetupAuthenticatedClient(","\tctx context.Context,","\tvcx provider.Interface,","\tkint kubeinteraction.Interface,","\trun *params.Run,","\tevent *info.Event,","\trepo *v1alpha1.Repository,","\tglobalRepo *v1alpha1.Repository,","\tpacInfo *info.PacOpts,","\tlogger *zap.SugaredLogger,",") error {","\t// Determine secret namespace BEFORE merging repos","\t// This preserves the ability to detect when credentials come from global repo","\tsecretNS := repo.GetNamespace()","\tif repo.Spec.GitProvider != nil \u0026\u0026 repo.Spec.GitProvider.Secret == nil \u0026\u0026","\t\tglobalRepo != nil \u0026\u0026 globalRepo.Spec.GitProvider != nil \u0026\u0026 globalRepo.Spec.GitProvider.Secret != nil {","\t\tsecretNS = globalRepo.GetNamespace()","\t}","\tlogger.Debugf(\"setupAuthenticatedClient: repo=%s/%s secret_namespace=%s\", repo.GetNamespace(), repo.GetName(), secretNS)","\t// merge global repo settings into local repo (after determining secret namespace)","\tif globalRepo != nil {","\t\tlogger.Debugf(\"setupAuthenticatedClient: merging global repo settings from %s/%s\", globalRepo.GetNamespace(), globalRepo.GetName())","\t\trepo.Spec.Merge(globalRepo.Spec)","\t}","","\t// GitHub Apps use controller secret, not Repository git_provider","\tif event.InstallationID \u003e 0 {","\t\tlogger.Debugf(\"setupAuthenticatedClient: github app installation id=%d, using controller webhook secret\", event.InstallationID)","\t\tevent.Provider.WebhookSecret, _ = GetCurrentNSWebhookSecret(ctx, kint, run)","\t} else {","\t\t// Non-GitHub App providers use git_provider section in Repository spec","\t\tscm := SecretFromRepository{","\t\t\tK8int:       kint,","\t\t\tConfig:      vcx.GetConfig(),","\t\t\tEvent:       event,","\t\t\tRepo:        repo,","\t\t\tWebhookType: pacInfo.WebhookType,","\t\t\tLogger:      logger,","\t\t\tNamespace:   secretNS,","\t\t}","\t\tif err := scm.Get(ctx); err != nil {","\t\t\treturn fmt.Errorf(\"cannot get secret from repository: %w\", err)","\t\t}","\t\tlogger.Debugf(\"setupAuthenticatedClient: loaded git provider credentials for repo=%s/%s\", repo.GetNamespace(), repo.GetName())","\t}","","\t// Set up the authenticated client","\teventEmitter := events.NewEventEmitter(run.Clients.Kube, logger)","","\t// Validate payload with webhook secret (skip for incoming webhooks)","\tif event.EventType != \"incoming\" {","\t\tlogger.Debugf(\"setupAuthenticatedClient: validating webhook payload for event_type=%s\", event.EventType)","\t\tif err := vcx.Validate(ctx, run, event); err != nil {","\t\t\t// check that webhook secret has no /n or space into it","\t\t\tif strings.ContainsAny(event.Provider.WebhookSecret, \"\\n \") {","\t\t\t\tmsg := `we have failed to validate the payload with the webhook secret,","it seems that we have detected a \\n or a space at the end of your webhook secret, ","is that what you want? make sure you use -n when generating the secret, eg: echo -n secret|base64`","\t\t\t\teventEmitter.EmitMessage(repo, zap.ErrorLevel, \"RepositorySecretValidation\", msg)","\t\t\t}","\t\t\treturn fmt.Errorf(\"could not validate payload, check your webhook secret?: %w\", err)","\t\t}","\t}","\t// Set up the authenticated client","\tif err := vcx.SetClient(ctx, run, event, repo, eventEmitter); err != nil {","\t\treturn fmt.Errorf(\"failed to set client: %w\", err)","\t}","\tlogger.Debugf(\"setupAuthenticatedClient: provider client initialized\")","","\t// Handle GitHub App token scoping for both global and repo-level configuration","\tif event.InstallationID \u003e 0 {","\t\tlogger.Debugf(\"setupAuthenticatedClient: scoping github app token\")","\t\ttoken, err := github.ScopeTokenToListOfRepos(ctx, vcx, pacInfo, repo, run, event, eventEmitter, logger)","\t\tif err != nil {","\t\t\treturn fmt.Errorf(\"failed to scope token: %w\", err)","\t\t}","\t\t// If Global and Repo level configurations are not provided then lets not override the provider token.","\t\tif token != \"\" {","\t\t\tevent.Provider.Token = token","\t\t}","\t}","","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,1,1,2,2,2,1,1,1,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,1,1,2,2,2,2,2,2,2,1,1,0,2,1,1,0,0,2,0]},{"id":90,"path":"pkg/pipelineascode/concurrency.go","lines":["package pipelineascode","","import (","\t\"fmt\"","\t\"sync\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/sort\"","\tv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"k8s.io/apimachinery/pkg/runtime\"",")","","const namePath = \"{.metadata.name}\"","","type ConcurrencyManager struct {","\tenabled      bool","\tpipelineRuns []*v1.PipelineRun","\tmutex        *sync.Mutex","}","","func NewConcurrencyManager() *ConcurrencyManager {","\treturn \u0026ConcurrencyManager{","\t\tpipelineRuns: []*v1.PipelineRun{},","\t\tmutex:        \u0026sync.Mutex{},","\t}","}","","func (c *ConcurrencyManager) AddPipelineRun(pr *v1.PipelineRun) {","\tif !c.enabled {","\t\treturn","\t}","\tif pr == nil {","\t\treturn","\t}","\tc.mutex.Lock()","\tdefer c.mutex.Unlock()","","\tc.pipelineRuns = append(c.pipelineRuns, pr)","}","","func (c *ConcurrencyManager) Enable() {","\tc.enabled = true","}","","func (c *ConcurrencyManager) GetExecutionOrder() (string, []*v1.PipelineRun) {","\tif !c.enabled {","\t\treturn \"\", nil","\t}","","\tif len(c.pipelineRuns) == 0 {","\t\treturn \"\", nil","\t}","","\truntimeObjs := []runtime.Object{}","\tfor _, pr := range c.pipelineRuns {","\t\tif pr != nil \u0026\u0026 pr.Name != \"\" {","\t\t\truntimeObjs = append(runtimeObjs, pr)","\t\t}","\t}","","\tif len(runtimeObjs) == 0 {","\t\treturn \"\", nil","\t}","","\t// sort runs by name","\tsort.ByField(namePath, runtimeObjs)","","\tsortedPipelineRuns := []*v1.PipelineRun{}","\tfor _, run := range runtimeObjs {","\t\tpr, _ := run.(*v1.PipelineRun)","\t\tsortedPipelineRuns = append(sortedPipelineRuns, pr)","\t}","\tc.pipelineRuns = sortedPipelineRuns","","\treturn getOrderByName(c.pipelineRuns), c.pipelineRuns","}","","func getOrderByName(runs []*v1.PipelineRun) string {","\tvar order string","\tfor _, run := range runs {","\t\tif order == \"\" {","\t\t\torder = fmt.Sprintf(\"%s/%s\", run.GetNamespace(), run.GetName())","\t\t\tcontinue","\t\t}","\t\torder = order + \",\" + fmt.Sprintf(\"%s/%s\", run.GetNamespace(), run.GetName())","\t}","\treturn order","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,2,2,2,2,2,1,1,2,2,2,2,0,0,2,2,2,0,2,2,2,2,0,2,1,1,0,2,2,2,2,2,0,0,2,1,1,0,0,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,0,2,0,2,0]},{"id":91,"path":"pkg/pipelineascode/errors.go","lines":["package pipelineascode","","import (","\t\"context\"","\t\"fmt\"","\t\"regexp\"","\t\"strings\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\tpacerrors \"github.com/openshift-pipelines/pipelines-as-code/pkg/errors\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"go.uber.org/zap\"",")","","const (","\ttektonDirMissingError = \".tekton/ directory doesn't exist in repository's root directory\"",")","","var regexpIgnoreErrors = regexp.MustCompile(`.*no kind.*is registered for version.*in scheme.*`)","","func (p *PacRun) checkAccessOrError(ctx context.Context, repo *v1alpha1.Repository, status provider.StatusOpts, viamsg string) (bool, error) {","\tp.debugf(\"checkAccessOrError: checking access for sender=%s via=%s\", p.event.Sender, viamsg)","\tallowed, err := p.vcx.IsAllowed(ctx, p.event)","\tif err != nil {","\t\treturn false, fmt.Errorf(\"unable to verify event authorization: %w\", err)","\t}","\tif allowed {","\t\tp.debugf(\"checkAccessOrError: access granted for sender=%s\", p.event.Sender)","\t\treturn true, nil","\t}","\tmsg := fmt.Sprintf(\"User %s is not allowed to trigger CI %s in this repo.\", p.event.Sender, viamsg)","\tif p.event.AccountID != \"\" {","\t\tmsg = fmt.Sprintf(\"User: %s AccountID: %s is not allowed to trigger CI %s in this repo.\", p.event.Sender, p.event.AccountID, viamsg)","\t}","\tp.eventEmitter.EmitMessage(repo, zap.InfoLevel, \"RepositoryPermissionDenied\", msg)","\tstatus.Text = msg","","\tif err := p.vcx.CreateStatus(ctx, p.event, status); err != nil {","\t\treturn false, fmt.Errorf(\"failed to run create status, user is not allowed to run the CI:: %w\", err)","\t}","\treturn false, nil","}","","// reportValidationErrors reports validation errors found in PipelineRuns by:","// 1. Creating error messages for each validation error","// 2. Emitting error messages to the event system","// 3. Creating a markdown formatted comment on the repository with all errors.","func (p *PacRun) reportValidationErrors(ctx context.Context, repo *v1alpha1.Repository, validationErrors []*pacerrors.PacYamlValidations) {","\tp.debugf(\"reportValidationErrors: count=%d repo=%s/%s\", len(validationErrors), repo.GetNamespace(), repo.GetName())","\terrorRows := make([]string, 0, len(validationErrors))","\tfor _, err := range validationErrors {","\t\t// if the error is a TektonConversionError, we don't want to report it since it may be a file that is not a tekton resource","\t\t// and we don't want to report it as a validation error.","\t\tif !regexpIgnoreErrors.MatchString(err.Err.Error()) \u0026\u0026 (strings.HasPrefix(err.Schema, tektonv1.SchemeGroupVersion.Group) || err.Schema == pacerrors.GenericBadYAMLValidation) {","\t\t\terrorRows = append(errorRows, fmt.Sprintf(\"| %s | `%s` |\", err.Name, err.Err.Error()))","\t\t}","\t\tp.eventEmitter.EmitMessage(repo, zap.ErrorLevel, \"PipelineRunValidationErrors\",","\t\t\tfmt.Sprintf(\"cannot read the PipelineRun: %s, error: %s\", err.Name, err.Err.Error()))","\t}","\tif len(errorRows) == 0 {","\t\treturn","\t}","\tmarkdownErrMessage := fmt.Sprintf(`%s","%s`, provider.ValidationErrorTemplate, strings.Join(errorRows, \"\\n\"))","","\teventID := \"unknown\"","\torg := \"unknown\"","\trepository := \"unknown\"","\tsourceBranch := \"unknown\"","\ttargetBranch := \"unknown\"","\tpr := 0","\tif p.event != nil {","\t\torg = p.event.Organization","\t\trepository = p.event.Repository","\t\tsourceBranch = p.event.HeadBranch","\t\ttargetBranch = p.event.BaseBranch","\t\tpr = p.event.PullRequestNumber","\t\tif p.event.Request != nil {","\t\t\tif id := p.event.Request.Header.Get(\"X-GitHub-Delivery\"); id != \"\" {","\t\t\t\teventID = id","\t\t\t}","\t\t}","\t}","\tp.debugf(\"reportValidationErrors: create_comment validation_error_count=%d event_id=%s pr=%d repo=%s/%s namespace=%s source_branch=%s target_branch=%s\",","\t\tlen(errorRows), eventID, pr, org, repository, repo.GetNamespace(), sourceBranch, targetBranch)","","\tif err := p.vcx.CreateComment(ctx, p.event, markdownErrMessage, provider.ValidationErrorTemplate); err != nil {","\t\tp.eventEmitter.EmitMessage(repo, zap.ErrorLevel, \"PipelineRunCommentCreationError\",","\t\t\tfmt.Sprintf(\"failed to create comment: %s\", err.Error()))","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,0,2,2,2,2,2,2,2,0]},{"id":92,"path":"pkg/pipelineascode/logging.go","lines":["package pipelineascode","","// debugf logs only when a logger is available.","// This avoids nil pointer panics in tests that don't wire a logger.","func (p *PacRun) debugf(format string, args ...any) {","\tif p == nil || p.logger == nil {","\t\treturn","\t}","\tp.logger.Debugf(format, args...)","}"],"coverage":[0,0,0,0,2,2,2,2,2,0]},{"id":93,"path":"pkg/pipelineascode/match.go","lines":["package pipelineascode","","import (","\t\"context\"","\t\"encoding/json\"","\t\"fmt\"","\t\"regexp\"","\t\"strings\"","","\tapipac \"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\tpacerrors \"github.com/openshift-pipelines/pipelines-as-code/pkg/errors\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/matcher\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/opscomments\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/resolve\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/secrets\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/templates\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"go.uber.org/zap\"",")","","func (p *PacRun) matchRepoPR(ctx context.Context) ([]matcher.Match, *v1alpha1.Repository, error) {","\tp.debugf(\"matchRepoPR: starting repo verification for url=%s\", p.event.URL)","\trepo, err := p.verifyRepoAndUser(ctx)","\tif err != nil {","\t\treturn nil, nil, err","\t}","\tif repo == nil {","\t\tp.debugf(\"matchRepoPR: no repository match for url=%s\", p.event.URL)","\t\treturn nil, nil, nil","\t}","","\tif p.event.CancelPipelineRuns {","\t\tp.debugf(\"matchRepoPR: cancel pipeline runs requested, skipping match\")","\t\treturn nil, repo, p.cancelPipelineRunsOpsComment(ctx, repo)","\t}","","\tp.debugf(\"matchRepoPR: fetching pipelineruns from repo=%s/%s\", repo.GetNamespace(), repo.GetName())","\tmatchedPRs, err := p.getPipelineRunsFromRepo(ctx, repo)","\tif err != nil {","\t\treturn nil, repo, err","\t}","","\tp.debugf(\"matchRepoPR: matched=%d repo=%s/%s\", len(matchedPRs), repo.GetNamespace(), repo.GetName())","\treturn matchedPRs, repo, nil","}","","// verifyRepoAndUser verifies if the Repo CR exists for the Git Repository,","// if the user has permission to run CI  and also initialise provider client.","func (p *PacRun) verifyRepoAndUser(ctx context.Context) (*v1alpha1.Repository, error) {","\t// Match the Event URL to a Repository URL,","\tp.debugf(\"verifyRepoAndUser: matching repository for url=%s\", p.event.URL)","\trepo, err := matcher.MatchEventURLRepo(ctx, p.run, p.event, \"\")","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"error matching Repository for event: %w\", err)","\t}","","\tif repo == nil {","\t\tmsg := fmt.Sprintf(\"cannot find a repository match for %s\", p.event.URL)","\t\tp.eventEmitter.EmitMessage(nil, zap.WarnLevel, \"RepositoryNamespaceMatch\", msg)","\t\treturn nil, nil","\t}","\tp.debugf(\"verifyRepoAndUser: matched repo=%s/%s\", repo.GetNamespace(), repo.GetName())","","\tp.logger = p.logger.With(\"namespace\", repo.Namespace)","\tp.vcx.SetLogger(p.logger)","\tp.eventEmitter.SetLogger(p.logger)","","\t// Set up authenticated client with proper token scoping","\t// NOTE: This is typically already done in sinker.processEvent() for all event types,","\t// but we call it here as a safety net for edge cases (e.g., tests calling Run() directly,","\t// or if the early setup in sinker failed/was skipped). The call is idempotent.","\t// SetupAuthenticatedClient will merge global repo settings after determining secret namespace.","\terr = SetupAuthenticatedClient(ctx, p.vcx, p.k8int, p.run, p.event, repo, p.globalRepo, p.pacInfo, p.logger)","\tif err != nil {","\t\treturn repo, err","\t}","\tp.debugf(\"verifyRepoAndUser: authenticated client setup complete\")","","\t// Get the SHA commit info, we want to get the URL and commit title","\tif p.event.SHA == \"\" || p.event.SHATitle == \"\" || p.event.SHAURL == \"\" {","\t\tp.debugf(\"verifyRepoAndUser: fetching commit info\")","\t\tif err = p.vcx.GetCommitInfo(ctx, p.event); err != nil {","\t\t\treturn repo, fmt.Errorf(\"could not find commit info: %w\", err)","\t\t}","\t\tp.debugf(\"verifyRepoAndUser: commit info loaded sha=%s title=%s\", p.event.SHA, p.event.SHATitle)","\t}","","\t// Verify whether the sender of the GitOps command (e.g., /test) has the appropriate permissions to","\t// trigger CI on the repository, as any user is able to comment on a pushed commit in open-source repositories.","\tif p.event.TriggerTarget == triggertype.Push \u0026\u0026 opscomments.IsAnyOpsEventType(p.event.EventType) {","\t\tp.debugf(\"verifyRepoAndUser: checking access for gitops comment on push\")","\t\tstatus := provider.StatusOpts{","\t\t\tStatus:       CompletedStatus,","\t\t\tTitle:        \"Permission denied\",","\t\t\tConclusion:   failureConclusion,","\t\t\tDetailsURL:   p.event.URL,","\t\t\tAccessDenied: true,","\t\t}","\t\tif allowed, err := p.checkAccessOrError(ctx, repo, status, \"by GitOps comment on push commit\"); !allowed {","\t\t\treturn nil, err","\t\t}","\t}","","\t// Check if the submitter is allowed to run this.","\t// on push we don't need to check the policy since the user has pushed to the repo so it has access to it.","\t// on comment we skip it for now, we are going to check later on","\tif p.event.TriggerTarget != triggertype.Push \u0026\u0026 p.event.EventType != opscomments.NoOpsCommentEventType.String() {","\t\tp.debugf(\"verifyRepoAndUser: checking access for trigger target=%s event_type=%s\", p.event.TriggerTarget, p.event.EventType)","\t\tstatus := provider.StatusOpts{","\t\t\tStatus:       queuedStatus,","\t\t\tTitle:        \"Pending approval, waiting for an /ok-to-test\",","\t\t\tConclusion:   pendingConclusion,","\t\t\tDetailsURL:   p.event.URL,","\t\t\tAccessDenied: true,","\t\t}","\t\tif allowed, err := p.checkAccessOrError(ctx, repo, status, \"via \"+p.event.TriggerTarget.String()); !allowed {","\t\t\treturn nil, err","\t\t}","\t\t// When /ok-to-test is approved, update the parent \"Pipelines as Code CI\" status to success","\t\t// to indicate the approval was successful before pipelines start running.","\t\tif p.event.EventType == opscomments.OkToTestCommentEventType.String() {","\t\t\tapprovalStatus := provider.StatusOpts{","\t\t\t\tStatus:     CompletedStatus,","\t\t\t\tTitle:      \"Approved\",","\t\t\t\tConclusion: successConclusion,","\t\t\t\tDetailsURL: p.event.URL,","\t\t\t}","\t\t\tif err := p.vcx.CreateStatus(ctx, p.event, approvalStatus); err != nil {","\t\t\t\tp.logger.Warnf(\"failed to update parent status on /ok-to-test approval: %v\", err)","\t\t\t}","\t\t}","\t}","\treturn repo, nil","}","","// getPipelineRunsFromRepo fetches pipelineruns from git repository and prepare them for creation.","func (p *PacRun) getPipelineRunsFromRepo(ctx context.Context, repo *v1alpha1.Repository) ([]matcher.Match, error) {","\tprovenance := \"source\"","\tif repo.Spec.Settings != nil \u0026\u0026 repo.Spec.Settings.PipelineRunProvenance != \"\" {","\t\tprovenance = repo.Spec.Settings.PipelineRunProvenance","\t}","\tp.debugf(\"getPipelineRunsFromRepo: repo=%s/%s provenance=%s\", repo.GetNamespace(), repo.GetName(), provenance)","\trawTemplates, err := p.vcx.GetTektonDir(ctx, p.event, tektonDir, provenance)","\tif err != nil \u0026\u0026 p.event.TriggerTarget == triggertype.PullRequest \u0026\u0026 strings.Contains(err.Error(), \"error unmarshalling yaml file\") {","\t\t// make the error a bit more friendly for users who don't know what marshalling or intricacies of the yaml parser works","\t\t// format is \"error unmarshalling yaml file pr-bad-format.yaml: yaml: line 3: could not find expected ':'\"","\t\t// get the filename with a regexp","\t\treg := regexp.MustCompile(`error unmarshalling yaml file\\s([^:]*):\\s*(yaml:\\s*)?(.*)`)","\t\tmatches := reg.FindStringSubmatch(err.Error())","\t\tif len(matches) == 4 {","\t\t\tp.reportValidationErrors(ctx, repo,","\t\t\t\t[]*pacerrors.PacYamlValidations{","\t\t\t\t\t{","\t\t\t\t\t\tName:   matches[1],","\t\t\t\t\t\tErr:    fmt.Errorf(\"yaml validation error: %s\", matches[3]),","\t\t\t\t\t\tSchema: pacerrors.GenericBadYAMLValidation,","\t\t\t\t\t},","\t\t\t\t},","\t\t\t)","\t\t\treturn nil, nil","\t\t}","","\t\treturn nil, err","\t}","","\tif err != nil {","\t\tp.debugf(\"getPipelineRunsFromRepo: GetTektonDir returned error: %v\", err)","\t} else {","\t\tp.debugf(\"getPipelineRunsFromRepo: fetched templates length=%d\", len(rawTemplates))","\t}","","\tif rawTemplates == \"\" \u0026\u0026 p.event.EventType == opscomments.OkToTestCommentEventType.String() {","\t\terr = p.createNeutralStatus(ctx, \".tekton directory not found\", tektonDirMissingError)","\t\tif err != nil {","\t\t\tp.eventEmitter.EmitMessage(nil, zap.ErrorLevel, \"RepositoryCreateStatus\", err.Error())","\t\t}","\t}","","\t// This is for push event error logging because we can't create comment for yaml validation errors on push","\tif err != nil || rawTemplates == \"\" {","\t\tmsg := \"\"","\t\treason := \"RepositoryPipelineRunNotFound\"","\t\tlogLevel := zap.InfoLevel","\t\tif err != nil {","\t\t\treason = \"RepositoryInvalidPipelineRunTemplate\"","\t\t\tlogLevel = zap.ErrorLevel","\t\t\tif strings.Contains(err.Error(), \"error unmarshalling yaml file\") {","\t\t\t\tmsg = \"PipelineRun YAML validation\"","\t\t\t}","\t\t\tmsg += fmt.Sprintf(\" err: %s\", err.Error())","\t\t} else {","\t\t\tmsg = fmt.Sprintf(\"cannot locate templates in %s/ directory for this repository in %s\", tektonDir, p.event.HeadBranch)","\t\t}","\t\tp.eventEmitter.EmitMessage(nil, logLevel, reason, msg)","\t\treturn nil, nil","\t}","","\t// check for condition if need update the pipelinerun with regexp from the","\t// \"raw\" pipelinerun string","\tif msg, needUpdate := p.checkNeedUpdate(rawTemplates); needUpdate {","\t\tp.eventEmitter.EmitMessage(repo, zap.InfoLevel, \"RepositoryNeedUpdate\", msg)","\t\treturn nil, fmt.Errorf(\"%s\", msg)","\t}","","\t// This is for bitbucket","\tif p.event.CloneURL == \"\" {","\t\tp.event.AccountID = \"\"","\t}","","\t// NOTE(chmouel): Initially, matching is performed here to accurately","\t// expand dynamic matching in events. This expansion is crucial for","\t// applying dynamic variables, such as setting the `event_type` to","\t// `on-comment` when matching a git provider's issue comment event with a","\t// comment in an annotation. Although matching occurs three times within","\t// this loop, which might seem inefficient, it's essential to maintain","\t// current functionality without introducing potential errors or behavior","\t// changes. Refactoring for optimization could lead to significant","\t// challenges in tracking down issues. Despite the repetition, the","\t// performance impact is minimal, involving only a loop and a few","\t// conditions.","\tif p.event.TargetTestPipelineRun == \"\" {","\t\trtypes, err := resolve.ReadTektonTypes(ctx, p.logger, rawTemplates)","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\tp.debugf(\"getPipelineRunsFromRepo: pre-parse types: pipelineruns=%d pipelines=%d tasks=%d\", len(rtypes.PipelineRuns), len(rtypes.Pipelines), len(rtypes.Tasks))","\t\t// Don't fail or do anything if we don't have a match yet, we will do it properly later in this function","\t\t_, _ = matcher.MatchPipelinerunByAnnotation(ctx, p.logger, rtypes.PipelineRuns, p.run, p.event, p.vcx, p.eventEmitter, repo, false)","\t}","\t// Replace those {{var}} placeholders user has in her template to the run.Info variable","\tallTemplates := p.makeTemplate(ctx, repo, rawTemplates)","\tp.debugf(\"getPipelineRunsFromRepo: templated data length=%d\", len(allTemplates))","","\ttypes, err := resolve.ReadTektonTypes(ctx, p.logger, allTemplates)","\tif err != nil {","\t\treturn nil, err","\t}","\tp.debugf(\"getPipelineRunsFromRepo: parsed types: pipelineruns=%d pipelines=%d tasks=%d validation_errors=%d\", len(types.PipelineRuns), len(types.Pipelines), len(types.Tasks), len(types.ValidationErrors))","","\tif len(types.ValidationErrors) \u003e 0 \u0026\u0026 p.event.TriggerTarget == triggertype.PullRequest {","\t\tp.reportValidationErrors(ctx, repo, types.ValidationErrors)","\t}","\tpipelineRuns := types.PipelineRuns","\tif len(pipelineRuns) == 0 {","\t\tmsg := fmt.Sprintf(\"cannot locate valid templates in %s/ directory for this repository in %s\", tektonDir, p.event.HeadBranch)","\t\tp.eventEmitter.EmitMessage(nil, zap.InfoLevel, \"RepositoryCannotLocatePipelineRun\", msg)","\t\treturn nil, nil","\t}","\tp.debugf(\"getPipelineRunsFromRepo: pipelineRuns count=%d\", len(pipelineRuns))","\tpipelineRuns, err = resolve.MetadataResolve(pipelineRuns)","\tif err != nil \u0026\u0026 len(pipelineRuns) == 0 {","\t\tp.eventEmitter.EmitMessage(repo, zap.ErrorLevel, \"FailedToResolvePipelineRunMetadata\", err.Error())","\t\treturn nil, err","\t}","\tp.debugf(\"getPipelineRunsFromRepo: metadata resolved for pipelineRuns count=%d\", len(pipelineRuns))","","\t// Match the PipelineRun with annotation","\tvar matchedPRs []matcher.Match","\tif p.event.TargetTestPipelineRun == \"\" {","\t\tif matchedPRs, err = matcher.MatchPipelinerunByAnnotation(ctx, p.logger, pipelineRuns, p.run, p.event, p.vcx, p.eventEmitter, repo, true); err != nil {","\t\t\t// Don't fail when you don't have a match between pipeline and annotations","\t\t\tp.eventEmitter.EmitMessage(nil, zap.WarnLevel, \"RepositoryNoMatch\", err.Error())","\t\t\t// In a scenario where an external user submits a pull request and the repository owner uses the","\t\t\t// GitOps command `/ok-to-test` to trigger CI, but no matching pull request is found,","\t\t\t// a neutral check-run will be created on the pull request to indicate that no PipelineRun was triggered","\t\t\tif p.event.EventType == opscomments.OkToTestCommentEventType.String() \u0026\u0026 len(matchedPRs) == 0 {","\t\t\t\ttext := fmt.Sprintf(\"No matching PipelineRun found for the '%s' event in .tekton/ directory. Please ensure that PipelineRun is configured for '%s' event.\", p.event.TriggerTarget.String(), p.event.TriggerTarget.String())","\t\t\t\terr = p.createNeutralStatus(ctx, \"No PipelineRun matched\", text)","\t\t\t\tif err != nil {","\t\t\t\t\tp.eventEmitter.EmitMessage(nil, zap.WarnLevel, \"RepositoryCreateStatus\", err.Error())","\t\t\t\t}","\t\t\t\tp.eventEmitter.EmitMessage(nil, zap.InfoLevel, \"RepositoryNoMatch\", text)","\t\t\t}","\t\t\treturn nil, nil","\t\t}","\t\tp.debugf(\"getPipelineRunsFromRepo: initial match count=%d\", len(matchedPRs))","\t}","","\t// if the event is a comment event, but we don't have any match from the keys.OnComment then do the ACL checks again","\t// we skipped previously so we can get the match from the event to the pipelineruns","\tif p.event.EventType == opscomments.NoOpsCommentEventType.String() || p.event.EventType == opscomments.OnCommentEventType.String() {","\t\tstatus := provider.StatusOpts{","\t\t\tStatus:       queuedStatus,","\t\t\tTitle:        \"Pending approval, waiting for an /ok-to-test\",","\t\t\tConclusion:   pendingConclusion,","\t\t\tDetailsURL:   p.event.URL,","\t\t\tAccessDenied: true,","\t\t}","\t\tif allowed, err := p.checkAccessOrError(ctx, repo, status, \"by GitOps comment on push commit\"); !allowed {","\t\t\treturn nil, err","\t\t}","\t}","","\t// if event type is incoming then filter out the pipelineruns related to incoming event","\tpipelineRuns = matcher.MatchRunningPipelineRunForIncomingWebhook(p.event.EventType, p.event.TargetPipelineRun, pipelineRuns)","\tif pipelineRuns == nil {","\t\tmsg := fmt.Sprintf(\"cannot find pipelinerun %s for matching an incoming event in this repository\", p.event.TargetPipelineRun)","\t\tp.eventEmitter.EmitMessage(repo, zap.InfoLevel, \"RepositoryCannotLocatePipelineRunForIncomingEvent\", msg)","\t\treturn nil, nil","\t}","\tp.debugf(\"getPipelineRunsFromRepo: incoming filter result count=%d\", len(pipelineRuns))","","\t// if /test command is used then filter out the pipelinerun","\tif p.event.TargetTestPipelineRun != \"\" {","\t\ttargetPR := filterRunningPipelineRunOnTargetTest(p.event.TargetTestPipelineRun, pipelineRuns)","\t\tif targetPR == nil {","\t\t\tmsg := fmt.Sprintf(\"cannot find the targeted pipelinerun %s in this repository\", p.event.TargetTestPipelineRun)","\t\t\tp.eventEmitter.EmitMessage(repo, zap.InfoLevel, \"RepositoryCannotLocatePipelineRun\", msg)","\t\t\treturn nil, nil","\t\t}","\t\tpipelineRuns = []*tektonv1.PipelineRun{targetPR}","\t\tp.debugf(\"getPipelineRunsFromRepo: filtered to target pipelinerun=%s\", p.event.TargetTestPipelineRun)","\t}","","\t// finally resolve with fetching the remote tasks (if enabled)","\tif p.pacInfo.RemoteTasks {","\t\t// only resolve on the matched pipelineruns if we don't do explicit /test of unmatched pipelineruns","\t\tif p.event.TargetTestPipelineRun == \"\" {","\t\t\ttypes.PipelineRuns = nil","\t\t\tfor _, match := range matchedPRs {","\t\t\t\tfor pr := range pipelineRuns {","\t\t\t\t\tif match.PipelineRun.GetName() == \"\" \u0026\u0026 match.PipelineRun.GetGenerateName() == pipelineRuns[pr].GenerateName ||","\t\t\t\t\t\tmatch.PipelineRun.GetName() != \"\" \u0026\u0026 match.PipelineRun.GetName() == pipelineRuns[pr].Name {","\t\t\t\t\t\ttypes.PipelineRuns = append(types.PipelineRuns, pipelineRuns[pr])","\t\t\t\t\t}","\t\t\t\t}","\t\t\t}","\t\t}","\t\tp.debugf(\"getPipelineRunsFromRepo: resolving remote tasks for pipelineRuns=%d\", len(types.PipelineRuns))","\t\tpipelineRuns, err = resolve.Resolve(ctx, p.run, p.logger, p.vcx, types, p.event, \u0026resolve.Opts{","\t\t\tGenerateName: true,","\t\t\tRemoteTasks:  true,","\t\t})","\t\tif err != nil {","\t\t\tp.eventEmitter.EmitMessage(repo, zap.ErrorLevel, \"RepositoryFailedToMatch\", fmt.Sprintf(\"failed to match pipelineRuns: %s\", err.Error()))","\t\t\treturn nil, err","\t\t}","\t}","","\terr = p.changePipelineRun(ctx, repo, pipelineRuns)","\tif err != nil {","\t\treturn nil, err","\t}","\tp.debugf(\"getPipelineRunsFromRepo: updated pipelineRuns count=%d\", len(pipelineRuns))","\t// if we are doing explicit /test command then we only want to run the one that has matched the /test","\tif p.event.TargetTestPipelineRun != \"\" {","\t\tp.eventEmitter.EmitMessage(repo, zap.InfoLevel, \"RepositoryMatchedPipelineRun\", fmt.Sprintf(\"explicit testing via /test of PipelineRun %s\", p.event.TargetTestPipelineRun))","\t\tselectedPr := filterRunningPipelineRunOnTargetTest(p.event.TargetTestPipelineRun, pipelineRuns)","\t\tif selectedPr == nil {","\t\t\tmsg := fmt.Sprintf(\"cannot find the targeted pipelinerun %s in this repository\", p.event.TargetTestPipelineRun)","\t\t\tp.eventEmitter.EmitMessage(repo, zap.InfoLevel, \"RepositoryCannotLocatePipelineRun\", msg)","\t\t\treturn nil, nil","\t\t}","\t\tselectedRepo := p.resolveTargetNamespaceRepo(ctx, repo, selectedPr)","\t\tif selectedRepo == nil {","\t\t\tmsg := fmt.Sprintf(\"skipping pipelinerun %s: target-namespace repo not found\", pipelineRunIdentifier(selectedPr))","\t\t\tp.eventEmitter.EmitMessage(repo, zap.InfoLevel, \"RepositoryTargetNamespaceNotFound\", msg)","\t\t\treturn nil, nil","\t\t}","\t\tp.debugf(\"getPipelineRunsFromRepo: explicit /test using repo=%s/%s for pipelinerun=%s\", selectedRepo.GetNamespace(), selectedRepo.GetName(), pipelineRunIdentifier(selectedPr))","\t\treturn []matcher.Match{{","\t\t\tPipelineRun: selectedPr,","\t\t\tRepo:        selectedRepo,","\t\t}}, nil","\t}","","\tmatchedPRs, err = matcher.MatchPipelinerunByAnnotation(ctx, p.logger, pipelineRuns, p.run, p.event, p.vcx, p.eventEmitter, repo, false)","\tif err != nil {","\t\t// Don't fail when you don't have a match between pipeline and annotations","\t\tp.eventEmitter.EmitMessage(nil, zap.WarnLevel, \"RepositoryNoMatch\", err.Error())","\t\treturn nil, nil","\t}","\tp.debugf(\"getPipelineRunsFromRepo: final match count=%d\", len(matchedPRs))","","\treturn matchedPRs, nil","}","","func filterRunningPipelineRunOnTargetTest(testPipeline string, prs []*tektonv1.PipelineRun) *tektonv1.PipelineRun {","\tfor _, pr := range prs {","\t\tif prName, ok := pr.GetAnnotations()[apipac.OriginalPRName]; ok {","\t\t\tif prName == testPipeline {","\t\t\t\treturn pr","\t\t\t}","\t\t}","\t}","\treturn nil","}","","func pipelineRunIdentifier(pr *tektonv1.PipelineRun) string {","\tif pr == nil {","\t\treturn \"\u003cnil\u003e\"","\t}","\tif pr.GetName() != \"\" {","\t\treturn pr.GetName()","\t}","\tif pr.GetGenerateName() != \"\" {","\t\treturn pr.GetGenerateName()","\t}","\treturn \"\u003cunnamed\u003e\"","}","","func (p *PacRun) resolveTargetNamespaceRepo(ctx context.Context, fallbackRepo *v1alpha1.Repository, pr *tektonv1.PipelineRun) *v1alpha1.Repository {","\tif fallbackRepo == nil || pr == nil {","\t\treturn fallbackRepo","\t}","","\ttargetNS, ok := pr.GetAnnotations()[apipac.TargetNamespace]","\tif !ok || targetNS == \"\" {","\t\treturn fallbackRepo","\t}","\tif targetNS == fallbackRepo.GetNamespace() {","\t\treturn fallbackRepo","\t}","","\ttargetRepo, err := matcher.MatchEventURLRepo(ctx, p.run, p.event, targetNS)","\tif err != nil {","\t\tp.logger.Warnf(\"resolveTargetNamespaceRepo: failed to lookup target namespace=%s for pipelinerun=%s: %v\", targetNS, pipelineRunIdentifier(pr), err)","\t\treturn nil","\t}","\tif targetRepo == nil {","\t\tp.logger.Warnf(\"resolveTargetNamespaceRepo: no repository found in target namespace=%s for pipelinerun=%s\", targetNS, pipelineRunIdentifier(pr))","\t\treturn nil","\t}","","\tp.debugf(\"resolveTargetNamespaceRepo: resolved pipelinerun=%s to repo=%s/%s via target-namespace=%s\", pipelineRunIdentifier(pr), targetRepo.GetNamespace(), targetRepo.GetName(), targetNS)","\treturn targetRepo","}","","// changePipelineRun go over each pipelineruns and modify things into it.","//","// - the secret template variable with a random one as generated from GetBasicAuthSecretName","// - the template variable with the one from the event (this includes the remote pipeline that has template variables).","func (p *PacRun) changePipelineRun(ctx context.Context, repo *v1alpha1.Repository, prs []*tektonv1.PipelineRun) error {","\tp.debugf(\"changePipelineRun: processing %d pipelineruns\", len(prs))","\tfor k, pr := range prs {","\t\tprName := pr.GetName()","\t\tif prName == \"\" {","\t\t\tprName = pr.GetGenerateName()","\t\t}","\t\tp.debugf(\"changePipelineRun: processing pipelinerun=%s\", prName)","","\t\tb, err := json.Marshal(pr)","\t\tif err != nil {","\t\t\treturn fmt.Errorf(\"failed to marshal PipelineRun %s: %w\", prName, err)","\t\t}","","\t\tname := secrets.GenerateBasicAuthSecretName()","\t\tprocessed := templates.ReplacePlaceHoldersVariables(string(b), map[string]string{","\t\t\t\"git_auth_secret\": name,","\t\t}, nil, nil, map[string]any{})","\t\tprocessed = p.makeTemplate(ctx, repo, processed)","","\t\tvar np *tektonv1.PipelineRun","\t\terr = json.Unmarshal([]byte(processed), \u0026np)","\t\tif err != nil {","\t\t\treturn fmt.Errorf(\"failed to unmarshal PipelineRun %s: %w\", prName, err)","\t\t}","\t\t// don't crash when we don't have any annotations","\t\tif np.Annotations == nil {","\t\t\tnp.Annotations = map[string]string{}","\t\t}","\t\tnp.Annotations[apipac.GitAuthSecret] = name","","\t\tprs[k] = np","\t\tp.debugf(\"changePipelineRun: updated pipelinerun=%s with git_auth_secret annotation\", prName)","\t}","\treturn nil","}","","// checkNeedUpdate checks if the template needs an update form the user, try to","// match some patterns for some issues in a template to let the user know they need to","// update.","//","// We otherwise fail with a descriptive error message to the user (check run","// interface on as comment for other providers) on how to update.","//","// Checks are deprecated/removed to n+2 release of OSP.","func (p *PacRun) checkNeedUpdate(_ string) (string, bool) {","\treturn \"\", false","}","","func (p *PacRun) createNeutralStatus(ctx context.Context, title, text string) error {","\tp.debugf(\"createNeutralStatus: title=%s\", title)","\tstatus := provider.StatusOpts{","\t\tStatus:     CompletedStatus,","\t\tTitle:      title,","\t\tText:       text,","\t\tConclusion: neutralConclusion,","\t\tDetailsURL: p.event.URL,","\t}","\tif err := p.vcx.CreateStatus(ctx, p.event, status); err != nil {","\t\treturn fmt.Errorf(\"failed to run create status, user is not allowed to run the CI:: %w\", err)","\t}","","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,0,2,1,1,1,0,2,2,2,2,2,0,2,2,0,0,0,0,2,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,2,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,1,1,1,1,1,1,1,1,1,0,0,2,0,0,0,2,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,1,0,0,2,2,2,2,2,0,2,2,2,2,2,0,0,0,2,2,2,2,2,2,2,2,1,1,2,1,1,1,2,2,0,0,0,0,2,1,1,1,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,1,1,2,2,2,0,0,2,2,2,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,0,2,0,0,0,0,2,1,1,1,1,1,1,1,1,1,1,0,0,0,2,2,1,1,1,1,2,2,2,2,2,2,1,1,1,1,2,2,0,0,0,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,2,2,2,0,0,2,2,1,1,2,2,2,2,2,2,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,1,1,1,1,2,2,2,0,0,2,2,2,2,2,2,0,0,2,0,0,2,2,1,1,2,2,2,2,2,2,1,0,0,2,2,1,1,0,2,2,2,2,2,1,1,0,2,2,1,1,1,2,2,2,2,0,2,2,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,2,0,2,0,0,0,0,0,0,0,0,0,0,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,0,1,0]},{"id":94,"path":"pkg/pipelineascode/pipelineascode.go","lines":["package pipelineascode","","import (","\t\"context\"","\t\"fmt\"","\t\"sync\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/action\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/customparams\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/events\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/formatting\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/matcher\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/opscomments\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/settings\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/secrets\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"go.uber.org/zap\"","\t\"k8s.io/apimachinery/pkg/api/errors\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"",")","","const (","\ttektonDir         = \".tekton\"","\tCompletedStatus   = \"completed\"","\tinProgressStatus  = \"in_progress\"","\tqueuedStatus      = \"queued\"","\tsuccessConclusion = \"success\"","\tfailureConclusion = \"failure\"","\tpendingConclusion = \"pending\"","\tneutralConclusion = \"neutral\"",")","","type PacRun struct {","\tevent        *info.Event","\tvcx          provider.Interface","\trun          *params.Run","\tk8int        kubeinteraction.Interface","\tlogger       *zap.SugaredLogger","\teventEmitter *events.EventEmitter","\tmanager      *ConcurrencyManager","\tpacInfo      *info.PacOpts","\tglobalRepo   *v1alpha1.Repository","}","","func NewPacs(event *info.Event, vcx provider.Interface, run *params.Run, pacInfo *info.PacOpts, k8int kubeinteraction.Interface, logger *zap.SugaredLogger, globalRepo *v1alpha1.Repository) PacRun {","\treturn PacRun{","\t\tevent: event, run: run, vcx: vcx, k8int: k8int, pacInfo: pacInfo, logger: logger, globalRepo: globalRepo,","\t\teventEmitter: events.NewEventEmitter(run.Clients.Kube, logger),","\t\tmanager:      NewConcurrencyManager(),","\t}","}","","func (p *PacRun) Run(ctx context.Context) error {","\tp.debugf(\"run start: trigger_target=%s event_type=%s repo_url=%s sha=%s pr=%d has_skip=%t cancel_pipeline_runs=%t\",","\t\tp.event.TriggerTarget,","\t\tp.event.EventType,","\t\tp.event.URL,","\t\tp.event.SHA,","\t\tp.event.PullRequestNumber,","\t\tp.event.HasSkipCommand,","\t\tp.event.CancelPipelineRuns,","\t)","\t// For PullRequestClosed events, skip matching logic and go straight to cancellation","\tif p.event.TriggerTarget == triggertype.PullRequestClosed {","\t\tp.debugf(\"pull request closed: verifying repo and cancelling in-progress pipelineRuns\")","\t\trepo, err := p.verifyRepoAndUser(ctx)","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\tif repo != nil {","\t\t\tp.debugf(\"repo verified: name=%s namespace=%s\", repo.GetName(), repo.GetNamespace())","\t\t\tif err := p.cancelAllInProgressBelongingToClosedPullRequest(ctx, repo); err != nil {","\t\t\t\treturn fmt.Errorf(\"error cancelling in progress pipelineRuns belonging to pull request %d: %w\", p.event.PullRequestNumber, err)","\t\t\t}","\t\t} else {","\t\t\tp.debugf(\"pull request closed: no repo match found for event url=%s\", p.event.URL)","\t\t}","\t\treturn nil","\t}","","\tmatchedPRs, repo, err := p.matchRepoPR(ctx)","\tif err != nil {","\t\tcreateStatusErr := p.vcx.CreateStatus(ctx, p.event, provider.StatusOpts{","\t\t\tStatus:     CompletedStatus,","\t\t\tConclusion: failureConclusion,","\t\t\tText:       fmt.Sprintf(\"There was an issue validating the commit: %q\", err),","\t\t\tDetailsURL: p.run.Clients.ConsoleUI().URL(),","\t\t})","\t\tp.eventEmitter.EmitMessage(repo, zap.ErrorLevel, \"RepositoryCreateStatus\", fmt.Sprintf(\"an error occurred: %s\", err))","\t\tif createStatusErr != nil {","\t\t\tp.eventEmitter.EmitMessage(repo, zap.ErrorLevel, \"RepositoryCreateStatus\", fmt.Sprintf(\"cannot create status: %s: %s\", err, createStatusErr))","\t\t}","\t}","\trepoName := \"\u003cnil\u003e\"","\trepoNamespace := \"\u003cnil\u003e\"","\tif repo != nil {","\t\trepoName = repo.GetName()","\t\trepoNamespace = repo.GetNamespace()","\t}","\tp.debugf(\"match results: matched=%d repo=%s/%s\", len(matchedPRs), repoNamespace, repoName)","\tif len(matchedPRs) == 0 {","\t\tp.debugf(\"no pipelineruns matched; returning without starting any runs\")","\t\treturn nil","\t}","\tif repo.Spec.ConcurrencyLimit != nil \u0026\u0026 *repo.Spec.ConcurrencyLimit != 0 {","\t\tp.debugf(\"enabling concurrency manager with limit=%d\", *repo.Spec.ConcurrencyLimit)","\t\tp.manager.Enable()","\t}","","\t// Defensive skip-CI check: this is a safety net in case events bypass the early check in sinker.","\t// Primary skip detection happens in sinker.processEvent() for performance, but this ensures","\t// nothing slips through (e.g., tests that call Run() directly, or edge cases).","\t// Skip only for non-GitOps events (GitOps commands can override skip-CI).","\tif p.event.HasSkipCommand \u0026\u0026 !opscomments.IsAnyOpsEventType(p.event.EventType) {","\t\tp.logger.Infof(\"CI skipped: commit contains skip command in message (secondary check)\")","\t\treturn nil","\t}","","\t// set params for the console driver, only used for the custom console ones","\tp.debugf(\"resolving custom params for console UI\")","\tcp := customparams.NewCustomParams(p.event, repo, p.run, p.k8int, p.eventEmitter, p.vcx)","\tmaptemplate, _, err := cp.GetParams(ctx)","\tif err != nil {","\t\tp.eventEmitter.EmitMessage(repo, zap.ErrorLevel, \"ParamsError\",","\t\t\tfmt.Sprintf(\"error processing repository CR custom params: %s\", err.Error()))","\t} else {","\t\tp.debugf(\"resolved %d custom params for console UI\", len(maptemplate))","\t}","\tp.run.Clients.ConsoleUI().SetParams(maptemplate)","","\tvar wg sync.WaitGroup","\tfor i, match := range matchedPRs {","\t\tif match.Repo == nil {","\t\t\tmatch.Repo = repo","\t\t}","","\t\t// After matchRepo func fetched repo from k8s api repo is updated and","\t\t// need to merge global repo again","\t\tif p.globalRepo != nil {","\t\t\tmatch.Repo.Spec.Merge(p.globalRepo.Spec)","\t\t}","","\t\twg.Add(1)","","\t\tgo func(match matcher.Match, i int) {","\t\t\tdefer wg.Done()","\t\t\tp.debugf(\"starting pipelinerun %s (index=%d)\", match.PipelineRun.GetGenerateName(), i)","\t\t\tpr, err := p.startPR(ctx, match)","\t\t\tif err != nil {","\t\t\t\terrMsg := fmt.Sprintf(\"There was an error starting the PipelineRun %s, %s\", match.PipelineRun.GetGenerateName(), err.Error())","\t\t\t\terrMsgM := fmt.Sprintf(\"There was an error creating the PipelineRun: \u003cb\u003e%s\u003c/b\u003e\\n\\n%s\", match.PipelineRun.GetGenerateName(), err.Error())","\t\t\t\tp.eventEmitter.EmitMessage(repo, zap.ErrorLevel, \"RepositoryPipelineRun\", errMsg)","\t\t\t\tprName := match.PipelineRun.GetName()","\t\t\t\tif prName == \"\" {","\t\t\t\t\tprName = match.PipelineRun.GetGenerateName()","\t\t\t\t}","\t\t\t\tcreateStatusErr := p.vcx.CreateStatus(ctx, p.event, provider.StatusOpts{","\t\t\t\t\tPipelineRunName:          prName,","\t\t\t\t\tPipelineRun:              match.PipelineRun,","\t\t\t\t\tOriginalPipelineRunName:  match.PipelineRun.GetAnnotations()[keys.OriginalPRName],","\t\t\t\t\tStatus:                   CompletedStatus,","\t\t\t\t\tConclusion:               failureConclusion,","\t\t\t\t\tTitle:                    \"pipelinerun start failure\",","\t\t\t\t\tText:                     errMsgM,","\t\t\t\t\tDetailsURL:               p.run.Clients.ConsoleUI().URL(),","\t\t\t\t\tInstanceCountForCheckRun: i,","\t\t\t\t})","\t\t\t\tif createStatusErr != nil {","\t\t\t\t\tp.eventEmitter.EmitMessage(repo, zap.ErrorLevel, \"RepositoryCreateStatus\", fmt.Sprintf(\"Cannot create status: %s: %s\", err, createStatusErr))","\t\t\t\t}","\t\t\t}","\t\t\tif pr != nil {","\t\t\t\tp.debugf(\"pipelinerun started: name=%s namespace=%s\", pr.GetName(), pr.GetNamespace())","\t\t\t}","\t\t\tp.manager.AddPipelineRun(pr)","\t\t\tif err := p.cancelInProgressMatchingPipelineRun(ctx, pr, repo); err != nil {","\t\t\t\tp.eventEmitter.EmitMessage(repo, zap.ErrorLevel, \"RepositoryPipelineRun\", fmt.Sprintf(\"error cancelling in progress pipelineRuns: %s\", err))","\t\t\t}","\t\t\tp.debugf(\"finished processing pipelinerun start: name=%s\", match.PipelineRun.GetGenerateName())","\t\t}(match, i)","\t}","\twg.Wait()","","\torder, prs := p.manager.GetExecutionOrder()","\tif order != \"\" {","\t\tp.debugf(\"patching execution order for %d pipelineruns: %s\", len(prs), order)","\t\tfor _, pr := range prs {","\t\t\twg.Add(1)","","\t\t\tgo func(order string, pr tektonv1.PipelineRun) {","\t\t\t\tdefer wg.Done()","\t\t\t\tif _, err := action.PatchPipelineRun(ctx, p.logger, \"execution order\", p.run.Clients.Tekton, \u0026pr, getExecutionOrderPatch(order)); err != nil {","\t\t\t\t\terrMsg := fmt.Sprintf(\"Failed to patch pipelineruns %s execution order: %s\", pr.GetGenerateName(), err.Error())","\t\t\t\t\tp.eventEmitter.EmitMessage(repo, zap.ErrorLevel, \"RepositoryPipelineRun\", errMsg)","\t\t\t\t\treturn","\t\t\t\t}","\t\t\t}(order, *pr)","\t\t}","\t}","\twg.Wait()","\treturn nil","}","","func (p *PacRun) startPR(ctx context.Context, match matcher.Match) (*tektonv1.PipelineRun, error) {","\tvar gitAuthSecretName string","\tprName := match.PipelineRun.GetName()","\tif prName == \"\" {","\t\tprName = match.PipelineRun.GetGenerateName()","\t}","\tp.debugf(\"startPR: pipelinerun=%s namespace=%s event_sha=%s target_branch=%s\",","\t\tprName,","\t\tmatch.Repo.GetNamespace(),","\t\tp.event.SHA,","\t\tp.event.BaseBranch,","\t)","","\t// Automatically create a secret with the token to be reused by git-clone task","\tif p.pacInfo.SecretAutoCreation {","\t\tif annotation, ok := match.PipelineRun.GetAnnotations()[keys.GitAuthSecret]; ok {","\t\t\tgitAuthSecretName = annotation","\t\t\tp.debugf(\"startPR: using git auth secret from annotation=%s\", gitAuthSecretName)","\t\t} else {","\t\t\treturn nil, fmt.Errorf(\"cannot get annotation %s as set on PR\", keys.GitAuthSecret)","\t\t}","","\t\tauthSecret, err := secrets.MakeBasicAuthSecret(p.event, gitAuthSecretName)","\t\tif err != nil {","\t\t\treturn nil, fmt.Errorf(\"making basic auth secret: %s has failed: %w \", gitAuthSecretName, err)","\t\t}","","\t\tif err = p.k8int.CreateSecret(ctx, match.Repo.GetNamespace(), authSecret); err != nil {","\t\t\t// NOTE: Handle AlreadyExists errors due to etcd/API server timing issues.","\t\t\t// Investigation found: slow etcd response causes API server retry, resulting in","\t\t\t// duplicate secret creation attempts for the same PR. This is a workaround, not","\t\t\t// designed behavior - reuse existing secret to prevent PipelineRun failure.","\t\t\tif errors.IsAlreadyExists(err) {","\t\t\t\tmsg := fmt.Sprintf(\"Secret %s already exists in namespace %s, reusing existing secret\",","\t\t\t\t\tauthSecret.GetName(), match.Repo.GetNamespace())","\t\t\t\tp.eventEmitter.EmitMessage(match.Repo, zap.WarnLevel, \"RepositorySecretReused\", msg)","\t\t\t} else {","\t\t\t\treturn nil, fmt.Errorf(\"creating basic auth secret: %s has failed: %w \", authSecret.GetName(), err)","\t\t\t}","\t\t} else {","\t\t\tp.debugf(\"startPR: created git auth secret %s in namespace %s\", authSecret.GetName(), match.Repo.GetNamespace())","\t\t}","\t}","","\t// Add labels and annotations to pipelinerun","\terr := kubeinteraction.AddLabelsAndAnnotations(p.event, match.PipelineRun, match.Repo, p.vcx.GetConfig(), p.run)","\tif err != nil {","\t\tp.logger.Errorf(\"Error adding labels/annotations to PipelineRun '%s' in namespace '%s': %v\", match.PipelineRun.GetName(), match.Repo.GetNamespace(), err)","\t} else {","\t\tp.debugf(\"startPR: added labels/annotations to pipelinerun=%s\", prName)","\t}","","\t// if concurrency is defined then start the pipelineRun in pending state","\tif match.Repo.Spec.ConcurrencyLimit != nil \u0026\u0026 *match.Repo.Spec.ConcurrencyLimit != 0 {","\t\t// pending status","\t\tmatch.PipelineRun.Spec.Status = tektonv1.PipelineRunSpecStatusPending","\t\tp.debugf(\"startPR: marking pipelinerun=%s as pending due to concurrency limit\", prName)","\t}","","\t// Create the actual pipelineRun","\tp.debugf(\"startPR: creating pipelinerun=%s in namespace=%s\", prName, match.Repo.GetNamespace())","\tpr, err := p.run.Clients.Tekton.TektonV1().PipelineRuns(match.Repo.GetNamespace()).Create(ctx,","\t\tmatch.PipelineRun, metav1.CreateOptions{})","\tif err != nil {","\t\t// cleanup the gitauth secret because ownerRef isn't set when the pipelineRun creation failed","\t\tif p.pacInfo.SecretAutoCreation {","\t\t\tif errDelSec := p.k8int.DeleteSecret(ctx, p.logger, match.Repo.GetNamespace(), gitAuthSecretName); errDelSec != nil {","\t\t\t\t// don't overshadow the pipelineRun creation error, just log","\t\t\t\tp.logger.Errorf(\"removing auto created secret: %s in namespace %s has failed: %w \", gitAuthSecretName, match.Repo.GetNamespace(), errDelSec)","\t\t\t}","\t\t}","\t\t// we need to make difference between markdown error and normal error that goes to namespace/controller stream","\t\treturn nil, fmt.Errorf(\"creating pipelinerun %s in namespace %s has failed.\\n\\nTekton Controller has reported this error: ```%w``` \", match.PipelineRun.GetGenerateName(),","\t\t\tmatch.Repo.GetNamespace(), err)","\t}","","\t// update ownerRef of secret with pipelineRun, so that it gets cleanedUp with pipelineRun","\tif p.pacInfo.SecretAutoCreation {","\t\terr := p.k8int.UpdateSecretWithOwnerRef(ctx, p.logger, pr.Namespace, gitAuthSecretName, pr)","\t\tif err != nil {","\t\t\t// we still return the created PR with error, and allow caller to decide what to do with the PR, and avoid","\t\t\t// unneeded SIGSEGV's","\t\t\treturn pr, fmt.Errorf(\"cannot update pipelinerun %s with ownerRef: %w\", pr.GetGenerateName(), err)","\t\t}","\t\tp.debugf(\"startPR: updated secret ownerRef for pipelinerun=%s secret=%s\", pr.GetName(), gitAuthSecretName)","\t}","","\t// Create status with the log url","\tp.logger.Infof(\"PipelineRun %s has been created in namespace %s with status %s for SHA: %s Target Branch: %s\",","\t\tpr.GetName(), match.Repo.GetNamespace(), pr.Spec.Status, p.event.SHA, p.event.BaseBranch)","","\tconsoleURL := p.run.Clients.ConsoleUI().DetailURL(pr)","\tmt := formatting.MessageTemplate{","\t\tPipelineRunName: pr.GetName(),","\t\tNamespace:       match.Repo.GetNamespace(),","\t\tConsoleName:     p.run.Clients.ConsoleUI().GetName(),","\t\tConsoleURL:      consoleURL,","\t\tTknBinary:       settings.TknBinaryName,","\t\tTknBinaryURL:    settings.TknBinaryURL,","\t}","","\tmsg, err := mt.MakeTemplate(p.vcx.GetTemplate(provider.StartingPipelineType))","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"cannot create message template: %w\", err)","\t}","\tstatus := provider.StatusOpts{","\t\tStatus:                  inProgressStatus,","\t\tConclusion:              pendingConclusion,","\t\tText:                    msg,","\t\tDetailsURL:              consoleURL,","\t\tPipelineRunName:         pr.GetName(),","\t\tPipelineRun:             pr,","\t\tOriginalPipelineRunName: pr.GetAnnotations()[keys.OriginalPRName],","\t}","","\t// Patch the pipelineRun with the appropriate annotations and labels.","\t// Set the state so the watcher will continue with reconciling the pipelineRun","\t// The watcher reconciles only pipelineRuns that has the state annotation.","\tpatchAnnotations := map[string]string{}","\tpatchLabels := map[string]string{}","\twhatPatching := \"\"","\t// if pipelineRun is in pending state then report status as queued","\t// The pipelineRun can be pending because of PAC's concurrency limit or because of an external mutatingwebhook","\tif pr.Spec.Status == tektonv1.PipelineRunSpecStatusPending {","\t\tstatus.Status = queuedStatus","\t\tif status.Text, err = mt.MakeTemplate(p.vcx.GetTemplate(provider.QueueingPipelineType)); err != nil {","\t\t\treturn nil, fmt.Errorf(\"cannot create message template: %w\", err)","\t\t}","\t\twhatPatching = \"annotations.state and labels.state\"","\t\tpatchAnnotations[keys.State] = kubeinteraction.StateQueued","\t\tpatchLabels[keys.State] = kubeinteraction.StateQueued","\t} else {","\t\t// Mark that the start will be reported to the Git provider","\t\tpatchAnnotations[keys.SCMReportingPLRStarted] = \"true\"","\t\tpatchAnnotations[keys.State] = kubeinteraction.StateStarted","\t\tpatchLabels[keys.State] = kubeinteraction.StateStarted","\t\twhatPatching = fmt.Sprintf(","\t\t\t\"annotation.%s and annotations.state and labels.state\",","\t\t\tkeys.SCMReportingPLRStarted,","\t\t)","\t}","","\tif err := p.vcx.CreateStatus(ctx, p.event, status); err != nil {","\t\t// we still return the created PR with error, and allow caller to decide what to do with the PR, and avoid","\t\t// unneeded SIGSEGV's","\t\treturn pr, fmt.Errorf(\"cannot use the API on the provider platform to create a in_progress status: %w\", err)","\t}","\tp.debugf(\"startPR: created status for pipelinerun=%s status=%s conclusion=%s\", pr.GetName(), status.Status, status.Conclusion)","","\t// Patch pipelineRun with logURL annotation, skips for GitHub App as we patch logURL while patching CheckrunID","\tif _, ok := pr.Annotations[keys.InstallationID]; !ok {","\t\tpatchAnnotations[keys.LogURL] = p.run.Clients.ConsoleUI().DetailURL(pr)","\t\twhatPatching = \"annotations.logURL, \" + whatPatching","\t}","","\tif len(patchAnnotations) \u003e 0 || len(patchLabels) \u003e 0 {","\t\tp.debugf(\"startPR: patching pipelinerun=%s patches=%s annotations=%d labels=%d\", pr.GetName(), whatPatching, len(patchAnnotations), len(patchLabels))","\t\tpr, err = action.PatchPipelineRun(ctx, p.logger, whatPatching, p.run.Clients.Tekton, pr, getMergePatch(patchAnnotations, patchLabels))","\t\tif err != nil {","\t\t\t// if PipelineRun patch is failed then do not return error, just log the error","\t\t\t// because its a false negative and on startPR return a failed check is being created","\t\t\t// due to this.","\t\t\tp.logger.Errorf(\"cannot patch pipelinerun %s: %w\", pr.GetGenerateName(), err)","\t\t\treturn pr, nil","\t\t}","\t\tcurrentReason := \"\"","\t\tif len(pr.Status.GetConditions()) \u003e 0 {","\t\t\tcurrentReason = pr.Status.GetConditions()[0].GetReason()","\t\t}","","\t\tp.logger.Infof(\"PipelineRun %s/%s patched successfully - Spec.Status: %s, State annotation: '%s', SCMReportingPLRStarted annotation: '%s', Status reason: '%s', Git provider status: '%s', Patched: %s\",","\t\t\tpr.GetNamespace(),","\t\t\tpr.GetName(),","\t\t\tpr.Spec.Status,","\t\t\tpr.GetAnnotations()[keys.State],","\t\t\tpr.GetAnnotations()[keys.SCMReportingPLRStarted],","\t\t\tcurrentReason,","\t\t\tstatus.Status,","\t\t\twhatPatching)","\t}","","\treturn pr, nil","}","","func getMergePatch(annotations, labels map[string]string) map[string]any {","\treturn map[string]any{","\t\t\"metadata\": map[string]any{","\t\t\t\"annotations\": annotations,","\t\t\t\"labels\":      labels,","\t\t},","\t}","}","","func getExecutionOrderPatch(order string) map[string]any {","\treturn map[string]any{","\t\t\"metadata\": map[string]any{","\t\t\t\"annotations\": map[string]string{","\t\t\t\tkeys.ExecutionOrder: order,","\t\t\t},","\t\t},","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,2,2,2,2,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,0,0,0,0,0,2,1,1,1,0,0,2,2,2,2,1,1,2,2,2,2,2,2,2,2,2,2,0,0,0,2,1,1,0,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,2,2,2,2,2,1,1,2,0,0,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,0,0,0,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,1,2,2,2,0,0,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,0,0,2,2,0,0,0,2,2,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,0,0,2,0,0,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2]},{"id":95,"path":"pkg/pipelineascode/secret.go","lines":["package pipelineascode","","import (","\t\"context\"","\t\"fmt\"","\t\"strings\"","","\tapipac \"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\tktypes \"github.com/openshift-pipelines/pipelines-as-code/pkg/secrets/types\"","\t\"go.uber.org/zap\"",")","","const (","\tDefaultGitProviderSecretKey                  = \"provider.token\"","\tDefaultGitProviderWebhookSecretKey           = \"webhook.secret\"","\tdefaultPipelinesAscodeSecretWebhookSecretKey = \"webhook.secret\"",")","","type SecretFromRepository struct {","\tK8int       kubeinteraction.Interface","\tConfig      *info.ProviderConfig","\tEvent       *info.Event","\tRepo        *apipac.Repository","\tWebhookType string","\tNamespace   string","\tLogger      *zap.SugaredLogger","}","","// Get grab the secret from the repository CRD.","func (s *SecretFromRepository) Get(ctx context.Context) error {","\tvar err error","\tif s.Repo.Spec.GitProvider == nil {","\t\treturn fmt.Errorf(\"failed to find git_provider details in repository spec: %v/%v\", s.Repo.Namespace, s.Repo.Name)","\t}","\tif s.Repo.Spec.GitProvider.URL == \"\" {","\t\ts.Repo.Spec.GitProvider.URL = s.Config.APIURL","\t} else {","\t\ts.Event.Provider.URL = s.Repo.Spec.GitProvider.URL","\t}","\ts.Logger.Debugf(\"secretFromRepository: repo=%s/%s provider_url=%s namespace=%s\", s.Repo.Namespace, s.Repo.Name, s.Repo.Spec.GitProvider.URL, s.Namespace)","","\tif s.Repo.Spec.GitProvider.Secret == nil {","\t\treturn fmt.Errorf(\"failed to find secret in git_provider section in repository spec: %v/%v\", s.Repo.Namespace, s.Repo.Name)","\t}","\tgitProviderSecretKey := s.Repo.Spec.GitProvider.Secret.Key","\tif gitProviderSecretKey == \"\" {","\t\tgitProviderSecretKey = DefaultGitProviderSecretKey","\t}","\ts.Logger.Debugf(\"secretFromRepository: reading provider token secret=%s key=%s\", s.Repo.Spec.GitProvider.Secret.Name, gitProviderSecretKey)","","\tif s.Event.Provider.Token, err = s.K8int.GetSecret(ctx, ktypes.GetSecretOpt{","\t\tNamespace: s.Namespace,","\t\tName:      s.Repo.Spec.GitProvider.Secret.Name,","\t\tKey:       gitProviderSecretKey,","\t}); err != nil {","\t\treturn err","\t}","","\t// if we don't have a provider token in repo crd we won't be able to do much with it","\t// let it go and it will fail later on when doing SetClients or success if it was done from a github app","\tif s.Event.Provider.Token == \"\" {","\t\treturn nil","\t}","\ts.Event.Provider.User = s.Repo.Spec.GitProvider.User","","\tif s.Repo.Spec.GitProvider.WebhookSecret == nil {","\t\t// repo.Spec.GitProvider.url/token without a webhook secret is probably going to be bitbucket cloud which","\t\t// doesn't have webhook support ðŸ™ƒ","\t\treturn nil","\t}","","\tgitProviderWebhookSecretKey := s.Repo.Spec.GitProvider.WebhookSecret.Key","\tif gitProviderWebhookSecretKey == \"\" {","\t\tgitProviderWebhookSecretKey = DefaultGitProviderWebhookSecretKey","\t}","\ts.Logger.Debugf(\"secretFromRepository: reading webhook secret=%s key=%s\", s.Repo.Spec.GitProvider.WebhookSecret.Name, gitProviderWebhookSecretKey)","\tlogmsg := fmt.Sprintf(\"Using git provider %s: apiurl=%s user=%s token-secret=%s token-key=%s\",","\t\ts.WebhookType,","\t\ts.Repo.Spec.GitProvider.URL,","\t\ts.Repo.Spec.GitProvider.User,","\t\ts.Repo.Spec.GitProvider.Secret.Name,","\t\tgitProviderSecretKey)","\tif s.Event.Provider.WebhookSecret, err = s.K8int.GetSecret(ctx, ktypes.GetSecretOpt{","\t\tNamespace: s.Namespace,","\t\tName:      s.Repo.Spec.GitProvider.WebhookSecret.Name,","\t\tKey:       gitProviderWebhookSecretKey,","\t}); err != nil {","\t\treturn err","\t}","\tif s.Event.Provider.WebhookSecret != \"\" {","\t\ts.Event.Provider.WebhookSecretFromRepo = true","\t\tlogmsg += fmt.Sprintf(\" webhook-secret=%s webhook-key=%s\",","\t\t\ts.Repo.Spec.GitProvider.WebhookSecret.Name,","\t\t\tgitProviderWebhookSecretKey)","\t} else {","\t\tlogmsg += \" webhook-secret=NOTFOUND\"","\t}","\ts.Logger.Infof(logmsg)","\treturn nil","}","","// GetCurrentNSWebhookSecret get secret from namespace as stored on context.","func GetCurrentNSWebhookSecret(ctx context.Context, k8int kubeinteraction.Interface, run *params.Run) (string, error) {","\tns := info.GetNS(ctx)","\ts, err := k8int.GetSecret(ctx, ktypes.GetSecretOpt{","\t\tNamespace: ns,","\t\tName:      run.Info.Controller.Secret,","\t\tKey:       defaultPipelinesAscodeSecretWebhookSecretKey,","\t})","\t// a lot of people have problem with this secret, when encoding it to base64 which add a \\n when we do :","\t// echo secret|base64 -w0","\t// so cleanup, if someone wants to have a \\n or a space in the secret, well then they can't :p","\treturn strings.TrimSpace(s), err","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,1,1,0,0,0,2,1,1,2,2,2,1,1,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,2,2,2,1,1,2,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2]},{"id":96,"path":"pkg/pipelineascode/template.go","lines":["package pipelineascode","","import (","\t\"context\"","\t\"fmt\"","\t\"net/http\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/customparams\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/templates\"","\t\"go.uber.org/zap\"",")","","// makeTemplate will process all templates replacing the value from the event and from the","// params as set on Repo CR.","func (p *PacRun) makeTemplate(ctx context.Context, repo *v1alpha1.Repository, template string) string {","\tcp := customparams.NewCustomParams(p.event, repo, p.run, p.k8int, p.eventEmitter, p.vcx)","\tmaptemplate, changedFiles, err := cp.GetParams(ctx)","\tif err != nil {","\t\tp.eventEmitter.EmitMessage(repo, zap.ErrorLevel, \"ParamsError\",","\t\t\tfmt.Sprintf(\"error processing repository CR custom params: %s\", err.Error()))","\t} else {","\t\tp.debugf(\"makeTemplate: resolved %d params and %d changed file groups\", len(maptemplate), len(changedFiles))","\t}","","\t// convert pull request number to string","\tif p.event.PullRequestNumber != 0 {","\t\tmaptemplate[\"pull_request_number\"] = fmt.Sprintf(\"%d\", p.event.PullRequestNumber)","\t}","","\t// replace placeholders variable as well as evaluate cel expressions","\theaders := http.Header{}","\tif p.event.Request != nil \u0026\u0026 p.event.Request.Header != nil {","\t\theaders = p.event.Request.Header","\t}","","\treturn templates.ReplacePlaceHoldersVariables(template, maptemplate, p.event.Event, headers, changedFiles)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,0,0,2,2,2,0,0,2,2,2,2,0,2,0]},{"id":97,"path":"pkg/pipelinerunmetrics/injection.go","lines":["/*","Copyright 2024 The Tekton Authors","","Licensed under the Apache License, Version 2.0 (the \"License\");","you may not use this file except in compliance with the License.","You may obtain a copy of the License at","","    http://www.apache.org/licenses/LICENSE-2.0","","Unless required by applicable law or agreed to in writing, software","distributed under the License is distributed on an \"AS IS\" BASIS,","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.","See the License for the specific language governing permissions and","limitations under the License.","*/","","package pipelinerunmetrics","","import (","\t\"context\"","","\tpipelineruninformer \"github.com/tektoncd/pipeline/pkg/client/injection/informers/pipeline/v1/pipelinerun\"","\tlisters \"github.com/tektoncd/pipeline/pkg/client/listers/pipeline/v1\"","\t\"k8s.io/client-go/rest\"","\t\"knative.dev/pkg/controller\"","\t\"knative.dev/pkg/injection\"","\t\"knative.dev/pkg/logging\"",")","","// nolint: gochecknoinits","func init() {","\tinjection.Default.RegisterClient(func(ctx context.Context, _ *rest.Config) context.Context { return WithClient(ctx) })","\tinjection.Default.RegisterInformer(WithInformer)","}","","// RecorderKey is used for associating the Recorder inside the context.Context.","type RecorderKey struct{}","","// WithClient adds a metrics recorder to the given context.","func WithClient(ctx context.Context) context.Context {","\trec, err := NewRecorder()","\tif err != nil {","\t\tlogging.FromContext(ctx).Errorf(\"Failed to create pipelinerun metrics recorder %v\", err)","\t}","\treturn context.WithValue(ctx, RecorderKey{}, rec)","}","","// Get extracts the pipelinerunmetrics.Recorder from the context.","func Get(ctx context.Context) *Recorder {","\tuntyped := ctx.Value(RecorderKey{})","\tif untyped == nil {","\t\tlogging.FromContext(ctx).Panic(\"Unable to fetch *pipelinerunmetrics.Recorder from context.\")","\t}","\t// nolint: forcetypeassert","\treturn untyped.(*Recorder)","}","","// InformerKey is used for associating the Informer inside the context.Context.","type InformerKey struct{}","","// WithInformer returns the given context, and a configured informer.","func WithInformer(ctx context.Context) (context.Context, controller.Informer) {","\treturn ctx, \u0026recorderInformer{","\t\tctx:     ctx,","\t\tmetrics: Get(ctx),","\t\tlister:  pipelineruninformer.Get(ctx).Lister(),","\t}","}","","type recorderInformer struct {","\tctx     context.Context","\tmetrics *Recorder","\tlister  listers.PipelineRunLister","}","","var _ controller.Informer = (*recorderInformer)(nil)","","// Run starts the recorder informer in a goroutine.","func (ri *recorderInformer) Run(stopCh \u003c-chan struct{}) {","\t// Turn the stopCh into a context for reporting metrics.","\tctx, cancel := context.WithCancel(ri.ctx)","\tgo func() {","\t\t\u003c-stopCh","\t\tcancel()","\t}()","","\tgo ri.metrics.ReportRunningPipelineRuns(ctx, ri.lister)","}","","// HasSynced returns whether the informer has synced, which in this case will always be true.","func (ri *recorderInformer) HasSynced() bool {","\treturn true","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,1,1,1,1,1,0,1,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,1,0,0,0,1,1,1]},{"id":98,"path":"pkg/pipelinerunmetrics/metrics.go","lines":["package pipelinerunmetrics","","import (","\t\"context\"","\t\"fmt\"","\t\"strings\"","\t\"sync\"","\t\"time\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\tlisters \"github.com/tektoncd/pipeline/pkg/client/listers/pipeline/v1\"","\t\"go.opencensus.io/stats\"","\t\"go.opencensus.io/stats/view\"","\t\"go.opencensus.io/tag\"","\t\"k8s.io/apimachinery/pkg/labels\"","\t\"knative.dev/pkg/logging\"","\t\"knative.dev/pkg/metrics\"",")","","var prCount = stats.Float64(\"pipelines_as_code_pipelinerun_count\",","\t\"number of pipelineruns by pipelines as code\",","\tstats.UnitDimensionless)","","var prDurationCount = stats.Float64(\"pipelines_as_code_pipelinerun_duration_seconds_sum\",","\t\"number of seconds all pipelineruns completed in by pipelines as code\",","\tstats.UnitDimensionless)","","var runningPRCount = stats.Float64(\"pipelines_as_code_running_pipelineruns_count\",","\t\"number of running pipelineruns by pipelines as code\",","\tstats.UnitDimensionless)","","var gitProviderAPIRequestCount = stats.Int64(","\t\"pipelines_as_code_git_provider_api_request_count\",","\t\"number of API requests from pipelines as code to git providers\",","\tstats.UnitDimensionless,",")","","// Recorder holds keys for metrics.","type Recorder struct {","\tinitialized     bool","\tprovider        tag.Key","\teventType       tag.Key","\tnamespace       tag.Key","\trepository      tag.Key","\tstatus          tag.Key","\treason          tag.Key","\tReportingPeriod time.Duration","}","","var (","\tOnce           sync.Once","\tR              *Recorder","\tErrRegistering error",")","","// NewRecorder creates a new metrics recorder instance","// to log the PAC PipelineRun related metrics.","func NewRecorder() (*Recorder, error) {","\tOnce.Do(func() {","\t\tR = \u0026Recorder{","\t\t\tinitialized: true,","","\t\t\t// Default to 30s intervals.","\t\t\tReportingPeriod: 30 * time.Second,","\t\t}","","\t\tprovider, errRegistering := tag.NewKey(\"provider\")","\t\tif errRegistering != nil {","\t\t\tErrRegistering = errRegistering","\t\t\treturn","\t\t}","\t\tR.provider = provider","","\t\teventType, errRegistering := tag.NewKey(\"event-type\")","\t\tif errRegistering != nil {","\t\t\tErrRegistering = errRegistering","\t\t\treturn","\t\t}","\t\tR.eventType = eventType","","\t\tnamespace, errRegistering := tag.NewKey(\"namespace\")","\t\tif errRegistering != nil {","\t\t\tErrRegistering = errRegistering","\t\t\treturn","\t\t}","\t\tR.namespace = namespace","","\t\trepository, errRegistering := tag.NewKey(\"repository\")","\t\tif errRegistering != nil {","\t\t\tErrRegistering = errRegistering","\t\t\treturn","\t\t}","\t\tR.repository = repository","","\t\tstatus, errRegistering := tag.NewKey(\"status\")","\t\tif errRegistering != nil {","\t\t\tErrRegistering = errRegistering","\t\t\treturn","\t\t}","\t\tR.status = status","","\t\treason, errRegistering := tag.NewKey(\"reason\")","\t\tif errRegistering != nil {","\t\t\tErrRegistering = errRegistering","\t\t\treturn","\t\t}","\t\tR.reason = reason","","\t\tvar (","\t\t\tprCountView = \u0026view.View{","\t\t\t\tDescription: prCount.Description(),","\t\t\t\tMeasure:     prCount,","\t\t\t\tAggregation: view.Count(),","\t\t\t\tTagKeys:     []tag.Key{R.provider, R.eventType, R.namespace, R.repository},","\t\t\t}","","\t\t\tprDurationView = \u0026view.View{","\t\t\t\tDescription: prDurationCount.Description(),","\t\t\t\tMeasure:     prDurationCount,","\t\t\t\tAggregation: view.Sum(),","\t\t\t\tTagKeys:     []tag.Key{R.namespace, R.repository, R.status, R.reason},","\t\t\t}","","\t\t\trunningPRView = \u0026view.View{","\t\t\t\tDescription: runningPRCount.Description(),","\t\t\t\tMeasure:     runningPRCount,","\t\t\t\tAggregation: view.LastValue(),","\t\t\t\tTagKeys:     []tag.Key{R.namespace, R.repository},","\t\t\t}","\t\t\tgitProviderAPIRequestView = \u0026view.View{","\t\t\t\tDescription: gitProviderAPIRequestCount.Description(),","\t\t\t\tMeasure:     gitProviderAPIRequestCount,","\t\t\t\tAggregation: view.Count(),","\t\t\t\tTagKeys:     []tag.Key{R.provider, R.eventType, R.namespace, R.repository},","\t\t\t}","\t\t)","","\t\tview.Unregister(prCountView, prDurationView, runningPRView, gitProviderAPIRequestView)","\t\terrRegistering = view.Register(prCountView, prDurationView, runningPRView, gitProviderAPIRequestView)","\t\tif errRegistering != nil {","\t\t\tErrRegistering = errRegistering","\t\t\tR.initialized = false","\t\t\treturn","\t\t}","\t})","","\treturn R, ErrRegistering","}","","func (r Recorder) assertInitialized() error {","\tif !r.initialized {","\t\treturn fmt.Errorf(","\t\t\t\"ignoring the metrics recording for pipelineruns, failed to initialize the metrics recorder\")","\t}","\treturn nil","}","","// Count logs number of times a pipelinerun is ran for a provider.","func (r *Recorder) Count(provider, event, namespace, repository string) error {","\tif err := r.assertInitialized(); err != nil {","\t\treturn err","\t}","","\tctx, err := tag.New(","\t\tcontext.Background(),","\t\ttag.Insert(r.provider, provider),","\t\ttag.Insert(r.eventType, event),","\t\ttag.Insert(r.namespace, namespace),","\t\ttag.Insert(r.repository, repository),","\t)","\tif err != nil {","\t\treturn err","\t}","","\tmetrics.Record(ctx, prCount.M(1))","\treturn nil","}","","// CountPRDuration collects duration taken by a pipelinerun in seconds accumulate them in prDurationCount.","func (r *Recorder) CountPRDuration(namespace, repository, status, reason string, duration time.Duration) error {","\tif err := r.assertInitialized(); err != nil {","\t\treturn err","\t}","","\tctx, err := tag.New(","\t\tcontext.Background(),","\t\ttag.Insert(r.namespace, namespace),","\t\ttag.Insert(r.repository, repository),","\t\ttag.Insert(r.status, status),","\t\ttag.Insert(r.reason, reason),","\t)","\tif err != nil {","\t\treturn err","\t}","","\tmetrics.Record(ctx, prDurationCount.M(duration.Seconds()))","\treturn nil","}","","// RunningPipelineRuns emits the number of running PipelineRuns for a repository and namespace.","func (r *Recorder) RunningPipelineRuns(namespace, repository string, runningPRs float64) error {","\tif err := r.assertInitialized(); err != nil {","\t\treturn err","\t}","","\tctx, err := tag.New(","\t\tcontext.Background(),","\t\ttag.Insert(r.namespace, namespace),","\t\ttag.Insert(r.repository, repository),","\t)","\tif err != nil {","\t\treturn err","\t}","","\tmetrics.Record(ctx, runningPRCount.M(runningPRs))","\treturn nil","}","","func (r *Recorder) EmitRunningPRsMetrics(prl []*tektonv1.PipelineRun) error {","\tif len(prl) == 0 {","\t\treturn nil","\t}","","\t// bifurcate PipelineRuns based on their namespace and repository","\trunningPRs := map[string]int{}","\tcompletedPRsKeys := map[string]struct{}{}","\tfor _, pr := range prl {","\t\t// Check if PipelineRun has Repository annotation it means PR is created by PAC.","\t\tif repository, ok := pr.GetAnnotations()[keys.Repository]; ok {","\t\t\tkey := fmt.Sprintf(\"%s/%s\", pr.GetNamespace(), repository)","\t\t\t// check if PipelineRun is running.","\t\t\tif !pr.IsDone() {","\t\t\t\trunningPRs[key]++","\t\t\t} else {","\t\t\t\t// add it in completed, and we don't want completed PipelineRuns count.","\t\t\t\tcompletedPRsKeys[key] = struct{}{}","\t\t\t}","\t\t}","\t}","","\tfor k, v := range runningPRs {","\t\tnsKeys := strings.Split(k, \"/\")","\t\tif err := r.RunningPipelineRuns(nsKeys[0], nsKeys[1], float64(v)); err != nil {","\t\t\treturn err","\t\t}","\t}","","\t// report zero for the keys which aren't in runningPRs.","\tfor key := range completedPRsKeys {","\t\t// if key isn't there in runningPRs then it should be reported 0","\t\t// otherwise it was reported in previous loop.","\t\tif _, ok := runningPRs[key]; !ok {","\t\t\tnsKeys := strings.Split(key, \"/\")","\t\t\tif err := r.RunningPipelineRuns(nsKeys[0], nsKeys[1], 0); err != nil {","\t\t\t\treturn err","\t\t\t}","\t\t}","\t}","","\treturn nil","}","","// ReportRunningPipelineRuns reports running PipelineRuns on our configured ReportingPeriod","// until the context is cancelled.","func (r *Recorder) ReportRunningPipelineRuns(ctx context.Context, lister listers.PipelineRunLister) {","\tlogger := logging.FromContext(ctx)","","\tfor {","\t\tdelay := time.NewTimer(r.ReportingPeriod)","\t\tselect {","\t\tcase \u003c-ctx.Done():","\t\t\t// When the context is cancelled, stop reporting.","\t\t\tif !delay.Stop() {","\t\t\t\t\u003c-delay.C","\t\t\t}","\t\t\treturn","","\t\tcase \u003c-delay.C:","\t\t\tprl, err := lister.List(labels.Everything())","\t\t\tif err != nil {","\t\t\t\tlogger.Warnf(\"Failed to list PipelineRuns : %v\", err)","\t\t\t\tcontinue","\t\t\t}","\t\t\t// Every 30s surface a metric for the number of running pipelines.","\t\t\tif err := r.EmitRunningPRsMetrics(prl); err != nil {","\t\t\t\tlogger.Warnf(\"Failed to log the metrics : %v\", err)","\t\t\t}","\t\t}","\t}","}","","func (r *Recorder) ReportGitProviderAPIUsage(provider, event, namespace, repository string) error {","\tif err := r.assertInitialized(); err != nil {","\t\treturn err","\t}","","\tctx, err := tag.New(","\t\tcontext.Background(),","\t\ttag.Insert(r.provider, provider),","\t\ttag.Insert(r.eventType, event),","\t\ttag.Insert(r.namespace, namespace),","\t\ttag.Insert(r.repository, repository),","\t)","\tif err != nil {","\t\treturn err","\t}","","\tmetrics.Record(ctx, gitProviderAPIRequestCount.M(1))","\treturn nil","}","","func ResetRecorder() {","\tOnce = sync.Once{}","\tR = nil","\tErrRegistering = nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,0,0,1,1,1,1,1,1,0,0,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,1,1,0,0,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,1,1,0,0,0,1,1,1,1,0,1,1,1,1,1,1,1,1,0,1,1,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,0,0,0,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,0,0,1,1,1,0,0,0,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,1,1,0,0,1,1,1,1,1]},{"id":99,"path":"pkg/policy/policy.go","lines":["package policy","","import (","\t\"context\"","\t\"fmt\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/events\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"go.uber.org/zap\"",")","","type Result int","","const (","\tResultNotSet     Result = 0","\tResultAllowed    Result = 1","\tResultDisallowed Result = 2",")","","type Policy struct {","\tRepository   *v1alpha1.Repository","\tEvent        *info.Event","\tVCX          provider.Interface","\tLogger       *zap.SugaredLogger","\tEventEmitter *events.EventEmitter","}","","// checkAllowed checks if the policy is set and allows the event to be processed.","func (p *Policy) checkAllowed(ctx context.Context, tType triggertype.Trigger) (Result, string) {","\tif p.Repository == nil {","\t\treturn ResultNotSet, \"\"","\t}","\tsettings := p.Repository.Spec.Settings","\tif settings == nil || settings.Policy == nil {","\t\treturn ResultNotSet, \"\"","\t}","","\tvar sType []string","\tswitch tType {","\t// NOTE: This make /retest /ok-to-test /test bound to the same policy, which is fine from a security standpoint but maybe we want to refine this in the future.","\tcase triggertype.OkToTest, triggertype.Retest:","\t\tsType = settings.Policy.OkToTest","\t// apply the same policy for PullRequest and comment","\t// we don't support comments on PRs yet but if we do on the future we will need our own policy","\tcase triggertype.PullRequest, triggertype.Comment, triggertype.PullRequestLabeled, triggertype.PullRequestClosed:","\t\tsType = settings.Policy.PullRequest","\t// NOTE: not supported yet, will imp if it gets requested and reasonable to implement","\tcase triggertype.Push, triggertype.Cancel, triggertype.CheckSuiteRerequested, triggertype.CheckRunRerequested, triggertype.Incoming:","\t\treturn ResultNotSet, \"\"","\tdefault:","\t\treturn ResultNotSet, \"\"","\t}","","\t// if policy is set but empty then it mean disallow everything","\tif len(sType) == 0 {","\t\treturn ResultDisallowed, \"no policy set\"","\t}","","\t// remove empty values from sType","\ttemp := []string{}","\tfor _, val := range sType {","\t\tif val != \"\" {","\t\t\ttemp = append(temp, val)","\t\t}","\t}","\tsType = temp","","\t// if policy is set but with empty values then bail out.","\tif len(sType) == 0 {","\t\treturn ResultDisallowed, \"policy set and empty with no groups\"","\t}","","\tallowed, reason := p.VCX.CheckPolicyAllowing(ctx, p.Event, sType)","\tif allowed {","\t\treturn ResultAllowed, \"\"","\t}","\treturn ResultDisallowed, fmt.Sprintf(\"policy check: %s, %s\", string(tType), reason)","}","","// IsAllowed determines if a given event trigger is permitted based on repository policy settings and OWNERS file.","//","// The function first checks if a policy is set for the repository and if the event trigger type is allowed by the policy.","// - If allowed by policy, it emits an informational event and returns ResultAllowed.","// - If disallowed by policy, it checks if the sender is allowed via the OWNERS file:","//   - If allowed via OWNERS, it emits an informational event and returns ResultAllowed.","//   - If not allowed, it emits a disallowed event and returns ResultDisallowed with a reason.","//","// - If no policy is set, it returns ResultNotSet.","//","// This function ensures that policy settings take precedence, but fallback to OWNERS file permissions if policy disallows the event.","func (p *Policy) IsAllowed(ctx context.Context, tType triggertype.Trigger) (Result, string) {","\tvar reason string","\tpolicyRes, reason := p.checkAllowed(ctx, tType)","\tswitch policyRes {","\tcase ResultAllowed:","\t\treason = fmt.Sprintf(\"policy check: policy is set for sender %s has been allowed to run CI via policy\", p.Event.Sender)","\t\tp.EventEmitter.EmitMessage(p.Repository, zap.InfoLevel, \"PolicySetAllowed\", reason)","\t\treturn ResultAllowed, \"\"","\tcase ResultDisallowed:","\t\tallowed, err := p.VCX.IsAllowedOwnersFile(ctx, p.Event)","\t\tif err != nil {","\t\t\treturn ResultDisallowed, err.Error()","\t\t}","\t\tif allowed {","\t\t\treason = fmt.Sprintf(\"policy check: policy is set, sender %s not in the allowed policy but allowed via OWNERS file\", p.Event.Sender)","\t\t\tp.EventEmitter.EmitMessage(p.Repository, zap.InfoLevel, \"PolicySetAllowed\", reason)","\t\t\treturn ResultAllowed, \"\"","\t\t}","\t\tif reason == \"\" {","\t\t\treason = fmt.Sprintf(\"policy check: policy is set but sender %s is not in the allowed groups\", p.Event.Sender)","\t\t}","\t\tp.EventEmitter.EmitMessage(p.Repository, zap.InfoLevel, \"PolicySetDisallowed\", reason)","\t\treturn ResultDisallowed, \"\"","\tcase ResultNotSet: // this is to make golangci-lint happy","\t}","\treturn ResultNotSet, reason","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,1,1,0,2,2,0,2,2,0,0,2,2,0,2,2,2,2,0,0,0,2,1,1,0,0,2,2,2,2,2,0,2,2,2,2,2,2,0,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,2,2,2,1,1,2,2,0,0,2,0]},{"id":100,"path":"pkg/provider/bitbucketcloud/acl.go","lines":["package bitbucketcloud","","import (","\t\"context\"","\t\"strconv\"","\t\"strings\"","","\t\"github.com/ktrysmt/go-bitbucket\"","\t\"github.com/mitchellh/mapstructure\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/acl\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/bitbucketcloud/types\"",")","","func (v *Provider) IsAllowed(ctx context.Context, event *info.Event) (bool, error) {","\t// Check first if the user is in the owner file or part of the workspace","\tallowed, err := v.checkMember(ctx, event)","\tif err != nil {","\t\treturn false, err","\t}","\tif allowed {","\t\treturn true, nil","\t}","","\t// Check then from comment if there is a approved user that has done a /ok-to-test","\treturn v.checkOkToTestCommentFromApprovedMember(ctx, event)","}","","func (v *Provider) isWorkspaceMember(event *info.Event) (bool, error) {","\tmembers, err := v.Client().Workspaces.Members(event.Organization)","\tif err != nil {","\t\treturn false, err","\t}","","\tfor _, member := range members.Members {","\t\tif member.AccountId == event.AccountID {","\t\t\treturn true, nil","\t\t}","\t}","\treturn false, nil","}","","// IsAllowedOwnersFile get the owner files (OWNERS, OWNERS_ALIASES) from main branch","// and check if we have explicitly allowed the user in there.","func (v *Provider) IsAllowedOwnersFile(ctx context.Context, event *info.Event) (bool, error) {","\townerContent, err := v.GetFileInsideRepo(ctx, event, \"OWNERS\", event.DefaultBranch)","\tif err != nil {","\t\tif strings.Contains(err.Error(), \"cannot find\") {","\t\t\t// no owner file, skipping","\t\t\treturn false, nil","\t\t}","\t\treturn false, err","\t}","\t// If there is OWNERS file, check for OWNERS_ALIASES","\townerAliasesContent, err := v.GetFileInsideRepo(ctx, event, \"OWNERS_ALIASES\", event.DefaultBranch)","\tif err != nil {","\t\tif !strings.Contains(err.Error(), \"cannot find\") {","\t\t\treturn false, err","\t\t}","\t}","","\treturn acl.UserInOwnerFile(ownerContent, ownerAliasesContent, event.AccountID)","}","","func (v *Provider) checkMember(ctx context.Context, event *info.Event) (bool, error) {","\t// If sender is a member that can write to the workspace then allow it.","\tallowed, err := v.isWorkspaceMember(event)","\tif err != nil {","\t\treturn false, err","\t} else if allowed {","\t\treturn true, err","\t}","","\t// Check if sender (which in bitbucket-cloud mean the accountID) is inside the Owner file","\t// in the 'main' branch Silently ignore error, which should be fine it","\t// probably means the OWNERS file is not created. If we had another error","\t// (ie: like API) we probably would have hit it already.","\tif allowed, _ := v.IsAllowedOwnersFile(ctx, event); allowed {","\t\treturn true, err","\t}","","\treturn false, nil","}","","func (v *Provider) checkOkToTestCommentFromApprovedMember(ctx context.Context, event *info.Event) (bool, error) {","\tcommentsIntf, err := v.Client().Repositories.PullRequests.GetComments(\u0026bitbucket.PullRequestsOptions{","\t\tOwner:    event.Organization,","\t\tRepoSlug: event.Repository,","\t\tID:       strconv.Itoa(event.PullRequestNumber),","\t})","\tif err != nil {","\t\treturn false, err","\t}","\tcomments := \u0026types.Comments{}","\terr = mapstructure.Decode(commentsIntf, comments)","\tif err != nil {","\t\treturn false, err","\t}","\tfor _, comment := range comments.Values {","\t\tif acl.MatchRegexp(acl.OKToTestCommentRegexp, comment.Content.Raw) {","\t\t\tcommenterEvent := info.NewEvent()","\t\t\tcommenterEvent.Event = event.Event","\t\t\tcommenterEvent.Sender = comment.User.Nickname","\t\t\tcommenterEvent.AccountID = comment.User.AccountID","\t\t\tcommenterEvent.BaseBranch = event.BaseBranch","\t\t\tcommenterEvent.HeadBranch = event.HeadBranch","\t\t\tcommenterEvent.Repository = event.Repository","\t\t\tcommenterEvent.Organization = event.Organization","\t\t\tcommenterEvent.DefaultBranch = event.DefaultBranch","\t\t\tallowed, err := v.checkMember(ctx, commenterEvent)","\t\t\tif err != nil {","\t\t\t\treturn false, err","\t\t\t}","\t\t\tif allowed {","\t\t\t\treturn true, nil","\t\t\t}","\t\t}","\t}","","\treturn false, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,1,1,2,2,2,0,0,2,0,0,2,2,2,1,1,0,2,2,2,2,0,2,0,0,0,0,2,2,2,2,2,2,2,1,0,0,2,2,2,1,1,0,0,2,0,0,2,2,2,2,1,2,2,2,0,0,0,0,0,2,2,2,0,2,0,0,2,2,2,2,2,2,2,1,1,2,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,0,0,0,2,0]},{"id":101,"path":"pkg/provider/bitbucketcloud/bitbucket.go","lines":["package bitbucketcloud","","import (","\t\"context\"","\t\"fmt\"","\t\"strconv\"","\t\"strings\"","","\t\"github.com/ktrysmt/go-bitbucket\"","\t\"github.com/mitchellh/mapstructure\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/changedfiles\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/events\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/bitbucketcloud/types\"","\tproviderMetrics \"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/providermetrics\"","\t\"go.uber.org/zap\"",")","","var _ provider.Interface = (*Provider)(nil)","","type Provider struct {","\tbbClient      *bitbucket.Client","\tLogger        *zap.SugaredLogger","\trun           *params.Run","\tpacInfo       *info.PacOpts","\tToken, APIURL *string","\tUsername      *string","\tprovenance    string","\teventEmitter  *events.EventEmitter","\trepo          *v1alpha1.Repository","\ttriggerEvent  string","}","","func (v *Provider) Client() *bitbucket.Client {","\tproviderMetrics.RecordAPIUsage(","\t\tv.Logger,","\t\tv.GetConfig().Name,","\t\tv.triggerEvent,","\t\tv.repo,","\t)","\treturn v.bbClient","}","","func (v *Provider) CreateComment(_ context.Context, _ *info.Event, _, _ string) error {","\treturn nil","}","","// CheckPolicyAllowing TODO: Implement ME.","func (v *Provider) CheckPolicyAllowing(_ context.Context, _ *info.Event, _ []string) (bool, string) {","\treturn false, \"\"","}","","// GetTaskURI TODO: Implement ME.","func (v *Provider) GetTaskURI(_ context.Context, _ *info.Event, _ string) (bool, string, error) {","\treturn false, \"\", nil","}","","func (v *Provider) SetPacInfo(pacInfo *info.PacOpts) {","\tv.pacInfo = pacInfo","}","","const taskStatusTemplate = `{{range $taskrun := .TaskRunList }} | **{{ formatCondition $taskrun.PipelineRunTaskRunStatus.Status.Conditions }}** | {{ $taskrun.ConsoleLogURL }} | *{{ formatDuration $taskrun.PipelineRunTaskRunStatus.Status.StartTime $taskrun.PipelineRunTaskRunStatus.Status.CompletionTime }}* |","{{ end }}`","","func (v *Provider) Validate(_ context.Context, _ *params.Run, _ *info.Event) error {","\treturn nil","}","","func (v *Provider) SetLogger(logger *zap.SugaredLogger) {","\tv.Logger = logger","}","","func (v *Provider) GetConfig() *info.ProviderConfig {","\treturn \u0026info.ProviderConfig{","\t\tTaskStatusTMPL: taskStatusTemplate,","\t\tAPIURL:         bitbucket.DEFAULT_BITBUCKET_API_BASE_URL,","\t\tName:           \"bitbucket-cloud\",","\t}","}","","func (v *Provider) CreateStatus(_ context.Context, event *info.Event, statusopts provider.StatusOpts) error {","\tswitch statusopts.Conclusion {","\tcase \"skipped\":","\t\tstatusopts.Conclusion = \"STOPPED\"","\t\tstatusopts.Title = \"âž– Skipping this commit\"","\tcase \"neutral\":","\t\tstatusopts.Conclusion = \"STOPPED\"","\t\tstatusopts.Title = \"âž– CI has stopped\"","\tcase \"failure\":","\t\tstatusopts.Conclusion = \"FAILED\"","\t\tstatusopts.Title = \"âŒ Failed\"","\tcase \"pending\":","\t\tstatusopts.Conclusion = \"INPROGRESS\"","\t\tstatusopts.Title = \"âš¡ CI has started\"","\tcase \"success\":","\t\tstatusopts.Conclusion = \"SUCCESSFUL\"","\t\tstatusopts.Title = \"âœ… Commit has been validated\"","\tcase \"completed\":","\t\tstatusopts.Conclusion = \"SUCCESSFUL\"","\t\tstatusopts.Title = \"âœ… Completed\"","\t}","\tdetailsURL := event.Provider.URL","\tif statusopts.DetailsURL != \"\" {","\t\tdetailsURL = statusopts.DetailsURL","\t}","","\tcso := \u0026bitbucket.CommitStatusOptions{","\t\tKey:         provider.GetCheckName(statusopts, v.pacInfo),","\t\tUrl:         detailsURL,","\t\tState:       statusopts.Conclusion,","\t\tDescription: statusopts.Title,","\t}","\tcmo := \u0026bitbucket.CommitsOptions{","\t\tOwner:    event.Organization,","\t\tRepoSlug: event.Repository,","\t\tRevision: event.SHA,","\t}","","\tif v.bbClient == nil {","\t\treturn fmt.Errorf(\"no token has been set, cannot set status\")","\t}","","\t_, err := v.Client().Repositories.Commits.CreateCommitStatus(cmo, cso)","\tif err != nil {","\t\t// Only emit an event to notify the user that something went wrong with the commit status API,","\t\t// and proceed with creating the comment (if applicable).","\t\tv.eventEmitter.EmitMessage(v.repo, zap.ErrorLevel, \"FailedToSetCommitStatus\",","\t\t\t\"cannot set status with the Bitbucket Cloud token because of: \"+err.Error())","\t}","","\teventType := triggertype.IsPullRequestType(event.EventType)","\tif statusopts.Conclusion != \"STOPPED\" \u0026\u0026 statusopts.Status == \"completed\" \u0026\u0026","\t\tstatusopts.Text != \"\" \u0026\u0026","\t\t(eventType == triggertype.PullRequest || event.TriggerTarget == triggertype.PullRequest) {","\t\tonPr := \"\"","\t\tif statusopts.OriginalPipelineRunName != \"\" {","\t\t\tonPr = \"/\" + statusopts.OriginalPipelineRunName","\t\t}","\t\t_, err = v.Client().Repositories.PullRequests.AddComment(","\t\t\t\u0026bitbucket.PullRequestCommentOptions{","\t\t\t\tOwner:         event.Organization,","\t\t\t\tRepoSlug:      event.Repository,","\t\t\t\tPullRequestID: strconv.Itoa(event.PullRequestNumber),","\t\t\t\tContent:       fmt.Sprintf(\"**%s%s** - %s\\n\\n%s\", v.pacInfo.ApplicationName, onPr, statusopts.Title, statusopts.Text),","\t\t\t})","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t}","\treturn nil","}","","func (v *Provider) GetTektonDir(_ context.Context, event *info.Event, path, provenance string) (string, error) {","\tv.provenance = provenance","\trepositoryFiles, err := v.getDir(event, path)","\tif err != nil {","\t\treturn \"\", err","\t}","","\treturn v.concatAllYamlFiles(repositoryFiles, event)","}","","func (v *Provider) getDir(event *info.Event, path string) ([]bitbucket.RepositoryFile, error) {","\t// default set provenance from the SHA","\trevision := event.SHA","\tif v.provenance == \"default_branch\" {","\t\trevision = event.DefaultBranch","\t\tv.Logger.Infof(\"Using PipelineRun definition from default_branch: %s\", event.DefaultBranch)","\t} else {","\t\tv.Logger.Infof(\"Using PipelineRun definition from source %s commit SHA: %s\", event.TriggerTarget.String(), event.SHA)","\t}","\trepoFileOpts := \u0026bitbucket.RepositoryFilesOptions{","\t\tOwner:    event.Organization,","\t\tRepoSlug: event.Repository,","\t\tRef:      revision,","\t\tPath:     path,","\t}","","\trepositoryFiles, err := v.Client().Repositories.Repository.ListFiles(repoFileOpts)","\tif err != nil {","\t\treturn nil, err","\t}","\treturn repositoryFiles, nil","}","","func (v *Provider) GetFileInsideRepo(_ context.Context, event *info.Event, path, _ string) (string, error) {","\trevision := event.SHA","\tif v.provenance == \"default_branch\" {","\t\trevision = event.DefaultBranch","\t}","\treturn v.getBlob(event, revision, path)","}","","func (v *Provider) SetClient(_ context.Context, run *params.Run, event *info.Event, repo *v1alpha1.Repository, eventEmitter *events.EventEmitter) error {","\tif event.Provider.Token == \"\" {","\t\treturn fmt.Errorf(\"no git_provider.secret has been set in the repo crd\")","\t}","\tif event.Provider.User == \"\" {","\t\treturn fmt.Errorf(\"no git_provider.user has been in repo crd\")","\t}","\tbbClient, err := bitbucket.NewBasicAuth(event.Provider.User, event.Provider.Token)","\tif err != nil {","\t\treturn fmt.Errorf(\"failed to create bitbucket client: %w\", err)","\t}","\tv.bbClient = bbClient","","\t// Added log for security audit purposes to log client access when a token is used","\trun.Clients.Log.Infof(\"bitbucket-cloud: initialized client with provided token for user=%s\", event.Provider.User)","","\tv.Token = \u0026event.Provider.Token","\tv.Username = \u0026event.Provider.User","\tv.run = run","\tv.eventEmitter = eventEmitter","\tv.repo = repo","\tv.triggerEvent = event.EventType","\treturn nil","}","","func (v *Provider) GetCommitInfo(_ context.Context, event *info.Event) error {","\t// If we don't have a SHA, get it from the branch first","\tsha := event.SHA","\tif sha == \"\" \u0026\u0026 event.HeadBranch != \"\" {","\t\tv.Logger.Infof(\"fetching branch info to get commit SHA for branch: %s\", event.HeadBranch)","\t\tbranchInfo, err := v.Client().Repositories.Repository.GetBranch(\u0026bitbucket.RepositoryBranchOptions{","\t\t\tOwner:      event.Organization,","\t\t\tRepoSlug:   event.Repository,","\t\t\tBranchName: event.HeadBranch,","\t\t})","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\t// Extract hash from Target map","\t\tif hash, ok := branchInfo.Target[\"hash\"].(string); ok {","\t\t\tsha = hash","\t\t} else {","\t\t\treturn fmt.Errorf(\"cannot extract commit hash from branch %s\", event.HeadBranch)","\t\t}","\t}","","\t// Use GetCommit API for direct single-commit fetch (no pagination)","\tv.Logger.Infof(\"fetching commit info using GetCommit API for SHA: %s\", sha)","\tresponse, err := v.Client().Repositories.Commits.GetCommit(\u0026bitbucket.CommitsOptions{","\t\tOwner:    event.Organization,","\t\tRepoSlug: event.Repository,","\t\tRevision: sha,","\t})","\tif err != nil {","\t\treturn err","\t}","","\tcommitMap, ok := response.(map[string]any)","\tif !ok {","\t\treturn fmt.Errorf(\"cannot convert commit response\")","\t}","","\tcommitinfo := \u0026types.Commit{}","\terr = mapstructure.Decode(commitMap, commitinfo)","\tif err != nil {","\t\treturn err","\t}","","\t// Some silliness since we get first the account id and we fill it properly after","\tevent.SHATitle = commitinfo.Message","\tevent.SHAURL = commitinfo.Links.HTML.HRef","\tevent.SHA = commitinfo.Hash","","\t// Populate full commit information for LLM context","\tevent.SHAMessage = commitinfo.Message","\t// Bitbucket Cloud API has limited commit author information","\t// Use display name or nickname if available","\tif commitinfo.Author.User.DisplayName != \"\" {","\t\tevent.SHAAuthorName = commitinfo.Author.User.DisplayName","\t} else if commitinfo.Author.Nickname != \"\" {","\t\tevent.SHAAuthorName = commitinfo.Author.Nickname","\t}","\t// Note: Bitbucket Cloud API doesn't provide author email or timestamps in the basic commit response","","\tevent.HasSkipCommand = provider.SkipCI(commitinfo.Message)","\t// now to get the default branch from repository.Get","\trepo, err := v.Client().Repositories.Repository.Get(\u0026bitbucket.RepositoryOptions{","\t\tOwner:    event.Organization,","\t\tRepoSlug: event.Repository,","\t})","\tif err != nil {","\t\treturn err","\t}","\tevent.DefaultBranch = repo.Mainbranch.Name","\treturn nil","}","","func (v *Provider) concatAllYamlFiles(objects []bitbucket.RepositoryFile, event *info.Event) (string, error) {","\tvar allTemplates string","","\trevision := event.SHA","\tif v.provenance == \"default_branch\" {","\t\trevision = event.DefaultBranch","\t}","\tfor _, value := range objects {","\t\tif value.Type == \"commit_directory\" {","\t\t\tobjects, err := v.getDir(event, value.Path)","\t\t\tif err != nil {","\t\t\t\treturn \"\", err","\t\t\t}","\t\t\tsubdirdata, err := v.concatAllYamlFiles(objects, event)","\t\t\tif err != nil {","\t\t\t\treturn \"\", err","\t\t\t}","\t\t\tif allTemplates != \"\" \u0026\u0026 !strings.HasPrefix(subdirdata, \"---\") {","\t\t\t\tallTemplates += \"---\"","\t\t\t}","\t\t\tallTemplates += fmt.Sprintf(\"\\n%s\\n\", subdirdata)","\t\t} else if strings.HasSuffix(value.Path, \".yaml\") ||","\t\t\tstrings.HasSuffix(value.Path, \".yml\") {","\t\t\tdata, err := v.getBlob(event, revision, value.Path)","\t\t\tif err != nil {","\t\t\t\treturn \"\", err","\t\t\t}","\t\t\tif err := provider.ValidateYaml([]byte(data), value.Path); err != nil {","\t\t\t\treturn \"\", err","\t\t\t}","","\t\t\tif allTemplates != \"\" \u0026\u0026 !strings.HasPrefix(data, \"---\") {","\t\t\t\tallTemplates += \"---\"","\t\t\t}","\t\t\tallTemplates += \"\\n\" + data + \"\\n\"","\t\t}","\t}","\treturn allTemplates, nil","}","","func (v *Provider) getBlob(runevent *info.Event, ref, path string) (string, error) {","\tblob, err := v.Client().Repositories.Repository.GetFileBlob(\u0026bitbucket.RepositoryBlobOptions{","\t\tOwner:    runevent.Organization,","\t\tRepoSlug: runevent.Repository,","\t\tRef:      ref,","\t\tPath:     path,","\t})","\tif err != nil {","\t\treturn \"\", fmt.Errorf(\"cannot find %s on branch %s in repo %s/%s\", path, ref, runevent.Organization, runevent.Repository)","\t}","\treturn blob.String(), nil","}","","func (v *Provider) GetFiles(_ context.Context, _ *info.Event) (changedfiles.ChangedFiles, error) {","\treturn changedfiles.ChangedFiles{}, nil","}","","func (v *Provider) CreateToken(_ context.Context, _ []string, _ *info.Event) (string, error) {","\treturn \"\", nil","}","","func (v *Provider) GetTemplate(commentType provider.CommentType) string {","\treturn provider.GetMarkdownTemplate(commentType)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,0,1,1,1,0,0,1,1,1,0,0,1,1,1,0,1,1,1,0,0,0,0,1,1,1,0,1,1,1,0,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,2,1,1,1,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,0,0,2,2,2,2,1,1,0,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,2,0,0,2,2,2,1,1,2,0,0,2,2,2,2,2,2,2,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,2,2,1,1,0,0,0,2,2,2,2,2,2,2,1,1,0,2,2,1,1,0,2,2,2,1,1,0,0,2,2,2,2,2,2,2,2,2,2,2,1,1,0,0,2,2,2,2,2,2,2,1,1,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,1,1,2,2,2,2,2,2,2,2,1,1,2,2,2,0,2,2,2,2,0,0,2,0,0,2,2,2,2,2,2,2,2,2,2,2,0,0,1,1,1,0,1,1,1,0,1,1,1]},{"id":102,"path":"pkg/provider/bitbucketcloud/detect.go","lines":["package bitbucketcloud","","import (","\t\"encoding/json\"","\t\"fmt\"","\t\"net/http\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/bitbucketcloud/types\"","\t\"go.uber.org/zap\"",")","","var (","\tpullRequestsClosed         = []string{\"pullrequest:closed\", \"pullrequest:fulfilled\", \"pullrequest:rejected\"}","\tpullRequestsCreated        = []string{\"pullrequest:created\", \"pullrequest:updated\"}","\tpullRequestsCommentCreated = []string{\"pullrequest:comment_created\"}","\tpushRepo                   = []string{\"repo:push\"}","\tPullRequestAllEvents       = append(append(append(append([]string{}, pullRequestsCreated...), pullRequestsCommentCreated...), pullRequestsClosed...), pushRepo...)",")","","func (v *Provider) Detect(req *http.Request, payload string, logger *zap.SugaredLogger) (bool, bool, *zap.SugaredLogger, string, error) {","\tisBitCloud := false","\treqHeader := req.Header","\tevent := reqHeader.Get(\"X-Event-Key\")","\tif event == \"\" {","\t\treturn false, false, logger, \"\", nil","\t}","","\teventInt, err := parsePayloadType(event, payload)","\tif err != nil || eventInt == nil {","\t\tlogger.Error(\"skip processing event\", zap.String(\"event\", event), zap.Error(err))","\t\treturn false, false, logger, \"\", err","\t}","","\t// it is a Bitbucket cloud event","\tisBitCloud = true","","\tsetLoggerAndProceed := func(processEvent bool, reason string, err error) (bool, bool, *zap.SugaredLogger,","\t\tstring, error,","\t) {","\t\tlogger = logger.With(\"provider\", \"bitbucket-cloud\", \"event-id\", reqHeader.Get(\"X-Request-Id\"))","\t\treturn isBitCloud, processEvent, logger, reason, err","\t}","","\t_ = json.Unmarshal([]byte(payload), \u0026eventInt)","","\tswitch e := eventInt.(type) {","\tcase *types.PullRequestEvent:","\t\tif provider.Valid(event, pullRequestsClosed) {","\t\t\treturn setLoggerAndProceed(true, \"\", nil)","\t\t}","","\t\tif provider.Valid(event, pullRequestsCreated) {","\t\t\treturn setLoggerAndProceed(true, \"\", nil)","\t\t}","","\t\tif provider.Valid(event, pullRequestsCommentCreated) {","\t\t\tif provider.IsTestRetestComment(e.Comment.Content.Raw) {","\t\t\t\treturn setLoggerAndProceed(true, \"\", nil)","\t\t\t}","\t\t\tif provider.IsOkToTestComment(e.Comment.Content.Raw) {","\t\t\t\treturn setLoggerAndProceed(true, \"\", nil)","\t\t\t}","\t\t\tif provider.IsCancelComment(e.Comment.Content.Raw) {","\t\t\t\treturn setLoggerAndProceed(true, \"\", nil)","\t\t\t}","\t\t}","\t\treturn setLoggerAndProceed(false, fmt.Sprintf(\"not a valid gitops comment: \\\"%s\\\"\", event), nil)","","\tcase *types.PushRequestEvent:","\t\tif provider.Valid(event, pushRepo) {","\t\t\tif e.Push.Changes != nil {","\t\t\t\treturn setLoggerAndProceed(true, \"\", nil)","\t\t\t}","\t\t}","\t\treturn setLoggerAndProceed(false, fmt.Sprintf(\"invalid push event: \\\"%s\\\"\", event), nil)","","\tdefault:","\t\treturn setLoggerAndProceed(false, \"\", fmt.Errorf(\"bitbucket-cloud: event \\\"%s\\\" is not supported\", event))","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,0,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,0,2,2,2,2,2,1,1,0,2,2,2,0,2,2,2,2,2,2,2,2,2,2,0,2,0,2,2,2,2,2,0,1,0,1,1,0,0]},{"id":103,"path":"pkg/provider/bitbucketcloud/parse_payload.go","lines":["package bitbucketcloud","","import (","\t\"context\"","\t\"encoding/json\"","\t\"fmt\"","\t\"net\"","\t\"net/http\"","\t\"strings\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/opscomments\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/bitbucketcloud/types\"",")","","const bitbucketCloudIPrangesList = \"https://ip-ranges.atlassian.com/\"","","// lastForwarderForIP get last ip from the X-Forwarded-For chain","// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For","func lastForwarderForIP(xff string) string {","\tsplit := strings.Split(xff, \",\")","\treturn split[len(split)-1]","}","","// checkFromPublicCloudIPS Grab public IP from public cloud and make sure we match it.","func (v *Provider) checkFromPublicCloudIPS(ctx context.Context, run *params.Run, sourceIP string) (bool, error) {","\tif !v.pacInfo.BitbucketCloudCheckSourceIP {","\t\treturn true, nil","\t}","","\tif sourceIP == \"\" {","\t\treturn false, fmt.Errorf(\"we need to check the source_ip but no source_ip has been passed\")","\t}","\tsourceIP = lastForwarderForIP(sourceIP)","","\tnetsourceIP := net.ParseIP(sourceIP)","\tdata, err := run.Clients.GetURL(ctx, bitbucketCloudIPrangesList)","\tif err != nil {","\t\treturn false, err","\t}","","\tipranges := \u0026types.IPRanges{}","\terr = json.Unmarshal(data, \u0026ipranges)","\tif err != nil {","\t\treturn false, err","\t}","","\textraIPEnv := v.pacInfo.BitbucketCloudAdditionalSourceIP","\tif extraIPEnv != \"\" {","\t\tfor _, value := range strings.Split(extraIPEnv, \",\") {","\t\t\tif !strings.Contains(value, \"/\") {","\t\t\t\tvalue = fmt.Sprintf(\"%s/32\", value)","\t\t\t}","\t\t\tipranges.Items = append(ipranges.Items, types.IPRangesItem{","\t\t\t\tCIDR: strings.TrimSpace(value),","\t\t\t})","\t\t}","\t}","\tfor _, value := range ipranges.Items {","\t\t_, cidr, err := net.ParseCIDR(value.CIDR)","\t\tif err != nil {","\t\t\treturn false, err","\t\t}","\t\tif cidr.Contains(netsourceIP) {","\t\t\treturn true, nil","\t\t}","\t}","\treturn false,","\t\tfmt.Errorf(\"payload from %s is not coming from the public bitbucket cloud ips as defined here: %s\",","\t\t\tsourceIP, bitbucketCloudIPrangesList)","}","","func parsePayloadType(event, rawPayload string) (any, error) {","\tvar payload any","","\tvar localEvent string","\tif strings.HasPrefix(event, \"pullrequest:\") {","\t\tif !provider.Valid(event, PullRequestAllEvents) {","\t\t\treturn nil, fmt.Errorf(\"event %s is not supported\", event)","\t\t}","\t\tlocalEvent = triggertype.PullRequest.String()","\t} else if provider.Valid(event, pushRepo) {","\t\tlocalEvent = \"push\"","\t}","","\tswitch localEvent {","\tcase triggertype.PullRequest.String():","\t\tpayload = \u0026types.PullRequestEvent{}","\tcase \"push\":","\t\tpayload = \u0026types.PushRequestEvent{}","\tdefault:","\t\treturn nil, nil","\t}","\terr := json.Unmarshal([]byte(rawPayload), payload)","\treturn payload, err","}","","func (v *Provider) ParsePayload(ctx context.Context, run *params.Run, request *http.Request, payload string) (*info.Event, error) {","\tprocessedEvent := info.NewEvent()","","\tevent := request.Header.Get(\"X-Event-Key\")","\teventInt, err := parsePayloadType(event, payload)","\tif err != nil || eventInt == nil {","\t\treturn info.NewEvent(), err","\t}","","\terr = json.Unmarshal([]byte(payload), \u0026eventInt)","\tif err != nil {","\t\treturn info.NewEvent(), err","\t}","","\tsourceIP := request.Header.Get(\"X-Forwarded-For\")","\tallowed, err := v.checkFromPublicCloudIPS(ctx, run, sourceIP)","\tif err != nil {","\t\treturn nil, err","\t}","\tif !allowed {","\t\treturn nil, fmt.Errorf(\"payload is not coming from the public bitbucket cloud ips as defined here: %s\",","\t\t\tbitbucketCloudIPrangesList)","\t}","","\tprocessedEvent.Event = eventInt","\tswitch e := eventInt.(type) {","\tcase *types.PullRequestEvent:","\t\tprocessedEvent.TriggerTarget = triggertype.PullRequest","\t\tswitch {","\t\tcase provider.Valid(event, pullRequestsCreated):","\t\t\tprocessedEvent.EventType = triggertype.PullRequest.String()","\t\tcase provider.Valid(event, pullRequestsCommentCreated):","\t\t\topscomments.SetEventTypeAndTargetPR(processedEvent, e.Comment.Content.Raw)","\t\tcase provider.Valid(event, pullRequestsClosed):","\t\t\tprocessedEvent.EventType = string(triggertype.PullRequestClosed)","\t\t\tprocessedEvent.TriggerTarget = triggertype.PullRequestClosed","\t\t}","\t\tprocessedEvent.Organization = e.Repository.Workspace.Slug","\t\tprocessedEvent.Repository = strings.Split(e.Repository.FullName, \"/\")[1]","\t\tprocessedEvent.SHA = e.PullRequest.Source.Commit.Hash","\t\tprocessedEvent.URL = e.Repository.Links.HTML.HRef","\t\tprocessedEvent.BaseBranch = e.PullRequest.Destination.Branch.Name","\t\tprocessedEvent.HeadBranch = e.PullRequest.Source.Branch.Name","\t\tprocessedEvent.BaseURL = e.PullRequest.Destination.Repository.Links.HTML.HRef","\t\tprocessedEvent.HeadURL = e.PullRequest.Source.Repository.Links.HTML.HRef","\t\tprocessedEvent.AccountID = e.PullRequest.Author.AccountID","\t\tprocessedEvent.Sender = e.PullRequest.Author.Nickname","\t\tprocessedEvent.PullRequestNumber = e.PullRequest.ID","\t\tprocessedEvent.PullRequestTitle = e.PullRequest.Title","\tcase *types.PushRequestEvent:","\t\tprocessedEvent.Event = \"push\"","\t\tprocessedEvent.TriggerTarget = \"push\"","\t\tprocessedEvent.EventType = \"push\"","\t\tprocessedEvent.Organization = e.Repository.Workspace.Slug","\t\tprocessedEvent.Repository = strings.Split(e.Repository.FullName, \"/\")[1]","\t\tprocessedEvent.SHA = e.Push.Changes[0].New.Target.Hash","\t\tprocessedEvent.URL = e.Repository.Links.HTML.HRef","\t\tprocessedEvent.HeadBranch = e.Push.Changes[0].Old.Name","\t\tprocessedEvent.BaseURL = e.Push.Changes[0].New.Target.Links.HTML.HRef","\t\tprocessedEvent.HeadURL = e.Push.Changes[0].Old.Target.Links.HTML.HRef","\t\tif e.Push.Changes[0].New.Type == \"tag\" {","\t\t\tprocessedEvent.BaseBranch = fmt.Sprintf(\"refs/tags/%s\", e.Push.Changes[0].New.Name)","\t\t} else {","\t\t\tprocessedEvent.BaseBranch = e.Push.Changes[0].New.Name","\t\t}","\t\tprocessedEvent.AccountID = e.Actor.AccountID","\t\tprocessedEvent.Sender = e.Actor.Nickname","\tdefault:","\t\treturn nil, fmt.Errorf(\"event %s is not recognized\", event)","\t}","\treturn processedEvent, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,0,2,2,2,2,0,2,1,1,2,2,2,2,2,1,1,0,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,0,0,2,2,2,1,1,2,2,2,0,2,2,2,0,0,2,2,2,2,2,2,1,1,2,2,2,2,0,2,2,2,2,2,2,2,0,2,2,0,0,2,2,2,2,2,2,1,1,0,2,2,1,1,0,2,2,2,2,2,2,1,1,1,0,2,2,2,2,2,2,2,2,2,1,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,0]},{"id":104,"path":"pkg/provider/bitbucketdatacenter/acl.go","lines":["package bitbucketdatacenter","","import (","\t\"context\"","\t\"fmt\"","\t\"strings\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/acl\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","","\t\"github.com/jenkins-x/go-scm/scm\"",")","","func (v *Provider) IsAllowed(ctx context.Context, event *info.Event) (bool, error) {","\tallowed, err := v.checkMemberShip(ctx, event)","\tif err != nil {","\t\treturn false, err","\t}","\tif allowed {","\t\treturn true, nil","\t}","","\t// Check then from comment if there is a approved user that has done a /ok-to-test","\treturn v.checkOkToTestCommentFromApprovedMember(ctx, event)","}","","// IsAllowedOwnersFile get the owner files (OWNERS, OWNERS_ALIASES) from main branch","// and check if we have explicitly allowed the user in there.","func (v *Provider) IsAllowedOwnersFile(ctx context.Context, event *info.Event) (bool, error) {","\townerContent, err := v.GetFileInsideRepo(ctx, event, \"OWNERS\", event.DefaultBranch)","\tif err != nil {","\t\treturn false, err","\t}","\townerAliasesContent, err := v.GetFileInsideRepo(ctx, event, \"OWNERS_ALIASES\", event.DefaultBranch)","\tif err != nil {","\t\tif !strings.Contains(err.Error(), \"cannot find\") {","\t\t\treturn false, err","\t\t}","\t}","","\treturn acl.UserInOwnerFile(ownerContent, ownerAliasesContent, event.AccountID)","}","","func (v *Provider) checkOkToTestCommentFromApprovedMember(ctx context.Context, event *info.Event) (bool, error) {","\tallComments := []*scm.Comment{}","\tOrgAndRepo := fmt.Sprintf(\"%s/%s\", event.Organization, event.Repository)","\topts := \u0026scm.ListOptions{Page: 1, Size: apiResponseLimit}","\tfor {","\t\tcomments, _, err := v.Client().PullRequests.ListComments(ctx, OrgAndRepo, v.pullRequestNumber, opts)","\t\tif err != nil {","\t\t\treturn false, err","\t\t}","","\t\tallComments = append(allComments, comments...)","","\t\tif len(comments) \u003c apiResponseLimit {","\t\t\tbreak","\t\t}","","\t\topts.Page++","\t}","","\tfor _, comment := range allComments {","\t\tif acl.MatchRegexp(acl.OKToTestCommentRegexp, comment.Body) {","\t\t\tcommenterEvent := info.NewEvent()","\t\t\tcommenterEvent.Sender = comment.Author.Login","\t\t\tcommenterEvent.AccountID = fmt.Sprintf(\"%d\", comment.Author.ID)","\t\t\tcommenterEvent.Event = event.Event","\t\t\tcommenterEvent.BaseBranch = event.BaseBranch","\t\t\tcommenterEvent.HeadBranch = event.HeadBranch","\t\t\tcommenterEvent.Repository = event.Repository","\t\t\tcommenterEvent.Organization = v.projectKey","\t\t\tcommenterEvent.DefaultBranch = event.DefaultBranch","\t\t\tallowed, err := v.checkMemberShip(ctx, commenterEvent)","\t\t\tif err != nil {","\t\t\t\treturn false, err","\t\t\t}","\t\t\tif allowed {","\t\t\t\t// TODO: show a log how come this has been allowed","\t\t\t\treturn true, nil","\t\t\t}","\t\t}","\t}","\treturn false, nil","}","","func (v *Provider) checkMemberShip(ctx context.Context, event *info.Event) (bool, error) {","\t// Get permissions from project","\tallowed, _, err := v.Client().Organizations.IsMember(ctx, event.Organization, event.Sender)","\tif err != nil {","\t\treturn false, err","\t}","\tif allowed {","\t\treturn true, nil","\t}","","\torgAndRepo := fmt.Sprintf(\"%s/%s\", event.Organization, event.Repository)","\t// Get permissions from repo","\tallowed, _, err = v.Client().Repositories.IsCollaborator(ctx, orgAndRepo, event.Sender)","\tif err != nil {","\t\treturn false, err","\t}","\tif allowed {","\t\treturn true, nil","\t}","","\t// Check if sender is inside the Owner file","\t// in the 'main' branch Silently ignore error, which should be fine it","\t// probably means the OWNERS file is not created. If we had another error","\t// (ie: like API) we probably would have hit it already.","\tallowed, err = v.IsAllowedOwnersFile(ctx, event)","\tif allowed {","\t\treturn true, err","\t}","","\treturn false, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,1,1,2,2,2,0,0,2,0,0,0,0,2,2,2,2,2,2,2,2,1,1,0,0,2,0,0,2,2,2,2,2,2,2,1,1,0,2,2,2,2,0,0,1,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,2,0,0,2,0,0,2,2,2,2,1,1,2,2,2,0,2,2,2,2,1,1,2,2,2,0,0,0,0,0,2,2,2,2,0,2,0]},{"id":105,"path":"pkg/provider/bitbucketdatacenter/bitbucketdatacenter.go","lines":["package bitbucketdatacenter","","import (","\t\"context\"","\t\"fmt\"","\t\"net/http\"","\t\"net/url\"","\t\"path/filepath\"","\t\"strings\"","","\t\"github.com/google/go-github/v81/github\"","\t\"github.com/jenkins-x/go-scm/scm\"","\t\"github.com/jenkins-x/go-scm/scm/driver/stash\"","\t\"github.com/jenkins-x/go-scm/scm/transport/oauth2\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/changedfiles\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/events\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\tproviderMetrics \"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/providermetrics\"","\t\"go.uber.org/zap\"",")","","const taskStatusTemplate = `{{range $taskrun := .TaskRunList }}| **{{ formatCondition $taskrun.PipelineRunTaskRunStatus.Status.Conditions }}** | {{ $taskrun.ConsoleLogURL }} | *{{ formatDuration $taskrun.Status.StartTime $taskrun.Status.CompletionTime }}* |","{{ end }}`","const apiResponseLimit = 100","","var _ provider.Interface = (*Provider)(nil)","","type Provider struct {","\tclient                    *scm.Client","\tLogger                    *zap.SugaredLogger","\trun                       *params.Run","\tpacInfo                   *info.PacOpts","\tbaseURL                   string","\tdefaultBranchLatestCommit string","\tpullRequestNumber         int","\tapiURL                    string","\tprovenance                string","\tprojectKey                string","\trepo                      *v1alpha1.Repository","\ttriggerEvent              string","\tcachedChangedFiles        *changedfiles.ChangedFiles","}","","func (v Provider) Client() *scm.Client {","\tproviderMetrics.RecordAPIUsage(","\t\tv.Logger,","\t\tv.GetConfig().Name,","\t\tv.triggerEvent,","\t\tv.repo,","\t)","\treturn v.client","}","","func (v *Provider) CreateComment(_ context.Context, _ *info.Event, _, _ string) error {","\treturn nil","}","","func (v *Provider) SetPacInfo(pacInfo *info.PacOpts) {","\tv.pacInfo = pacInfo","}","","func (v *Provider) CheckPolicyAllowing(_ context.Context, _ *info.Event, _ []string) (bool, string) {","\treturn false, \"\"","}","","// GetTaskURI TODO: Implement ME.","func (v *Provider) GetTaskURI(_ context.Context, _ *info.Event, _ string) (bool, string, error) {","\treturn false, \"\", nil","}","","func (v *Provider) SetLogger(logger *zap.SugaredLogger) {","\tv.Logger = logger","}","","func (v *Provider) Validate(_ context.Context, _ *params.Run, event *info.Event) error {","\tsignature := event.Request.Header.Get(\"X-Hub-Signature\")","\tif event.Provider.WebhookSecret == \"\" \u0026\u0026 signature != \"\" {","\t\treturn fmt.Errorf(\"bitbucket-datacenter failed validation: failed to find webhook secret\")","\t}","\treturn github.ValidateSignature(signature, event.Request.Payload, []byte(event.Provider.WebhookSecret))","}","","// sanitizeTitle make sure we only get the tile by remove everything after \\n.","func sanitizeTitle(s string) string {","\treturn strings.Split(s, \"\\n\")[0]","}","","func (v *Provider) CreateStatus(ctx context.Context, event *info.Event, statusOpts provider.StatusOpts) error {","\tdetailsURL := event.Provider.URL","\tswitch statusOpts.Conclusion {","\tcase \"skipped\":","\t\tstatusOpts.Conclusion = \"FAILED\"","\t\tstatusOpts.Title = \"âž– Skipping this commit\"","\tcase \"neutral\":","\t\tstatusOpts.Conclusion = \"FAILED\"","\t\tstatusOpts.Title = \"âž– CI has stopped\"","\tcase \"failure\":","\t\tstatusOpts.Conclusion = \"FAILED\"","\t\tstatusOpts.Title = \"âŒ Failed\"","\tcase \"pending\":","\t\tif statusOpts.Status == \"queued\" {","\t\t\tstatusOpts.Conclusion = \"UNKNOWN\"","\t\t} else {","\t\t\tstatusOpts.Conclusion = \"INPROGRESS\"","\t\t\tstatusOpts.Title = \"âš¡ CI has started\"","\t\t}","\tcase \"success\":","\t\tstatusOpts.Conclusion = \"SUCCESSFUL\"","\t\tstatusOpts.Title = \"Commit has been validated\"","\tcase \"completed\":","\t\tstatusOpts.Conclusion = \"SUCCESSFUL\"","\t\tstatusOpts.Title = \"Completed\"","\t}","\tif statusOpts.DetailsURL != \"\" {","\t\tdetailsURL = statusOpts.DetailsURL","\t}","\tif v.client == nil {","\t\treturn fmt.Errorf(\"no token has been set, cannot set status\")","\t}","","\tkey := statusOpts.PipelineRunName","\tif key == \"\" {","\t\tkey = statusOpts.Title","\t}","","\tif v.pacInfo.ApplicationName != \"\" {","\t\tkey = fmt.Sprintf(\"%s / %s\", v.pacInfo.ApplicationName, key)","\t}","","\tOrgAndRepo := fmt.Sprintf(\"%s/%s\", event.Organization, event.Repository)","\topts := \u0026scm.StatusInput{","\t\tState: convertState(statusOpts.Conclusion),","\t\tLabel: key,","\t\tDesc:  statusOpts.Text,","\t\tLink:  detailsURL,","\t}","\t_, _, err := v.Client().Repositories.CreateStatus(ctx, OrgAndRepo, event.SHA, opts)","\tif err != nil {","\t\treturn err","\t}","","\tonPr := \"\"","\tif statusOpts.OriginalPipelineRunName != \"\" {","\t\tonPr = \"/\" + statusOpts.OriginalPipelineRunName","\t}","\tbbComment := fmt.Sprintf(\"**%s%s** - %s\\n\\n%s\", v.pacInfo.ApplicationName, onPr, statusOpts.Title, statusOpts.Text)","","\tif statusOpts.Conclusion == \"SUCCESSFUL\" \u0026\u0026 statusOpts.Status == \"completed\" \u0026\u0026","\t\tstatusOpts.Text != \"\" \u0026\u0026 event.TriggerTarget == triggertype.PullRequest \u0026\u0026 event.PullRequestNumber \u003e 0 {","\t\tinput := \u0026scm.CommentInput{","\t\t\tBody: bbComment,","\t\t}","\t\t_, _, err := v.Client().PullRequests.CreateComment(ctx, OrgAndRepo, event.PullRequestNumber, input)","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t}","","\t// TODO: Completed status","\treturn nil","}","","func convertState(from string) scm.State {","\tswitch from {","\tcase \"FAILED\":","\t\treturn scm.StateFailure","\tcase \"INPROGRESS\":","\t\treturn scm.StatePending","\tcase \"SUCCESSFUL\":","\t\treturn scm.StateSuccess","\tcase \"UNKNOWN\":","\t\treturn scm.StateUnknown","\tdefault:","\t\treturn scm.StateUnknown","\t}","}","","func (v *Provider) concatAllYamlFiles(ctx context.Context, objects []string, sha string, runevent *info.Event) (string, error) {","\tvar allTemplates string","\tfor _, value := range objects {","\t\tif strings.HasSuffix(value, \".yaml\") ||","\t\t\tstrings.HasSuffix(value, \".yml\") {","\t\t\t// if sha is empty string then it fetches raw file from","\t\t\t// default branch which we can use for PAC provenance.","\t\t\tdata, err := v.getRaw(ctx, runevent, sha, value)","\t\t\tif err != nil {","\t\t\t\treturn \"\", err","\t\t\t}","","\t\t\tif err := provider.ValidateYaml([]byte(data), value); err != nil {","\t\t\t\treturn \"\", err","\t\t\t}","","\t\t\tif allTemplates != \"\" \u0026\u0026 !strings.HasPrefix(data, \"---\") {","\t\t\t\tallTemplates += \"---\"","\t\t\t}","\t\t\tallTemplates += \"\\n\" + data + \"\\n\"","\t\t}","\t}","\treturn allTemplates, nil","}","","func (v *Provider) getRaw(ctx context.Context, runevent *info.Event, revision, path string) (string, error) {","\trepo := fmt.Sprintf(\"%s/%s\", runevent.Organization, runevent.Repository)","\tcontent, _, err := v.Client().Contents.Find(ctx, repo, path, revision)","\tif err != nil {","\t\treturn \"\", fmt.Errorf(\"cannot find %s inside the %s repository: %w\", path, runevent.Repository, err)","\t}","\treturn string(content.Data), nil","}","","func (v *Provider) GetTektonDir(ctx context.Context, event *info.Event, path, provenance string) (string, error) {","\tv.provenance = provenance","\t// If \"at\" is empty string \"\" then default branch will be used as source","\tat := \"\"","\tif v.provenance == \"source\" {","\t\tat = event.SHA","\t\tv.Logger.Infof(\"Using PipelineRun definition from source %s commit SHA: %s\", event.TriggerTarget.String(), event.SHA)","\t} else {","\t\tv.Logger.Infof(\"Using PipelineRun definition from default_branch: %s\", event.DefaultBranch)","\t}","","\torgAndRepo := fmt.Sprintf(\"%s/%s\", event.Organization, event.Repository)","\tvar fileEntries []*scm.FileEntry","\topts := \u0026scm.ListOptions{Page: 1, Size: apiResponseLimit}","\tfor {","\t\tentries, _, err := v.Client().Contents.List(ctx, orgAndRepo, path, at, opts)","\t\tif err != nil {","\t\t\treturn \"\", fmt.Errorf(\"cannot list content of %s directory: %w\", path, err)","\t\t}","\t\tfileEntries = append(fileEntries, entries...)","","\t\tif len(entries) \u003c apiResponseLimit {","\t\t\tbreak","\t\t}","","\t\topts.Page++","\t}","","\tfpathTmpl := []string{}","\tfor _, e := range fileEntries {","\t\tfpathTmpl = append(fpathTmpl, filepath.Join(path, e.Path))","\t}","","\treturn v.concatAllYamlFiles(ctx, fpathTmpl, at, event)","}","","func (v *Provider) GetFileInsideRepo(ctx context.Context, event *info.Event, path, targetBranch string) (string, error) {","\tbranch := event.SHA","\t// TODO: this may be buggy? we need to figure out how to get the fromSource ref","\tif targetBranch == event.DefaultBranch {","\t\tbranch = v.defaultBranchLatestCommit","\t}","","\tret, err := v.getRaw(ctx, event, branch, path)","\treturn ret, err","}","","func removeLastSegment(urlStr string) string {","\tu, _ := url.Parse(urlStr)","\tsegments := strings.Split(u.Path, \"/\")","\tswitch {","\tcase len(segments) \u003e 1:","\t\tsegments = segments[:len(segments)-1]","\tcase (len(segments) == 1 \u0026\u0026 segments[0] != \"\") || u.Path == \"/\":","\t\tsegments = []string{\"\"}","\t}","","\tnewPath := strings.Join(segments, \"/\")","\tif newPath == \"\" \u0026\u0026 strings.HasPrefix(u.Path, \"/\") {","\t\tnewPath = \"/\" // Ensure root path is correctly represented as \"/\"","\t}","","\tu.Path = newPath","\treturn u.String()","}","","func (v *Provider) SetClient(ctx context.Context, run *params.Run, event *info.Event, repo *v1alpha1.Repository, _ *events.EventEmitter) error {","\tif event.Provider.User == \"\" {","\t\treturn fmt.Errorf(\"no spec.git_provider.user has been set in the repo crd\")","\t}","\tif event.Provider.Token == \"\" {","\t\treturn fmt.Errorf(\"no spec.git_provider.secret has been set in the repo crd\")","\t}","\tif event.Provider.URL == \"\" {","\t\treturn fmt.Errorf(\"no spec.git_provider.url has been set in the repo crd\")","\t}","","\t// make sure we have /rest at the end of the url","\tif !strings.HasSuffix(event.Provider.URL, \"/rest\") {","\t\tevent.Provider.URL += \"/rest\"","\t}","","\t// make sure we strip slashes from the end of the URL","\tevent.Provider.URL = strings.TrimSuffix(event.Provider.URL, \"/\")","\tv.apiURL = event.Provider.URL","","\tif v.client == nil {","\t\tclient, err := stash.New(removeLastSegment(event.Provider.URL)) // remove `/rest` from url","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\tclient.Client = \u0026http.Client{","\t\t\tTransport: \u0026oauth2.Transport{","\t\t\t\tSource: oauth2.StaticTokenSource(","\t\t\t\t\t\u0026scm.Token{","\t\t\t\t\t\tToken: event.Provider.Token,","\t\t\t\t\t},","\t\t\t\t),","\t\t\t},","\t\t}","\t\tv.client = client","","\t\t// Added for security audit purposes to log client access when a token is used","\t\trun.Clients.Log.Infof(\"bitbucket-datacenter: initialized client with provided token for user=%s providerURL=%s\", event.Provider.User, event.Provider.URL)","\t}","\tv.run = run","\tv.repo = repo","\tv.triggerEvent = event.EventType","\t_, resp, err := v.Client().Users.FindLogin(ctx, event.Provider.User)","\tif resp != nil \u0026\u0026 resp.Status == http.StatusUnauthorized {","\t\treturn fmt.Errorf(\"cannot get user %s with token: %w\", event.Provider.User, err)","\t}","\tif err != nil {","\t\treturn fmt.Errorf(\"cannot get user %s: %w\", event.Provider.User, err)","\t}","","\treturn nil","}","","func (v *Provider) GetCommitInfo(_ context.Context, event *info.Event) error {","\tOrgAndRepo := fmt.Sprintf(\"%s/%s\", event.Organization, event.Repository)","\tcommit, _, err := v.Client().Git.FindCommit(context.Background(), OrgAndRepo, event.SHA)","\tif err != nil {","\t\treturn err","\t}","\tevent.SHATitle = sanitizeTitle(commit.Message)","\tevent.SHAURL = fmt.Sprintf(\"%s/projects/%s/repos/%s/commits/%s\", v.baseURL, v.projectKey, event.Repository, event.SHA)","\tevent.HasSkipCommand = provider.SkipCI(commit.Message)","","\t// Populate full commit information for LLM context","\tevent.SHAMessage = commit.Message","\tevent.SHAAuthorName = commit.Author.Name","\tevent.SHAAuthorEmail = commit.Author.Email","\tif !commit.Author.Date.IsZero() {","\t\tevent.SHAAuthorDate = commit.Author.Date","\t}","\tevent.SHACommitterName = commit.Committer.Name","\tevent.SHACommitterEmail = commit.Committer.Email","\tif !commit.Committer.Date.IsZero() {","\t\tevent.SHACommitterDate = commit.Committer.Date","\t}","","\tref, _, err := v.Client().Git.GetDefaultBranch(context.Background(), OrgAndRepo)","\tif err != nil {","\t\treturn err","\t}","","\tv.defaultBranchLatestCommit = ref.Sha","\tevent.DefaultBranch = ref.Name","\treturn nil","}","","func (v *Provider) GetConfig() *info.ProviderConfig {","\treturn \u0026info.ProviderConfig{","\t\tTaskStatusTMPL: taskStatusTemplate,","\t\tName:           \"bitbucket-datacenter\",","\t}","}","","// GetFiles gets and caches the list of files changed by a given event.","func (v *Provider) GetFiles(ctx context.Context, runevent *info.Event) (changedfiles.ChangedFiles, error) {","\tif v.cachedChangedFiles == nil {","\t\tchanges, err := v.fetchChangedFiles(ctx, runevent)","\t\tif err != nil {","\t\t\treturn changedfiles.ChangedFiles{}, err","\t\t}","\t\tv.cachedChangedFiles = \u0026changes","\t}","\treturn *v.cachedChangedFiles, nil","}","","func (v *Provider) fetchChangedFiles(ctx context.Context, runevent *info.Event) (changedfiles.ChangedFiles, error) {","\tchangedFiles := changedfiles.ChangedFiles{}","","\torgAndRepo := fmt.Sprintf(\"%s/%s\", runevent.Organization, runevent.Repository)","","\tswitch runevent.TriggerTarget {","\tcase triggertype.PullRequest:","\t\topts := \u0026scm.ListOptions{Page: 1, Size: apiResponseLimit}","\t\tfor {","\t\t\tchanges, _, err := v.Client().PullRequests.ListChanges(ctx, orgAndRepo, runevent.PullRequestNumber, opts)","\t\t\tif err != nil {","\t\t\t\treturn changedfiles.ChangedFiles{}, fmt.Errorf(\"failed to list changes for pull request: %w\", err)","\t\t\t}","","\t\t\tfor _, c := range changes {","\t\t\t\tchangedFiles.All = append(changedFiles.All, c.Path)","\t\t\t\tif c.Added {","\t\t\t\t\tchangedFiles.Added = append(changedFiles.Added, c.Path)","\t\t\t\t}","\t\t\t\tif c.Modified {","\t\t\t\t\tchangedFiles.Modified = append(changedFiles.Modified, c.Path)","\t\t\t\t}","\t\t\t\tif c.Renamed {","\t\t\t\t\tchangedFiles.Renamed = append(changedFiles.Renamed, c.Path)","\t\t\t\t}","\t\t\t\tif c.Deleted {","\t\t\t\t\tchangedFiles.Deleted = append(changedFiles.Deleted, c.Path)","\t\t\t\t}","\t\t\t}","","\t\t\t// In the Jenkins-x/go-scm package, the `isLastPage` field is not available, and the value of","\t\t\t// `response.Page.Last` is set to `0`. Therefore, to determine if there are more items to fetch,","\t\t\t// we can check if the length of the currently fetched items is less than the specified limit.","\t\t\t// If the length is less than the limit, it indicates that there are no more items to retrieve.","\t\t\tif len(changes) \u003c apiResponseLimit {","\t\t\t\tbreak","\t\t\t}","","\t\t\topts.Page++","\t\t}","\tcase triggertype.Push:","\t\topts := \u0026scm.ListOptions{Page: 1, Size: apiResponseLimit}","\t\tfor {","\t\t\tchanges, _, err := v.Client().Git.ListChanges(ctx, orgAndRepo, runevent.SHA, opts)","\t\t\tif err != nil {","\t\t\t\treturn changedfiles.ChangedFiles{}, fmt.Errorf(\"failed to list changes for commit %s: %w\", runevent.SHA, err)","\t\t\t}","","\t\t\tfor _, c := range changes {","\t\t\t\tchangedFiles.All = append(changedFiles.All, c.Path)","\t\t\t\tif c.Added {","\t\t\t\t\tchangedFiles.Added = append(changedFiles.Added, c.Path)","\t\t\t\t}","\t\t\t\tif c.Modified {","\t\t\t\t\tchangedFiles.Modified = append(changedFiles.Modified, c.Path)","\t\t\t\t}","\t\t\t\tif c.Renamed {","\t\t\t\t\tchangedFiles.Renamed = append(changedFiles.Renamed, c.Path)","\t\t\t\t}","\t\t\t\tif c.Deleted {","\t\t\t\t\tchangedFiles.Deleted = append(changedFiles.Deleted, c.Path)","\t\t\t\t}","\t\t\t}","","\t\t\tif len(changes) \u003c apiResponseLimit {","\t\t\t\tbreak","\t\t\t}","","\t\t\topts.Page++","\t\t}","\tdefault:","\t\t// No action necessary","\t}","\treturn changedFiles, nil","}","","func (v *Provider) CreateToken(_ context.Context, _ []string, _ *info.Event) (string, error) {","\treturn \"\", nil","}","","func (v *Provider) GetTemplate(commentType provider.CommentType) string {","\treturn provider.GetMarkdownTemplate(commentType)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,0,1,1,1,0,1,1,1,0,1,1,1,0,0,1,1,1,0,1,1,1,0,2,2,2,2,2,2,0,0,0,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,0,2,2,2,2,0,2,2,2,0,2,2,2,2,2,2,2,2,2,1,1,0,2,2,1,1,2,2,2,2,1,1,1,1,1,1,1,0,0,0,2,0,0,2,2,2,2,2,2,2,2,2,2,1,1,0,0,0,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,0,2,2,2,2,0,0,2,0,0,2,2,2,2,2,2,2,0,0,2,2,2,2,2,1,1,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,0,0,1,0,0,2,2,2,2,0,2,0,0,2,2,2,2,2,2,0,2,2,0,0,2,2,2,2,2,2,1,1,0,0,2,2,2,2,0,2,2,0,0,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,0,0,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,2,2,2,2,2,2,2,2,2,2,0,2,0,0,2,2,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,1,1,0,2,2,2,0,0,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,0,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,0,2,2,0,0,1,0,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,0,0,1,0,0,0,0,2,0,0,1,1,1,0,1,1,1]},{"id":106,"path":"pkg/provider/bitbucketdatacenter/detect.go","lines":["package bitbucketdatacenter","","import (","\t\"encoding/json\"","\t\"fmt\"","\t\"net/http\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/bitbucketdatacenter/types\"","\t\"go.uber.org/zap\"",")","","// Detect processes event and detect if it is a bitbucket data center event, whether to process or reject it","// returns (if is a bitbucket data center event, whether to process or reject, error if any occurred).","func (v *Provider) Detect(req *http.Request, payload string, logger *zap.SugaredLogger) (bool, bool, *zap.SugaredLogger, string, error) {","\tisBitDataCenter := false","\tevent := req.Header.Get(\"X-Event-Key\")","\tif event == \"\" {","\t\treturn false, false, logger, \"\", nil","\t}","","\teventPayload, err := parsePayloadType(event)","\tif err != nil || eventPayload == nil {","\t\treturn false, false, logger, \"\", err","\t}","","\t// it is a Bitbucket data center event","\tisBitDataCenter = true","","\tsetLoggerAndProceed := func(processEvent bool, reason string, err error) (bool, bool, *zap.SugaredLogger, string,","\t\terror,","\t) {","\t\tlogger = logger.With(\"provider\", \"bitbucket-datacenter\", \"event-id\", req.Header.Get(\"X-Request-Id\"))","\t\treturn isBitDataCenter, processEvent, logger, reason, err","\t}","","\t_ = json.Unmarshal([]byte(payload), \u0026eventPayload)","","\tswitch e := eventPayload.(type) {","\tcase *types.PullRequestEvent:","\t\tif provider.Valid(event, []string{\"pr:from_ref_updated\", \"pr:opened\"}) {","\t\t\treturn setLoggerAndProceed(true, \"\", nil)","\t\t}","\t\tif provider.Valid(event, []string{\"pr:comment:added\"}) {","\t\t\tif provider.IsTestRetestComment(e.Comment.Text) {","\t\t\t\treturn setLoggerAndProceed(true, \"\", nil)","\t\t\t}","\t\t\tif provider.IsOkToTestComment(e.Comment.Text) {","\t\t\t\treturn setLoggerAndProceed(true, \"\", nil)","\t\t\t}","\t\t\tif provider.IsCancelComment(e.Comment.Text) {","\t\t\t\treturn setLoggerAndProceed(true, \"\", nil)","\t\t\t}","\t\t}","\t\treturn setLoggerAndProceed(false, fmt.Sprintf(\"not a recognized bitbucket event: \\\"%s\\\"\", event), nil)","","\tcase *types.PushRequestEvent:","\t\tif provider.Valid(event, []string{\"repo:refs_changed\"}) {","\t\t\tif e.Changes != nil {","\t\t\t\treturn setLoggerAndProceed(true, \"\", nil)","\t\t\t}","\t\t}","\t\treturn setLoggerAndProceed(false, fmt.Sprintf(\"not an event we support: \\\"%s\\\"\", event), nil)","","\tdefault:","\t\treturn setLoggerAndProceed(false, \"\", fmt.Errorf(\"bitbucket-datacenter: event \\\"%s\\\" is not supported\", event))","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,2,2,2,2,0,0,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,0,2,2,2,2,2,0,1,0,1,1,0,0]},{"id":107,"path":"pkg/provider/bitbucketdatacenter/parse_payload.go","lines":["package bitbucketdatacenter","","import (","\t\"context\"","\t\"encoding/json\"","\t\"fmt\"","\t\"net/http\"","\t\"net/url\"","\t\"strings\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/bitbucketdatacenter/types\"",")","","// checkValidPayload checks if the payload is valid.","func checkValidPayload(e *types.PullRequestEvent) error {","\tif e.PullRequest.ToRef.Repository.Project == nil {","\t\treturn fmt.Errorf(\"bitbucket toRef project is nil\")","\t}","\tif e.PullRequest.ToRef.Repository.Project.Key == \"\" {","\t\treturn fmt.Errorf(\"bitbucket toRef project key is empty\")","\t}","\tif e.PullRequest.ToRef.Repository.Name == \"\" {","\t\treturn fmt.Errorf(\"bitbucket toRef repository name is empty\")","\t}","\tif e.PullRequest.ToRef.LatestCommit == \"\" {","\t\treturn fmt.Errorf(\"bitbucket toRef latest commit is empty\")","\t}","","\tif e.PullRequest.FromRef.Repository.Project == nil {","\t\treturn fmt.Errorf(\"bitbucket fromRef project is nil\")","\t}","\tif e.PullRequest.FromRef.Repository.Project.Key == \"\" {","\t\treturn fmt.Errorf(\"bitbucket fromRef project key is empty\")","\t}","\tif e.PullRequest.FromRef.Repository.Name == \"\" {","\t\treturn fmt.Errorf(\"bitbucket fromRef repository name is empty\")","\t}","\tif e.PullRequest.FromRef.LatestCommit == \"\" {","\t\treturn fmt.Errorf(\"bitbucket fromRef latest commit is empty\")","\t}","\tif e.PullRequest.ID == 0 {","\t\treturn fmt.Errorf(\"bitbucket pull request ID is zero\")","\t}","","\tif e.PullRequest.ToRef.Repository.Links == nil || len(e.PullRequest.ToRef.Repository.Links.Self) == 0 {","\t\treturn fmt.Errorf(\"bitbucket toRef repository links are nil or empty\")","\t}","\tif e.PullRequest.ToRef.DisplayID == \"\" {","\t\treturn fmt.Errorf(\"bitbucket toRef display ID is empty\")","\t}","\tif e.PullRequest.FromRef.DisplayID == \"\" {","\t\treturn fmt.Errorf(\"bitbucket fromRef display ID is empty\")","\t}","\tif e.PullRequest.FromRef.Repository.Links == nil || len(e.PullRequest.FromRef.Repository.Links.Self) == 0 {","\t\treturn fmt.Errorf(\"bitbucket fromRef repository links are nil or empty\")","\t}","\tif len(e.PullRequest.ToRef.Repository.Links.Clone) == 0 {","\t\treturn fmt.Errorf(\"bitbucket toRef repository clone links are empty\")","\t}","\tif len(e.PullRequest.FromRef.Repository.Links.Clone) == 0 {","\t\treturn fmt.Errorf(\"bitbucket fromRef repository clone links are empty\")","\t}","","\tif e.Actor.ID == 0 {","\t\treturn fmt.Errorf(\"bitbucket actor ID is zero\")","\t}","\tif e.Actor.Name == \"\" {","\t\treturn fmt.Errorf(\"bitbucket actor name is empty\")","\t}","\treturn nil","}","","// sanitizeEventURL returns the URL to the event without the /browse.","func sanitizeEventURL(eventURL string) string {","\tif strings.HasSuffix(eventURL, \"/browse\") {","\t\treturn eventURL[:len(eventURL)-len(\"/browse\")]","\t}","\treturn eventURL","}","","// sanitizeOwner remove ~ from OWNER in case of personal repos.","func sanitizeOwner(owner string) string {","\treturn strings.ReplaceAll(owner, \"~\", \"\")","}","","// ParsePayload parses the payload from the event.","func (v *Provider) ParsePayload(_ context.Context, _ *params.Run, request *http.Request,","\tpayload string,",") (*info.Event, error) {","\tprocessedEvent := info.NewEvent()","","\teventType := request.Header.Get(\"X-Event-Key\")","\teventPayload, err := parsePayloadType(eventType)","\tif err != nil {","\t\treturn info.NewEvent(), err","\t}","","\tif err := json.Unmarshal([]byte(payload), \u0026eventPayload); err != nil {","\t\treturn info.NewEvent(), err","\t}","","\tprocessedEvent.Event = eventPayload","","\tswitch e := eventPayload.(type) {","\tcase *types.PullRequestEvent:","\t\tif provider.Valid(eventType, []string{\"pr:from_ref_updated\", \"pr:opened\"}) {","\t\t\tprocessedEvent.TriggerTarget = triggertype.PullRequest","\t\t\tprocessedEvent.EventType = triggertype.PullRequest.String()","\t\t} else if provider.Valid(eventType, []string{\"pr:comment:added\", \"pr:comment:edited\"}) {","\t\t\tswitch {","\t\t\tcase provider.IsTestRetestComment(e.Comment.Text):","\t\t\t\tprocessedEvent.TriggerTarget = triggertype.PullRequest","\t\t\t\tif strings.Contains(e.Comment.Text, \"/test\") {","\t\t\t\t\tprocessedEvent.EventType = \"test-comment\"","\t\t\t\t} else {","\t\t\t\t\tprocessedEvent.EventType = \"retest-comment\"","\t\t\t\t}","\t\t\t\tprocessedEvent.TargetTestPipelineRun = provider.GetPipelineRunFromTestComment(e.Comment.Text)","\t\t\tcase provider.IsOkToTestComment(e.Comment.Text):","\t\t\t\tprocessedEvent.TriggerTarget = triggertype.PullRequest","\t\t\t\tprocessedEvent.EventType = \"ok-to-test-comment\"","\t\t\tcase provider.IsCancelComment(e.Comment.Text):","\t\t\t\tprocessedEvent.TriggerTarget = triggertype.PullRequest","\t\t\t\tprocessedEvent.EventType = \"cancel-comment\"","\t\t\t\tprocessedEvent.CancelPipelineRuns = true","\t\t\t\tprocessedEvent.TargetCancelPipelineRun = provider.GetPipelineRunFromCancelComment(e.Comment.Text)","\t\t\t}","\t\t\tprocessedEvent.TriggerComment = e.Comment.Text","\t\t}","","\t\tif err := checkValidPayload(e); err != nil {","\t\t\treturn nil, err","\t\t}","","\t\t// TODO: It's Really not an OWNER but a PROJECT","\t\tprocessedEvent.Organization = e.PullRequest.ToRef.Repository.Project.Key","\t\tprocessedEvent.Repository = e.PullRequest.ToRef.Repository.Name","\t\tprocessedEvent.SHA = e.PullRequest.FromRef.LatestCommit","\t\tprocessedEvent.PullRequestNumber = e.PullRequest.ID","\t\tprocessedEvent.URL = e.PullRequest.ToRef.Repository.Links.Self[0].Href","\t\tprocessedEvent.BaseBranch = e.PullRequest.ToRef.DisplayID","\t\tprocessedEvent.HeadBranch = e.PullRequest.FromRef.DisplayID","\t\tprocessedEvent.BaseURL = e.PullRequest.ToRef.Repository.Links.Self[0].Href","\t\tprocessedEvent.HeadURL = e.PullRequest.FromRef.Repository.Links.Self[0].Href","\t\tprocessedEvent.AccountID = fmt.Sprintf(\"%d\", e.Actor.ID)","\t\tprocessedEvent.Sender = e.Actor.Name","\t\tfor _, value := range e.PullRequest.FromRef.Repository.Links.Clone {","\t\t\tif value.Name == \"http\" {","\t\t\t\tprocessedEvent.CloneURL = value.Href","\t\t\t}","\t\t}","\t\tv.pullRequestNumber = e.PullRequest.ID","\tcase *types.PushRequestEvent:","\t\tprocessedEvent.TriggerTarget = triggertype.Push","\t\tprocessedEvent.EventType = triggertype.Push.String()","\t\tprocessedEvent.Organization = e.Repository.Project.Key","\t\tprocessedEvent.Repository = e.Repository.Slug","","\t\tif len(e.Changes) == 0 {","\t\t\treturn nil, fmt.Errorf(\"push event contains no commits under 'changes'; cannot proceed\")","\t\t}","","\t\t// Check for branch deletion - if any change is a DELETE type with zero hash, skip processing","\t\tfor _, change := range e.Changes {","\t\t\tif provider.IsZeroSHA(change.ToHash) \u0026\u0026 change.Type == \"DELETE\" {","\t\t\t\treturn nil, fmt.Errorf(\"branch delete event is not supported; cannot proceed\")","\t\t\t}","\t\t}","","\t\tif len(e.Commits) == 0 {","\t\t\treturn nil, fmt.Errorf(\"push event contains no commits; cannot proceed\")","\t\t}","","\t\tprocessedEvent.SHA = e.Changes[0].ToHash","\t\tprocessedEvent.URL = e.Repository.Links.Self[0].Href","\t\tprocessedEvent.BaseBranch = e.Changes[0].RefID","\t\tprocessedEvent.HeadBranch = e.Changes[0].RefID","\t\tprocessedEvent.BaseURL = e.Repository.Links.Self[0].Href","\t\tprocessedEvent.HeadURL = e.Repository.Links.Self[0].Href","\t\tprocessedEvent.AccountID = fmt.Sprintf(\"%d\", e.Actor.ID)","\t\tprocessedEvent.Sender = e.Actor.Name","\t\t// Should we care about clone via SSH or just only do HTTP clones?","\t\tfor _, value := range e.Repository.Links.Clone {","\t\t\tif value.Name == \"http\" {","\t\t\t\tprocessedEvent.CloneURL = value.Href","\t\t\t}","\t\t}","\tdefault:","\t\treturn nil, fmt.Errorf(\"event %s is not supported\", eventType)","\t}","","\tv.projectKey = processedEvent.Organization","\tprocessedEvent.Organization = sanitizeOwner(processedEvent.Organization)","\tprocessedEvent.URL = sanitizeEventURL(processedEvent.URL)","","\t// TODO: is this the right way? I guess i have no way to know what is the","\t// baseURL of a bitbucket data center unless there is something in the API?","\t// remove everything after /project in the URL to get the basePath","\tpURL, err := url.Parse(processedEvent.URL)","\tif err != nil {","\t\treturn nil, err","\t}","","\tv.baseURL = fmt.Sprintf(\"%s://%s\", pURL.Scheme, pURL.Host)","\treturn processedEvent, nil","}","","func parsePayloadType(event string) (any, error) {","\t// bitbucket data center event type has `pr:` prefix for pull request","\t// but in case of push event it is `repo:` prefix for both bitbucket data center","\t// and cloud, so we check the event name directly","\tvar localEvent string","\tif strings.HasPrefix(event, \"pr:\") {","\t\tif !provider.Valid(event, []string{","\t\t\t\"pr:from_ref_updated\", \"pr:opened\", \"pr:comment:added\", \"pr:comment:edited\",","\t\t}) {","\t\t\treturn nil, fmt.Errorf(\"event \\\"%s\\\" is not supported\", event)","\t\t}","\t\tlocalEvent = triggertype.PullRequest.String()","\t} else if event == \"repo:refs_changed\" {","\t\tlocalEvent = \"push\"","\t}","","\tvar intfType any","\tswitch localEvent {","\tcase triggertype.PullRequest.String():","\t\tintfType = \u0026types.PullRequestEvent{}","\tcase \"push\":","\t\tintfType = \u0026types.PushRequestEvent{}","\tdefault:","\t\tintfType = nil","\t}","\treturn intfType, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,0,0,0,2,2,2,2,2,0,0,0,2,2,2,0,0,0,0,2,2,2,2,2,2,2,2,0,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,0,0,2,1,1,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,0,0,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,1,1,0,0,2,2,2,2,2,2,2,2,2,2,2,0,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,0,2,0]},{"id":108,"path":"pkg/provider/gitea/acl.go","lines":["package gitea","","import (","\t\"context\"","\t\"fmt\"","\t\"net/http\"","\t\"strings\"","","\t\"codeberg.org/mvdkleijn/forgejo-sdk/forgejo/v2\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/acl\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/policy\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/gitea/forgejostructs\"",")","","func (v *Provider) CheckPolicyAllowing(_ context.Context, event *info.Event, allowedTeams []string) (bool, string) {","\tif event.Organization == event.Repository {","\t\treturn true, \"\"","\t}","\t// TODO: caching","\torgTeams, resp, err := v.Client().ListOrgTeams(event.Organization, forgejo.ListTeamsOptions{})","\tif resp.StatusCode == http.StatusNotFound {","\t\t// we explicitly disallow the policy when there is no team on org","\t\treturn false, fmt.Sprintf(\"no teams on org %s\", event.Organization)","\t}","\tif err != nil {","\t\t// probably a 500 or another api error, no need to try again and again with other teams","\t\treturn false, fmt.Sprintf(\"error while getting org team, error: %s\", err.Error())","\t}","\tfor _, allowedTeam := range allowedTeams {","\t\tfor _, orgTeam := range orgTeams {","\t\t\tif orgTeam.Name == allowedTeam {","\t\t\t\tteamMember, _, err := v.Client().GetTeamMember(orgTeam.ID, event.Sender)","\t\t\t\tif err != nil {","\t\t\t\t\tv.Logger.Infof(\"error while getting team member: %s, error: %s\", event.Sender, err.Error())","\t\t\t\t\tcontinue","\t\t\t\t}","\t\t\t\tif teamMember.ID != 0 {","\t\t\t\t\treturn true, fmt.Sprintf(\"allowing user: %s as a member of the team: %s\", event.Sender, orgTeam.Name)","\t\t\t\t}","\t\t\t}","\t\t}","\t}","\treturn false, fmt.Sprintf(\"user: %s is not a member of any of the allowed teams: %v\", event.Sender, allowedTeams)","}","","func (v *Provider) IsAllowed(ctx context.Context, event *info.Event) (bool, error) {","\taclPolicy := policy.Policy{","\t\tRepository:   v.repo,","\t\tEventEmitter: v.eventEmitter,","\t\tEvent:        event,","\t\tVCX:          v,","\t\tLogger:       v.Logger,","\t}","","\t// Try to detect a policy rule allowed it","\ttType, _ := detectTriggerTypeFromPayload(\"\", event.Event)","\tpolicyAllowed, policyReason := aclPolicy.IsAllowed(ctx, tType)","\tswitch policyAllowed {","\tcase policy.ResultAllowed:","\t\treturn true, nil","\tcase policy.ResultDisallowed:","\t\treturn false, nil","\tcase policy.ResultNotSet: // this is to make golangci-lint happy","\t}","","\t// Check all the ACL rules","\tallowed, err := v.aclCheckAll(ctx, event)","\tif err != nil {","\t\treturn false, err","\t}","\tif allowed {","\t\treturn true, nil","\t}","","\t// Try to parse the comment from an owner who has issues a /ok-to-test","\townerAllowed, err := v.aclAllowedOkToTestFromAnOwner(ctx, event)","\tif err != nil {","\t\treturn false, err","\t}","\tif ownerAllowed {","\t\treturn true, nil","\t}","","\t// error with the policy reason if it was set","\tif policyReason != \"\" {","\t\treturn false, fmt.Errorf(\"%s\", policyReason)","\t}","","\t// finally silently return false if no rules allowed this","\treturn false, nil","}","","// allowedOkToTestFromAnOwner Go over comments in a pull request and check","// if there is a /ok-to-test in there running an aclCheck again on the comment","// Sender if she is an OWNER and then allow it to run CI.","// TODO: pull out the github logic from there in an agnostic way.","func (v *Provider) aclAllowedOkToTestFromAnOwner(ctx context.Context, event *info.Event) (bool, error) {","\trevent := info.NewEvent()","\tevent.DeepCopyInto(revent)","\trevent.EventType = \"\"","\trevent.TriggerTarget = \"\"","\tif revent.Event == nil {","\t\treturn false, nil","\t}","","\tswitch event := revent.Event.(type) {","\tcase *forgejostructs.IssueCommentPayload:","\t\t// if we don't need to check old comments, then on issue comment we","\t\t// need to check if comment have /ok-to-test and is from allowed user","\t\tif !v.pacInfo.RememberOKToTest {","\t\t\treturn v.aclAllowedOkToTestCurrentComment(ctx, revent, event.Comment.ID)","\t\t}","\t\trevent.URL = event.Issue.URL","\tcase *forgejostructs.PullRequestPayload:","\t\t// if we don't need to check old comments, then on push event we don't need","\t\t// to check anything for the non-allowed user","\t\tif !v.pacInfo.RememberOKToTest {","\t\t\treturn false, nil","\t\t}","\t\trevent.URL = event.PullRequest.HTMLURL","\tdefault:","\t\treturn false, nil","\t}","","\tcomments, err := v.GetStringPullRequestComment(ctx, revent, acl.OKToTestCommentRegexp)","\tif err != nil {","\t\treturn false, err","\t}","","\tfor _, comment := range comments {","\t\trevent.Sender = comment.Poster.UserName","\t\tallowed, err := v.aclCheckAll(ctx, revent)","\t\tif err != nil {","\t\t\treturn false, err","\t\t}","\t\tif allowed {","\t\t\treturn true, nil","\t\t}","\t}","\treturn false, nil","}","","// aclAllowedOkToTestCurrentEvent only check if this is issue comment event","// have /ok-to-test regex and sender is allowed.","func (v *Provider) aclAllowedOkToTestCurrentComment(ctx context.Context, revent *info.Event, id int64) (bool, error) {","\tcomment, _, err := v.Client().GetIssueComment(revent.Organization, revent.Repository, id)","\tif err != nil {","\t\treturn false, err","\t}","\tif acl.MatchRegexp(acl.OKToTestCommentRegexp, comment.Body) {","\t\trevent.Sender = comment.Poster.UserName","\t\tallowed, err := v.aclCheckAll(ctx, revent)","\t\tif err != nil {","\t\t\treturn false, err","\t\t}","\t\tif allowed {","\t\t\treturn true, nil","\t\t}","\t}","\treturn false, nil","}","","// aclCheck check if we are allowed to run the pipeline on that PR.","func (v *Provider) aclCheckAll(ctx context.Context, rev *info.Event) (bool, error) {","\tif rev.Organization == rev.Sender {","\t\treturn true, nil","\t}","","\tcheckSenderRepoMembership, err := v.checkSenderRepoMembership(ctx, rev)","\tif err != nil {","\t\treturn false, err","\t}","\tif checkSenderRepoMembership {","\t\treturn true, nil","\t}","","\treturn v.IsAllowedOwnersFile(ctx, rev)","}","","// IsAllowedOwnersFile get the OWNERS files from main branch and check if we have","// explicitly allowed the user in there.","func (v *Provider) IsAllowedOwnersFile(ctx context.Context, rev *info.Event) (bool, error) {","\t// If we have a OWNERS and OWNERS_ALIASE files in the defaultBranch (ie: master) then","\t// parse them and check if sender is in there.","\townerContent, err := v.getFileFromDefaultBranch(ctx, \"OWNERS\", rev)","\tif err != nil {","\t\tif strings.Contains(err.Error(), \"cannot find\") {","\t\t\t// no owner file, skipping","\t\t\treturn false, nil","\t\t}","\t\treturn false, err","\t}","\t// If there is OWNERS file, check for OWNERS_ALIASES. OWNERS can exist without OWNERS_ALIASES.","\t// OWNERS_ALIASES can't exist without OWNERS.","\townerAliasesContent, err := v.getFileFromDefaultBranch(ctx, \"OWNERS_ALIASES\", rev)","\tif err != nil {","\t\tif !strings.Contains(err.Error(), \"cannot find\") {","\t\t\treturn false, err","\t\t}","\t}","","\treturn acl.UserInOwnerFile(ownerContent, ownerAliasesContent, rev.Sender)","}","","func (v *Provider) checkSenderRepoMembership(_ context.Context, runevent *info.Event) (bool, error) {","\tret, _, err := v.Client().IsCollaborator(runevent.Organization, runevent.Repository, runevent.Sender)","\treturn ret, err","}","","// getFileFromDefaultBranch will get a file directly from the Default BaseBranch as","// configured in runinfo which is directly set in webhook by Github.","func (v *Provider) getFileFromDefaultBranch(ctx context.Context, path string, runevent *info.Event) (string, error) {","\ttektonyaml, err := v.GetFileInsideRepo(ctx, runevent, path, runevent.DefaultBranch)","\tif err != nil {","\t\treturn \"\", fmt.Errorf(\"cannot find %s inside the %s branch: %w\", path, runevent.DefaultBranch, err)","\t}","\treturn tektonyaml, err","}","","// GetStringPullRequestComment return the comment if we find a regexp in one of","// the comments text of a pull request.","func (v *Provider) GetStringPullRequestComment(_ context.Context, runevent *info.Event, reg string) ([]*forgejo.Comment, error) {","\tvar ret []*forgejo.Comment","\tprNumber, err := convertPullRequestURLtoNumber(runevent.URL)","\tif err != nil {","\t\treturn nil, err","\t}","","\tcomments, _, err := v.Client().ListIssueComments(runevent.Organization, runevent.Repository, int64(prNumber), forgejo.ListIssueCommentOptions{})","\tif err != nil {","\t\treturn nil, err","\t}","\tfor _, v := range comments {","\t\tif acl.MatchRegexp(reg, v.Body) {","\t\t\tret = append(ret, v)","\t\t}","\t}","\treturn ret, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,2,2,0,0,0,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,0,0,0,0,2,2,1,1,2,1,1,0,0,2,2,1,1,2,2,2,0,0,2,1,1,0,0,2,0,0,0,0,0,0,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,1,1,0,2,2,2,2,1,1,2,2,2,0,2,0,0,0,0,2,2,2,1,1,2,2,2,2,1,1,2,2,2,0,2,0,0,0,2,2,2,2,0,2,2,1,1,2,2,2,0,2,0,0,0,0,2,2,2,2,2,2,2,2,2,1,0,0,0,2,2,2,1,1,0,0,2,0,0,2,2,2,2,0,0,0,2,2,2,2,2,2,0,0,0,0,2,2,2,2,1,1,0,2,2,1,1,2,2,2,2,0,2,0]},{"id":109,"path":"pkg/provider/gitea/detect.go","lines":["package gitea","","import (","\t\"encoding/json\"","\t\"fmt\"","\t\"net/http\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/gitea/forgejostructs\"","\t\"go.uber.org/zap\"",")","","var (","\tpullRequestOpenSyncEvent = []string{\"opened\", \"synchronize\", \"synchronized\", \"reopened\"}","\tpullRequestLabelUpdated  = \"label_updated\"","\tpullRequestLabelClosed   = \"closed\"",")","","// Detect processes event and detect if it is a gitea event, whether to process or reject it","// returns (if is a Gitea event, whether to process or reject, logger with event metadata,, error if any occurred).","func (v *Provider) Detect(req *http.Request, payload string, logger *zap.SugaredLogger) (bool, bool, *zap.SugaredLogger, string, error) {","\tisGitea := false","\teventType := req.Header.Get(\"X-Gitea-Event-Type\")","\tif eventType == \"\" {","\t\treturn false, false, logger, \"not a gitea event\", nil","\t}","","\tisGitea = true","\tsetLoggerAndProceed := func(processEvent bool, reason string, err error) (bool, bool, *zap.SugaredLogger,","\t\tstring, error,","\t) {","\t\tlogger = logger.With(\"provider\", \"gitea\", \"event-id\", req.Header.Get(\"X-Gitea-Delivery\"))","\t\treturn isGitea, processEvent, logger, reason, err","\t}","","\teventInt, err := parseWebhook(whEventType(eventType), []byte(payload))","\tif err != nil {","\t\treturn setLoggerAndProceed(false, \"\", err)","\t}","\t_ = json.Unmarshal([]byte(payload), \u0026eventInt)","\teType, errReason := detectTriggerTypeFromPayload(eventType, eventInt)","\tif eType != \"\" {","\t\treturn setLoggerAndProceed(true, \"\", nil)","\t}","","\treturn setLoggerAndProceed(false, errReason, nil)","}","","// detectTriggerTypeFromPayload will detect the event type from the payload,","// filtering out the events that are not supported.","func detectTriggerTypeFromPayload(ghEventType string, eventInt any) (triggertype.Trigger, string) {","\tswitch event := eventInt.(type) {","\tcase *forgejostructs.PushPayload:","\t\tif event.Pusher != nil {","\t\t\treturn triggertype.Push, \"\"","\t\t}","\t\treturn \"\", \"invalid payload: no pusher in event\"","\tcase *forgejostructs.PullRequestPayload:","\t\tif provider.Valid(string(event.Action), append(pullRequestOpenSyncEvent, pullRequestLabelUpdated, pullRequestLabelClosed)) {","\t\t\treturn triggertype.PullRequest, \"\"","\t\t}","\t\treturn \"\", fmt.Sprintf(\"pull_request: unsupported action \\\"%s\\\"\", event.Action)","\tcase *forgejostructs.IssueCommentPayload:","\t\tif event.Action == \"created\" \u0026\u0026","\t\t\tevent.Issue.PullRequest != nil \u0026\u0026","\t\t\tevent.Issue.State == \"open\" {","\t\t\tif provider.IsTestRetestComment(event.Comment.Body) {","\t\t\t\treturn triggertype.Retest, \"\"","\t\t\t}","\t\t\tif provider.IsOkToTestComment(event.Comment.Body) {","\t\t\t\treturn triggertype.OkToTest, \"\"","\t\t\t}","\t\t\tif provider.IsCancelComment(event.Comment.Body) {","\t\t\t\treturn triggertype.Cancel, \"\"","\t\t\t}","\t\t\t// this ignores the comment if it is not a PAC gitops comment and not return an error","\t\t\treturn triggertype.Comment, \"\"","\t\t}","\t\treturn \"\", \"skip: not a PAC gitops comment\"","\t}","\treturn \"\", fmt.Sprintf(\"gitea: event \\\"%v\\\" is not supported\", ghEventType)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,0,2,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,0,2,0,2,0]},{"id":110,"path":"pkg/provider/gitea/forgejostructs/types.go","lines":["// Copyright 2014 The Gogs Authors. All rights reserved.","// Copyright 2017 The Gitea Authors. All rights reserved.","// SPDX-License-Identifier: MIT.","","// Package forgejostructs contains Gitea webhook payload types.","// These types are copied from code.gitea.io/gitea/modules/structs v1.25.3.","// to avoid importing the entire Gitea codebase and its massive dependency tree.","package forgejostructs","","import (","\t\"encoding/json\"","\t\"time\"",")","","// -----------------------------------------------------------------------------","// User types.","// -----------------------------------------------------------------------------","","// User represents a user.","type User struct {","\tID        int64  `json:\"id\"`","\tUserName  string `json:\"login\"`","\tLoginName string `json:\"login_name\"`","\tSourceID  int64  `json:\"source_id\"`","\tFullName  string `json:\"full_name\"`","\tEmail     string `json:\"email\"`","\tAvatarURL string `json:\"avatar_url\"`","\tHTMLURL   string `json:\"html_url\"`","\tLanguage  string `json:\"language\"`","\tIsAdmin   bool   `json:\"is_admin\"`","\t// swagger:strfmt date-time.","\tLastLogin time.Time `json:\"last_login\"`","\t// swagger:strfmt date-time.","\tCreated       time.Time `json:\"created\"`","\tRestricted    bool      `json:\"restricted\"`","\tIsActive      bool      `json:\"active\"`","\tProhibitLogin bool      `json:\"prohibit_login\"`","\tLocation      string    `json:\"location\"`","\tWebsite       string    `json:\"website\"`","\tDescription   string    `json:\"description\"`","\tVisibility    string    `json:\"visibility\"`","\tFollowers     int       `json:\"followers_count\"`","\tFollowing     int       `json:\"following_count\"`","\tStarredRepos  int       `json:\"starred_repos_count\"`","}","","// MarshalJSON implements the json.Marshaler interface for User, adding field(s) for backward compatibility.","func (u User) MarshalJSON() ([]byte, error) {","\ttype shadow User","\treturn json.Marshal(struct {","\t\tshadow","\t\tCompatUserName string `json:\"username\"`","\t}{shadow(u), u.UserName})","}","","// userAlias is used to avoid recursion in UnmarshalJSON.","type userAlias User","","// userWithCompat includes the backwards-compatible username field that Gitea sends in webhook payloads.","type userWithCompat struct {","\tuserAlias","\tCompatUserName string `json:\"username\"`","}","","// UnmarshalJSON implements json.Unmarshaler, reading the \"username\" field into UserName if \"login\" is empty.","func (u *User) UnmarshalJSON(data []byte) error {","\tvar compat userWithCompat","\tif err := json.Unmarshal(data, \u0026compat); err != nil {","\t\treturn err","\t}","\t*u = User(compat.userAlias)","\tif u.UserName == \"\" \u0026\u0026 compat.CompatUserName != \"\" {","\t\tu.UserName = compat.CompatUserName","\t}","\treturn nil","}","","// -----------------------------------------------------------------------------","// Permission types.","// -----------------------------------------------------------------------------","","// Permission represents a set of permissions.","type Permission struct {","\tAdmin bool `json:\"admin\"`","\tPush  bool `json:\"push\"`","\tPull  bool `json:\"pull\"`","}","","// -----------------------------------------------------------------------------","// Repository types.","// -----------------------------------------------------------------------------","","// InternalTracker represents settings for internal tracker.","type InternalTracker struct {","\tEnableTimeTracker                bool `json:\"enable_time_tracker\"`","\tAllowOnlyContributorsToTrackTime bool `json:\"allow_only_contributors_to_track_time\"`","\tEnableIssueDependencies          bool `json:\"enable_issue_dependencies\"`","}","","// ExternalTracker represents settings for external tracker.","type ExternalTracker struct {","\tExternalTrackerURL           string `json:\"external_tracker_url\"`","\tExternalTrackerFormat        string `json:\"external_tracker_format\"`","\tExternalTrackerStyle         string `json:\"external_tracker_style\"`","\tExternalTrackerRegexpPattern string `json:\"external_tracker_regexp_pattern\"`","}","","// ExternalWiki represents setting for external wiki.","type ExternalWiki struct {","\tExternalWikiURL string `json:\"external_wiki_url\"`","}","","// RepoTransfer represents a pending repo transfer.","type RepoTransfer struct {","\tDoer      *User   `json:\"doer\"`","\tRecipient *User   `json:\"recipient\"`","\tTeams     []*Team `json:\"teams\"`","}","","// Repository represents a repository.","type Repository struct {","\tID                            int64            `json:\"id\"`","\tOwner                         *User            `json:\"owner\"`","\tName                          string           `json:\"name\"`","\tFullName                      string           `json:\"full_name\"`","\tDescription                   string           `json:\"description\"`","\tEmpty                         bool             `json:\"empty\"`","\tPrivate                       bool             `json:\"private\"`","\tFork                          bool             `json:\"fork\"`","\tTemplate                      bool             `json:\"template\"`","\tParent                        *Repository      `json:\"parent,omitempty\"`","\tMirror                        bool             `json:\"mirror\"`","\tSize                          int              `json:\"size\"`","\tLanguage                      string           `json:\"language\"`","\tLanguagesURL                  string           `json:\"languages_url\"`","\tHTMLURL                       string           `json:\"html_url\"`","\tURL                           string           `json:\"url\"`","\tLink                          string           `json:\"link\"`","\tSSHURL                        string           `json:\"ssh_url\"`","\tCloneURL                      string           `json:\"clone_url\"`","\tOriginalURL                   string           `json:\"original_url\"`","\tWebsite                       string           `json:\"website\"`","\tStars                         int              `json:\"stars_count\"`","\tForks                         int              `json:\"forks_count\"`","\tWatchers                      int              `json:\"watchers_count\"`","\tOpenIssues                    int              `json:\"open_issues_count\"`","\tOpenPulls                     int              `json:\"open_pr_counter\"`","\tReleases                      int              `json:\"release_counter\"`","\tDefaultBranch                 string           `json:\"default_branch\"`","\tArchived                      bool             `json:\"archived\"`","\tCreated                       time.Time        `json:\"created_at\"`","\tUpdated                       time.Time        `json:\"updated_at\"`","\tArchivedAt                    time.Time        `json:\"archived_at\"`","\tPermissions                   *Permission      `json:\"permissions,omitempty\"`","\tHasCode                       bool             `json:\"has_code\"`","\tHasIssues                     bool             `json:\"has_issues\"`","\tInternalTracker               *InternalTracker `json:\"internal_tracker,omitempty\"`","\tExternalTracker               *ExternalTracker `json:\"external_tracker,omitempty\"`","\tHasWiki                       bool             `json:\"has_wiki\"`","\tExternalWiki                  *ExternalWiki    `json:\"external_wiki,omitempty\"`","\tHasPullRequests               bool             `json:\"has_pull_requests\"`","\tHasProjects                   bool             `json:\"has_projects\"`","\tProjectsMode                  string           `json:\"projects_mode\"`","\tHasReleases                   bool             `json:\"has_releases\"`","\tHasPackages                   bool             `json:\"has_packages\"`","\tHasActions                    bool             `json:\"has_actions\"`","\tIgnoreWhitespaceConflicts     bool             `json:\"ignore_whitespace_conflicts\"`","\tAllowMerge                    bool             `json:\"allow_merge_commits\"`","\tAllowRebase                   bool             `json:\"allow_rebase\"`","\tAllowRebaseMerge              bool             `json:\"allow_rebase_explicit\"`","\tAllowSquash                   bool             `json:\"allow_squash_merge\"`","\tAllowFastForwardOnly          bool             `json:\"allow_fast_forward_only_merge\"`","\tAllowRebaseUpdate             bool             `json:\"allow_rebase_update\"`","\tAllowManualMerge              bool             `json:\"allow_manual_merge\"`","\tAutodetectManualMerge         bool             `json:\"autodetect_manual_merge\"`","\tDefaultDeleteBranchAfterMerge bool             `json:\"default_delete_branch_after_merge\"`","\tDefaultMergeStyle             string           `json:\"default_merge_style\"`","\tDefaultAllowMaintainerEdit    bool             `json:\"default_allow_maintainer_edit\"`","\tAvatarURL                     string           `json:\"avatar_url\"`","\tInternal                      bool             `json:\"internal\"`","\tMirrorInterval                string           `json:\"mirror_interval\"`","\tObjectFormatName              string           `json:\"object_format_name\"`","\tMirrorUpdated                 time.Time        `json:\"mirror_updated\"`","\tRepoTransfer                  *RepoTransfer    `json:\"repo_transfer,omitempty\"`","\tTopics                        []string         `json:\"topics\"`","\tLicenses                      []string         `json:\"licenses\"`","}","","// -----------------------------------------------------------------------------","// Label types.","// -----------------------------------------------------------------------------","","// Label a label to an issue or a pr.","type Label struct {","\tID          int64  `json:\"id\"`","\tName        string `json:\"name\"`","\tExclusive   bool   `json:\"exclusive\"`","\tIsArchived  bool   `json:\"is_archived\"`","\tColor       string `json:\"color\"`","\tDescription string `json:\"description\"`","\tURL         string `json:\"url\"`","}","","// -----------------------------------------------------------------------------","// Milestone types.","// -----------------------------------------------------------------------------","","// StateType issue state type.","type StateType string","","const (","\tStateOpen   StateType = \"open\"","\tStateClosed StateType = \"closed\"","\tStateAll    StateType = \"all\"",")","","// Milestone milestone is a collection of issues on one repository.","type Milestone struct {","\tID           int64      `json:\"id\"`","\tTitle        string     `json:\"title\"`","\tDescription  string     `json:\"description\"`","\tState        StateType  `json:\"state\"`","\tOpenIssues   int        `json:\"open_issues\"`","\tClosedIssues int        `json:\"closed_issues\"`","\tCreated      time.Time  `json:\"created_at\"`","\tUpdated      *time.Time `json:\"updated_at\"`","\tClosed       *time.Time `json:\"closed_at\"`","\tDeadline     *time.Time `json:\"due_on\"`","}","","// -----------------------------------------------------------------------------","// Comment types.","// -----------------------------------------------------------------------------","","// Comment represents a comment on a commit or issue.","type Comment struct {","\tID               int64         `json:\"id\"`","\tHTMLURL          string        `json:\"html_url\"`","\tPRURL            string        `json:\"pull_request_url\"`","\tIssueURL         string        `json:\"issue_url\"`","\tPoster           *User         `json:\"user\"`","\tOriginalAuthor   string        `json:\"original_author\"`","\tOriginalAuthorID int64         `json:\"original_author_id\"`","\tBody             string        `json:\"body\"`","\tAttachments      []*Attachment `json:\"assets\"`","\tCreated          time.Time     `json:\"created_at\"`","\tUpdated          time.Time     `json:\"updated_at\"`","}","","// -----------------------------------------------------------------------------","// Attachment types.","// -----------------------------------------------------------------------------","","// Attachment a generic attachment.","type Attachment struct {","\tID            int64     `json:\"id\"`","\tName          string    `json:\"name\"`","\tSize          int64     `json:\"size\"`","\tDownloadCount int64     `json:\"download_count\"`","\tCreated       time.Time `json:\"created_at\"`","\tUUID          string    `json:\"uuid\"`","\tDownloadURL   string    `json:\"browser_download_url\"`","}","","// -----------------------------------------------------------------------------","// Issue types.","// -----------------------------------------------------------------------------","","// PullRequestMeta PR info if an issue is a PR.","type PullRequestMeta struct {","\tHasMerged        bool       `json:\"merged\"`","\tMerged           *time.Time `json:\"merged_at\"`","\tIsWorkInProgress bool       `json:\"draft\"`","\tHTMLURL          string     `json:\"html_url\"`","}","","// RepositoryMeta basic repository information.","type RepositoryMeta struct {","\tID       int64  `json:\"id\"`","\tName     string `json:\"name\"`","\tOwner    string `json:\"owner\"`","\tFullName string `json:\"full_name\"`","}","","// Issue represents an issue in a repository.","type Issue struct {","\tID               int64            `json:\"id\"`","\tURL              string           `json:\"url\"`","\tHTMLURL          string           `json:\"html_url\"`","\tIndex            int64            `json:\"number\"`","\tPoster           *User            `json:\"user\"`","\tOriginalAuthor   string           `json:\"original_author\"`","\tOriginalAuthorID int64            `json:\"original_author_id\"`","\tTitle            string           `json:\"title\"`","\tBody             string           `json:\"body\"`","\tRef              string           `json:\"ref\"`","\tAttachments      []*Attachment    `json:\"assets\"`","\tLabels           []*Label         `json:\"labels\"`","\tMilestone        *Milestone       `json:\"milestone\"`","\tAssignee         *User            `json:\"assignee\"`","\tAssignees        []*User          `json:\"assignees\"`","\tState            StateType        `json:\"state\"`","\tIsLocked         bool             `json:\"is_locked\"`","\tComments         int              `json:\"comments\"`","\tCreated          time.Time        `json:\"created_at\"`","\tUpdated          time.Time        `json:\"updated_at\"`","\tClosed           *time.Time       `json:\"closed_at\"`","\tDeadline         *time.Time       `json:\"due_date\"`","\tTimeEstimate     int64            `json:\"time_estimate\"`","\tPullRequest      *PullRequestMeta `json:\"pull_request\"`","\tRepo             *RepositoryMeta  `json:\"repository\"`","\tPinOrder         int              `json:\"pin_order\"`","}","","// -----------------------------------------------------------------------------","// Pull Request types.","// -----------------------------------------------------------------------------","","// PRBranchInfo information about a branch.","type PRBranchInfo struct {","\tName       string      `json:\"label\"`","\tRef        string      `json:\"ref\"`","\tSha        string      `json:\"sha\"`","\tRepoID     int64       `json:\"repo_id\"`","\tRepository *Repository `json:\"repo\"`","}","","// PullRequest represents a pull request.","type PullRequest struct {","\tID                      int64         `json:\"id\"`","\tURL                     string        `json:\"url\"`","\tIndex                   int64         `json:\"number\"`","\tPoster                  *User         `json:\"user\"`","\tTitle                   string        `json:\"title\"`","\tBody                    string        `json:\"body\"`","\tLabels                  []*Label      `json:\"labels\"`","\tMilestone               *Milestone    `json:\"milestone\"`","\tAssignee                *User         `json:\"assignee\"`","\tAssignees               []*User       `json:\"assignees\"`","\tRequestedReviewers      []*User       `json:\"requested_reviewers\"`","\tRequestedReviewersTeams []*Team       `json:\"requested_reviewers_teams\"`","\tState                   StateType     `json:\"state\"`","\tDraft                   bool          `json:\"draft\"`","\tIsLocked                bool          `json:\"is_locked\"`","\tComments                int           `json:\"comments\"`","\tReviewComments          int           `json:\"review_comments,omitempty\"`","\tAdditions               *int          `json:\"additions,omitempty\"`","\tDeletions               *int          `json:\"deletions,omitempty\"`","\tChangedFiles            *int          `json:\"changed_files,omitempty\"`","\tHTMLURL                 string        `json:\"html_url\"`","\tDiffURL                 string        `json:\"diff_url\"`","\tPatchURL                string        `json:\"patch_url\"`","\tMergeable               bool          `json:\"mergeable\"`","\tHasMerged               bool          `json:\"merged\"`","\tMerged                  *time.Time    `json:\"merged_at\"`","\tMergedCommitID          *string       `json:\"merge_commit_sha\"`","\tMergedBy                *User         `json:\"merged_by\"`","\tAllowMaintainerEdit     bool          `json:\"allow_maintainer_edit\"`","\tBase                    *PRBranchInfo `json:\"base\"`","\tHead                    *PRBranchInfo `json:\"head\"`","\tMergeBase               string        `json:\"merge_base\"`","\tDeadline                *time.Time    `json:\"due_date\"`","\tCreated                 *time.Time    `json:\"created_at\"`","\tUpdated                 *time.Time    `json:\"updated_at\"`","\tClosed                  *time.Time    `json:\"closed_at\"`","\tPinOrder                int           `json:\"pin_order\"`","}","","// -----------------------------------------------------------------------------","// Release types.","// -----------------------------------------------------------------------------","","// Release represents a repository release.","type Release struct {","\tID           int64         `json:\"id\"`","\tTagName      string        `json:\"tag_name\"`","\tTarget       string        `json:\"target_commitish\"`","\tTitle        string        `json:\"name\"`","\tNote         string        `json:\"body\"`","\tURL          string        `json:\"url\"`","\tHTMLURL      string        `json:\"html_url\"`","\tTarURL       string        `json:\"tarball_url\"`","\tZipURL       string        `json:\"zipball_url\"`","\tUploadURL    string        `json:\"upload_url\"`","\tIsDraft      bool          `json:\"draft\"`","\tIsPrerelease bool          `json:\"prerelease\"`","\tCreatedAt    time.Time     `json:\"created_at\"`","\tPublishedAt  time.Time     `json:\"published_at\"`","\tPublisher    *User         `json:\"author\"`","\tAttachments  []*Attachment `json:\"assets\"`","}","","// -----------------------------------------------------------------------------","// Organization types.","// -----------------------------------------------------------------------------","","// Organization represents an organization.","type Organization struct {","\tID                        int64  `json:\"id\"`","\tName                      string `json:\"name\"`","\tFullName                  string `json:\"full_name\"`","\tEmail                     string `json:\"email\"`","\tAvatarURL                 string `json:\"avatar_url\"`","\tDescription               string `json:\"description\"`","\tWebsite                   string `json:\"website\"`","\tLocation                  string `json:\"location\"`","\tVisibility                string `json:\"visibility\"`","\tRepoAdminChangeTeamAccess bool   `json:\"repo_admin_change_team_access\"`","\tUserName                  string `json:\"username\"`","}","","// -----------------------------------------------------------------------------","// Team types.","// -----------------------------------------------------------------------------","","// Team represents a team in an organization.","type Team struct {","\tID                      int64             `json:\"id\"`","\tName                    string            `json:\"name\"`","\tDescription             string            `json:\"description\"`","\tOrganization            *Organization     `json:\"organization\"`","\tIncludesAllRepositories bool              `json:\"includes_all_repositories\"`","\tPermission              string            `json:\"permission\"`","\tUnits                   []string          `json:\"units\"`","\tUnitsMap                map[string]string `json:\"units_map\"`","\tCanCreateOrgRepo        bool              `json:\"can_create_org_repo\"`","}","","// -----------------------------------------------------------------------------","// Hook/Webhook types.","// -----------------------------------------------------------------------------","","// PayloadUser represents the author or committer of a commit.","type PayloadUser struct {","\tName     string `json:\"name\"`","\tEmail    string `json:\"email\"`","\tUserName string `json:\"username\"`","}","","// PayloadCommitVerification represents the GPG verification of a commit.","type PayloadCommitVerification struct {","\tVerified  bool         `json:\"verified\"`","\tReason    string       `json:\"reason\"`","\tSignature string       `json:\"signature\"`","\tSigner    *PayloadUser `json:\"signer\"`","\tPayload   string       `json:\"payload\"`","}","","// PayloadCommit represents a commit.","type PayloadCommit struct {","\tID           string                     `json:\"id\"`","\tMessage      string                     `json:\"message\"`","\tURL          string                     `json:\"url\"`","\tAuthor       *PayloadUser               `json:\"author\"`","\tCommitter    *PayloadUser               `json:\"committer\"`","\tVerification *PayloadCommitVerification `json:\"verification\"`","\tTimestamp    time.Time                  `json:\"timestamp\"`","\tAdded        []string                   `json:\"added\"`","\tRemoved      []string                   `json:\"removed\"`","\tModified     []string                   `json:\"modified\"`","}","","// PusherType define the type to push.","type PusherType string","","const (","\tPusherTypeUser PusherType = \"user\"",")","","// HookIssueAction defines hook issue action.","type HookIssueAction string","","const (","\tHookIssueOpened               HookIssueAction = \"opened\"","\tHookIssueClosed               HookIssueAction = \"closed\"","\tHookIssueReOpened             HookIssueAction = \"reopened\"","\tHookIssueEdited               HookIssueAction = \"edited\"","\tHookIssueDeleted              HookIssueAction = \"deleted\"","\tHookIssueAssigned             HookIssueAction = \"assigned\"","\tHookIssueUnassigned           HookIssueAction = \"unassigned\"","\tHookIssueLabelUpdated         HookIssueAction = \"label_updated\"","\tHookIssueLabelCleared         HookIssueAction = \"label_cleared\"","\tHookIssueSynchronized         HookIssueAction = \"synchronized\"","\tHookIssueMilestoned           HookIssueAction = \"milestoned\"","\tHookIssueDemilestoned         HookIssueAction = \"demilestoned\"","\tHookIssueReviewed             HookIssueAction = \"reviewed\"","\tHookIssueReviewRequested      HookIssueAction = \"review_requested\"","\tHookIssueReviewRequestRemoved HookIssueAction = \"review_request_removed\"",")","","// HookIssueCommentAction defines hook issue comment action.","type HookIssueCommentAction string","","const (","\tHookIssueCommentCreated HookIssueCommentAction = \"created\"","\tHookIssueCommentEdited  HookIssueCommentAction = \"edited\"","\tHookIssueCommentDeleted HookIssueCommentAction = \"deleted\"",")","","// HookReleaseAction defines hook release action type.","type HookReleaseAction string","","const (","\tHookReleasePublished HookReleaseAction = \"published\"","\tHookReleaseUpdated   HookReleaseAction = \"updated\"","\tHookReleaseDeleted   HookReleaseAction = \"deleted\"",")","","// HookRepoAction defines hook repository action type.","type HookRepoAction string","","const (","\tHookRepoCreated HookRepoAction = \"created\"","\tHookRepoDeleted HookRepoAction = \"deleted\"",")","","// ChangesFromPayload represents the previous value before a change.","type ChangesFromPayload struct {","\tFrom string `json:\"from\"`","}","","// ChangesPayload represents the payload information of issue change.","type ChangesPayload struct {","\tTitle         *ChangesFromPayload `json:\"title,omitempty\"`","\tBody          *ChangesFromPayload `json:\"body,omitempty\"`","\tRef           *ChangesFromPayload `json:\"ref,omitempty\"`","\tAddedLabels   []*Label            `json:\"added_labels\"`","\tRemovedLabels []*Label            `json:\"removed_labels\"`","}","","// ReviewPayload represents review information.","type ReviewPayload struct {","\tType    string `json:\"type\"`","\tContent string `json:\"content\"`","}","","// -----------------------------------------------------------------------------","// Webhook Payload types.","// -----------------------------------------------------------------------------","","// CreatePayload represents a payload information of create event.","type CreatePayload struct {","\tSha     string      `json:\"sha\"`","\tRef     string      `json:\"ref\"`","\tRefType string      `json:\"ref_type\"`","\tRepo    *Repository `json:\"repository\"`","\tSender  *User       `json:\"sender\"`","}","","// DeletePayload represents delete payload.","type DeletePayload struct {","\tRef        string      `json:\"ref\"`","\tRefType    string      `json:\"ref_type\"`","\tPusherType PusherType  `json:\"pusher_type\"`","\tRepo       *Repository `json:\"repository\"`","\tSender     *User       `json:\"sender\"`","}","","// ForkPayload represents fork payload.","type ForkPayload struct {","\tForkee *Repository `json:\"forkee\"`","\tRepo   *Repository `json:\"repository\"`","\tSender *User       `json:\"sender\"`","}","","// PushPayload represents a payload information of push event.","type PushPayload struct {","\tRef          string           `json:\"ref\"`","\tBefore       string           `json:\"before\"`","\tAfter        string           `json:\"after\"`","\tCompareURL   string           `json:\"compare_url\"`","\tCommits      []*PayloadCommit `json:\"commits\"`","\tTotalCommits int              `json:\"total_commits\"`","\tHeadCommit   *PayloadCommit   `json:\"head_commit\"`","\tRepo         *Repository      `json:\"repository\"`","\tPusher       *User            `json:\"pusher\"`","\tSender       *User            `json:\"sender\"`","}","","// IssuePayload represents the payload information that is sent along with an issue event.","type IssuePayload struct {","\tAction     HookIssueAction `json:\"action\"`","\tIndex      int64           `json:\"number\"`","\tChanges    *ChangesPayload `json:\"changes,omitempty\"`","\tIssue      *Issue          `json:\"issue\"`","\tRepository *Repository     `json:\"repository\"`","\tSender     *User           `json:\"sender\"`","\tCommitID   string          `json:\"commit_id\"`","}","","// IssueCommentPayload represents a payload information of issue comment event.","type IssueCommentPayload struct {","\tAction      HookIssueCommentAction `json:\"action\"`","\tIssue       *Issue                 `json:\"issue\"`","\tPullRequest *PullRequest           `json:\"pull_request,omitempty\"`","\tComment     *Comment               `json:\"comment\"`","\tChanges     *ChangesPayload        `json:\"changes,omitempty\"`","\tRepository  *Repository            `json:\"repository\"`","\tSender      *User                  `json:\"sender\"`","\tIsPull      bool                   `json:\"is_pull\"`","}","","// PullRequestPayload represents a payload information of pull request event.","type PullRequestPayload struct {","\tAction            HookIssueAction `json:\"action\"`","\tIndex             int64           `json:\"number\"`","\tChanges           *ChangesPayload `json:\"changes,omitempty\"`","\tPullRequest       *PullRequest    `json:\"pull_request\"`","\tRequestedReviewer *User           `json:\"requested_reviewer\"`","\tRepository        *Repository     `json:\"repository\"`","\tSender            *User           `json:\"sender\"`","\tCommitID          string          `json:\"commit_id\"`","\tReview            *ReviewPayload  `json:\"review\"`","}","","// ReleasePayload represents a payload information of release event.","type ReleasePayload struct {","\tAction     HookReleaseAction `json:\"action\"`","\tRelease    *Release          `json:\"release\"`","\tRepository *Repository       `json:\"repository\"`","\tSender     *User             `json:\"sender\"`","}","","// RepositoryPayload payload for repository webhooks.","type RepositoryPayload struct {","\tAction       HookRepoAction `json:\"action\"`","\tRepository   *Repository    `json:\"repository\"`","\tOrganization *User          `json:\"organization\"`","\tSender       *User          `json:\"sender\"`","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]},{"id":111,"path":"pkg/provider/gitea/gitea.go","lines":["package gitea","","import (","\t\"context\"","\t\"crypto/hmac\"","\t\"crypto/sha256\"","\t\"crypto/subtle\"","\t\"encoding/base64\"","\t\"encoding/hex\"","\t\"encoding/json\"","\t\"fmt\"","\t\"net/http\"","\t\"path\"","\t\"regexp\"","\t\"strconv\"","\t\"strings\"","\t\"time\"","","\t\"codeberg.org/mvdkleijn/forgejo-sdk/forgejo/v2\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/changedfiles\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/events\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/opscomments\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\tproviderMetrics \"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/providermetrics\"","\t\"go.uber.org/zap\"",")","","const (","\ttaskStatusTemplate = `","\u003ctable\u003e","  \u003ctr\u003e\u003cth\u003eStatus\u003c/th\u003e\u003cth\u003eDuration\u003c/th\u003e\u003cth\u003eName\u003c/th\u003e\u003c/tr\u003e","","{{- range $taskrun := .TaskRunList }}","\u003ctr\u003e","\u003ctd\u003e{{ formatCondition $taskrun.PipelineRunTaskRunStatus.Status.Conditions }}\u003c/td\u003e","\u003ctd\u003e{{ formatDuration $taskrun.PipelineRunTaskRunStatus.Status.StartTime $taskrun.Status.CompletionTime }}\u003c/td\u003e\u003ctd\u003e","","{{ $taskrun.ConsoleLogURL }}","","\u003c/td\u003e\u003c/tr\u003e","{{- end }}","\u003c/table\u003e`","","\tForgejoSignatureHeader = \"X-Forgejo-Signature\"","\tGiteaSignatureHeader   = \"X-Gitea-Signature\"",")","","// validate the struct to interface.","var _ provider.Interface = (*Provider)(nil)","","type Provider struct {","\tgiteaClient      *forgejo.Client","\tLogger           *zap.SugaredLogger","\tpacInfo          *info.PacOpts","\tToken            *string","\tgiteaInstanceURL string","\t// only exposed for e2e tests","\tPassword     string","\trepo         *v1alpha1.Repository","\teventEmitter *events.EventEmitter","\trun          *params.Run","\ttriggerEvent string","}","","func (v *Provider) Client() *forgejo.Client {","\tproviderMetrics.RecordAPIUsage(","\t\tv.Logger,","\t\t// URL used instead of \"gitea\" to differentiate in the case of a CI cluster which","\t\t// serves multiple Gitea instances","\t\tv.giteaInstanceURL,","\t\tv.triggerEvent,","\t\tv.repo,","\t)","\treturn v.giteaClient","}","","func (v *Provider) SetGiteaClient(client *forgejo.Client) {","\tv.giteaClient = client","}","","func (v *Provider) CreateComment(_ context.Context, event *info.Event, commit, updateMarker string) error {","\tif v.giteaClient == nil {","\t\treturn fmt.Errorf(\"no gitea client has been initialized\")","\t}","","\tif event.PullRequestNumber == 0 {","\t\treturn fmt.Errorf(\"create comment only works on pull requests\")","\t}","","\t// List comments of the PR","\tif updateMarker != \"\" {","\t\tcomments, _, err := v.Client().ListIssueComments(event.Organization, event.Repository, int64(event.PullRequestNumber), forgejo.ListIssueCommentOptions{})","\t\tif err != nil {","\t\t\treturn err","\t\t}","","\t\tre := regexp.MustCompile(updateMarker)","\t\tfor _, comment := range comments {","\t\t\tif re.MatchString(comment.Body) {","\t\t\t\t_, _, err := v.Client().EditIssueComment(event.Organization, event.Repository, comment.ID, forgejo.EditIssueCommentOption{","\t\t\t\t\tBody: commit,","\t\t\t\t})","\t\t\t\treturn err","\t\t\t}","\t\t}","\t}","","\t_, _, err := v.Client().CreateIssueComment(event.Organization, event.Repository, int64(event.PullRequestNumber), forgejo.CreateIssueCommentOption{","\t\tBody: commit,","\t})","","\treturn err","}","","func (v *Provider) SetPacInfo(pacInfo *info.PacOpts) {","\tv.pacInfo = pacInfo","}","","// GetTaskURI TODO: Implement ME.","func (v *Provider) GetTaskURI(_ context.Context, _ *info.Event, _ string) (bool, string, error) {","\treturn false, \"\", nil","}","","func (v *Provider) SetLogger(logger *zap.SugaredLogger) {","\tv.Logger = logger","}","","func (v *Provider) Validate(_ context.Context, _ *params.Run, event *info.Event) error {","\tsignature := event.Request.Header.Get(ForgejoSignatureHeader)","\tif signature == \"\" {","\t\tsignature = event.Request.Header.Get(GiteaSignatureHeader)","\t}","\tif signature == \"\" {","\t\treturn fmt.Errorf(\"no signature has been detected, for security reason we are not allowing webhooks without a secret\")","\t}","","\tsecret := event.Provider.WebhookSecret","\tif secret == \"\" {","\t\treturn fmt.Errorf(\"no webhook secret has been set, in repository CR or secret\")","\t}","","\treturn validateSignature(signature, event.Request.Payload, []byte(secret))","}","","func validateSignature(signature string, payload, secret []byte) error {","\tsignatureBytes, err := hex.DecodeString(signature)","\tif err != nil {","\t\treturn fmt.Errorf(\"gitea/forgejo webhook signature is not valid hex: %w\", err)","\t}","","\tmac := hmac.New(sha256.New, secret)","\tmac.Write(payload)","\texpectedMAC := mac.Sum(nil)","","\tif subtle.ConstantTimeCompare(signatureBytes, expectedMAC) != 1 {","\t\treturn fmt.Errorf(\"gitea/forgejo webhook signature validation failed\")","\t}","\treturn nil","}","","func convertPullRequestURLtoNumber(pullRequest string) (int, error) {","\tprNumber, err := strconv.Atoi(path.Base(pullRequest))","\tif err != nil {","\t\treturn -1, fmt.Errorf(\"bad pull request number html_url number: %w\", err)","\t}","\treturn prNumber, nil","}","","func (v *Provider) GetConfig() *info.ProviderConfig {","\treturn \u0026info.ProviderConfig{","\t\tTaskStatusTMPL: taskStatusTemplate,","\t\tAPIURL:         v.giteaInstanceURL,","\t\tName:           \"gitea\",","\t\tSkipEmoji:      true,","\t}","}","","func (v *Provider) SetClient(_ context.Context, run *params.Run, runevent *info.Event, repo *v1alpha1.Repository, emitter *events.EventEmitter) error {","\tvar err error","\tapiURL := runevent.Provider.URL","\t// password is not exposed to CRD, it's only used from the e2e tests","\tif v.Password != \"\" \u0026\u0026 runevent.Provider.User != \"\" {","\t\tv.giteaClient, err = forgejo.NewClient(apiURL, forgejo.SetBasicAuth(runevent.Provider.User, v.Password))","\t} else {","\t\tif runevent.Provider.Token == \"\" {","\t\t\treturn fmt.Errorf(\"no git_provider.secret has been set in the repo crd\")","\t\t}","\t\tv.giteaClient, err = forgejo.NewClient(apiURL, forgejo.SetToken(runevent.Provider.Token))","\t}","\tif err != nil {","\t\treturn err","\t}","","\t// Added log for security audit purposes to log client access when a token is used","\trun.Clients.Log.Infof(\"gitea: initialized API client with provided credentials user=%s providerURL=%s\", runevent.Provider.User, apiURL)","","\tv.giteaInstanceURL = runevent.Provider.URL","\tv.eventEmitter = emitter","\tv.repo = repo","\tv.run = run","\tv.triggerEvent = runevent.EventType","\treturn nil","}","","func (v *Provider) CreateStatus(_ context.Context, event *info.Event, statusOpts provider.StatusOpts) error {","\tif v.giteaClient == nil {","\t\treturn fmt.Errorf(\"cannot set status on gitea no token or url set\")","\t}","\tswitch statusOpts.Conclusion {","\tcase \"success\":","\t\tstatusOpts.Title = \"Success\"","\t\tstatusOpts.Summary = \"has \u003cb\u003esuccessfully\u003c/b\u003e validated your commit.\"","\tcase \"failure\":","\t\tstatusOpts.Title = \"Failed\"","\t\tstatusOpts.Summary = \"has \u003cb\u003efailed\u003c/b\u003e.\"","\tcase \"pending\":","\t\t// for concurrency set title as pending","\t\tif statusOpts.Title == \"\" {","\t\t\tstatusOpts.Title = \"Pending\"","\t\t}","\t\t// for unauthorized user set title as Pending approval","\t\tstatusOpts.Summary = \"is skipping this commit.\"","\tcase \"neutral\":","\t\tstatusOpts.Title = \"Unknown\"","\t\tstatusOpts.Summary = \"doesn't know what happened with this commit.\"","\t}","","\tif statusOpts.Status == \"in_progress\" {","\t\tstatusOpts.Title = \"CI has Started\"","\t\tstatusOpts.Summary = \"is running.\\n\"","\t}","","\tonPr := \"\"","\tif statusOpts.PipelineRunName != \"\" {","\t\tonPr = fmt.Sprintf(\"/%s\", statusOpts.PipelineRunName)","\t}","\t// gitea show weirdly the \u003cbr\u003e","\tstatusOpts.Summary = fmt.Sprintf(\"%s%s %s\", v.pacInfo.ApplicationName, onPr, statusOpts.Summary)","","\treturn v.createStatusCommit(event, v.pacInfo, statusOpts)","}","","func (v *Provider) createStatusCommit(event *info.Event, pacopts *info.PacOpts, status provider.StatusOpts) error {","\tstate := forgejo.StatusState(status.Conclusion)","\tswitch status.Conclusion {","\tcase \"neutral\":","\t\tstate = forgejo.StatusSuccess // We don't have a choice than setting as success, no pending here.c","\tcase \"pending\":","\t\tif status.Title != \"\" {","\t\t\tstate = forgejo.StatusPending","\t\t}","\t}","\tif status.Status == \"in_progress\" {","\t\tstate = forgejo.StatusPending","\t}","","\tgStatus := forgejo.CreateStatusOption{","\t\tState:       state,","\t\tTargetURL:   status.DetailsURL,","\t\tDescription: status.Title,","\t\tContext:     provider.GetCheckName(status, pacopts),","\t}","","\t// Retry logic for transient errors (e.g., \"user does not exist\" under concurrent load)","\tmaxRetries := 3","\tvar lastErr error","\tfor i := range maxRetries {","\t\tif _, _, err := v.Client().CreateStatus(event.Organization, event.Repository, event.SHA, gStatus); err != nil {","\t\t\tlastErr = err","\t\t\t// Only retry on transient \"user does not exist\" errors","\t\t\tif strings.Contains(err.Error(), \"user does not exist\") {","\t\t\t\tv.Logger.Warnf(\"CreateStatus failed with transient error, retrying %d/%d: %v\", i+1, maxRetries, err)","\t\t\t\ttime.Sleep(time.Duration(i+1) * 500 * time.Millisecond)","\t\t\t\tcontinue","\t\t\t}","\t\t\treturn err","\t\t}","\t\tlastErr = nil","\t\tbreak","\t}","\tif lastErr != nil {","\t\treturn lastErr","\t}","","\teventType := triggertype.IsPullRequestType(event.EventType)","\tif opscomments.IsAnyOpsEventType(eventType.String()) {","\t\teventType = triggertype.PullRequest","\t}","\tif status.Text != \"\" \u0026\u0026 (eventType == triggertype.PullRequest || event.TriggerTarget == triggertype.PullRequest) {","\t\tstatus.Text = strings.ReplaceAll(strings.TrimSpace(status.Text), \"\u003cbr\u003e\", \"\\n\")","\t\t_, _, err := v.Client().CreateIssueComment(event.Organization, event.Repository,","\t\t\tint64(event.PullRequestNumber), forgejo.CreateIssueCommentOption{","\t\t\t\tBody: fmt.Sprintf(\"%s\\n%s\", status.Summary, status.Text),","\t\t\t},","\t\t)","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t}","\treturn nil","}","","func (v *Provider) GetTektonDir(_ context.Context, event *info.Event, path, provenance string) (string, error) {","\t// default set provenance from the SHA","\trevision := event.SHA","\tif provenance == \"default_branch\" {","\t\trevision = event.DefaultBranch","\t\tv.Logger.Infof(\"Using PipelineRun definition from default_branch: %s\", event.DefaultBranch)","\t} else {","\t\tv.Logger.Infof(\"Using PipelineRun definition from source %s commit SHA: %s\", event.TriggerTarget.String(), event.SHA)","\t}","","\ttektonDirSha := \"\"","\topt := forgejo.GetTreesOptions{","\t\tRecursive: false,","\t}","\trootobjects, _, err := v.Client().GetTrees(event.Organization, event.Repository, revision, opt)","\tif err != nil {","\t\treturn \"\", err","\t}","\tfor _, object := range rootobjects.Entries {","\t\tif object.Path == path {","\t\t\tif object.Type != \"tree\" {","\t\t\t\treturn \"\", fmt.Errorf(\"%s has been found but is not a directory\", path)","\t\t\t}","\t\t\ttektonDirSha = object.SHA","\t\t}","\t}","","\t// If we didn't find a .tekton directory then just silently ignore the error.","\tif tektonDirSha == \"\" {","\t\treturn \"\", nil","\t}","\t// Get all files in the .tekton directory recursively","\t// TODO: figure out if there is a object limit we need to handle here","\topts := forgejo.GetTreesOptions{Recursive: false}","\ttektonDirObjects, _, err := v.Client().GetTrees(event.Organization, event.Repository, tektonDirSha, opts)","\tif err != nil {","\t\treturn \"\", err","\t}","\treturn v.concatAllYamlFiles(tektonDirObjects.Entries, event)","}","","func (v *Provider) concatAllYamlFiles(objects []forgejo.GitEntry, event *info.Event) (string,","\terror,",") {","\tvar allTemplates string","","\tfor _, value := range objects {","\t\tif strings.HasSuffix(value.Path, \".yaml\") ||","\t\t\tstrings.HasSuffix(value.Path, \".yml\") {","\t\t\tdata, err := v.getObject(value.SHA, event)","\t\t\tif err != nil {","\t\t\t\treturn \"\", err","\t\t\t}","\t\t\tif err := provider.ValidateYaml(data, value.Path); err != nil {","\t\t\t\treturn \"\", err","\t\t\t}","\t\t\tif allTemplates != \"\" \u0026\u0026 !strings.HasPrefix(string(data), \"---\") {","\t\t\t\tallTemplates += \"---\"","\t\t\t}","\t\t\tallTemplates += \"\\n\" + string(data) + \"\\n\"","\t\t}","\t}","\treturn allTemplates, nil","}","","func (v *Provider) getObject(sha string, event *info.Event) ([]byte, error) {","\tblob, _, err := v.Client().GetBlob(event.Organization, event.Repository, sha)","\tif err != nil {","\t\treturn nil, err","\t}","\tdecoded, err := base64.StdEncoding.DecodeString(blob.Content)","\tif err != nil {","\t\treturn nil, err","\t}","\treturn decoded, err","}","","func (v *Provider) GetFileInsideRepo(_ context.Context, runevent *info.Event, path, target string) (string, error) {","\tref := runevent.SHA","\tif target != \"\" {","\t\tref = runevent.BaseBranch","\t}","","\tcontent, _, err := v.Client().GetContents(runevent.Organization, runevent.Repository, ref, path)","\tif err != nil {","\t\treturn \"\", err","\t}","\t// base64 decode to string","\tdecoded, err := base64.StdEncoding.DecodeString(*content.Content)","\tif err != nil {","\t\treturn \"\", err","\t}","\treturn string(decoded), nil","}","","func (v *Provider) GetCommitInfo(_ context.Context, runevent *info.Event) error {","\tif v.giteaClient == nil {","\t\treturn fmt.Errorf(\"no gitea client has been initialized, \" +","\t\t\t\"exiting... (hint: did you forget setting a secret on your repo?)\")","\t}","","\tsha := runevent.SHA","\tif sha == \"\" \u0026\u0026 runevent.HeadBranch != \"\" {","\t\tbranchinfo, _, err := v.Client().GetRepoBranch(runevent.Organization, runevent.Repository, runevent.HeadBranch)","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\tsha = branchinfo.Commit.ID","\t} else if sha == \"\" \u0026\u0026 runevent.PullRequestNumber != 0 {","\t\tpr, _, err := v.Client().GetPullRequest(runevent.Organization, runevent.Repository, int64(runevent.PullRequestNumber))","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\trunevent.SHA = pr.Head.Sha","\t\trunevent.HeadBranch = pr.Head.Ref","\t\trunevent.BaseBranch = pr.Base.Ref","\t\tsha = pr.Head.Sha","\t}","\tcommit, _, err := v.Client().GetSingleCommit(runevent.Organization, runevent.Repository, sha)","\tif err != nil {","\t\treturn err","\t}","\trunevent.SHAURL = commit.HTMLURL","\trunevent.SHATitle = strings.Split(commit.RepoCommit.Message, \"\\n\\n\")[0]","\trunevent.SHA = commit.SHA","","\t// Populate full commit information for LLM context","\trunevent.SHAMessage = commit.RepoCommit.Message","\tif commit.RepoCommit.Author != nil {","\t\trunevent.SHAAuthorName = commit.RepoCommit.Author.Name","\t\trunevent.SHAAuthorEmail = commit.RepoCommit.Author.Email","\t\tif commit.RepoCommit.Author.Date != \"\" {","\t\t\tif authorDate, err := time.Parse(time.RFC3339, commit.RepoCommit.Author.Date); err == nil {","\t\t\t\trunevent.SHAAuthorDate = authorDate","\t\t\t}","\t\t}","\t}","\tif commit.RepoCommit.Committer != nil {","\t\trunevent.SHACommitterName = commit.RepoCommit.Committer.Name","\t\trunevent.SHACommitterEmail = commit.RepoCommit.Committer.Email","\t\tif commit.RepoCommit.Committer.Date != \"\" {","\t\t\tif committerDate, err := time.Parse(time.RFC3339, commit.RepoCommit.Committer.Date); err == nil {","\t\t\t\trunevent.SHACommitterDate = committerDate","\t\t\t}","\t\t}","\t}","\trunevent.HasSkipCommand = provider.SkipCI(commit.RepoCommit.Message)","","\treturn nil","}","","func ShouldGetNextPage(resp *forgejo.Response, currentPage int) (bool, int) {","\tval, exists := resp.Header[http.CanonicalHeaderKey(\"x-pagecount\")]","\tif !exists {","\t\treturn false, 0","\t}","\ti, err := strconv.Atoi(val[0])","\tif err != nil {","\t\treturn false, 0","\t}","\tif i \u003e= currentPage {","\t\treturn false, i","\t}","\treturn true, (currentPage + 1)","}","","type PushPayload struct {","\tCommits []forgejo.PayloadCommit `json:\"commits,omitempty\"`","}","","func (v *Provider) GetFiles(_ context.Context, runevent *info.Event) (changedfiles.ChangedFiles, error) {","\tchangedFiles := changedfiles.ChangedFiles{}","","\t//nolint:exhaustive // we don't need to handle all cases","\tswitch runevent.TriggerTarget {","\tcase triggertype.PullRequest, triggertype.PullRequestClosed:","\t\topt := forgejo.ListPullRequestFilesOptions{ListOptions: forgejo.ListOptions{Page: 1, PageSize: 50}}","\t\tshouldGetNextPage := false","\t\tfor {","\t\t\tprChangedFiles, resp, err := v.Client().ListPullRequestFiles(runevent.Organization, runevent.Repository, int64(runevent.PullRequestNumber), opt)","\t\t\tif err != nil {","\t\t\t\treturn changedfiles.ChangedFiles{}, err","\t\t\t}","\t\t\tfor j := range prChangedFiles {","\t\t\t\tchangedFiles.All = append(changedFiles.All, prChangedFiles[j].Filename)","\t\t\t\tif prChangedFiles[j].Status == \"added\" {","\t\t\t\t\tchangedFiles.Added = append(changedFiles.Added, prChangedFiles[j].Filename)","\t\t\t\t}","\t\t\t\tif prChangedFiles[j].Status == \"deleted\" {","\t\t\t\t\tchangedFiles.Deleted = append(changedFiles.Deleted, prChangedFiles[j].Filename)","\t\t\t\t}","\t\t\t\tif prChangedFiles[j].Status == \"changed\" {","\t\t\t\t\tchangedFiles.Modified = append(changedFiles.Modified, prChangedFiles[j].Filename)","\t\t\t\t}","\t\t\t\tif prChangedFiles[j].Status == \"renamed\" {","\t\t\t\t\tchangedFiles.Renamed = append(changedFiles.Renamed, prChangedFiles[j].Filename)","\t\t\t\t}","\t\t\t}","","\t\t\tshouldGetNextPage, opt.Page = ShouldGetNextPage(resp, opt.Page)","\t\t\tif !shouldGetNextPage {","\t\t\t\tbreak","\t\t\t}","\t\t}","\tcase triggertype.Push:","\t\tpushPayload := PushPayload{}","\t\terr := json.Unmarshal(runevent.Request.Payload, \u0026pushPayload)","\t\tif err != nil {","\t\t\tv.Logger.Errorf(\"failed to unmarshal the push payload to get changed files - %v\", err)","\t\t\treturn changedfiles.ChangedFiles{}, fmt.Errorf(\"failed to unmarshal the push payload to get changed files - %w\", err)","\t\t}","","\t\tfor _, commit := range pushPayload.Commits {","\t\t\tfor _, file := range commit.Added {","\t\t\t\tchangedFiles.All = append(changedFiles.All, file)","\t\t\t\tchangedFiles.Added = append(changedFiles.Added, file)","\t\t\t}","\t\t\tfor _, file := range commit.Modified {","\t\t\t\tchangedFiles.All = append(changedFiles.All, file)","\t\t\t\tchangedFiles.Modified = append(changedFiles.Modified, file)","\t\t\t}","\t\t\tfor _, file := range commit.Removed {","\t\t\t\tchangedFiles.All = append(changedFiles.All, file)","\t\t\t\tchangedFiles.Deleted = append(changedFiles.Deleted, file)","\t\t\t}","\t\t}","\tdefault:","\t\tv.Logger.Errorf(\"unable to get changed files. Unknown trigger type of '%s'. Expected pull_request or push\", runevent.TriggerTarget)","\t\treturn changedFiles, fmt.Errorf(\"unable to get changed files. Unknown trigger type of '%s'. Expected pull_request or push\", runevent.TriggerTarget)","\t}","","\treturn changedFiles, nil","}","","func (v *Provider) CreateToken(_ context.Context, _ []string, _ *info.Event) (string, error) {","\treturn \"\", nil","}","","func (v *Provider) GetTemplate(commentType provider.CommentType) string {","\treturn provider.GetHTMLTemplate(commentType)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,0,1,1,1,0,2,2,2,2,0,2,2,2,0,0,2,2,2,1,1,0,2,2,2,2,2,2,2,2,0,0,0,2,2,2,2,2,0,0,1,1,1,0,0,1,1,1,0,1,1,1,0,2,2,2,2,2,2,2,2,0,2,2,2,2,0,2,0,0,2,2,2,2,2,0,2,2,2,2,2,2,2,2,0,0,2,2,2,1,1,2,0,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,0,0,1,1,1,1,1,1,1,1,0,0,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,0,0,2,2,2,2,0,2,2,2,2,0,2,2,2,0,0,2,2,2,2,2,2,2,2,2,0,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,0,2,2,0,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,0,0,2,2,2,2,1,1,2,2,2,0,2,2,2,2,2,2,1,1,2,2,2,1,1,2,0,0,0,0,2,1,1,0,0,2,2,2,1,1,2,0,0,0,0,2,2,2,2,2,2,2,2,1,1,2,2,2,1,1,1,1,0,0,1,0,0,2,2,2,1,1,2,2,1,1,2,0,0,2,2,2,2,2,0,2,2,2,2,0,2,2,1,1,2,0,0,2,2,2,2,2,0,2,2,1,1,1,1,1,2,1,1,1,1,1,1,1,1,0,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,0,0,2,2,2,0,0,2,2,2,2,2,1,1,1,1,1,1,1,1,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,0,0,2,2,2,2,1,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,1,1,1,0,0,2,0,0,1,1,1,0,1,1,1]},{"id":112,"path":"pkg/provider/gitea/parse_payload.go","lines":["package gitea","","import (","\t\"context\"","\t\"encoding/json\"","\t\"fmt\"","\t\"net/http\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/opscomments\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/gitea/forgejostructs\"",")","","func (v *Provider) ParsePayload(_ context.Context, _ *params.Run, request *http.Request,","\tpayload string,",") (*info.Event, error) {","\t// TODO: parse request to figure out which event","\tvar processedEvent *info.Event","","\teventType := request.Header.Get(\"X-Gitea-Event-Type\")","\tif eventType == \"\" {","\t\treturn nil, fmt.Errorf(\"failed to find event type in request header\")","\t}","","\tpayloadB := []byte(payload)","\teventInt, err := parseWebhook(whEventType(eventType), payloadB)","\tif err != nil {","\t\treturn nil, err","\t}","\t_ = json.Unmarshal(payloadB, \u0026eventInt)","","\tswitch gitEvent := eventInt.(type) {","\tcase *forgejostructs.PullRequestPayload:","\t\tprocessedEvent = info.NewEvent()","\t\t// // Organization:  event.GetRepo().GetOwner().GetLogin(),","\t\tprocessedEvent.Sender = gitEvent.Sender.UserName","\t\tprocessedEvent.DefaultBranch = gitEvent.Repository.DefaultBranch","\t\tprocessedEvent.URL = gitEvent.Repository.HTMLURL","\t\tprocessedEvent.SHA = gitEvent.PullRequest.Head.Sha","\t\tprocessedEvent.SHAURL = fmt.Sprintf(\"%s/commit/%s\", gitEvent.PullRequest.HTMLURL, processedEvent.SHA)","\t\tprocessedEvent.HeadBranch = gitEvent.PullRequest.Head.Ref","\t\tprocessedEvent.BaseBranch = gitEvent.PullRequest.Base.Ref","\t\tprocessedEvent.HeadURL = gitEvent.PullRequest.Head.Repository.HTMLURL","\t\tprocessedEvent.BaseURL = gitEvent.PullRequest.Base.Repository.HTMLURL","\t\tprocessedEvent.PullRequestNumber = int(gitEvent.Index)","\t\tprocessedEvent.PullRequestTitle = gitEvent.PullRequest.Title","\t\tprocessedEvent.Organization = gitEvent.Repository.Owner.UserName","\t\tprocessedEvent.Repository = gitEvent.Repository.Name","\t\tprocessedEvent.TriggerTarget = triggertype.PullRequest","\t\tprocessedEvent.EventType = triggertype.PullRequest.String()","\t\tif provider.Valid(string(gitEvent.Action), []string{pullRequestLabelUpdated}) {","\t\t\tprocessedEvent.EventType = string(triggertype.PullRequestLabeled)","\t\t}","\t\tfor _, label := range gitEvent.PullRequest.Labels {","\t\t\tprocessedEvent.PullRequestLabel = append(processedEvent.PullRequestLabel, label.Name)","\t\t}","\t\tif gitEvent.Action == forgejostructs.HookIssueClosed {","\t\t\tprocessedEvent.TriggerTarget = triggertype.PullRequestClosed","\t\t}","\tcase *forgejostructs.PushPayload:","\t\tprocessedEvent = info.NewEvent()","\t\tprocessedEvent.SHA = gitEvent.HeadCommit.ID","\t\tif processedEvent.SHA == \"\" {","\t\t\tprocessedEvent.SHA = gitEvent.Before","\t\t}","\t\tprocessedEvent.SHAURL = gitEvent.HeadCommit.URL","\t\tprocessedEvent.SHATitle = gitEvent.HeadCommit.Message","\t\tprocessedEvent.Organization = gitEvent.Repo.Owner.UserName","\t\tprocessedEvent.Repository = gitEvent.Repo.Name","\t\tprocessedEvent.DefaultBranch = gitEvent.Repo.DefaultBranch","\t\tprocessedEvent.URL = gitEvent.Repo.HTMLURL","\t\tprocessedEvent.Sender = gitEvent.Sender.UserName","\t\tprocessedEvent.BaseBranch = gitEvent.Ref","\t\tprocessedEvent.EventType = eventType","\t\tprocessedEvent.HeadBranch = processedEvent.BaseBranch // in push events Head Branch is the same as Basebranch","\t\tprocessedEvent.BaseURL = gitEvent.Repo.HTMLURL","\t\tprocessedEvent.HeadURL = processedEvent.BaseURL // in push events Head URL is the same as BaseURL","\t\tprocessedEvent.TriggerTarget = \"push\"","\tcase *forgejostructs.IssueCommentPayload:","\t\tif gitEvent.Issue.PullRequest == nil {","\t\t\treturn info.NewEvent(), fmt.Errorf(\"issue comment is not coming from a pull_request\")","\t\t}","\t\tprocessedEvent = info.NewEvent()","\t\tprocessedEvent.Organization = gitEvent.Repository.Owner.UserName","\t\tprocessedEvent.Repository = gitEvent.Repository.Name","\t\tprocessedEvent.Sender = gitEvent.Sender.UserName","\t\tprocessedEvent.TriggerTarget = triggertype.PullRequest","\t\topscomments.SetEventTypeAndTargetPR(processedEvent, gitEvent.Comment.Body)","\t\tprocessedEvent.PullRequestNumber, err = convertPullRequestURLtoNumber(gitEvent.Issue.URL)","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\tprocessedEvent.URL = gitEvent.Repository.HTMLURL","\t\tprocessedEvent.DefaultBranch = gitEvent.Repository.DefaultBranch","\tdefault:","\t\treturn nil, fmt.Errorf(\"event %s is not supported\", eventType)","\t}","","\tprocessedEvent.Event = eventInt","\treturn processedEvent, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,0]},{"id":113,"path":"pkg/provider/gitea/webhook.go","lines":["package gitea","","import (","\t\"encoding/json\"","\t\"fmt\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/gitea/forgejostructs\"",")","","// whEventType represents a Gitea webhook event.","type whEventType string","","// List of supported events","//","// Find them in Gitea's source at /models/webhook.go as HookEventType.","// To correlate with when each of these trigger, see the Trigger On -\u003e Custom Events options","// when editing a repo's webhook in a Gitea project. Those descriptions are helpful.","const (","\tEventTypeCreate              whEventType = \"create\"","\tEventTypeDelete              whEventType = \"delete\"","\tEventTypeFork                whEventType = \"fork\"","\tEventTypePush                whEventType = \"push\"","\tEventTypeIssues              whEventType = \"issues\"","\tEventTypeIssueComment        whEventType = \"issue_comment\"","\tEventTypeRepository          whEventType = \"repository\"","\tEventTypeRelease             whEventType = \"release\"","\tEventTypePullRequest         whEventType = \"pull_request\"","\tEventTypePullRequestApproved whEventType = \"pull_request_approved\"","\tEventTypePullRequestRejected whEventType = \"pull_request_rejected\"","\tEventTypePullRequestLabel    whEventType = \"pull_request_label\"","\tEventTypePullRequestComment  whEventType = \"pull_request_comment\"","\tEventTypePullRequestSync     whEventType = \"pull_request_sync\"",")","","func parseWebhook(eventType whEventType, payload []byte) (event any, err error) {","\tswitch eventType {","\tcase EventTypePush:","\t\tevent = \u0026forgejostructs.PushPayload{}","\tcase EventTypeCreate:","\t\tevent = \u0026forgejostructs.CreatePayload{}","\tcase EventTypeDelete:","\t\tevent = \u0026forgejostructs.DeletePayload{}","\tcase EventTypeFork:","\t\tevent = \u0026forgejostructs.ForkPayload{}","\tcase EventTypeIssues:","\t\tevent = \u0026forgejostructs.IssuePayload{}","\tcase EventTypeIssueComment:","\t\tevent = \u0026forgejostructs.IssueCommentPayload{}","\tcase EventTypeRepository:","\t\tevent = \u0026forgejostructs.RepositoryPayload{}","\tcase EventTypeRelease:","\t\tevent = \u0026forgejostructs.ReleasePayload{}","\tcase EventTypePullRequestComment:","\t\tevent = \u0026forgejostructs.IssueCommentPayload{}","\tcase EventTypePullRequest, EventTypePullRequestApproved, EventTypePullRequestSync, EventTypePullRequestRejected, EventTypePullRequestLabel:","\t\tevent = \u0026forgejostructs.PullRequestPayload{}","\tdefault:","\t\treturn nil, fmt.Errorf(\"unexpected event type: %s\", eventType)","\t}","","\tif err := json.Unmarshal(payload, event); err != nil {","\t\treturn nil, err","\t}","","\treturn event, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,1,1,1,1,1,1,2,2,1,1,1,1,1,1,2,2,2,2,0,0,2,2,2,0,2,0]},{"id":114,"path":"pkg/provider/github/acl.go","lines":["package github","","import (","\t\"context\"","\t\"fmt\"","\t\"net/http\"","\t\"strings\"","","\t\"github.com/google/go-github/v81/github\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/acl\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/policy\"",")","","// CheckPolicyAllowing check that policy is allowing the event to be processed","// we  check the membership of the team allowed","// if the team is not found we explicitly disallow the policy, user have to correct the setting.","func (v *Provider) CheckPolicyAllowing(ctx context.Context, event *info.Event, allowedTeams []string) (bool, string) {","\tfor _, team := range allowedTeams {","\t\t// TODO: caching","\t\topt := github.ListOptions{PerPage: v.PaginedNumber}","\t\tfor {","\t\t\tmembers, resp, err := wrapAPI(v, \"list_team_members_by_slug\", func() ([]*github.User, *github.Response, error) {","\t\t\t\treturn v.Client().Teams.ListTeamMembersBySlug(ctx, event.Organization, team, \u0026github.TeamListTeamMembersOptions{ListOptions: opt})","\t\t\t})","\t\t\tif resp.StatusCode == http.StatusNotFound {","\t\t\t\t// we explicitly disallow the policy when the team is not found","\t\t\t\t// maybe we should ignore it instead? i'd rather keep this explicit","\t\t\t\t// and conservative since being security related.","\t\t\t\treturn false, fmt.Sprintf(\"team: %s is not found on the organization: %s\", team, event.Organization)","\t\t\t}","\t\t\tif err != nil {","\t\t\t\t// probably a 500 or another api error, no need to try again and again with other teams","\t\t\t\treturn false, fmt.Sprintf(\"error while getting team membership for user: %s in team: %s, error: %s\", event.Sender, team, err.Error())","\t\t\t}","\t\t\tfor _, member := range members {","\t\t\t\tif member.GetLogin() == event.Sender {","\t\t\t\t\treturn true, fmt.Sprintf(\"allowing user: %s as a member of the team: %s\", event.Sender, team)","\t\t\t\t}","\t\t\t}","\t\t\tif resp.NextPage == 0 {","\t\t\t\tbreak","\t\t\t}","\t\t\topt.Page = resp.NextPage","\t\t}","\t}","","\treturn false, fmt.Sprintf(\"user: %s is not a member of any of the allowed teams: %v\", event.Sender, allowedTeams)","}","","// IsAllowedOwnersFile get the owner files (OWNERS, OWNERS_ALIASES) from main branch","// and check if we have explicitly allowed the user in there.","func (v *Provider) IsAllowedOwnersFile(ctx context.Context, event *info.Event) (bool, error) {","\townerContent, err := v.getFileFromDefaultBranch(ctx, \"OWNERS\", event)","\tif err != nil {","\t\tif strings.Contains(err.Error(), \"cannot find\") {","\t\t\t// no owner file, skipping","\t\t\treturn false, nil","\t\t}","\t\treturn false, err","\t}","\t// If there is OWNERS file, check for OWNERS_ALIASES","\townerAliasesContent, err := v.getFileFromDefaultBranch(ctx, \"OWNERS_ALIASES\", event)","\tif err != nil {","\t\tif !strings.Contains(err.Error(), \"cannot find\") {","\t\t\treturn false, err","\t\t}","\t}","","\treturn acl.UserInOwnerFile(ownerContent, ownerAliasesContent, event.Sender)","}","","func (v *Provider) IsAllowed(ctx context.Context, event *info.Event) (bool, error) {","\taclPolicy := policy.Policy{","\t\tRepository:   v.repo,","\t\tEventEmitter: v.eventEmitter,","\t\tEvent:        event,","\t\tVCX:          v,","\t\tLogger:       v.Logger,","\t}","","\t// Try to detect a policy rule allowing this","\ttType, _ := v.detectTriggerTypeFromPayload(\"\", event.Event)","\tpolicyAllowed, policyReason := aclPolicy.IsAllowed(ctx, tType)","","\tswitch policyAllowed {","\tcase policy.ResultAllowed:","\t\treturn true, nil","\tcase policy.ResultDisallowed:","\t\treturn false, nil","\tcase policy.ResultNotSet: // this is to make golangci-lint happy","\t}","","\t// Check all the ACL rules","\tallowed, err := v.aclCheckAll(ctx, event)","\tif err != nil {","\t\treturn false, err","\t}","\tif allowed {","\t\treturn true, nil","\t}","","\t// Try to parse the comment from an owner who has issues a /ok-to-test","\townerAllowed, err := v.aclAllowedOkToTestFromAnOwner(ctx, event)","\tif err != nil {","\t\treturn false, err","\t}","\tif ownerAllowed {","\t\treturn true, nil","\t}","","\t// error with the policy reason if it was set","\tif policyReason != \"\" {","\t\treturn false, fmt.Errorf(\"%s\", policyReason)","\t}","","\t// finally silently return false if no rules allowed this","\treturn false, nil","}","","// allowedOkToTestFromAnOwner Go over comments in a pull request and check","// if there is a /ok-to-test in there running an aclCheck again on the comment","// Sender if she is an OWNER and then allow it to run CI.","// TODO: pull out the github logic from there in an agnostic way.","func (v *Provider) aclAllowedOkToTestFromAnOwner(ctx context.Context, event *info.Event) (bool, error) {","\trevent := info.NewEvent()","\tevent.DeepCopyInto(revent)","\trevent.EventType = \"\"","\trevent.TriggerTarget = \"\"","\tif revent.Event == nil {","\t\treturn false, nil","\t}","","\tswitch event := revent.Event.(type) {","\tcase *github.IssueCommentEvent:","\t\t// if we don't need to check old comments, then on issue comment we","\t\t// need to check if comment have /ok-to-test and is from allowed user","\t\tif !v.pacInfo.RememberOKToTest {","\t\t\treturn v.aclAllowedOkToTestCurrentComment(ctx, revent, event.Comment.GetID())","\t\t}","\t\trevent.URL = event.Issue.GetPullRequestLinks().GetHTMLURL()","\tcase *github.PullRequestEvent:","\t\t// if we don't need to check old comments, then on push event we don't need","\t\t// to check anything for the non-allowed user","\t\tif !v.pacInfo.RememberOKToTest {","\t\t\treturn false, nil","\t\t}","\t\trevent.URL = event.GetPullRequest().GetHTMLURL()","\tdefault:","\t\treturn false, nil","\t}","","\tcomments, err := v.GetStringPullRequestComment(ctx, revent, acl.OKToTestCommentRegexp)","\tif err != nil {","\t\treturn false, err","\t}","","\tfor _, comment := range comments {","\t\trevent.Sender = comment.User.GetLogin()","\t\tallowed, err := v.aclCheckAll(ctx, revent)","\t\tif err != nil {","\t\t\treturn false, err","\t\t}","\t\tif allowed {","\t\t\treturn true, nil","\t\t}","\t}","\treturn false, nil","}","","// aclAllowedOkToTestCurrentEvent only check if this is issue comment event","// have /ok-to-test regex and sender is allowed.","func (v *Provider) aclAllowedOkToTestCurrentComment(ctx context.Context, revent *info.Event, id int64) (bool, error) {","\tcomment, _, err := wrapAPI(v, \"get_issue_comment\", func() (*github.IssueComment, *github.Response, error) {","\t\treturn v.Client().Issues.GetComment(ctx, revent.Organization, revent.Repository, id)","\t})","\tif err != nil {","\t\treturn false, err","\t}","\tif acl.MatchRegexp(acl.OKToTestCommentRegexp, comment.GetBody()) {","\t\trevent.Sender = comment.User.GetLogin()","\t\tallowed, err := v.aclCheckAll(ctx, revent)","\t\tif err != nil {","\t\t\treturn false, err","\t\t}","\t\tif allowed {","\t\t\treturn true, nil","\t\t}","\t}","\treturn false, nil","}","","// aclCheck check if we are allowed to run the pipeline on that PR.","func (v *Provider) aclCheckAll(ctx context.Context, rev *info.Event) (bool, error) {","\t// if the sender own the repo, then allow it to run","\tif rev.Organization == rev.Sender {","\t\treturn true, nil","\t}","","\t// If the user who has submitted the PR is not a owner or public member or Collaborator or not there in OWNERS file","\t// but has permission to push to branches then allow the CI to be run.","\t// This can only happen with GithubApp and Bots.","\t// Ex: dependabot, bots","\tif rev.PullRequestNumber != 0 {","\t\tisSameCloneURL, err := v.checkPullRequestForSameURL(ctx, rev)","\t\tif err != nil {","\t\t\treturn false, err","\t\t}","\t\tif isSameCloneURL {","\t\t\treturn true, nil","\t\t}","\t}","","\t// If the user who has submitted the pr is a owner on the repo then allows","\t// the CI to be run.","\tisUserMemberRepo, err := v.checkSenderOrgMembership(ctx, rev)","\tif err != nil {","\t\treturn false, err","\t}","\tif isUserMemberRepo {","\t\treturn true, nil","\t}","","\tcheckSenderRepoMembership, err := v.checkSenderRepoMembership(ctx, rev)","\tif err != nil {","\t\treturn false, err","\t}","\tif checkSenderRepoMembership {","\t\treturn true, nil","\t}","","\t// If we have a prow OWNERS file in the defaultBranch (ie: master) then","\t// parse it in approvers and reviewers field and check if sender is in there.","\treturn v.IsAllowedOwnersFile(ctx, rev)","}","","// checkPullRequestForSameURL checks If PullRequests are for same clone URL and different branches","// means if the user has access to create a branch in the repository without forking or having any permissions then PAC should allow to run CI.","//","//\tex: dependabot, *[bot] etc...","func (v *Provider) checkPullRequestForSameURL(ctx context.Context, runevent *info.Event) (bool, error) {","\tpr, resp, err := wrapAPI(v, \"get_pull_request\", func() (*github.PullRequest, *github.Response, error) {","\t\treturn v.Client().PullRequests.Get(ctx, runevent.Organization, runevent.Repository, runevent.PullRequestNumber)","\t})","\tif err != nil {","\t\treturn false, err","\t}","\tif resp != nil \u0026\u0026 resp.StatusCode == http.StatusNotFound {","\t\treturn false, nil","\t}","","\tif pr.GetHead().GetRepo().GetCloneURL() == pr.GetBase().GetRepo().GetCloneURL() \u0026\u0026 pr.GetHead().GetRef() != pr.GetBase().GetRef() {","\t\treturn true, nil","\t}","","\treturn false, nil","}","","// checkSenderOrgMembership Get sender user's organization. We can","// only get the one that the user sets as public ðŸ¤·.","func (v *Provider) checkSenderOrgMembership(ctx context.Context, runevent *info.Event) (bool, error) {","\topt := \u0026github.ListMembersOptions{","\t\tListOptions: github.ListOptions{PerPage: v.PaginedNumber},","\t}","","\tfor {","\t\tusers, resp, err := wrapAPI(v, \"list_org_members\", func() ([]*github.User, *github.Response, error) {","\t\t\treturn v.Client().Organizations.ListMembers(ctx, runevent.Organization, opt)","\t\t})","\t\t// If we are 404 it means we are checking a repo owner and not a org so let's bail out with grace","\t\tif resp != nil \u0026\u0026 resp.StatusCode == http.StatusNotFound {","\t\t\treturn false, nil","\t\t}","","\t\tif err != nil {","\t\t\treturn false, err","\t\t}","\t\tfor _, v := range users {","\t\t\tif v.GetLogin() == runevent.Sender {","\t\t\t\treturn true, nil","\t\t\t}","\t\t}","\t\tif resp.NextPage == 0 {","\t\t\tbreak","\t\t}","\t\topt.Page = resp.NextPage","\t}","\treturn false, nil","}","","// checkSenderRepoMembership check if user is allowed to run CI.","func (v *Provider) checkSenderRepoMembership(ctx context.Context, runevent *info.Event) (bool, error) {","\tisCollab, _, err := wrapAPI(v, \"is_collaborator\", func() (bool, *github.Response, error) {","\t\treturn v.Client().Repositories.IsCollaborator(ctx,","\t\t\trunevent.Organization,","\t\t\trunevent.Repository,","\t\t\trunevent.Sender)","\t})","","\treturn isCollab, err","}","","// getFileFromDefaultBranch will get a file directly from the Default BaseBranch as","// configured in runinfo which is directly set in webhook by Github.","func (v *Provider) getFileFromDefaultBranch(ctx context.Context, path string, runevent *info.Event) (string, error) {","\ttektonyaml, err := v.GetFileInsideRepo(ctx, runevent, path, runevent.DefaultBranch)","\tif err != nil {","\t\treturn \"\", fmt.Errorf(\"cannot find %s inside the %s branch: %w\", path, runevent.DefaultBranch, err)","\t}","\treturn tektonyaml, err","}","","// GetStringPullRequestComment return the comment if we find a regexp in one of","// the comments text of a pull request.","func (v *Provider) GetStringPullRequestComment(ctx context.Context, runevent *info.Event, reg string) ([]*github.IssueComment, error) {","\tvar ret []*github.IssueComment","\tprNumber, err := convertPullRequestURLtoNumber(runevent.URL)","\tif err != nil {","\t\treturn nil, err","\t}","","\topt := \u0026github.IssueListCommentsOptions{","\t\tListOptions: github.ListOptions{PerPage: v.PaginedNumber},","\t}","\tfor {","\t\tcomments, resp, err := wrapAPI(v, \"list_issue_comments\", func() ([]*github.IssueComment, *github.Response, error) {","\t\t\treturn v.Client().Issues.ListComments(ctx, runevent.Organization, runevent.Repository,","\t\t\t\tprNumber, opt)","\t\t})","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\t\tfor _, v := range comments {","\t\t\tif acl.MatchRegexp(reg, v.GetBody()) {","\t\t\t\tret = append(ret, v)","\t\t\t}","\t\t}","\t\tif resp.NextPage == 0 {","\t\t\tbreak","\t\t}","\t\topt.Page = resp.NextPage","\t}","\treturn ret, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,0,2,0,0,0,2,0,0,0,0,2,2,2,2,2,2,2,1,0,0,2,2,2,1,1,0,0,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,0,0,0,0,2,2,1,1,2,1,1,0,0,2,2,1,1,2,2,2,0,0,2,1,1,0,0,2,0,0,0,0,0,0,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,1,1,0,2,2,2,2,1,1,2,2,2,0,2,0,0,0,0,2,2,2,2,2,1,1,2,2,2,2,1,1,2,2,2,0,2,0,0,0,2,2,2,2,2,0,0,0,0,0,2,2,2,2,2,2,2,2,0,0,0,0,2,2,2,2,2,2,2,0,2,2,1,1,2,2,2,0,0,0,2,0,0,0,0,0,0,2,2,2,2,2,2,2,2,1,1,0,2,2,2,0,2,0,0,0,0,2,2,2,2,2,2,2,2,2,0,2,2,2,0,2,2,2,2,2,2,2,0,2,2,0,2,0,2,0,0,0,2,2,2,2,2,2,2,0,2,0,0,0,0,2,2,2,2,2,2,0,0,0,0,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,1,1,2,2,2,2,0,2,2,0,2,0,2,0]},{"id":115,"path":"pkg/provider/github/app/token.go","lines":["package app","","import (","\t\"context\"","\t\"fmt\"","\t\"net/http\"","\t\"net/url\"","\t\"strings\"","\t\"time\"","","\t\"github.com/golang-jwt/jwt/v4\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/github\"","\t\"knative.dev/pkg/logging\"",")","","type Install struct {","\trequest   *http.Request","\trun       *params.Run","\trepo      *v1alpha1.Repository","\tghClient  *github.Provider","\tnamespace string","}","","func NewInstallation(req *http.Request, run *params.Run, repo *v1alpha1.Repository, gh *github.Provider, namespace string) *Install {","\tif req == nil {","\t\treq = \u0026http.Request{}","\t}","\treturn \u0026Install{","\t\trequest:   req,","\t\trun:       run,","\t\trepo:      repo,","\t\tghClient:  gh,","\t\tnamespace: namespace,","\t}","}","","// GetAndUpdateInstallationID retrieves and updates the installation ID for the GitHub App.","// It generates a JWT token, and directly fetches the installation for the","// repository.","func (ip *Install) GetAndUpdateInstallationID(ctx context.Context) (string, string, int64, error) {","\tlogger := logging.FromContext(ctx)","","\t// Generate a JWT token for authentication","\tjwtToken, err := ip.GenerateJWT(ctx)","\tif err != nil {","\t\treturn \"\", \"\", 0, err","\t}","","\t// Get owner and repo from the repository URL","\trepoURL, err := url.Parse(ip.repo.Spec.URL)","\tif err != nil {","\t\treturn \"\", \"\", 0, fmt.Errorf(\"failed to parse repository URL: %w\", err)","\t}","\tpathParts := strings.Split(strings.Trim(repoURL.Path, \"/\"), \"/\")","\tif len(pathParts) != 2 {","\t\treturn \"\", \"\", 0, fmt.Errorf(\"invalid repository URL path: %s\", repoURL.Path)","\t}","\towner := pathParts[0]","\trepoName := pathParts[1]","\tif owner == \"\" || repoName == \"\" {","\t\treturn \"\", \"\", 0, fmt.Errorf(\"invalid repository URL: owner or repo name is empty\")","\t}","","\tif ip.ghClient.APIURL == nil {","\t\treturn \"\", \"\", 0, fmt.Errorf(\"github client APIURL is nil\")","\t}","\tapiURL := *ip.ghClient.APIURL","\tenterpriseHost := ip.request.Header.Get(\"X-GitHub-Enterprise-Host\")","\tif enterpriseHost != \"\" {","\t\tapiURL = fmt.Sprintf(\"https://%s/api/v3\", strings.TrimSuffix(enterpriseHost, \"/\"))","\t}","","\tclient, _, _ := github.MakeClient(ctx, apiURL, jwtToken)","\t// Directly get the installation for the repository","\tinstallation, _, err := client.Apps.FindRepositoryInstallation(ctx, owner, repoName)","\tif err != nil {","\t\t// Fallback to finding organization installation if repository installation is not found","\t\tinstallation, _, err = client.Apps.FindOrganizationInstallation(ctx, owner)","\t\tif err != nil {","\t\t\t// Fallback to finding user installation if organization installation is not found","\t\t\tinstallation, _, err = client.Apps.FindUserInstallation(ctx, owner)","\t\t}","\t}","","\tif err != nil {","\t\treturn \"\", \"\", 0, fmt.Errorf(\"could not find repository, organization or user installation for %s/%s: %w\", owner, repoName, err)","\t}","","\tif installation.ID == nil {","\t\treturn \"\", \"\", 0, fmt.Errorf(\"github App installation found but contained no ID. This is likely a bug\")","\t}","","\tinstallationID := *installation.ID","\ttoken, err := ip.ghClient.GetAppToken(ctx, ip.run.Clients.Kube, enterpriseHost, installationID, ip.namespace)","\tif err != nil {","\t\tlogger.Warnf(\"Could not get a token for installation ID %d: %v\", installationID, err)","\t\t// Return with the installation ID even if token generation fails,","\t\t// as some operations might only need the ID.","\t\treturn enterpriseHost, \"\", installationID, nil","\t}","","\treturn enterpriseHost, token, installationID, nil","}","","// JWTClaim represents the JWT claims for the GitHub App.","type JWTClaim struct {","\tIssuer int64 `json:\"iss\"`","\tjwt.RegisteredClaims","}","","// GenerateJWT generates a JWT token for the GitHub App.","// It retrieves the application ID and private key, sets the claims, and signs the token.","func (ip *Install) GenerateJWT(ctx context.Context) (string, error) {","\t// TODO: move this out of here","\tgh := github.New()","\tgh.Run = ip.run","\tapplicationID, privateKey, err := gh.GetAppIDAndPrivateKey(ctx, ip.namespace, ip.run.Clients.Kube)","\tif err != nil {","\t\treturn \"\", err","\t}","","\t// The expirationTime claim identifies the expiration time on or after which the JWT MUST NOT be accepted for processing.","\t// Value cannot be longer duration.","\t// See https://datatracker.ietf.org/doc/html/rfc7519#section-4.1.4","\texpirationTime := time.Now().Add(5 * time.Minute)","\tclaims := \u0026JWTClaim{","\t\tIssuer: applicationID,","\t\tRegisteredClaims: jwt.RegisteredClaims{","\t\t\tExpiresAt: jwt.NewNumericDate(expirationTime),","\t\t\tIssuedAt:  jwt.NewNumericDate(time.Now()),","\t\t},","\t}","\ttoken := jwt.NewWithClaims(jwt.SigningMethodRS256, claims)","","\tparsedPK, err := jwt.ParseRSAPrivateKeyFromPEM(privateKey)","\tif err != nil {","\t\treturn \"\", fmt.Errorf(\"failed to parse private key: %w\", err)","\t}","","\ttokenString, err := token.SignedString(parsedPK)","\tif err != nil {","\t\treturn \"\", fmt.Errorf(\"failed to sign private key: %w\", err)","\t}","\treturn tokenString, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,1,1,2,2,2,2,2,2,2,0,0,0,0,0,2,2,2,2,2,2,1,1,0,0,2,2,1,1,2,2,2,2,2,2,2,1,1,0,2,1,1,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,0,2,1,1,0,2,2,2,1,1,1,1,1,0,2,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,1,1,2,0]},{"id":116,"path":"pkg/provider/github/detect.go","lines":["package github","","import (","\t\"encoding/json\"","\t\"fmt\"","\t\"net/http\"","","\t\"github.com/google/go-github/v81/github\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"go.uber.org/zap\"",")","","var (","\tpullRequestOpenSyncEvent = []string{\"opened\", \"synchronize\", \"synchronized\", \"reopened\", \"ready_for_review\"}","\tpullRequestLabelEvent    = []string{\"labeled\"}",")","","// Detect processes event and detect if it is a github event, whether to process or reject it","// returns (if is a GH event, whether to process or reject, error if any occurred).","func (v *Provider) Detect(req *http.Request, payload string, logger *zap.SugaredLogger) (bool, bool, *zap.SugaredLogger, string, error) {","\t// gitea set x-github-event too, so skip it for the gitea driver","\tif h := req.Header.Get(\"X-Gitea-Event-Type\"); h != \"\" {","\t\treturn false, false, logger, \"\", nil","\t}","\tisGH := false","\teventType := req.Header.Get(\"X-Github-Event\")","\tif eventType == \"\" {","\t\treturn false, false, logger, \"\", nil","\t}","","\t// it is a Github event","\tisGH = true","","\tsetLoggerAndProceed := func(processEvent bool, reason string, err error) (bool, bool, *zap.SugaredLogger,","\t\tstring, error,","\t) {","\t\tlogger = logger.With(\"provider\", \"github\", \"event-id\", req.Header.Get(\"X-GitHub-Delivery\"))","\t\treturn isGH, processEvent, logger, reason, err","\t}","","\teventInt, err := github.ParseWebHook(eventType, []byte(payload))","\tif err != nil {","\t\treturn setLoggerAndProceed(false, \"\", err)","\t}","","\t_ = json.Unmarshal([]byte(payload), \u0026eventInt)","\teType, errReason := v.detectTriggerTypeFromPayload(eventType, eventInt)","\tif eType != \"\" {","\t\treturn setLoggerAndProceed(true, \"\", nil)","\t}","","\treturn setLoggerAndProceed(false, errReason, nil)","}","","// detectTriggerTypeFromPayload will detect the event type from the payload,","// filtering out the events that are not supported.","// first arg will get the event type and the second one will get an error string explaining why it's not supported.","func (v *Provider) detectTriggerTypeFromPayload(ghEventType string, eventInt any) (triggertype.Trigger, string) {","\tswitch event := eventInt.(type) {","\tcase *github.PushEvent:","\t\tif event.GetPusher() != nil {","\t\t\treturn triggertype.Push, \"\"","\t\t}","\t\treturn \"\", \"no pusher in payload\"","\tcase *github.PullRequestEvent:","\t\tif event.GetAction() == \"closed\" {","\t\t\treturn triggertype.PullRequestClosed, \"\"","\t\t}","","\t\tif provider.Valid(event.GetAction(), pullRequestOpenSyncEvent) || provider.Valid(event.GetAction(), pullRequestLabelEvent) {","\t\t\treturn triggertype.PullRequest, \"\"","\t\t}","\t\treturn \"\", fmt.Sprintf(\"pull_request: unsupported action \\\"%s\\\"\", event.GetAction())","\tcase *github.IssueCommentEvent:","\t\tif event.GetAction() == \"created\" \u0026\u0026","\t\t\tevent.GetIssue().IsPullRequest() \u0026\u0026","\t\t\tevent.GetIssue().GetState() == \"open\" {","\t\t\tif provider.IsTestRetestComment(event.GetComment().GetBody()) {","\t\t\t\treturn triggertype.Retest, \"\"","\t\t\t}","\t\t\tif provider.IsOkToTestComment(event.GetComment().GetBody()) {","\t\t\t\treturn triggertype.OkToTest, \"\"","\t\t\t}","\t\t\tif provider.IsCancelComment(event.GetComment().GetBody()) {","\t\t\t\treturn triggertype.Cancel, \"\"","\t\t\t}","\t\t}","\t\treturn triggertype.Comment, \"\"","\tcase *github.CheckSuiteEvent:","\t\tif event.GetAction() == \"rerequested\" \u0026\u0026 event.GetCheckSuite() != nil {","\t\t\treturn triggertype.CheckSuiteRerequested, \"\"","\t\t}","\t\treturn \"\", fmt.Sprintf(\"check_suite: unsupported action \\\"%s\\\"\", event.GetAction())","\tcase *github.CheckRunEvent:","\t\tif event.GetAction() == \"rerequested\" \u0026\u0026 event.GetCheckRun() != nil {","\t\t\treturn triggertype.CheckRunRerequested, \"\"","\t\t}","\t\treturn \"\", fmt.Sprintf(\"check_run: unsupported action \\\"%s\\\"\", event.GetAction())","\tcase *github.CommitCommentEvent:","\t\tif event.GetAction() == \"created\" {","\t\t\tif provider.IsTestRetestComment(event.GetComment().GetBody()) {","\t\t\t\treturn triggertype.Retest, \"\"","\t\t\t}","\t\t\tif provider.IsCancelComment(event.GetComment().GetBody()) {","\t\t\t\treturn triggertype.Cancel, \"\"","\t\t\t}","\t\t\t// Here, the `/ok-to-test` command is ignored because it is intended for pull requests.","\t\t\t// For unauthorized users, it has no relevance to pushed commits, as only authorized users","\t\t\t// are allowed to run CI on pushed commits. Therefore, the `ok-to-test` command holds no significance in this context.","\t\t\t// However, it is left to be processed by the `on-comment` annotation rather than returning an error.","\t\t}","\t\treturn triggertype.Comment, \"\"","\t}","\treturn \"\", fmt.Sprintf(\"github: event \\\"%v\\\" is not supported\", ghEventType)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,1,1,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,0,2,2,2,2,0,2,2,2,2,2,0,2,0,0,0,0,0,2,2,2,2,2,2,1,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,1,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,2,0,2,0]},{"id":117,"path":"pkg/provider/github/github.go","lines":["package github","","import (","\t\"context\"","\t\"crypto/sha256\"","\t\"encoding/base64\"","\t\"encoding/hex\"","\t\"fmt\"","\t\"math/rand\"","\t\"net/http\"","\t\"net/url\"","\t\"regexp\"","\t\"strings\"","\t\"sync\"","\t\"time\"","","\t\"github.com/gobwas/glob\"","\t\"github.com/google/go-github/v81/github\"","\t\"github.com/jonboulle/clockwork\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/changedfiles\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/events\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"go.uber.org/zap\"","\t\"golang.org/x/oauth2\"","\t\"k8s.io/client-go/kubernetes\"",")","","const (","\tapiPublicURL = \"https://api.github.com/\"","\t// TODO: makes this configurable for GHE in the ConfigMap.","\t// on our GHE instance, it looks like this :","\t// https://raw.ghe.openshiftpipelines.com/pac/chmouel-test/main/README.md","\t// we can perhaps do some autodetection with event.Provider.GHEURL and adding","\t// a raw into it.","\tpublicRawURLHost = \"raw.githubusercontent.com\"","","\tdefaultPaginedNumber = 100",")","","var _ provider.Interface = (*Provider)(nil)","","type Provider struct {","\tghClient      *github.Client","\tLogger        *zap.SugaredLogger","\tRun           *params.Run","\tpacInfo       *info.PacOpts","\tToken, APIURL *string","\tApplicationID *int64","\tproviderName  string","\tprovenance    string","\tRepositoryIDs []int64","\trepo          *v1alpha1.Repository","\teventEmitter  *events.EventEmitter","\tPaginedNumber int","\tuserType      string // The type of user i.e bot or not","\tskippedRun","\ttriggerEvent       string","\tcachedChangedFiles *changedfiles.ChangedFiles","\tcommitInfo         *github.Commit","}","","type skippedRun struct {","\tmutex      *sync.Mutex","\tcheckRunID int64","}","","func New() *Provider {","\treturn \u0026Provider{","\t\tAPIURL:        github.Ptr(keys.PublicGithubAPIURL),","\t\tPaginedNumber: defaultPaginedNumber,","\t\tskippedRun: skippedRun{","\t\t\tmutex: \u0026sync.Mutex{},","\t\t},","\t}","}","","func (v *Provider) Client() *github.Client {","\treturn v.ghClient","}","","func (v *Provider) SetGithubClient(client *github.Client) {","\tv.ghClient = client","}","","func (v *Provider) SetPacInfo(pacInfo *info.PacOpts) {","\tv.pacInfo = pacInfo","}","","// detectGHERawURL Detect if we have a raw URL in GHE.","func detectGHERawURL(event *info.Event, taskHost string) bool {","\tgheURL, err := url.Parse(event.GHEURL)","\tif err != nil {","\t\t// should not happen but may as well make sure","\t\treturn false","\t}","\treturn taskHost == fmt.Sprintf(\"raw.%s\", gheURL.Host)","}","","// splitGithubURL Take a Github url and split it with org/repo path ref, supports rawURL.","func splitGithubURL(event *info.Event, uri string) (string, string, string, string, error) {","\tpURL, err := url.Parse(uri)","\tif err != nil {","\t\treturn \"\", \"\", \"\", \"\", fmt.Errorf(\"URL %s is not a valid provider URL: %w\", uri, err)","\t}","\tpath := pURL.Path","\tif pURL.RawPath != \"\" {","\t\tpath = pURL.RawPath","\t}","\tsplit := strings.Split(path, \"/\")","\tif len(split) \u003c= 3 {","\t\treturn \"\", \"\", \"\", \"\", fmt.Errorf(\"URL %s does not seem to be a proper provider url: %w\", uri, err)","\t}","\tvar spOrg, spRepo, spRef, spPath string","\tswitch {","\tcase (pURL.Host == publicRawURLHost || detectGHERawURL(event, pURL.Host)) \u0026\u0026 len(split) \u003e= 5:","\t\tspOrg = split[1]","\t\tspRepo = split[2]","\t\tspRef = split[3]","\t\tspPath = strings.Join(split[4:], \"/\")","\tcase split[3] == \"blob\" \u0026\u0026 len(split) \u003e= 5:","\t\tspOrg = split[1]","\t\tspRepo = split[2]","\t\tspRef = split[4]","\t\tspPath = strings.Join(split[5:], \"/\")","\tdefault:","\t\treturn \"\", \"\", \"\", \"\", fmt.Errorf(\"cannot recognize task as a GitHub URL to fetch: %s\", uri)","\t}","\t// url decode the org, repo, ref and path","\tif spRef, err = url.QueryUnescape(spRef); err != nil {","\t\treturn \"\", \"\", \"\", \"\", fmt.Errorf(\"cannot decode ref: %w\", err)","\t}","\tif spPath, err = url.QueryUnescape(spPath); err != nil {","\t\treturn \"\", \"\", \"\", \"\", fmt.Errorf(\"cannot decode path: %w\", err)","\t}","\tif spOrg, err = url.QueryUnescape(spOrg); err != nil {","\t\treturn \"\", \"\", \"\", \"\", fmt.Errorf(\"cannot decode org: %w\", err)","\t}","\tif spRepo, err = url.QueryUnescape(spRepo); err != nil {","\t\treturn \"\", \"\", \"\", \"\", fmt.Errorf(\"cannot decode repo: %w\", err)","\t}","\treturn spOrg, spRepo, spPath, spRef, nil","}","","func (v *Provider) GetTaskURI(ctx context.Context, event *info.Event, uri string) (bool, string, error) {","\tif ret := provider.CompareHostOfURLS(uri, event.URL); !ret {","\t\treturn false, \"\", nil","\t}","","\tspOrg, spRepo, spPath, spRef, err := splitGithubURL(event, uri)","\tif err != nil {","\t\treturn false, \"\", err","\t}","\tnEvent := info.NewEvent()","\tnEvent.Organization = spOrg","\tnEvent.Repository = spRepo","\tnEvent.BaseBranch = spRef","\tret, err := v.GetFileInsideRepo(ctx, nEvent, spPath, spRef)","\tif err != nil {","\t\treturn false, \"\", err","\t}","\treturn true, ret, nil","}","","func (v *Provider) InitAppClient(ctx context.Context, kube kubernetes.Interface, event *info.Event) error {","\tvar err error","\t// TODO: move this out of here when we move al config inside context","\tns := info.GetNS(ctx)","\tevent.Provider.Token, err = v.GetAppToken(ctx, kube, event.GHEURL, event.InstallationID, ns)","\tif err != nil {","\t\treturn err","\t}","","\treturn nil","}","","func (v *Provider) SetLogger(logger *zap.SugaredLogger) {","\tv.Logger = logger","}","","func (v *Provider) Validate(_ context.Context, _ *params.Run, event *info.Event) error {","\tsignature := event.Request.Header.Get(github.SHA256SignatureHeader)","","\tif signature == \"\" {","\t\tsignature = event.Request.Header.Get(github.SHA1SignatureHeader)","\t}","\tif signature == \"\" || signature == \"sha1=\" {","\t\t// if no signature is present then don't validate, because user hasn't set one","\t\treturn fmt.Errorf(\"no signature has been detected, for security reason we are not allowing webhooks that has no secret\")","\t}","\tif event.Provider.WebhookSecret == \"\" {","\t\treturn fmt.Errorf(\"no webhook secret has been set, in repository CR or secret\")","\t}","\treturn github.ValidateSignature(signature, event.Request.Payload, []byte(event.Provider.WebhookSecret))","}","","func (v *Provider) GetConfig() *info.ProviderConfig {","\treturn \u0026info.ProviderConfig{","\t\tTaskStatusTMPL: taskStatusTemplate,","\t\tAPIURL:         apiPublicURL,","\t\tName:           v.providerName,","\t}","}","","func MakeClient(ctx context.Context, apiURL, token string) (*github.Client, string, *string) {","\tvar client *github.Client","\tts := oauth2.StaticTokenSource(","\t\t\u0026oauth2.Token{AccessToken: token},","\t)","","\ttc := oauth2.NewClient(ctx, ts)","\tif apiURL != \"\" {","\t\tif !strings.HasPrefix(apiURL, \"https\") \u0026\u0026 !strings.HasPrefix(apiURL, \"http\") {","\t\t\tapiURL = \"https://\" + apiURL","\t\t}","\t}","","\tproviderName := \"github\"","\tif apiURL != \"\" \u0026\u0026 apiURL != apiPublicURL {","\t\tproviderName = \"github-enterprise\"","\t\tuploadURL := apiURL + \"/api/uploads\"","\t\tclient, _ = github.NewClient(tc).WithEnterpriseURLs(apiURL, uploadURL)","\t} else {","\t\tclient = github.NewClient(tc)","\t\tapiURL = client.BaseURL.String()","\t}","","\treturn client, providerName, github.Ptr(apiURL)","}","","func parseTS(headerTS string) (time.Time, error) {","\tts := time.Time{}","\t// Normal UTC: 2023-01-31 23:00:00 UTC","\tif t, err := time.Parse(\"2006-01-02 15:04:05 MST\", headerTS); err == nil {","\t\tts = t","\t}","","\t// With TZ(???), ie: a token from Christoph 2023-04-26 23:23:26 +2000","\tif t, err := time.Parse(\"2006-01-02 15:04:05 -0700\", headerTS); err == nil {","\t\tts = t","\t}","\tif ts.Year() == 1 {","\t\treturn ts, fmt.Errorf(\"cannot parse token expiration date: %s\", headerTS)","\t}","","\treturn ts, nil","}","","// checkWebhookSecretValidity check the webhook secret is valid and not","// ratelimited. we try to check first the header is set (unlimited life token  would","// not have an expiration) we would anyway get a 401 error when trying to use it","// but this gives a nice hint to the user into their namespace event of where","// the issue was.","func (v *Provider) checkWebhookSecretValidity(ctx context.Context, cw clockwork.Clock) error {","\trl, resp, err := wrapAPI(v, \"check_rate_limit\", func() (*github.RateLimits, *github.Response, error) {","\t\treturn v.Client().RateLimit.Get(ctx)","\t})","\tif resp != nil \u0026\u0026 resp.StatusCode == http.StatusNotFound {","\t\tv.Logger.Info(\"skipping checking if token has expired, rate_limit api is not enabled on token\")","\t\treturn nil","\t}","","\tif err != nil {","\t\treturn fmt.Errorf(\"error making request to the GitHub API checking rate limit: %w\", err)","\t}","","\tif resp != nil \u0026\u0026 resp.Header.Get(\"GitHub-Authentication-Token-Expiration\") != \"\" {","\t\tts, err := parseTS(resp.Header.Get(\"GitHub-Authentication-Token-Expiration\"))","\t\tif err != nil {","\t\t\treturn fmt.Errorf(\"error parsing token expiration date: %w\", err)","\t\t}","","\t\tif cw.Now().After(ts) {","\t\t\terrMsg := fmt.Sprintf(\"token has expired at %s\", resp.TokenExpiration.Format(time.RFC1123))","\t\t\treturn fmt.Errorf(\"%s\", errMsg)","\t\t}","\t}","","\t// Guard against nil rl or rl.SCIM which could lead to a panic.","\tif rl == nil || rl.SCIM == nil {","\t\tv.Logger.Info(\"skipping token expiration check, SCIM rate limit API is not available for this token\")","\t\treturn nil","\t}","","\tif rl.SCIM.Remaining == 0 {","\t\treturn fmt.Errorf(\"api rate limit exceeded. Access will be restored at %s\", rl.SCIM.Reset.Format(time.RFC1123))","\t}","\treturn nil","}","","func (v *Provider) SetClient(ctx context.Context, run *params.Run, event *info.Event, repo *v1alpha1.Repository, eventsEmitter *events.EventEmitter) error {","\tclient, providerName, apiURL := MakeClient(ctx, event.Provider.URL, event.Provider.Token)","\tv.providerName = providerName","\tv.Run = run","\tv.repo = repo","\tv.eventEmitter = eventsEmitter","\tv.triggerEvent = event.EventType","","\t// check that the Client is not already set, so we don't override our fakeclient","\t// from unittesting.","\tif v.ghClient == nil {","\t\tv.ghClient = client","\t}","\tif v.ghClient == nil {","\t\treturn fmt.Errorf(\"no github client has been initialized\")","\t}","","\t// Added log for security audit purposes to log client access when a token is used","\tintegration := \"github-webhook\"","\tif event.InstallationID != 0 {","\t\tintegration = \"github-app\"","\t}","\trun.Clients.Log.Infof(integration+\": initialized OAuth2 client for providerName=%s providerURL=%s\", v.providerName, event.Provider.URL)","","\tv.APIURL = apiURL","","\tif event.Provider.WebhookSecretFromRepo {","\t\t// check the webhook secret is valid and not ratelimited","\t\tif err := v.checkWebhookSecretValidity(ctx, clockwork.NewRealClock()); err != nil {","\t\t\treturn fmt.Errorf(\"the webhook secret is not valid: %w\", err)","\t\t}","\t}","","\treturn nil","}","","// GetTektonDir retrieves all YAML files from the .tekton directory and returns them as a single concatenated multi-document YAML file.","func (v *Provider) GetTektonDir(ctx context.Context, runevent *info.Event, path, provenance string) (string, error) {","\ttektonDirSha := \"\"","","\tv.provenance = provenance","\t// default set provenance from the SHA","\trevision := runevent.SHA","\tif provenance == \"default_branch\" {","\t\trevision = runevent.DefaultBranch","\t\tv.Logger.Infof(\"Using PipelineRun definition from default_branch: %s\", runevent.DefaultBranch)","\t} else {","\t\tprInfo := \"\"","\t\tif runevent.TriggerTarget == triggertype.PullRequest {","\t\t\tprInfo = fmt.Sprintf(\"%s/%s#%d\", runevent.Organization, runevent.Repository, runevent.PullRequestNumber)","\t\t}","\t\tv.Logger.Infof(\"Using PipelineRun definition from source %s %s on commit SHA %s\", runevent.TriggerTarget.String(), prInfo, runevent.SHA)","\t}","","\trootobjects, _, err := wrapAPI(v, \"get_root_tree\", func() (*github.Tree, *github.Response, error) {","\t\treturn v.Client().Git.GetTree(ctx, runevent.Organization, runevent.Repository, revision, false)","\t})","\tif err != nil {","\t\treturn \"\", err","\t}","\tfor _, object := range rootobjects.Entries {","\t\tif object.GetPath() == path {","\t\t\tif object.GetType() != \"tree\" {","\t\t\t\treturn \"\", fmt.Errorf(\"%s has been found but is not a directory\", path)","\t\t\t}","\t\t\ttektonDirSha = object.GetSHA()","\t\t}","\t}","","\t// If we didn't find a .tekton directory then just silently ignore the error.","\tif tektonDirSha == \"\" {","\t\treturn \"\", nil","\t}","","\t// Get all files in the .tekton directory recursively","\t// there is a limit on this recursive calls to 500 entries, as documented here:","\t// https://docs.github.com/en/rest/reference/git#get-a-tree","\t// so we may need to address it in the future.","\ttektonDirObjects, _, err := wrapAPI(v, \"get_tekton_tree\", func() (*github.Tree, *github.Response, error) {","\t\treturn v.Client().Git.GetTree(ctx, runevent.Organization, runevent.Repository, tektonDirSha,","\t\t\ttrue)","\t})","\tif err != nil {","\t\treturn \"\", err","\t}","\treturn v.concatAllYamlFiles(ctx, tektonDirObjects.Entries, runevent)","}","","// GetCommitInfo get info (url and title) on a commit in runevent, this needs to","// be run after sewebhook while we already matched a token.","func (v *Provider) GetCommitInfo(ctx context.Context, runevent *info.Event) error {","\tif v.ghClient == nil {","\t\treturn fmt.Errorf(\"no github client has been initialized, \" +","\t\t\t\"exiting... (hint: did you forget setting a secret on your repo?)\")","\t}","","\t// if we don't have a sha we may have a branch (ie: incoming webhook) then","\t// use the branch as sha since github supports it","\tvar commit *github.Commit","\tsha := runevent.SHA","\tif runevent.SHA == \"\" \u0026\u0026 runevent.HeadBranch != \"\" {","\t\tbranchinfo, _, err := wrapAPI(v, \"get_branch_info\", func() (*github.Branch, *github.Response, error) {","\t\t\treturn v.Client().Repositories.GetBranch(ctx, runevent.Organization, runevent.Repository, runevent.HeadBranch, 1)","\t\t})","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\tsha = branchinfo.Commit.GetSHA()","\t}","\tvar err error","","\t// check if the commit info is already cached in provider","\tif v.commitInfo == nil {","\t\tcommit, _, err = wrapAPI(v, \"get_commit\", func() (*github.Commit, *github.Response, error) {","\t\t\treturn v.Client().Git.GetCommit(ctx, runevent.Organization, runevent.Repository, sha)","\t\t})","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t} else {","\t\tcommit = v.commitInfo","\t}","","\trunevent.SHAURL = commit.GetHTMLURL()","\trunevent.SHATitle = strings.Split(commit.GetMessage(), \"\\n\\n\")[0]","\trunevent.SHA = commit.GetSHA()","\trunevent.HasSkipCommand = provider.SkipCI(commit.GetMessage())","","\t// Populate full commit information for LLM context","\trunevent.SHAMessage = commit.GetMessage()","\tif commit.Author != nil {","\t\trunevent.SHAAuthorName = commit.Author.GetName()","\t\trunevent.SHAAuthorEmail = commit.Author.GetEmail()","\t\tif commit.Author.Date != nil {","\t\t\trunevent.SHAAuthorDate = commit.Author.Date.Time","\t\t}","\t}","\tif commit.Committer != nil {","\t\trunevent.SHACommitterName = commit.Committer.GetName()","\t\trunevent.SHACommitterEmail = commit.Committer.GetEmail()","\t\tif commit.Committer.Date != nil {","\t\t\trunevent.SHACommitterDate = commit.Committer.Date.Time","\t\t}","\t}","","\treturn nil","}","","// GetFileInsideRepo Get a file via Github API using the runinfo information, we","// branch is true, the user the branch as ref instead of the SHA","// TODO: merge GetFileInsideRepo amd GetTektonDir.","func (v *Provider) GetFileInsideRepo(ctx context.Context, runevent *info.Event, path, target string) (string, error) {","\tref := runevent.SHA","\tif target != \"\" {","\t\tref = runevent.BaseBranch","\t} else if v.provenance == \"default_branch\" {","\t\tref = runevent.DefaultBranch","\t}","","\tfp, objects, _, err := wrapAPIGetContents(v, \"get_file_contents\", func() (*github.RepositoryContent, []*github.RepositoryContent, *github.Response, error) {","\t\treturn v.Client().Repositories.GetContents(ctx, runevent.Organization,","\t\t\trunevent.Repository, path, \u0026github.RepositoryContentGetOptions{Ref: ref})","\t})","\tif err != nil {","\t\treturn \"\", err","\t}","\tif objects != nil {","\t\treturn \"\", fmt.Errorf(\"referenced file inside the Github Repository %s is a directory\", path)","\t}","","\tgetobj, err := v.getObject(ctx, fp.GetSHA(), runevent)","\tif err != nil {","\t\treturn \"\", err","\t}","","\treturn string(getobj), nil","}","","// concatAllYamlFiles concat all yaml files from a directory as one big multi document yaml string.","func (v *Provider) concatAllYamlFiles(ctx context.Context, objects []*github.TreeEntry, runevent *info.Event) (string, error) {","\tvar allTemplates string","","\tfor _, value := range objects {","\t\tif strings.HasSuffix(value.GetPath(), \".yaml\") ||","\t\t\tstrings.HasSuffix(value.GetPath(), \".yml\") {","\t\t\tdata, err := v.getObject(ctx, value.GetSHA(), runevent)","\t\t\tif err != nil {","\t\t\t\treturn \"\", err","\t\t\t}","\t\t\tif err := provider.ValidateYaml(data, value.GetPath()); err != nil {","\t\t\t\treturn \"\", err","\t\t\t}","\t\t\tif allTemplates != \"\" \u0026\u0026 !strings.HasPrefix(string(data), \"---\") {","\t\t\t\tallTemplates += \"---\"","\t\t\t}","\t\t\tallTemplates += \"\\n\" + string(data) + \"\\n\"","\t\t}","\t}","\treturn allTemplates, nil","}","","// getPullRequest get a pull request details.","func (v *Provider) getPullRequest(ctx context.Context, runevent *info.Event) (*info.Event, error) {","\tpr, _, err := wrapAPI(v, \"get_pull_request\", func() (*github.PullRequest, *github.Response, error) {","\t\treturn v.Client().PullRequests.Get(ctx, runevent.Organization, runevent.Repository, runevent.PullRequestNumber)","\t})","\tif err != nil {","\t\treturn runevent, err","\t}","\t// Make sure to use the Base for Default BaseBranch or there would be a potential hijack","\trunevent.DefaultBranch = pr.GetBase().GetRepo().GetDefaultBranch()","\trunevent.URL = pr.GetBase().GetRepo().GetHTMLURL()","\trunevent.SHA = pr.GetHead().GetSHA()","\trunevent.SHAURL = fmt.Sprintf(\"%s/commit/%s\", pr.GetHTMLURL(), pr.GetHead().GetSHA())","\trunevent.PullRequestTitle = pr.GetTitle()","","\t// TODO: check if we really need this","\tif runevent.Sender == \"\" {","\t\trunevent.Sender = pr.GetUser().GetLogin()","\t}","\trunevent.HeadBranch = pr.GetHead().GetRef()","\trunevent.BaseBranch = pr.GetBase().GetRef()","\trunevent.HeadURL = pr.GetHead().GetRepo().GetHTMLURL()","\trunevent.BaseURL = pr.GetBase().GetRepo().GetHTMLURL()","\tif runevent.EventType == \"\" {","\t\trunevent.EventType = triggertype.PullRequest.String()","\t}","","\tfor _, label := range pr.Labels {","\t\trunevent.PullRequestLabel = append(runevent.PullRequestLabel, label.GetName())","\t}","","\tv.RepositoryIDs = []int64{","\t\tpr.GetBase().GetRepo().GetID(),","\t}","\treturn runevent, nil","}","","// GetFiles gets and caches the list of files changed by a given event.","func (v *Provider) GetFiles(ctx context.Context, runevent *info.Event) (changedfiles.ChangedFiles, error) {","\tif v.cachedChangedFiles == nil {","\t\tchanges, err := v.fetchChangedFiles(ctx, runevent)","\t\tif err != nil {","\t\t\treturn changedfiles.ChangedFiles{}, err","\t\t}","\t\tv.cachedChangedFiles = \u0026changes","\t}","\treturn *v.cachedChangedFiles, nil","}","","func (v *Provider) fetchChangedFiles(ctx context.Context, runevent *info.Event) (changedfiles.ChangedFiles, error) {","\tchangedFiles := changedfiles.ChangedFiles{}","","\tswitch runevent.TriggerTarget {","\tcase triggertype.PullRequest:","\t\topt := \u0026github.ListOptions{PerPage: v.PaginedNumber}","\t\tfor {","\t\t\trepoCommit, resp, err := wrapAPI(v, \"list_pull_request_files\", func() ([]*github.CommitFile, *github.Response, error) {","\t\t\t\treturn v.Client().PullRequests.ListFiles(ctx, runevent.Organization, runevent.Repository, runevent.PullRequestNumber, opt)","\t\t\t})","\t\t\tif err != nil {","\t\t\t\treturn changedfiles.ChangedFiles{}, err","\t\t\t}","\t\t\tfor j := range repoCommit {","\t\t\t\tchangedFiles.All = append(changedFiles.All, *repoCommit[j].Filename)","\t\t\t\tif *repoCommit[j].Status == \"added\" {","\t\t\t\t\tchangedFiles.Added = append(changedFiles.Added, *repoCommit[j].Filename)","\t\t\t\t}","\t\t\t\tif *repoCommit[j].Status == \"removed\" {","\t\t\t\t\tchangedFiles.Deleted = append(changedFiles.Deleted, *repoCommit[j].Filename)","\t\t\t\t}","\t\t\t\tif *repoCommit[j].Status == \"modified\" {","\t\t\t\t\tchangedFiles.Modified = append(changedFiles.Modified, *repoCommit[j].Filename)","\t\t\t\t}","\t\t\t\tif *repoCommit[j].Status == \"renamed\" {","\t\t\t\t\tchangedFiles.Renamed = append(changedFiles.Renamed, *repoCommit[j].Filename)","\t\t\t\t}","\t\t\t}","\t\t\tif resp.NextPage == 0 {","\t\t\t\tbreak","\t\t\t}","\t\t\topt.Page = resp.NextPage","\t\t}","\tcase triggertype.Push:","\t\trC, _, err := wrapAPI(v, \"get_commit_files\", func() (*github.RepositoryCommit, *github.Response, error) {","\t\t\treturn v.Client().Repositories.GetCommit(ctx, runevent.Organization, runevent.Repository, runevent.SHA, \u0026github.ListOptions{})","\t\t})","\t\tif err != nil {","\t\t\treturn changedfiles.ChangedFiles{}, err","\t\t}","\t\tfor i := range rC.Files {","\t\t\tchangedFiles.All = append(changedFiles.All, *rC.Files[i].Filename)","\t\t\tif *rC.Files[i].Status == \"added\" {","\t\t\t\tchangedFiles.Added = append(changedFiles.Added, *rC.Files[i].Filename)","\t\t\t}","\t\t\tif *rC.Files[i].Status == \"removed\" {","\t\t\t\tchangedFiles.Deleted = append(changedFiles.Deleted, *rC.Files[i].Filename)","\t\t\t}","\t\t\tif *rC.Files[i].Status == \"modified\" {","\t\t\t\tchangedFiles.Modified = append(changedFiles.Modified, *rC.Files[i].Filename)","\t\t\t}","\t\t\tif *rC.Files[i].Status == \"renamed\" {","\t\t\t\tchangedFiles.Renamed = append(changedFiles.Renamed, *rC.Files[i].Filename)","\t\t\t}","\t\t}","\tdefault:","\t\t// No action necessary","\t}","\treturn changedFiles, nil","}","","// getObject Get an object from a repository.","func (v *Provider) getObject(ctx context.Context, sha string, runevent *info.Event) ([]byte, error) {","\tblob, _, err := wrapAPI(v, \"get_blob\", func() (*github.Blob, *github.Response, error) {","\t\treturn v.Client().Git.GetBlob(ctx, runevent.Organization, runevent.Repository, sha)","\t})","\tif err != nil {","\t\treturn nil, err","\t}","","\tdecoded, err := base64.StdEncoding.DecodeString(blob.GetContent())","\tif err != nil {","\t\treturn nil, err","\t}","\treturn decoded, err","}","","// ListRepos lists all the repos for a particular token.","func ListRepos(ctx context.Context, v *Provider) ([]string, error) {","\tif v.ghClient == nil {","\t\treturn []string{}, fmt.Errorf(\"no github client has been initialized, \" +","\t\t\t\"exiting... (hint: did you forget setting a secret on your repo?)\")","\t}","","\topt := \u0026github.ListOptions{PerPage: v.PaginedNumber}","\trepoURLs := []string{}","\tfor {","\t\trepoList, resp, err := wrapAPI(v, \"list_app_repos\", func() (*github.ListRepositories, *github.Response, error) {","\t\t\treturn v.Client().Apps.ListRepos(ctx, opt)","\t\t})","\t\tif err != nil {","\t\t\treturn []string{}, err","\t\t}","\t\tfor i := range repoList.Repositories {","\t\t\trepoURLs = append(repoURLs, *repoList.Repositories[i].HTMLURL)","\t\t}","\t\tif resp.NextPage == 0 {","\t\t\tbreak","\t\t}","\t\topt.Page = resp.NextPage","\t}","\treturn repoURLs, nil","}","","func (v *Provider) CreateToken(ctx context.Context, repository []string, event *info.Event) (string, error) {","\tvar appReposCache []*github.Repository","","\tfor _, r := range repository {","\t\t// Check if this is a glob pattern","\t\tif strings.ContainsAny(r, \"*?[\") {","\t\t\tif err := v.expandGlobAndAddRepoIDs(ctx, r, \u0026appReposCache); err != nil {","\t\t\t\tv.Logger.Warn(\"failed to expand glob pattern %q: %v\", r, err)","\t\t\t}","\t\t\tcontinue","\t\t}","","\t\tsplit := strings.Split(r, \"/\")","\t\tinfoData, _, err := wrapAPI(v, \"get_repository\", func() (*github.Repository, *github.Response, error) {","\t\t\treturn v.Client().Repositories.Get(ctx, split[0], split[1])","\t\t})","\t\tif err != nil {","\t\t\tv.Logger.Warn(\"we have an invalid repository: `%s` or no access to it: %v\", r, err)","\t\t\tcontinue","\t\t}","\t\tv.RepositoryIDs = uniqueRepositoryID(v.RepositoryIDs, infoData.GetID())","\t}","\tns := info.GetNS(ctx)","\ttoken, err := v.GetAppToken(ctx, v.Run.Clients.Kube, event.Provider.URL, event.InstallationID, ns)","\tif err != nil {","\t\treturn \"\", err","\t}","\treturn token, nil","}","","func (v *Provider) expandGlobAndAddRepoIDs(ctx context.Context, repoPattern string, cache *[]*github.Repository) error {","\t// We can skip error check here as all the glob compilation has been checked","\t// before this method is called.","\treposToScope, _ := glob.Compile(repoPattern)","","\tif *cache == nil {","\t\trepos, err := v.listAppRepos(ctx)","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\t*cache = repos","\t}","","\tfor _, repo := range *cache {","\t\trepoFullName := repo.GetFullName()","\t\tif reposToScope.Match(repoFullName) {","\t\t\tv.RepositoryIDs = uniqueRepositoryID(v.RepositoryIDs, repo.GetID())","\t\t}","\t}","","\treturn nil","}","","func (v *Provider) listAppRepos(ctx context.Context) ([]*github.Repository, error) {","\tvar allRepos []*github.Repository","","\topt := \u0026github.ListOptions{PerPage: v.PaginedNumber}","\tfor {","\t\trepoList, resp, err := wrapAPI(v, \"list_app_repos\", func() (*github.ListRepositories, *github.Response, error) {","\t\t\treturn v.Client().Apps.ListRepos(ctx, opt)","\t\t})","\t\tif err != nil {","\t\t\treturn nil, fmt.Errorf(\"failed to list app repos: %w\", err)","\t\t}","","\t\tallRepos = append(allRepos, repoList.Repositories...)","","\t\tif resp.NextPage == 0 {","\t\t\tbreak","\t\t}","\t\topt.Page = resp.NextPage","\t}","","\treturn allRepos, nil","}","","func uniqueRepositoryID(repoIDs []int64, id int64) []int64 {","\tr := repoIDs","\tm := make(map[int64]bool)","\tfor _, val := range repoIDs {","\t\tif _, ok := m[val]; !ok {","\t\t\tm[val] = true","\t\t}","\t}","\tif _, ok := m[id]; !ok {","\t\tr = append(r, id)","\t}","\treturn r","}","","// isHeadCommitOfBranch checks whether provided branch is valid or not and SHA is HEAD commit of the branch.","func (v *Provider) isHeadCommitOfBranch(ctx context.Context, runevent *info.Event, branchName string) error {","\tif v.ghClient == nil {","\t\treturn fmt.Errorf(\"no github client has been initialized, \" +","\t\t\t\"exiting... (hint: did you forget setting a secret on your repo?)\")","\t}","","\tbranchInfo, _, err := wrapAPI(v, \"get_branch\", func() (*github.Branch, *github.Response, error) {","\t\treturn v.Client().Repositories.GetBranch(ctx, runevent.Organization, runevent.Repository, branchName, 1)","\t})","\tif err != nil {","\t\treturn err","\t}","\tif branchInfo.Commit.GetSHA() == runevent.SHA {","\t\treturn nil","\t}","\treturn fmt.Errorf(\"provided SHA %s is not the HEAD commit of the branch %s\", runevent.SHA, branchName)","}","","func (v *Provider) GetTemplate(commentType provider.CommentType) string {","\treturn provider.GetHTMLTemplate(commentType)","}","","type commentTraceLogContext struct {","\tdedupTrace      string","\teventID         string","\tmarkerHash      string","\tmarkerLen       int","\tcontrollerLabel string","}","","func newDedupTraceID() string {","\t//nolint:gosec // best-effort correlation ID for debug logs only","\treturn fmt.Sprintf(\"%x-%04x\", time.Now().UnixNano(), rand.Intn(1\u003c\u003c16))","}","","func markerHash(marker string) string {","\tif marker == \"\" {","\t\treturn \"none\"","\t}","\tsum := sha256.Sum256([]byte(marker))","\tdigest := hex.EncodeToString(sum[:])","\tif len(digest) \u003e 12 {","\t\treturn digest[:12]","\t}","\treturn digest","}","","func formatCommentTime(ts github.Timestamp) string {","\tif ts.IsZero() {","\t\treturn \"unknown\"","\t}","\treturn ts.UTC().Format(time.RFC3339)","}","","func compactCommentIDs(comments []*github.IssueComment) []string {","\tout := make([]string, 0, len(comments))","\tfor _, comment := range comments {","\t\tout = append(out, fmt.Sprintf(\"%d@%s\", comment.GetID(), formatCommentTime(comment.GetCreatedAt())))","\t}","\treturn out","}","","func responseStatusCode(resp *github.Response) int {","\tif resp == nil {","\t\treturn 0","\t}","\treturn resp.StatusCode","}","","func eventID(event *info.Event) string {","\tif event == nil || event.Request == nil {","\t\treturn \"unknown\"","\t}","\tif id := event.Request.Header.Get(\"X-GitHub-Delivery\"); id != \"\" {","\t\treturn id","\t}","\treturn \"unknown\"","}","","func (v *Provider) controllerLabel(ctx context.Context) string {","\tif name := info.GetCurrentControllerName(ctx); name != \"\" {","\t\treturn name","\t}","\tif v.Run != nil \u0026\u0026 v.Run.Info.Controller != nil \u0026\u0026 v.Run.Info.Controller.Name != \"\" {","\t\treturn v.Run.Info.Controller.Name","\t}","\treturn \"unknown\"","}","","func (v *Provider) newCommentTraceLogContext(ctx context.Context, event *info.Event, marker string) commentTraceLogContext {","\treturn commentTraceLogContext{","\t\tdedupTrace:      newDedupTraceID(),","\t\teventID:         eventID(event),","\t\tmarkerHash:      markerHash(marker),","\t\tmarkerLen:       len(marker),","\t\tcontrollerLabel: v.controllerLabel(ctx),","\t}","}","","func (v *Provider) debugCommentPhase(event *info.Event, trace commentTraceLogContext, phase string, kv ...any) {","\tif v.Logger == nil {","\t\treturn","\t}","","\torg := \"unknown\"","\trepo := \"unknown\"","\tpr := 0","\tif event != nil {","\t\torg = event.Organization","\t\trepo = event.Repository","\t\tpr = event.PullRequestNumber","\t}","","\tbaseFields := []any{","\t\t\"phase\", phase,","\t\t\"organization\", org,","\t\t\"repository\", repo,","\t\t\"pr\", pr,","\t\t\"event_id\", trace.eventID,","\t\t\"dedup_trace\", trace.dedupTrace,","\t\t\"marker_hash\", trace.markerHash,","\t\t\"marker_len\", trace.markerLen,","\t\t\"controller_label\", trace.controllerLabel,","\t}","\tv.Logger.Debugw(\"github comment dedup flow\", append(baseFields, kv...)...)","}","","func (v *Provider) listCommentsByMarker(","\tctx context.Context,","\tevent *info.Event,","\tmarker, phase string,","\ttrace commentTraceLogContext,",") ([]*github.IssueComment, error) {","\tcomments, _, err := wrapAPI(v, \"list_comments\", func() ([]*github.IssueComment, *github.Response, error) {","\t\treturn v.Client().Issues.ListComments(ctx, event.Organization, event.Repository, event.PullRequestNumber, \u0026github.IssueListCommentsOptions{","\t\t\tListOptions: github.ListOptions{","\t\t\t\tPage:    1,","\t\t\t\tPerPage: 100,","\t\t\t},","\t\t})","\t})","\tif err != nil {","\t\treturn nil, err","\t}","","\tre := regexp.MustCompile(regexp.QuoteMeta(marker))","\tmatchedComments := make([]*github.IssueComment, 0, len(comments))","\tfor _, comment := range comments {","\t\tif re.MatchString(comment.GetBody()) {","\t\t\tmatchedComments = append(matchedComments, comment)","\t\t}","\t}","","\tv.debugCommentPhase(event, trace, phase,","\t\t\"fetched_count\", len(comments),","\t\t\"matched_count\", len(matchedComments),","\t\t\"matched_comments\", compactCommentIDs(matchedComments),","\t)","","\treturn matchedComments, nil","}","","func (v *Provider) ensureSingleMarkerComment(","\tctx context.Context,","\tevent *info.Event,","\tcomments []*github.IssueComment,","\tcommit string,","\ttrace commentTraceLogContext,",") error {","\tif len(comments) == 0 {","\t\treturn nil","\t}","","\tif len(comments) \u003e 1 {","\t\tv.debugCommentPhase(event, trace, \"duplicate_detected\", \"matched_count\", len(comments))","\t}","","\tprimaryComment := comments[0]","\tfor _, comment := range comments {","\t\tif comment.GetBody() == commit {","\t\t\tprimaryComment = comment","\t\t\tbreak","\t\t}","\t}","","\tv.debugCommentPhase(event, trace, \"dedup_select_primary\",","\t\t\"matched_count\", len(comments),","\t\t\"primary_comment_id\", primaryComment.GetID(),","\t)","","\tif primaryComment.GetBody() != commit {","\t\tif _, _, err := wrapAPI(v, \"edit_comment\", func() (*github.IssueComment, *github.Response, error) {","\t\t\treturn v.Client().Issues.EditComment(ctx, event.Organization, event.Repository, primaryComment.GetID(), \u0026github.IssueComment{","\t\t\t\tBody: github.Ptr(commit),","\t\t\t})","\t\t}); err != nil {","\t\t\treturn err","\t\t}","\t}","","\t// Best-effort cleanup to collapse duplicates into a single canonical marker comment.","\tfor _, comment := range comments {","\t\tif comment.GetID() == primaryComment.GetID() {","\t\t\tcontinue","\t\t}","","\t\tv.debugCommentPhase(event, trace, \"dedup_delete_attempt\", \"delete_comment_id\", comment.GetID())","\t\t_, resp, err := wrapAPI(v, \"delete_comment\", func() (struct{}, *github.Response, error) {","\t\t\tresp, err := v.Client().Issues.DeleteComment(ctx, event.Organization, event.Repository, comment.GetID())","\t\t\treturn struct{}{}, resp, err","\t\t})","\t\tv.debugCommentPhase(event, trace, \"dedup_delete_done\",","\t\t\t\"delete_comment_id\", comment.GetID(),","\t\t\t\"status_code\", responseStatusCode(resp),","\t\t\t\"delete_error\", err != nil,","\t\t)","\t\tif err != nil \u0026\u0026 v.Logger != nil {","\t\t\tv.Logger.Warnf(\"failed to delete duplicate comment %d on %s/%s#%d: %v\",","\t\t\t\tcomment.GetID(), event.Organization, event.Repository, event.PullRequestNumber, err)","\t\t}","\t}","\tv.debugCommentPhase(event, trace, \"dedup_complete\", \"final_expected_count\", 1)","\treturn nil","}","","// CreateComment creates a comment on a Pull Request.","func (v *Provider) CreateComment(ctx context.Context, event *info.Event, commit, updateMarker string) error {","\tif v.ghClient == nil {","\t\treturn fmt.Errorf(\"no github client has been initialized\")","\t}","","\tif event.PullRequestNumber == 0 {","\t\treturn fmt.Errorf(\"create comment only works on pull requests\")","\t}","","\ttrace := v.newCommentTraceLogContext(ctx, event, updateMarker)","","\tif updateMarker != \"\" {","\t\texistingComments, err := v.listCommentsByMarker(ctx, event, updateMarker, \"initial_list\", trace)","\t\tif err != nil {","\t\t\treturn err","\t\t}","","\t\tif len(existingComments) \u003e 1 {","\t\t\tv.debugCommentPhase(event, trace, \"duplicate_detected\", \"matched_count\", len(existingComments))","\t\t}","","\t\tif len(existingComments) \u003e 0 {","\t\t\treturn v.ensureSingleMarkerComment(ctx, event, existingComments, commit, trace)","\t\t}","","\t\t//nolint:gosec // No need for crypto/rand here, just reducing timing window","\t\tjitter := time.Duration(rand.Intn(500)) * time.Millisecond","\t\tv.debugCommentPhase(event, trace, \"jitter_wait\", \"jitter_ms\", jitter.Milliseconds())","\t\ttimer := time.NewTimer(jitter)","\t\tdefer timer.Stop()","","\t\tselect {","\t\tcase \u003c-ctx.Done():","\t\t\treturn ctx.Err()","\t\tcase \u003c-timer.C:","\t\t}","","\t\t// Re-check after jitter in case another processor already created the marker comment.","\t\texistingComments, err = v.listCommentsByMarker(ctx, event, updateMarker, \"post_jitter_list\", trace)","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\tif len(existingComments) \u003e 1 {","\t\t\tv.debugCommentPhase(event, trace, \"duplicate_detected\", \"matched_count\", len(existingComments))","\t\t}","\t\tif len(existingComments) \u003e 0 {","\t\t\treturn v.ensureSingleMarkerComment(ctx, event, existingComments, commit, trace)","\t\t}","","\t\tv.debugCommentPhase(event, trace, \"pre_create_race_window\", \"matched_count\", len(existingComments))","\t}","","\tv.debugCommentPhase(event, trace, \"create_comment_start\")","\tcreatedComment, createResp, err := wrapAPI(v, \"create_comment\", func() (*github.IssueComment, *github.Response, error) {","\t\treturn v.Client().Issues.CreateComment(ctx, event.Organization, event.Repository, event.PullRequestNumber, \u0026github.IssueComment{","\t\t\tBody: github.Ptr(commit),","\t\t})","\t})","\tif err != nil {","\t\tv.debugCommentPhase(event, trace, \"create_comment_done\",","\t\t\t\"status_code\", responseStatusCode(createResp),","\t\t\t\"create_error\", err.Error(),","\t\t)","\t\treturn err","\t}","\tv.debugCommentPhase(event, trace, \"create_comment_done\",","\t\t\"status_code\", responseStatusCode(createResp),","\t\t\"created_comment_id\", createdComment.GetID(),","\t)","","\tif updateMarker == \"\" {","\t\treturn nil","\t}","","\t// Best-effort post-create reconciliation to collapse duplicates created by","\t// concurrent processors handling the same event.","\tmatchedComments, listErr := v.listCommentsByMarker(ctx, event, updateMarker, \"post_create_list\", trace)","\tif listErr != nil {","\t\treturn nil","\t}","\tif len(matchedComments) \u003e 1 {","\t\tv.debugCommentPhase(event, trace, \"duplicate_detected\", \"matched_count\", len(matchedComments))","\t}","\treturn v.ensureSingleMarkerComment(ctx, event, matchedComments, commit, trace)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,0,2,2,2,0,2,2,2,0,2,2,2,0,0,2,2,2,1,1,1,2,0,0,0,2,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,1,1,2,1,1,2,1,1,2,1,1,2,0,0,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,1,1,2,0,0,1,1,1,1,1,1,1,1,0,1,0,0,1,1,1,0,2,2,2,2,2,2,2,1,1,1,2,2,2,2,0,0,1,1,1,1,1,1,1,0,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,0,2,0,0,2,2,2,2,2,2,0,0,2,2,2,2,2,2,0,2,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,0,2,2,2,0,2,2,2,1,1,0,2,2,2,2,0,0,0,2,2,2,2,0,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,0,2,2,2,2,2,2,2,2,2,1,1,1,1,0,0,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,1,1,2,2,2,2,2,2,0,0,0,0,2,2,2,0,0,0,0,0,2,2,2,2,2,1,1,2,0,0,0,0,2,2,2,2,2,0,0,0,2,2,2,1,1,1,1,1,1,1,0,2,2,2,2,2,2,2,2,2,2,1,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,0,0,2,0,0,0,0,0,2,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,0,2,0,0,0,2,2,2,2,2,2,2,2,1,1,2,2,2,2,2,2,2,0,0,2,0,0,0,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,1,1,0,2,2,2,2,0,0,0,2,2,2,2,1,1,2,0,2,0,0,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,0,2,0,2,2,2,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,2,0,0,0,2,2,2,2,2,2,2,0,2,2,2,2,2,0,0,0,2,2,1,1,1,0,2,2,2,2,2,2,2,1,1,2,2,2,2,2,0,2,0,2,0,0,2,2,2,2,2,2,1,1,1,1,0,0,2,2,2,2,2,1,1,0,2,0,2,2,2,2,2,2,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,0,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1,0,0,1,0,0,2,2,2,2,2,2,2,0,2,2,2,2,0,0,0,2,2,1,1,1,0,2,2,2,2,2,2,2,2,2,2,0,0,1,1,1,0,0,0,0,0,0,0,0,0,2,2,2,2,0,2,2,2,2,2,2,2,2,2,1,0,0,2,2,2,2,2,0,0,2,2,2,2,2,2,0,0,2,2,1,1,2,0,0,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,1,1,2,0,0,2,2,2,2,2,2,2,2,2,0,2,2,2,2,0,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,0,0,0,0,0,0,0,0,2,2,2,2,0,2,2,2,0,2,2,2,2,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,1,1,0,0,0,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,1,1,1,0,2,2,0,0,0,2,2,2,2,0,2,2,2,0,2,2,2,2,2,1,1,0,2,2,2,0,2,2,2,0,0,2,2,2,2,2,2,1,1,0,0,0,0,2,2,1,1,2,1,1,2,1,1,0,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,1,1,2,2,2,2,0]},{"id":118,"path":"pkg/provider/github/parse_payload.go","lines":["package github","","import (","\t\"context\"","\t\"encoding/json\"","\t\"errors\"","\t\"fmt\"","\t\"net/http\"","\t\"os\"","\t\"path\"","\t\"strconv\"","\t\"strings\"","\t\"time\"","","\tghinstallation \"github.com/bradleyfalzon/ghinstallation/v2\"","\tgithub75 \"github.com/google/go-github/v75/github\"","\t\"github.com/google/go-github/v81/github\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/opscomments\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/client-go/kubernetes\"",")","","const (","\tmaxRetriesForMergeCommit = 3","\tgithubNoreplyEmail       = \"noreply@github.com\"","\tgithubWebFlowUser        = \"web-flow\"",")","","// GetAppIDAndPrivateKey retrieves the GitHub application ID and private key from a secret in the specified namespace.","// It takes a context, namespace, and Kubernetes client as input parameters.","// It returns the application ID (int64), private key ([]byte), and an error if any.","func (v *Provider) GetAppIDAndPrivateKey(ctx context.Context, ns string, kube kubernetes.Interface) (int64, []byte, error) {","\tparamsinfo := \u0026v.Run.Info","\tsecret, err := kube.CoreV1().Secrets(ns).Get(ctx, paramsinfo.Controller.Secret, metav1.GetOptions{})","\tif err != nil {","\t\treturn 0, []byte{}, fmt.Errorf(\"could not get the secret %s in ns %s: %w\", paramsinfo.Controller.Secret, ns, err)","\t}","","\tappID := secret.Data[keys.GithubApplicationID]","\tapplicationID, err := strconv.ParseInt(strings.TrimSpace(string(appID)), 10, 64)","\tif err != nil {","\t\treturn 0, []byte{}, fmt.Errorf(\"could not parse the github application_id number from secret: %w\", err)","\t}","","\tprivateKey := secret.Data[keys.GithubPrivateKey]","\treturn applicationID, privateKey, nil","}","","func (v *Provider) GetAppToken(ctx context.Context, kube kubernetes.Interface, gheURL string, installationID int64, ns string) (string, error) {","\tapplicationID, privateKey, err := v.GetAppIDAndPrivateKey(ctx, ns, kube)","\tif err != nil {","\t\treturn \"\", err","\t}","\tv.ApplicationID = \u0026applicationID","\ttr := http.DefaultTransport","","\titr, err := ghinstallation.New(tr, applicationID, installationID, privateKey)","\tif err != nil {","\t\treturn \"\", err","\t}","\titr.InstallationTokenOptions = \u0026github75.InstallationTokenOptions{","\t\tRepositoryIDs: v.RepositoryIDs,","\t}","","\t// This is a hack when we have auth and api disassociated like in our","\t// unittests since we are using a custom http server with httptest","\treqTokenURL := os.Getenv(\"PAC_GIT_PROVIDER_TOKEN_APIURL\")","\tif reqTokenURL != \"\" {","\t\titr.BaseURL = reqTokenURL","\t\tv.APIURL = \u0026reqTokenURL","\t\tgheURL = strings.TrimSuffix(reqTokenURL, \"/api/v3\")","\t}","","\tif gheURL != \"\" {","\t\tif !strings.HasPrefix(gheURL, \"https://\") \u0026\u0026 !strings.HasPrefix(gheURL, \"http://\") {","\t\t\tgheURL = \"https://\" + gheURL","\t\t}","\t\tuploadURL := gheURL + \"/api/uploads\"","\t\tv.ghClient, _ = github.NewClient(\u0026http.Client{Transport: itr}).WithEnterpriseURLs(gheURL, uploadURL)","\t\titr.BaseURL = strings.TrimSuffix(v.Client().BaseURL.String(), \"/\")","\t} else {","\t\tv.ghClient = github.NewClient(\u0026http.Client{Transport: itr})","\t}","","\t// Get a token ASAP because we need it for setting private repos","\ttoken, err := itr.Token(ctx)","\tif err != nil {","\t\treturn \"\", err","\t}","\tv.Token = github.Ptr(token)","","\treturn token, err","}","","func (v *Provider) parseEventType(request *http.Request, event *info.Event) error {","\tevent.EventType = request.Header.Get(\"X-GitHub-Event\")","\tif event.EventType == \"\" {","\t\treturn fmt.Errorf(\"failed to find event type in request header\")","\t}","","\tevent.Provider.URL = request.Header.Get(\"X-GitHub-Enterprise-Host\")","","\tif event.EventType == \"push\" {","\t\tevent.TriggerTarget = triggertype.Push","\t} else {","\t\tevent.TriggerTarget = triggertype.PullRequest","\t}","","\treturn nil","}","","type Payload struct {","\tInstallation struct {","\t\tID *int64 `json:\"id\"`","\t} `json:\"installation\"`","}","","func getInstallationIDFromPayload(payload string) (int64, error) {","\tvar data Payload","\terr := json.Unmarshal([]byte(payload), \u0026data)","\tif err != nil {","\t\treturn -1, err","\t}","\tif data.Installation.ID != nil {","\t\treturn *data.Installation.ID, nil","\t}","\treturn -1, nil","}","","// ParsePayload will parse the payload and return the event","// it generate the github app token targeting the installation id","// this pieces of code is a bit messy because we need first getting a token to","// before parsing the payload.","//","// We need to get the token at first because in some case when coming from pull request","// comment (or recheck from the UI) we will use that token to get","// information about the PR that is not part of the payload.","//","// We then regenerate a second time the token scoped to the repo where the","// payload come from so we can avoid the scenario where an admin install the","// app on a github org which has a mixed of private and public repos and some of","// the public users should not have access to the private repos.","//","// Another thing: The payload is protected by the webhook signature so it cannot be tempered but even tho if it's","// tempered with and somehow a malicious user found the token and set their own github endpoint to hijack and","// exfiltrate the token, it would fail since the jwt token generation will fail, so we are safe here.","// a bit too far fetched but i don't see any way we can exploit this.","func (v *Provider) ParsePayload(ctx context.Context, run *params.Run, request *http.Request, payload string) (*info.Event, error) {","\t// ParsePayload is really happening before SetClient so let's set this at first here.","\t// Only apply for GitHub provider since we do fancy token creation at payload parsing","\tv.Run = run","\tevent := info.NewEvent()","\tsystemNS := info.GetNS(ctx)","\tif err := v.parseEventType(request, event); err != nil {","\t\treturn nil, err","\t}","","\tinstallationIDFrompayload, err := getInstallationIDFromPayload(payload)","\tif err != nil {","\t\treturn nil, err","\t}","\tif installationIDFrompayload != -1 {","\t\tvar err error","\t\t// TODO: move this out of here when we move al config inside context","\t\tif event.Provider.Token, err = v.GetAppToken(ctx, run.Clients.Kube, event.Provider.URL, installationIDFrompayload, systemNS); err != nil {","\t\t\treturn nil, err","\t\t}","\t}","","\teventInt, err := github.ParseWebHook(event.EventType, []byte(payload))","\tif err != nil {","\t\treturn nil, err","\t}","","\t// should not get invalid json since we already check it in github.ParseWebHook","\t_ = json.Unmarshal([]byte(payload), \u0026eventInt)","","\tprocessedEvent, err := v.processEvent(ctx, event, eventInt)","\tif err != nil {","\t\treturn nil, err","\t}","","\tif processedEvent == nil {","\t\treturn nil, nil","\t}","","\tprocessedEvent.Event = eventInt","\tprocessedEvent.InstallationID = installationIDFrompayload","\tprocessedEvent.GHEURL = event.Provider.URL","\tprocessedEvent.Provider.URL = event.Provider.URL","","\treturn processedEvent, nil","}","","// getPullRequestsWithCommit lists the all pull requests associated with given commit.","// It implements retry logic with exponential backoff to handle cases where GitHub's API","// hasn't yet indexed the PR-to-commit association (e.g., immediately after a merge commit).","func (v *Provider) getPullRequestsWithCommit(ctx context.Context, sha, org, repo string, isMergeCommit bool) ([]*github.PullRequest, error) {","\tif v.ghClient == nil {","\t\treturn nil, fmt.Errorf(\"github client is not initialized\")","\t}","","\t// Validate input parameters","\tif sha == \"\" {","\t\treturn nil, fmt.Errorf(\"sha cannot be empty\")","\t}","\tif org == \"\" {","\t\treturn nil, fmt.Errorf(\"organization cannot be empty\")","\t}","\tif repo == \"\" {","\t\treturn nil, fmt.Errorf(\"repository cannot be empty\")","\t}","","\t// For merge commits, retry the API call to handle potential delays in GitHub's API indexing the PR-to-commit association.","\tmaxRetries := 0","","\tif isMergeCommit {","\t\tmaxRetries = maxRetriesForMergeCommit","\t}","","\tconst initialBackoff = 1 * time.Second","","\tfor attempt := 0; attempt \u003c= maxRetries; attempt++ {","\t\topts := \u0026github.ListOptions{","\t\t\tPerPage: 100, // GitHub's maximum per page","\t\t}","","\t\tpullRequests := []*github.PullRequest{}","","\t\tfor {","\t\t\t// Use the \"List pull requests associated with a commit\" API to check if the commit is part of any open PR","\t\t\tprs, resp, err := wrapAPI(v, \"list_pull_requests_with_commit\", func() ([]*github.PullRequest, *github.Response, error) {","\t\t\t\treturn v.Client().PullRequests.ListPullRequestsWithCommit(ctx, org, repo, sha, opts)","\t\t\t})","\t\t\tif err != nil {","\t\t\t\t// Log the error for debugging purposes","\t\t\t\tv.Logger.Debugf(\"Failed to list pull requests for commit %s in %s/%s: %v\", sha, org, repo, err)","\t\t\t\treturn nil, fmt.Errorf(\"failed to list pull requests for commit %s: %w\", sha, err)","\t\t\t}","","\t\t\tpullRequests = append(pullRequests, prs...)","","\t\t\t// Check if there are more pages","\t\t\tif resp.NextPage == 0 {","\t\t\t\tbreak","\t\t\t}","\t\t\topts.Page = resp.NextPage","\t\t}","","\t\t// If we found pull requests, return them immediately","\t\tif len(pullRequests) \u003e 0 {","\t\t\treturn pullRequests, nil","\t\t}","","\t\t// If this is not the last attempt and we got an empty result, retry with exponential backoff","\t\t// This handles the case where GitHub's API hasn't indexed the PR-to-commit association yet","\t\t// (common when a PR is merged via Merge Commit strategy)","\t\tif attempt \u003c maxRetries {","\t\t\tbackoff := time.Duration(1\u003c\u003cuint(attempt)) * initialBackoff // #nosec G115","\t\t\tv.Logger.Debugf(\"No pull requests found for commit %s in %s/%s (attempt %d/%d), retrying after %v\", sha, org, repo, attempt+1, maxRetries+1, backoff)","","\t\t\t// Wait with exponential backoff, but respect context cancellation","\t\t\tselect {","\t\t\tcase \u003c-ctx.Done():","\t\t\t\treturn nil, ctx.Err()","\t\t\tcase \u003c-time.After(backoff):","\t\t\t}","\t\t}","\t}","","\t// After all retries, return empty list (no error, as this is a valid state)","\treturn []*github.PullRequest{}, nil","}","","// isCommitPartOfPullRequest checks if the commit from a push event is part of an open pull request","// If it is, it returns true and the PR number.","func (v *Provider) isCommitPartOfPullRequest(sha, org, repo string, prs []*github.PullRequest) (bool, int) {","\t// Check if any of the returned PRs are open","\tfor _, pr := range prs {","\t\tif pr.GetState() == \"open\" {","\t\t\tv.Logger.Debugf(\"Commit %s is part of open PR #%d in %s/%s\", sha, pr.GetNumber(), org, repo)","\t\t\treturn true, pr.GetNumber()","\t\t}","\t}","","\tv.Logger.Debugf(\"Commit %s is not part of any open pull request in %s/%s\", sha, org, repo)","\treturn false, 0","}","","func (v *Provider) processEvent(ctx context.Context, event *info.Event, eventInt any) (*info.Event, error) {","\tvar processedEvent *info.Event","\tvar err error","","\tprocessedEvent = info.NewEvent()","","\tswitch gitEvent := eventInt.(type) {","\tcase *github.CheckRunEvent:","\t\tif v.ghClient == nil {","\t\t\treturn nil, fmt.Errorf(\"check run rerequest is only supported with github apps integration\")","\t\t}","","\t\tif *gitEvent.Action != \"rerequested\" {","\t\t\treturn nil, fmt.Errorf(\"only issue recheck is supported in checkrunevent\")","\t\t}","\t\treturn v.handleReRequestEvent(ctx, gitEvent)","\tcase *github.CheckSuiteEvent:","\t\tif v.ghClient == nil {","\t\t\treturn nil, fmt.Errorf(\"check suite rerequest is only supported with github apps integration\")","\t\t}","","\t\tif *gitEvent.Action != \"rerequested\" {","\t\t\treturn nil, fmt.Errorf(\"only issue recheck is supported in checkrunevent\")","\t\t}","\t\treturn v.handleCheckSuites(ctx, gitEvent)","\tcase *github.IssueCommentEvent:","\t\tif v.ghClient == nil {","\t\t\treturn nil, fmt.Errorf(\"no github client has been initialized, \" +","\t\t\t\t\"exiting... (hint: did you forget setting a secret on your repo?)\")","\t\t}","\t\tif gitEvent.GetAction() != \"created\" {","\t\t\treturn nil, fmt.Errorf(\"only newly created comment is supported, received: %s\", gitEvent.GetAction())","\t\t}","\t\tprocessedEvent, err = v.handleIssueCommentEvent(ctx, gitEvent)","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\tcase *github.CommitCommentEvent:","\t\tif v.ghClient == nil {","\t\t\treturn nil, fmt.Errorf(\"no github client has been initialized, \" +","\t\t\t\t\"exiting... (hint: did you forget setting a secret on your repo?)\")","\t\t}","\t\tprocessedEvent, err = v.handleCommitCommentEvent(ctx, gitEvent)","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","\tcase *github.PushEvent:","\t\tif gitEvent.GetRepo() == nil {","\t\t\treturn nil, errors.New(\"error parsing payload the repository should not be nil\")","\t\t}","","\t\t// When a branch is deleted via repository UI, it triggers a push event.","\t\t// However, Pipelines as Code does not support handling branch delete events,","\t\t// so we return an error here to indicate this unsupported operation.","\t\tif gitEvent.After != nil {","\t\t\tif provider.IsZeroSHA(*gitEvent.After) {","\t\t\t\treturn nil, fmt.Errorf(\"branch %s has been deleted, exiting\", gitEvent.GetRef())","\t\t\t}","\t\t}","","\t\t// Check if this push commit is part of an open pull request","\t\tsha := gitEvent.GetHeadCommit().GetID()","\t\tif sha == \"\" {","\t\t\tsha = gitEvent.GetBefore()","\t\t}","\t\torg := gitEvent.GetRepo().GetOwner().GetLogin()","\t\trepoName := gitEvent.GetRepo().GetName()","","\t\t// when the commit is a merge commit, either email is 'noreply@github.com' or name is 'web-flow'","\t\tisMergeCommit := gitEvent.GetHeadCommit().GetCommitter().GetEmail() == githubNoreplyEmail ||","\t\t\tgitEvent.GetHeadCommit().GetCommitter().GetName() == githubWebFlowUser","","\t\t// First get all the pull requests associated with this commit so that we can reuse the output to check","\t\t// whether the commit is included in any PR or not, and if this push is generated on PR merge event, we can","\t\t// assign PR number to `pull_request_number` variable.","\t\tprs, err := v.getPullRequestsWithCommit(ctx, sha, org, repoName, isMergeCommit)","\t\tif err != nil {","\t\t\tv.Logger.Warnf(\"Error getting pull requests associated with the commit in this push event: %v\", err)","\t\t}","","\t\tisGitTagEvent := strings.HasPrefix(gitEvent.GetRef(), \"refs/tags/\")","","\t\tif v.pacInfo.SkipPushEventForPRCommits \u0026\u0026 isGitTagEvent {","\t\t\tv.Logger.Infof(\"Processing tag push event for commit %s despite skip-push-events-for-pr-commits being enabled (tag events are excluded from this setting)\", sha)","\t\t}","","\t\t// Only check if the flag is enabled, and there are pull requests associated with this commit, and it's not a tag push event.","\t\tif v.pacInfo.SkipPushEventForPRCommits \u0026\u0026 len(prs) \u003e 0 \u0026\u0026 !isGitTagEvent {","\t\t\tisPartOfPR, prNumber := v.isCommitPartOfPullRequest(sha, org, repoName, prs)","","\t\t\t// If the commit is part of a PR, skip processing the push event","\t\t\tif isPartOfPR {","\t\t\t\tv.Logger.Infof(\"Skipping push event for commit %s as it belongs to pull request #%d\", sha, prNumber)","\t\t\t\treturn nil, nil","\t\t\t}","\t\t}","","\t\t// if there are pull requests associated with this commit, first pull request number will be used","\t\t// for `pull_request_number` dynamic variable.","\t\tif len(prs) \u003e 0 {","\t\t\tprocessedEvent.PullRequestNumber = *prs[0].Number","\t\t}","","\t\tprocessedEvent.Organization = gitEvent.GetRepo().GetOwner().GetLogin()","\t\tprocessedEvent.Repository = gitEvent.GetRepo().GetName()","\t\tprocessedEvent.DefaultBranch = gitEvent.GetRepo().GetDefaultBranch()","\t\tprocessedEvent.URL = gitEvent.GetRepo().GetHTMLURL()","\t\tv.RepositoryIDs = []int64{gitEvent.GetRepo().GetID()}","\t\tprocessedEvent.SHA = sha","\t\tprocessedEvent.SHAURL = gitEvent.GetHeadCommit().GetURL()","\t\tprocessedEvent.SHATitle = gitEvent.GetHeadCommit().GetMessage()","\t\tprocessedEvent.Sender = gitEvent.GetSender().GetLogin()","\t\tprocessedEvent.BaseBranch = gitEvent.GetRef()","\t\tprocessedEvent.EventType = event.TriggerTarget.String()","\t\tprocessedEvent.HeadBranch = processedEvent.BaseBranch // in push events Head Branch is the same as Basebranch","\t\tprocessedEvent.BaseURL = gitEvent.GetRepo().GetHTMLURL()","\t\tprocessedEvent.HeadURL = processedEvent.BaseURL // in push events Head URL is the same as BaseURL","\t\tv.userType = gitEvent.GetSender().GetType()","\tcase *github.PullRequestEvent:","\t\tprocessedEvent.Repository = gitEvent.GetRepo().GetName()","\t\tif gitEvent.GetRepo() == nil {","\t\t\treturn nil, errors.New(\"error parsing payload the repository should not be nil\")","\t\t}","\t\tprocessedEvent.Organization = gitEvent.GetRepo().Owner.GetLogin()","\t\tprocessedEvent.DefaultBranch = gitEvent.GetRepo().GetDefaultBranch()","\t\tprocessedEvent.SHA = gitEvent.GetPullRequest().Head.GetSHA()","\t\tprocessedEvent.URL = gitEvent.GetRepo().GetHTMLURL()","\t\tprocessedEvent.BaseBranch = gitEvent.GetPullRequest().Base.GetRef()","\t\tprocessedEvent.HeadBranch = gitEvent.GetPullRequest().Head.GetRef()","\t\tprocessedEvent.BaseURL = gitEvent.GetPullRequest().Base.GetRepo().GetHTMLURL()","\t\tprocessedEvent.HeadURL = gitEvent.GetPullRequest().Head.GetRepo().GetHTMLURL()","\t\tprocessedEvent.Sender = gitEvent.GetPullRequest().GetUser().GetLogin()","\t\tprocessedEvent.EventType = event.EventType","\t\tv.userType = gitEvent.GetPullRequest().GetUser().GetType()","","\t\tif gitEvent.Action != nil \u0026\u0026 provider.Valid(*gitEvent.Action, pullRequestLabelEvent) {","\t\t\tprocessedEvent.EventType = string(triggertype.PullRequestLabeled)","\t\t}","","\t\tif gitEvent.GetAction() == \"closed\" {","\t\t\tprocessedEvent.TriggerTarget = triggertype.PullRequestClosed","\t\t}","","\t\tprocessedEvent.PullRequestNumber = gitEvent.GetPullRequest().GetNumber()","\t\tprocessedEvent.PullRequestTitle = gitEvent.GetPullRequest().GetTitle()","\t\t// getting the repository ids of the base and head of the pull request","\t\t// to scope the token to","\t\tv.RepositoryIDs = []int64{","\t\t\tgitEvent.GetPullRequest().GetBase().GetRepo().GetID(),","\t\t}","\t\tfor _, label := range gitEvent.GetPullRequest().Labels {","\t\t\tprocessedEvent.PullRequestLabel = append(processedEvent.PullRequestLabel, label.GetName())","\t\t}","\tdefault:","\t\treturn nil, errors.New(\"this event is not supported\")","\t}","","\t// check before overriding the value for TriggerTarget","\tif processedEvent.TriggerTarget == \"\" {","\t\tprocessedEvent.TriggerTarget = event.TriggerTarget","\t}","\tprocessedEvent.Provider.Token = event.Provider.Token","","\treturn processedEvent, nil","}","","func (v *Provider) handleReRequestEvent(ctx context.Context, event *github.CheckRunEvent) (*info.Event, error) {","\trunevent := info.NewEvent()","\tif event.GetRepo() == nil {","\t\treturn nil, errors.New(\"error parsing payload the repository should not be nil\")","\t}","\trunevent.Organization = event.GetRepo().GetOwner().GetLogin()","\trunevent.Repository = event.GetRepo().GetName()","\trunevent.URL = event.GetRepo().GetHTMLURL()","\trunevent.DefaultBranch = event.GetRepo().GetDefaultBranch()","\trunevent.SHA = event.GetCheckRun().GetCheckSuite().GetHeadSHA()","\trunevent.HeadBranch = event.GetCheckRun().GetCheckSuite().GetHeadBranch()","\trunevent.HeadURL = event.GetCheckRun().GetCheckSuite().GetRepository().GetHTMLURL()","\t// If we don't have a pull_request in this it probably mean a push","\tif len(event.GetCheckRun().GetCheckSuite().PullRequests) == 0 {","\t\trunevent.BaseBranch = runevent.HeadBranch","\t\trunevent.BaseURL = runevent.HeadURL","\t\trunevent.EventType = \"push\"","\t\t// we allow the rerequest user here, not the push user, i guess it's","\t\t// fine because you can't do a rereq without being a github owner?","\t\trunevent.Sender = event.GetSender().GetLogin()","\t\tv.userType = event.GetSender().GetType()","\t\treturn runevent, nil","\t}","\trunevent.PullRequestNumber = event.GetCheckRun().GetCheckSuite().PullRequests[0].GetNumber()","\trunevent.TriggerTarget = triggertype.PullRequest","\tv.Logger.Infof(\"Recheck of PR %s/%s#%d has been requested\", runevent.Organization, runevent.Repository, runevent.PullRequestNumber)","\treturn v.getPullRequest(ctx, runevent)","}","","func (v *Provider) handleCheckSuites(ctx context.Context, event *github.CheckSuiteEvent) (*info.Event, error) {","\trunevent := info.NewEvent()","\tif event.GetRepo() == nil {","\t\treturn nil, errors.New(\"error parsing payload the repository should not be nil\")","\t}","\trunevent.Organization = event.GetRepo().GetOwner().GetLogin()","\trunevent.Repository = event.GetRepo().GetName()","\trunevent.URL = event.GetRepo().GetHTMLURL()","\trunevent.DefaultBranch = event.GetRepo().GetDefaultBranch()","\trunevent.SHA = event.GetCheckSuite().GetHeadSHA()","\trunevent.HeadBranch = event.GetCheckSuite().GetHeadBranch()","\trunevent.HeadURL = event.GetCheckSuite().GetRepository().GetHTMLURL()","\t// If we don't have a pull_request in this it probably mean a push","\t// we are not able to know which","\tif len(event.GetCheckSuite().PullRequests) == 0 {","\t\trunevent.BaseBranch = runevent.HeadBranch","\t\trunevent.BaseURL = runevent.HeadURL","\t\trunevent.EventType = \"push\"","\t\trunevent.TriggerTarget = \"push\"","\t\t// we allow the rerequest user here, not the push user, i guess it's","\t\t// fine because you can't do a rereq without being a github owner?","\t\trunevent.Sender = event.GetSender().GetLogin()","\t\tv.userType = event.GetSender().GetType()","\t\treturn runevent, nil","\t\t// return nil, fmt.Errorf(\"check suite event is not supported for push events\")","\t}","\trunevent.PullRequestNumber = event.GetCheckSuite().PullRequests[0].GetNumber()","\trunevent.TriggerTarget = triggertype.PullRequest","\tv.Logger.Infof(\"Rerun of all check on PR %s/%s#%d has been requested\", runevent.Organization, runevent.Repository, runevent.PullRequestNumber)","\treturn v.getPullRequest(ctx, runevent)","}","","func convertPullRequestURLtoNumber(pullRequest string) (int, error) {","\tprNumber, err := strconv.Atoi(path.Base(pullRequest))","\tif err != nil {","\t\treturn -1, fmt.Errorf(\"bad pull request number html_url number: %w\", err)","\t}","\treturn prNumber, nil","}","","const (","\terrSHANotProvided        = \"a SHA is required in `/ok-to-test` comments, but none was provided\"","\terrSHANotProvidedComment = \"The `/ok-to-test` needs to be followed by a SHA to verify which commit to test. Try again with:\\n\\n`/ok-to-test %s`\"","\terrSHAPrefixMismatch     = \"the SHA provided in the `/ok-to-test` comment (`%s`) is not a prefix of the pull request's HEAD SHA (`%s`)\"","\terrSHANotMatch           = \"the SHA provided in the `/ok-to-test` comment (`%s`) does not match the pull request's HEAD SHA (`%s`)\"",")","","func (v *Provider) handleIssueCommentEvent(ctx context.Context, event *github.IssueCommentEvent) (*info.Event, error) {","\taction := \"recheck\"","\trunevent := info.NewEvent()","\trunevent.Organization = event.GetRepo().GetOwner().GetLogin()","\trunevent.Repository = event.GetRepo().GetName()","\trunevent.Sender = event.GetSender().GetLogin()","\t// Always set the trigger target as pull_request on issue comment events","\trunevent.TriggerTarget = triggertype.PullRequest","\tif !event.GetIssue().IsPullRequest() {","\t\treturn info.NewEvent(), fmt.Errorf(\"issue comment is not coming from a pull_request\")","\t}","\tv.userType = event.GetSender().GetType()","\topscomments.SetEventTypeAndTargetPR(runevent, event.GetComment().GetBody())","","\t// We are getting the full URL so we have to get the last part to get the PR number,","\t// we don\\'t have to care about URL query string/hash and other stuff because","\t// that comes up from the API.","\tvar err error","\trunevent.PullRequestNumber, err = convertPullRequestURLtoNumber(event.GetIssue().GetPullRequestLinks().GetHTMLURL())","\tif err != nil {","\t\treturn info.NewEvent(), err","\t}","","\tv.Logger.Infof(\"issue_comment: pipelinerun %s on %s/%s#%d has been requested\", action, runevent.Organization, runevent.Repository, runevent.PullRequestNumber)","\tpr, err := v.getPullRequest(ctx, runevent)","\tif err != nil {","\t\treturn nil, err","\t}","","\tcommentBody := event.GetComment().GetBody()","\tif opscomments.IsOkToTestComment(commentBody) \u0026\u0026 v.pacInfo.RequireOkToTestSHA {","\t\tshaFromCommentRaw := opscomments.GetSHAFromOkToTestComment(commentBody)","\t\tif shaFromCommentRaw == \"\" {","\t\t\tv.Logger.Errorf(errSHANotProvided)","\t\t\tif err := v.CreateComment(ctx, runevent, fmt.Sprintf(errSHANotProvidedComment, pr.SHA), \"\"); err != nil {","\t\t\t\tv.Logger.Errorf(\"failed to create comment: %v\", err)","\t\t\t}","\t\t\treturn info.NewEvent(), errors.New(errSHANotProvided)","\t\t}","\t\tshaFromComment := strings.ToLower(shaFromCommentRaw)","\t\tprSHALower := strings.ToLower(pr.SHA)","\t\tshaLen := len(shaFromCommentRaw)","","\t\t// Validate SHA-1 based on length:","\t\t// - Short SHAs (\u003c 40 chars): must be a prefix of PR HEAD SHA","\t\t// - Full SHA-1 (40 chars): must match exactly","\t\tif shaLen \u003c 40 {","\t\t\t// Short SHA: verify it's a valid prefix","\t\t\tif !strings.HasPrefix(prSHALower, shaFromComment) {","\t\t\t\tmsg := fmt.Sprintf(errSHAPrefixMismatch, shaFromCommentRaw, pr.SHA)","\t\t\t\tv.Logger.Errorf(msg)","\t\t\t\tif err := v.CreateComment(ctx, runevent, msg, \"\"); err != nil {","\t\t\t\t\tv.Logger.Errorf(\"failed to create comment: %v\", err)","\t\t\t\t}","\t\t\t\treturn info.NewEvent(), fmt.Errorf(errSHAPrefixMismatch, shaFromCommentRaw, pr.SHA)","\t\t\t}","\t\t} else if shaLen == 40 {","\t\t\t// Full SHA-1: verify exact match","\t\t\tif prSHALower != shaFromComment {","\t\t\t\tmsg := fmt.Sprintf(errSHANotMatch, shaFromCommentRaw, pr.SHA)","\t\t\t\tv.Logger.Errorf(msg)","\t\t\t\tif err := v.CreateComment(ctx, runevent, msg, \"\"); err != nil {","\t\t\t\t\tv.Logger.Errorf(\"failed to create comment: %v\", err)","\t\t\t\t}","\t\t\t\treturn info.NewEvent(), fmt.Errorf(errSHANotMatch, shaFromCommentRaw, pr.SHA)","\t\t\t}","\t\t}","\t}","","\treturn pr, nil","}","","func (v *Provider) handleCommitCommentEvent(ctx context.Context, event *github.CommitCommentEvent) (*info.Event, error) {","\taction := \"push\"","\trunevent := info.NewEvent()","\tif event.GetRepo() == nil {","\t\treturn nil, errors.New(\"error parsing payload the repository should not be nil\")","\t}","\trunevent.Organization = event.GetRepo().GetOwner().GetLogin()","\trunevent.Repository = event.GetRepo().GetName()","\trunevent.Sender = event.GetSender().GetLogin()","\tv.userType = event.GetSender().GetType()","\trunevent.URL = event.GetRepo().GetHTMLURL()","\trunevent.SHA = event.GetComment().GetCommitID()","\trunevent.HeadURL = runevent.URL","\trunevent.BaseURL = runevent.HeadURL","\trunevent.TriggerTarget = triggertype.Push","\topscomments.SetEventTypeAndTargetPR(runevent, event.GetComment().GetBody())","\tv.userType = event.GetSender().GetType()","","\tdefaultBranch := event.GetRepo().GetDefaultBranch()","\t// Set Event.Repository.DefaultBranch as default branch to runevent.HeadBranch, runevent.BaseBranch","","\tcommit, _, err := v.Client().Git.GetCommit(ctx, runevent.Organization, runevent.Repository, runevent.SHA)","\tif err != nil {","\t\treturn runevent, fmt.Errorf(\"error getting commit %s: %w\", runevent.SHA, err)","\t}","","\t// as we're going to make GetCommit API again in GetCommitInfo func, it will be cached in provider","\t// so that we're wasting one API call","\tv.commitInfo = commit","","\t// when the commit is a merge commit, either email is 'noreply@github.com' or name is 'web-flow'","\tisMergeCommit := commit.GetCommitter().GetEmail() == githubNoreplyEmail ||","\t\tcommit.GetCommitter().GetName() == githubWebFlowUser","","\tprs, err := v.getPullRequestsWithCommit(ctx, runevent.SHA, runevent.Organization, runevent.Repository, isMergeCommit)","\tif err != nil {","\t\tv.Logger.Warnf(\"Error getting pull requests associated with the commit in this commit comment event: %v\", err)","\t}","\tif len(prs) \u003e 0 {","\t\trunevent.PullRequestNumber = prs[0].GetNumber()","\t}","","\trunevent.HeadBranch, runevent.BaseBranch = defaultBranch, defaultBranch","\tvar (","\t\tbranchName string","\t\tprName     string","\t\ttagName    string","\t)","","\t// If it is a /test or /retest comment with pipelinerun name figure out the pipelinerun name","\tif provider.IsTestRetestComment(event.GetComment().GetBody()) {","\t\tprName, branchName, tagName, err = provider.GetPipelineRunAndBranchOrTagNameFromTestComment(event.GetComment().GetBody())","\t\tif err != nil {","\t\t\treturn runevent, err","\t\t}","\t\trunevent.TargetTestPipelineRun = prName","\t}","\t// Check for /cancel comment","\tif provider.IsCancelComment(event.GetComment().GetBody()) {","\t\taction = \"cancellation\"","\t\tprName, branchName, tagName, err = provider.GetPipelineRunAndBranchOrTagNameFromCancelComment(event.GetComment().GetBody())","\t\tif err != nil {","\t\t\treturn runevent, err","\t\t}","\t\trunevent.CancelPipelineRuns = true","\t\trunevent.TargetCancelPipelineRun = prName","\t}","","\tif tagName != \"\" {","\t\ttagPath := fmt.Sprintf(\"refs/tags/%s\", tagName)","\t\t// here in GitHub TAG_SHA and the commit which is tagged for a tag are different","\t\t// so we need to get the ref for the tag and then get the tag object to get the tag SHA","\t\tref, _, err := wrapAPI(v, \"get_ref\", func() (*github.Reference, *github.Response, error) {","\t\t\treturn v.Client().Git.GetRef(ctx, runevent.Organization, runevent.Repository, tagPath)","\t\t})","\t\tif err != nil {","\t\t\treturn runevent, fmt.Errorf(\"error getting ref for tag %s: %w\", tagName, err)","\t\t}","","\t\ttagSha := \"\"","","\t\tswitch ref.GetObject().GetType() {","\t\tcase \"tag\":","\t\t\t// annotated tag - get the tag object to resolve the commit SHA","\t\t\ttag, _, err := wrapAPI(v, \"get_tag\", func() (*github.Tag, *github.Response, error) {","\t\t\t\treturn v.Client().Git.GetTag(ctx, runevent.Organization, runevent.Repository, ref.GetObject().GetSHA())","\t\t\t})","\t\t\tif err != nil {","\t\t\t\treturn runevent, fmt.Errorf(\"error getting tag %s: %w\", tagName, err)","\t\t\t}","\t\t\ttagSha = tag.GetObject().GetSHA()","\t\tcase \"commit\":","\t\t\t// lightweight tag - ref contains the commit SHA directly.","\t\t\t// trying to get the tag object would return an error.","\t\t\ttagSha = ref.GetObject().GetSHA()","\t\tdefault:","\t\t\treturn runevent, fmt.Errorf(\"invalid object type for tag %s: %s\", tagName, ref.GetObject().GetType())","\t\t}","","\t\tif tagSha != runevent.SHA {","\t\t\treturn runevent, fmt.Errorf(\"provided SHA %s is not the tagged commit for the tag %s\", runevent.SHA, tagName)","\t\t}","\t\trunevent.HeadBranch = tagPath","\t\trunevent.BaseBranch = tagPath","\t\treturn runevent, nil","\t}","","\t// If no branch is specified in GitOps comments, use runevent.HeadBranch","\tif branchName == \"\" {","\t\tbranchName = runevent.HeadBranch","\t}","","\t// Check if the specified branch contains the commit","\tif err = v.isHeadCommitOfBranch(ctx, runevent, branchName); err != nil {","\t\tif provider.IsCancelComment(event.GetComment().GetBody()) {","\t\t\trunevent.CancelPipelineRuns = false","\t\t}","\t\treturn runevent, err","\t}","\t// Finally update branch information to runevent.HeadBranch and runevent.BaseBranch","\trunevent.HeadBranch = branchName","\trunevent.BaseBranch = branchName","","\tv.Logger.Infof(\"github commit_comment: pipelinerun %s on %s/%s#%s has been requested\", action, runevent.Organization, runevent.Repository, runevent.SHA)","\treturn runevent, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,2,2,2,2,2,0,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,1,1,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,0,0,2,2,2,1,1,0,2,2,2,2,2,2,2,0,2,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,0,0,2,2,2,2,2,2,0,2,2,2,0,2,2,2,2,2,2,0,0,0,0,0,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,0,2,0,0,0,2,2,2,0,0,0,0,2,2,2,2,2,2,1,1,0,0,0,0,0,0,2,0,0,0,0,2,2,2,2,2,2,2,0,0,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,1,1,0,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,2,2,2,2,0,0,0,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,0,0,0,0,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,2,2,0,2,2,2,2,2,2,2,2,1,1,2,2,0,0,0,2,2,2,2,2,2,0,0,2,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,1,1,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,0,0,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,1,1,1,1,1,1,1,1,1,0,0,0,0,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,1,1,2,2,0,0,2,2,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,1,1,2,2,2,2,2,2,2,0,0,2,1,1,2,2,2,0,0,0,2,2,2,0,0,2,2,2,2,2,0,0,2,2,2,2,2,0]},{"id":119,"path":"pkg/provider/github/profiler.go","lines":["package github","","import (","\t\"fmt\"","\t\"net/http\"","\t\"strconv\"","\t\"time\"","","\t\"github.com/google/go-github/v81/github\"","\tproviderMetrics \"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/providermetrics\"",")","","const (","\t// Rate limit warning thresholds.","\trateLimitCritical = 50  // Warn when remaining calls \u003c 50","\trateLimitWarning  = 100 // Warn when remaining calls \u003c 100","\trateLimitInfo     = 500 // Info when remaining calls \u003c 500",")","","// checkRateLimit monitors GitHub API rate limits and logs warnings when limits are running low.","func (v *Provider) checkRateLimit(resp *github.Response) (remaining string) {","\tif resp == nil || resp.Response == nil {","\t\treturn \"\"","\t}","","\t// Extract rate limit information from headers (using canonical header keys)","\tvalues := resp.Header[http.CanonicalHeaderKey(\"X-RateLimit-Remaining\")]","\tif len(values) == 0 {","\t\treturn \"\"","\t}","","\tremaining = values[0]","\tif remaining == \"\" {","\t\treturn remaining","\t}","","\t// Parse remaining count","\tremainingCount, err := strconv.Atoi(remaining)","\tif err != nil {","\t\tv.Logger.Debugf(\"Failed to parse rate limit remaining: %s\", remaining)","\t\treturn remaining","\t}","","\t// Get additional rate limit context","\tlimit := \"\"","\treset := \"\"","\tif limitValues := resp.Header[http.CanonicalHeaderKey(\"X-RateLimit-Limit\")]; len(limitValues) \u003e 0 {","\t\tlimit = limitValues[0]","\t}","\tif resetValues := resp.Header[http.CanonicalHeaderKey(\"X-RateLimit-Reset\")]; len(resetValues) \u003e 0 {","\t\treset = resetValues[0]","\t\t// Convert Unix timestamp to human readable time","\t\tif resetTimestamp, err := strconv.ParseInt(reset, 10, 64); err == nil {","\t\t\tresetTime := time.Unix(resetTimestamp, 0)","\t\t\treset = fmt.Sprintf(\"%s (%s)\", reset, resetTime.Format(\"15:04:05 MST\"))","\t\t}","\t}","","\trepoName := \"\"","\tif v.repo != nil {","\t\trepoName = fmt.Sprintf(\"%s/%s\", v.repo.Namespace, v.repo.Name)","\t}","","\t// Log warnings based on thresholds","\tlogFields := []any{","\t\t\"repo\", repoName,","\t\t\"remaining\", remainingCount,","\t\t\"limit\", limit,","\t\t\"reset\", reset,","\t}","\tswitch {","\tcase remainingCount \u003c rateLimitCritical:","\t\tv.Logger.Errorw(\"GitHub API rate limit critically low\", logFields...)","\tcase remainingCount \u003c rateLimitWarning:","\t\tv.Logger.Warnw(\"GitHub API rate limit running low\", logFields...)","\tcase remainingCount \u003c rateLimitInfo:","\t\tv.Logger.Infow(\"GitHub API rate limit moderate\", logFields...)","\t}","","\treturn remaining","}","","// wrapAPI wraps a GitHub API call with logging, metrics, and operation context.","func wrapAPI[T any](v *Provider, operation string, call func() (T, *github.Response, error)) (T, *github.Response, error) {","\t// This check ensures we only profile if a logger is available.","\tif v.Logger == nil {","\t\treturn call()","\t}","","\tstart := time.Now()","\tdata, resp, err := call()","\tduration := time.Since(start)","","\tv.logAPICall(operation, duration, resp, err)","","\treturn data, resp, err","}","","func (v *Provider) logAPICall(operation string, duration time.Duration, resp *github.Response, err error) {","\tproviderMetrics.RecordAPIUsage(v.Logger, v.providerName, v.triggerEvent, v.repo)","","\t// Build structured logging context","\tlogFields := []any{","\t\t\"operation\", operation,","\t\t\"duration_ms\", duration.Milliseconds(),","\t\t\"provider\", \"github\",","\t}","","\t// Add repository context if available","\tif v.repo != nil {","\t\tlogFields = append(logFields, \"repo\", fmt.Sprintf(\"%s/%s\", v.repo.Namespace, v.repo.Name))","\t}","","\t// Add response context if available","\tif resp != nil {","\t\tremaining := v.checkRateLimit(resp)","\t\tlogFields = append(logFields,","\t\t\t\"url_path\", resp.Request.URL.Path,","\t\t\t\"rate_limit_remaining\", remaining,","\t\t)","\t\tif resp.StatusCode \u003e 0 {","\t\t\tlogFields = append(logFields, \"status_code\", resp.StatusCode)","\t\t}","\t}","","\t// Log based on success/failure with appropriate level","\tif err != nil {","\t\tlogFields = append(logFields, \"error\", err.Error())","\t\tv.Logger.Errorw(\"GitHub API call failed\", logFields...)","\t} else {","\t\tv.Logger.Debugw(\"GitHub API call completed\", logFields...)","\t}","}","","// wrapAPIGetContents wraps the GetContents API call with operation context.","func wrapAPIGetContents(v *Provider, operation string, call func() (*github.RepositoryContent, []*github.RepositoryContent, *github.Response, error)) (*github.RepositoryContent, []*github.RepositoryContent, *github.Response, error) {","\t// This check ensures we only profile if a logger is available.","\tif v.Logger == nil {","\t\treturn call()","\t}","","\tstart := time.Now()","\tfile, dir, resp, err := call()","\tduration := time.Since(start)","","\tv.logAPICall(operation, duration, resp, err)","","\treturn file, dir, resp, err","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,1,1,0,0,2,2,2,2,0,2,2,1,1,0,0,2,2,1,1,1,0,0,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,0,0,0,2,2,2,2,2,0,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,0,0,0,2,2,2,2,2,0,2,2,2,2,2,2,2,0]},{"id":120,"path":"pkg/provider/github/repository.go","lines":["package github","","import (","\t\"context\"","\t\"encoding/json\"","\t\"fmt\"","\t\"net/http\"","","\t\"github.com/google/go-github/v81/github\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/formatting\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/clients\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/templates\"","\t\"go.uber.org/zap\"","\tcorev1 \"k8s.io/api/core/v1\"","\t\"k8s.io/apimachinery/pkg/api/errors\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"",")","","const (","\tdefaultNsTemplate   = \"%v-pipelines\"","\tdefaultRepoTemplate = \"%v-repo-cr\"",")","","func ConfigureRepository(ctx context.Context, run *params.Run, req *http.Request, payload string, pacInfo *info.PacOpts, logger *zap.SugaredLogger) (bool, bool, error) {","\t// check if repo auto configuration is enabled","\tif !pacInfo.AutoConfigureNewGitHubRepo {","\t\treturn false, false, nil","\t}","\t// gitea set x-github-event too, so skip it for the gitea driver","\tif h := req.Header.Get(\"X-Gitea-Event-Type\"); h != \"\" {","\t\treturn false, false, nil","\t}","\tevent := req.Header.Get(\"X-Github-Event\")","\tif event != \"repository\" {","\t\treturn false, false, nil","\t}","","\teventInt, err := github.ParseWebHook(event, []byte(payload))","\tif err != nil {","\t\treturn true, false, err","\t}","\t_ = json.Unmarshal([]byte(payload), \u0026eventInt)","\trepoEvent, _ := eventInt.(*github.RepositoryEvent)","","\tif repoEvent.GetAction() != \"created\" {","\t\tlogger.Infof(\"github: repository event \\\"%v\\\" is not supported\", repoEvent.GetAction())","\t\treturn true, false, nil","\t}","","\tlogger.Infof(\"github: configuring repository cr for repo: %v\", repoEvent.Repo.GetHTMLURL())","\tnsTemplate := pacInfo.AutoConfigureRepoNamespaceTemplate","\trepoTemplate := pacInfo.AutoConfigureRepoRepositoryTemplate","\tif err := createRepository(ctx, nsTemplate, repoTemplate, run.Clients, repoEvent, logger); err != nil {","\t\tlogger.Errorf(\"failed repository creation: %v\", err)","\t\treturn true, true, err","\t}","","\treturn true, true, nil","}","","func createRepository(ctx context.Context, nsTemplate, repoTemplate string, clients clients.Clients, gitEvent *github.RepositoryEvent, logger *zap.SugaredLogger) error {","\trepoNsName, repoCRName, err := generateNamespaceAndRepositoryName(nsTemplate, repoTemplate, gitEvent)","\tif err != nil {","\t\treturn fmt.Errorf(\"failed to generate namespace for repo: %w\", err)","\t}","","\tlogger.Info(\"github: generated namespace name: \", repoNsName)","","\t// create namespace","\trepoNs := \u0026corev1.Namespace{","\t\tObjectMeta: metav1.ObjectMeta{","\t\t\tName: repoNsName,","\t\t},","\t}","\trepoNs, err = clients.Kube.CoreV1().Namespaces().Create(ctx, repoNs, metav1.CreateOptions{})","\tif err != nil \u0026\u0026 !errors.IsAlreadyExists(err) {","\t\treturn fmt.Errorf(\"failed to create namespace %v: %w\", repoNs.Name, err)","\t}","","\tif errors.IsAlreadyExists(err) {","\t\tlogger.Infof(\"github: namespace %v already exists, creating repository\", repoNsName)","\t} else {","\t\tlogger.Info(\"github: created repository namespace: \", repoNs.Name)","\t}","","\t// create repository","\trepo := \u0026v1alpha1.Repository{","\t\tObjectMeta: metav1.ObjectMeta{","\t\t\tName:      repoCRName,","\t\t\tNamespace: repoNsName,","\t\t},","\t\tSpec: v1alpha1.RepositorySpec{","\t\t\tURL: gitEvent.Repo.GetHTMLURL(),","\t\t},","\t}","\trepo, err = clients.PipelineAsCode.PipelinesascodeV1alpha1().Repositories(repoNsName).Create(ctx, repo, metav1.CreateOptions{})","\tif err != nil {","\t\treturn fmt.Errorf(\"failed to create repository for repo: %v: %w\", gitEvent.Repo.GetHTMLURL(), err)","\t}","\tlogger = logger.With(\"namespace\", repo.Namespace)","\tlogger.Infof(\"github: repository created: %s/%s \", repo.Namespace, repo.Name)","\treturn nil","}","","func generateNamespaceAndRepositoryName(nsTemplate, repoTemplate string, gitEvent *github.RepositoryEvent) (string, string, error) {","\trepoOwner, repoName, err := formatting.GetRepoOwnerSplitted(gitEvent.Repo.GetHTMLURL())","\tif err != nil {","\t\treturn \"\", \"\", fmt.Errorf(\"failed to parse git repo url: %w\", err)","\t}","","\tnsName := \"\"","\trepoCRName := \"\"","\tplaceholders := map[string]string{","\t\t\"repo_owner\": repoOwner,","\t\t\"repo_name\":  repoName,","\t}","","\tif nsTemplate == \"\" {","\t\tnsName = fmt.Sprintf(defaultNsTemplate, repoName)","\t} else {","\t\tnsName = templates.ReplacePlaceHoldersVariables(nsTemplate, placeholders, nil, http.Header{}, map[string]any{})","\t}","","\tif repoTemplate == \"\" {","\t\trepoCRName = fmt.Sprintf(defaultRepoTemplate, repoName)","\t} else {","\t\trepoCRName = templates.ReplacePlaceHoldersVariables(repoTemplate, placeholders, nil, http.Header{}, map[string]any{})","\t}","\treturn nsName, repoCRName, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,1,1,0,2,1,1,2,2,2,2,0,2,2,1,1,2,2,2,2,2,2,2,0,2,2,2,2,1,1,1,0,2,0,0,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,1,1,0,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,0,0,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,0]},{"id":121,"path":"pkg/provider/github/scope.go","lines":["package github","","import (","\t\"context\"","\t\"errors\"","\t\"fmt\"","\t\"net/url\"","\t\"strings\"","","\t\"github.com/gobwas/glob\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/events\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/settings\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"go.uber.org/zap\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"",")","","func ScopeTokenToListOfRepos(ctx context.Context, vcx provider.Interface, pacInfo *info.PacOpts, repo *v1alpha1.Repository, run *params.Run,","\tevent *info.Event, eventEmitter *events.EventEmitter, logger *zap.SugaredLogger,",") (string, error) {","\tvar (","\t\tlistRepos bool","\t\ttoken     string","\t)","\tlistURLs := []string{}","\trepoListToScopeToken := []string{}","","\t// This is a Global config to provide list of repos to scope token","\tif pacInfo.SecretGhAppTokenScopedExtraRepos != \"\" {","\t\tfor _, configValue := range strings.Split(pacInfo.SecretGhAppTokenScopedExtraRepos, \",\") {","\t\t\tconfigValueS := strings.TrimSpace(configValue)","\t\t\tif configValueS == \"\" {","\t\t\t\tcontinue","\t\t\t}","\t\t\t// May or may not be a glob","\t\t\tif _, err := glob.Compile(configValueS); err != nil {","\t\t\t\tmsg := fmt.Sprintf(\"invalid repo glob specified %s\", configValueS)","\t\t\t\teventEmitter.EmitMessage(nil, zap.ErrorLevel, \"InvalidRepoGlobSpecified\", msg)","","\t\t\t\treturn \"\", errors.New(msg)","\t\t\t}","","\t\t\trepoListToScopeToken = append(repoListToScopeToken, configValueS)","\t\t}","\t\tlistRepos = true","\t\tlogger.Infof(\"configured Global configuration to %v to scope Github token \", repoListToScopeToken)","\t}","\tif repo.Spec.Settings != nil \u0026\u0026 len(repo.Spec.Settings.GithubAppTokenScopeRepos) != 0 {","\t\tns := repo.Namespace","\t\trepoListInPerticularNamespace, err := run.Clients.PipelineAsCode.PipelinesascodeV1alpha1().Repositories(ns).List(ctx, metav1.ListOptions{})","\t\tif err != nil {","\t\t\treturn \"\", err","\t\t}","\t\tfor i := range repoListInPerticularNamespace.Items {","\t\t\tsplitData, err := getURLPathData(repoListInPerticularNamespace.Items[i].Spec.URL)","\t\t\tif err != nil {","\t\t\t\treturn \"\", err","\t\t\t}","\t\t\tlistURLs = append(listURLs, splitData[1]+\"/\"+splitData[2])","\t\t}","\t\tfor i := range repo.Spec.Settings.GithubAppTokenScopeRepos {","\t\t\t// May or may not be a glob","\t\t\trepoToScope, err := glob.Compile(repo.Spec.Settings.GithubAppTokenScopeRepos[i])","\t\t\tif err != nil {","\t\t\t\tmsg := fmt.Sprintf(\"invalid repo glob specified %s\", repo.Spec.Settings.GithubAppTokenScopeRepos[i])","\t\t\t\teventEmitter.EmitMessage(nil, zap.ErrorLevel, \"InvalidRepoGlobSpecified\", msg)","","\t\t\t\treturn \"\", errors.New(msg)","\t\t\t}","\t\t\tglobMatchFound := false","\t\t\t// Match glob to repos list.","\t\t\tfor _, repoKey := range listURLs {","\t\t\t\tif repoToScope.Match(repoKey) {","\t\t\t\t\trepoListToScopeToken = append(repoListToScopeToken, repoKey)","\t\t\t\t\tglobMatchFound = true","\t\t\t\t}","\t\t\t}","\t\t\tif !globMatchFound {","\t\t\t\tmsg := fmt.Sprintf(\"failed to scope GitHub token as repo with pattern %s does not exist in namespace %s\", repo.Spec.Settings.GithubAppTokenScopeRepos[i], ns)","\t\t\t\teventEmitter.EmitMessage(nil, zap.ErrorLevel, \"RepoDoesNotExistInNamespace\", msg)","\t\t\t\treturn \"\", errors.New(msg)","\t\t\t}","\t\t}","\t\t// When the global configuration is not set then check for secret-github-app-token-scoped key for the repo level configuration","\t\tif pacInfo.SecretGHAppRepoScoped \u0026\u0026 pacInfo.SecretGhAppTokenScopedExtraRepos == \"\" {","\t\t\tmsg := fmt.Sprintf(`failed to scope GitHub token as repo scoped key %s is enabled. Hint: update key %s from pipelines-as-code configmap to false`,","\t\t\t\tsettings.SecretGhAppTokenRepoScopedKey, settings.SecretGhAppTokenRepoScopedKey)","\t\t\teventEmitter.EmitMessage(nil, zap.ErrorLevel, \"SecretGHAppTokenRepoScopeIsEnabled\", msg)","\t\t\treturn \"\", errors.New(msg)","\t\t}","\t\tlistRepos = true","\t\tlogger.Infof(\"configured repo level configuration to %v to scope Github token \", repo.Spec.Settings.GithubAppTokenScopeRepos)","\t}","\tif listRepos {","\t\trepoInfoFromWhichEventCame, err := getURLPathData(repo.Spec.URL)","\t\tif err != nil {","\t\t\treturn \"\", err","\t\t}","\t\t// adding the repo info from which event came so that repositoryID will be added while scoping the token","\t\trepoListToScopeToken = append(repoListToScopeToken, repoInfoFromWhichEventCame[1]+\"/\"+repoInfoFromWhichEventCame[2])","\t\ttoken, err = vcx.CreateToken(ctx, repoListToScopeToken, event)","\t\tif err != nil {","\t\t\treturn \"\", fmt.Errorf(\"failed to scope token to repositories with error : %w\", err)","\t\t}","\t\tlogger.Infof(\"Github token scope extended to %v \", repoListToScopeToken)","\t}","\treturn token, nil","}","","func getURLPathData(urlInfo string) ([]string, error) {","\turlData, err := url.ParseRequestURI(urlInfo)","\tif err != nil {","\t\treturn []string{}, err","\t}","\treturn strings.Split(urlData.Path, \"/\"), nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,1,0,0,2,2,2,2,2,2,0,2,0,2,2,0,2,2,2,2,1,1,2,2,2,1,1,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,0,2,2,2,1,1,0,2,2,2,1,1,2,0,2,0,0,2,2,2,1,1,2,0]},{"id":122,"path":"pkg/provider/github/status.go","lines":["package github","","import (","\t\"context\"","\t\"fmt\"","\t\"regexp\"","\t\"strconv\"","\t\"strings\"","\t\"time\"","","\t\"github.com/google/go-github/v81/github\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/action\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction\"","\tkstatus \"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction/status\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/opscomments\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"",")","","const (","\tbotType         = \"Bot\"","\tpendingApproval = \"Pending approval, waiting for an /ok-to-test\"",")","","const taskStatusTemplate = `","\u003ctable\u003e","  \u003ctr\u003e\u003cth\u003eStatus\u003c/th\u003e\u003cth\u003eDuration\u003c/th\u003e\u003cth\u003eName\u003c/th\u003e\u003c/tr\u003e","","{{- range $taskrun := .TaskRunList }}","\u003ctr\u003e","\u003ctd\u003e{{ formatCondition $taskrun.PipelineRunTaskRunStatus.Status.Conditions }}\u003c/td\u003e","\u003ctd\u003e{{ formatDuration $taskrun.PipelineRunTaskRunStatus.Status.StartTime $taskrun.PipelineRunTaskRunStatus.Status.CompletionTime }}\u003c/td\u003e\u003ctd\u003e","","{{ $taskrun.ConsoleLogURL }}","","\u003c/td\u003e\u003c/tr\u003e","{{- end }}","\u003c/table\u003e`","","func (v *Provider) getExistingCheckRunID(ctx context.Context, runevent *info.Event, status provider.StatusOpts) (*int64, error) {","\topt := github.ListOptions{PerPage: v.PaginedNumber}","\tfor {","\t\tres, resp, err := wrapAPI(v, \"list_check_runs_for_ref\", func() (*github.ListCheckRunsResults, *github.Response, error) {","\t\t\treturn v.Client().Checks.ListCheckRunsForRef(ctx, runevent.Organization, runevent.Repository,","\t\t\t\trunevent.SHA, \u0026github.ListCheckRunsOptions{","\t\t\t\t\tAppID:       v.ApplicationID,","\t\t\t\t\tListOptions: opt,","\t\t\t\t})","\t\t})","\t\tif err != nil {","\t\t\treturn nil, err","\t\t}","","\t\tfor _, checkrun := range res.CheckRuns {","\t\t\t// if it is a Pending approval CheckRun then overwrite it","\t\t\tif isPendingApprovalCheckrun(checkrun) || isFailedCheckrun(checkrun) {","\t\t\t\tif v.canIUseCheckrunID(checkrun.ID) {","\t\t\t\t\treturn checkrun.ID, nil","\t\t\t\t}","\t\t\t}","\t\t\tif *checkrun.ExternalID == status.PipelineRunName {","\t\t\t\treturn checkrun.ID, nil","\t\t\t}","\t\t}","\t\tif resp.NextPage == 0 {","\t\t\tbreak","\t\t}","\t\topt.Page = resp.NextPage","\t}","","\treturn nil, nil","}","","func isPendingApprovalCheckrun(run *github.CheckRun) bool {","\tif run == nil || run.Output == nil {","\t\treturn false","\t}","\tif run.Output.Title != nil \u0026\u0026 strings.Contains(*run.Output.Title, \"Pending\") \u0026\u0026","\t\trun.Output.Summary != nil \u0026\u0026","\t\tstrings.Contains(*run.Output.Summary, \"is waiting for approval\") {","\t\treturn true","\t}","\treturn false","}","","func isFailedCheckrun(run *github.CheckRun) bool {","\tif run == nil || run.Output == nil {","\t\treturn false","\t}","\tif run.Output.Title != nil \u0026\u0026 strings.Contains(*run.Output.Title, \"pipelinerun start failure\") \u0026\u0026","\t\trun.Output.Summary != nil \u0026\u0026","\t\tstrings.Contains(*run.Output.Summary, \"failed\") {","\t\treturn true","\t}","\treturn false","}","","func (v *Provider) canIUseCheckrunID(checkrunid *int64) bool {","\tv.mutex.Lock()","\tdefer v.mutex.Unlock()","","\tif v.checkRunID == 0 {","\t\tv.checkRunID = *checkrunid","\t\treturn true","\t}","\treturn false","}","","func (v *Provider) createCheckRunStatus(ctx context.Context, runevent *info.Event, status provider.StatusOpts) (*int64, error) {","\tnow := github.Timestamp{Time: time.Now()}","\tcheckrunoption := github.CreateCheckRunOptions{","\t\tName:    provider.GetCheckName(status, v.pacInfo),","\t\tHeadSHA: runevent.SHA,","\t\tStatus:  github.Ptr(status.Status), // take status from statusOpts because it can be in_progress, queued, or failure // same for conclusion as well","\t\tOutput: \u0026github.CheckRunOutput{","\t\t\tTitle:   github.Ptr(status.Title),","\t\t\tSummary: github.Ptr(status.Summary),","\t\t\tText:    github.Ptr(status.Text),","\t\t},","\t\tDetailsURL: github.Ptr(status.DetailsURL),","\t\tExternalID: github.Ptr(status.PipelineRunName),","\t\tStartedAt:  \u0026now,","\t}","","\tif status.Status != \"in_progress\" \u0026\u0026 status.Status != \"queued\" {","\t\tcheckrunoption.Conclusion = github.Ptr(status.Conclusion)","\t}","","\tcheckRun, _, err := wrapAPI(v, \"create_check_run\", func() (*github.CheckRun, *github.Response, error) {","\t\treturn v.Client().Checks.CreateCheckRun(ctx, runevent.Organization, runevent.Repository, checkrunoption)","\t})","\tif err != nil {","\t\treturn nil, err","\t}","\treturn checkRun.ID, nil","}","","func (v *Provider) getFailuresMessageAsAnnotations(ctx context.Context, pr *tektonv1.PipelineRun, pacopts *info.PacOpts) []*github.CheckRunAnnotation {","\tannotations := []*github.CheckRunAnnotation{}","\tr, err := regexp.Compile(pacopts.ErrorDetectionSimpleRegexp)","\tif err != nil {","\t\tv.Logger.Errorf(\"invalid regexp for filtering failure messages: %v\", pacopts.ErrorDetectionSimpleRegexp)","\t\treturn annotations","\t}","\tintf, err := kubeinteraction.NewKubernetesInteraction(v.Run)","\tif err != nil {","\t\tv.Logger.Errorf(\"failed to create kubeinteraction: %v\", err)","\t\treturn annotations","\t}","\ttaskinfos := kstatus.CollectFailedTasksLogSnippet(ctx, v.Run, intf, pr, int64(pacopts.ErrorDetectionNumberOfLines))","\tfor _, taskinfo := range taskinfos {","\t\tfor _, errline := range strings.Split(taskinfo.LogSnippet, \"\\n\") {","\t\t\tresults := map[string]string{}","\t\t\tif !r.MatchString(errline) {","\t\t\t\tcontinue","\t\t\t}","\t\t\tmatches := r.FindStringSubmatch(errline)","\t\t\tfor i, name := range r.SubexpNames() {","\t\t\t\tif i != 0 \u0026\u0026 name != \"\" {","\t\t\t\t\tresults[name] = matches[i]","\t\t\t\t}","\t\t\t}","","\t\t\t// check if we  have file in results","\t\t\tvar linenumber, errmsg, filename string","\t\t\tvar ok bool","","\t\t\tif filename, ok = results[\"filename\"]; !ok {","\t\t\t\tv.Logger.Errorf(\"regexp for filtering failure messages does not contain a filename regexp group: %v\", pacopts.ErrorDetectionSimpleRegexp)","\t\t\t\tcontinue","\t\t\t}","\t\t\t// remove ./ cause it would bug github otherwise","\t\t\tfilename = strings.TrimPrefix(filename, \"./\")","","\t\t\tif linenumber, ok = results[\"line\"]; !ok {","\t\t\t\tv.Logger.Errorf(\"regexp for filtering failure messages does not contain a line regexp group: %v\", pacopts.ErrorDetectionSimpleRegexp)","\t\t\t\tcontinue","\t\t\t}","","\t\t\tif errmsg, ok = results[\"error\"]; !ok {","\t\t\t\tv.Logger.Errorf(\"regexp for filtering failure messages does not contain a error regexp group: %v\", pacopts.ErrorDetectionSimpleRegexp)","\t\t\t\tcontinue","\t\t\t}","","\t\t\tilinenumber, err := strconv.Atoi(linenumber)","\t\t\tif err != nil {","\t\t\t\t// can't do much regexp has probably failed to detect","\t\t\t\tv.Logger.Errorf(\"cannot convert %s as integer: %v\", linenumber, err)","\t\t\t\tcontinue","\t\t\t}","\t\t\tannotations = append(annotations, \u0026github.CheckRunAnnotation{","\t\t\t\tPath:            github.Ptr(filename),","\t\t\t\tStartLine:       github.Ptr(ilinenumber),","\t\t\t\tEndLine:         github.Ptr(ilinenumber),","\t\t\t\tAnnotationLevel: github.Ptr(\"failure\"),","\t\t\t\tMessage:         github.Ptr(errmsg),","\t\t\t})","\t\t}","\t}","\treturn annotations","}","","// getOrUpdateCheckRunStatus create a status via the checkRun API, which is only","// available with GitHub apps tokens.","func (v *Provider) getOrUpdateCheckRunStatus(ctx context.Context, runevent *info.Event, statusOpts provider.StatusOpts) error {","\tvar err error","\tvar checkRunID *int64","\tvar found bool","\tpacopts := v.pacInfo","","\t// The purpose of this condition is to limit the generation of checkrun IDs","\t// when multiple pipelineruns fail. In such cases, generate only one checkrun ID,","\t// regardless of the number of failed pipelineruns.","\tif statusOpts.Title == \"Failed\" \u0026\u0026 statusOpts.PipelineRunName == \"\" {","\t\t// setting different title to handle multiple checkrun cases","\t\tstatusOpts.Title = \"pipelinerun start failure\"","\t\tif statusOpts.InstanceCountForCheckRun \u003e= 1 {","\t\t\treturn nil","\t\t}","\t}","","\t// check if pipelineRun has the label with checkRun-id","\tif statusOpts.PipelineRun != nil {","\t\tvar id string","\t\tid, found = statusOpts.PipelineRun.GetAnnotations()[keys.CheckRunID]","\t\tif found {","\t\t\tcheckID, err := strconv.Atoi(id)","\t\t\tif err != nil {","\t\t\t\treturn fmt.Errorf(\"api error: cannot convert checkrunid\")","\t\t\t}","\t\t\tcheckRunID = github.Ptr(int64(checkID))","\t\t}","\t}","\tif !found {","\t\tif checkRunID, _ = v.getExistingCheckRunID(ctx, runevent, statusOpts); checkRunID == nil {","\t\t\tcheckRunID, err = v.createCheckRunStatus(ctx, runevent, statusOpts)","\t\t\tif err != nil {","\t\t\t\treturn err","\t\t\t}","\t\t}","","\t\t// Patch the pipelineRun with the checkRunID and logURL only when the pipelineRun is not nil and has a name","\t\t// because on validation failed PipelineRun will provide PipelineRun struct but it is not a valid resource","\t\t// created in cluster so if its only validation error report then ignore patching the pipelineRun.","\t\tif statusOpts.PipelineRun != nil \u0026\u0026 (statusOpts.PipelineRun.GetName() != \"\" || statusOpts.PipelineRun.GetGenerateName() != \"\") {","\t\t\tif _, err := action.PatchPipelineRun(ctx, v.Logger, \"checkRunID and logURL\", v.Run.Clients.Tekton, statusOpts.PipelineRun, metadataPatch(checkRunID, statusOpts.DetailsURL)); err != nil {","\t\t\t\treturn err","\t\t\t}","\t\t}","\t}","","\ttext := statusOpts.Text","\tcheckRunOutput := \u0026github.CheckRunOutput{","\t\tTitle:   \u0026statusOpts.Title,","\t\tSummary: \u0026statusOpts.Summary,","\t}","","\tif statusOpts.PipelineRun != nil {","\t\tif pacopts.ErrorDetection {","\t\t\tcheckRunOutput.Annotations = v.getFailuresMessageAsAnnotations(ctx, statusOpts.PipelineRun, pacopts)","\t\t}","\t}","","\tcheckRunOutput.Text = github.Ptr(text)","","\topts := github.UpdateCheckRunOptions{","\t\tName:   provider.GetCheckName(statusOpts, pacopts),","\t\tStatus: github.Ptr(statusOpts.Status),","\t\tOutput: checkRunOutput,","\t}","\tif statusOpts.PipelineRunName != \"\" {","\t\topts.ExternalID = github.Ptr(statusOpts.PipelineRunName)","\t}","\tif statusOpts.DetailsURL != \"\" {","\t\topts.DetailsURL = \u0026statusOpts.DetailsURL","\t}","","\t// Only set completed-at if conclusion is set (which means finished)","\tif statusOpts.Conclusion != \"\" \u0026\u0026 statusOpts.Conclusion != \"pending\" {","\t\topts.CompletedAt = \u0026github.Timestamp{Time: time.Now()}","\t\topts.Conclusion = \u0026statusOpts.Conclusion","\t}","\tif isPipelineRunCancelledOrStopped(statusOpts.PipelineRun) {","\t\topts.Conclusion = github.Ptr(\"cancelled\")","\t}","","\t_, _, err = wrapAPI(v, \"update_check_run\", func() (*github.CheckRun, *github.Response, error) {","\t\treturn v.Client().Checks.UpdateCheckRun(ctx, runevent.Organization, runevent.Repository, *checkRunID, opts)","\t})","\treturn err","}","","func isPipelineRunCancelledOrStopped(run *tektonv1.PipelineRun) bool {","\tif run == nil {","\t\treturn false","\t}","\tif run.IsCancelled() || run.IsGracefullyCancelled() || run.IsGracefullyStopped() {","\t\treturn true","\t}","\treturn false","}","","func metadataPatch(checkRunID *int64, logURL string) map[string]any {","\treturn map[string]any{","\t\t\"metadata\": map[string]any{","\t\t\t\"labels\": map[string]string{","\t\t\t\tkeys.CheckRunID: strconv.FormatInt(*checkRunID, 10),","\t\t\t},","\t\t\t\"annotations\": map[string]string{","\t\t\t\tkeys.LogURL:     logURL,","\t\t\t\tkeys.CheckRunID: strconv.FormatInt(*checkRunID, 10),","\t\t\t},","\t\t},","\t}","}","","// createStatusCommit use the classic/old statuses API which is available when we","// don't have a github app token.","func (v *Provider) createStatusCommit(ctx context.Context, runevent *info.Event, status provider.StatusOpts) error {","\tvar err error","\tnow := time.Now()","\tswitch status.Conclusion {","\tcase \"neutral\":","\t\tstatus.Conclusion = \"success\" // We don't have a choice than setting as success, no pending here.","\tcase \"pending\":","\t\tif status.Title != \"\" {","\t\t\tstatus.Conclusion = \"pending\"","\t\t}","\t}","\tif status.Status == \"in_progress\" {","\t\tstatus.Conclusion = \"pending\"","\t}","","\tghstatus := github.RepoStatus{","\t\tState:       github.Ptr(status.Conclusion),","\t\tTargetURL:   github.Ptr(status.DetailsURL),","\t\tDescription: github.Ptr(status.Title),","\t\tContext:     github.Ptr(provider.GetCheckName(status, v.pacInfo)),","\t\tCreatedAt:   \u0026github.Timestamp{Time: now},","\t}","","\tif _, _, err := wrapAPI(v, \"create_status\", func() (*github.RepoStatus, *github.Response, error) {","\t\treturn v.Client().Repositories.CreateStatus(ctx,","\t\t\trunevent.Organization, runevent.Repository, runevent.SHA, ghstatus)","\t}); err != nil {","\t\treturn err","\t}","\teventType := triggertype.IsPullRequestType(runevent.EventType)","\tif opscomments.IsAnyOpsEventType(eventType.String()) {","\t\teventType = triggertype.PullRequest","\t}","","\tvar commentStrategy string","\tif v.repo != nil \u0026\u0026 v.repo.Spec.Settings != nil \u0026\u0026 v.repo.Spec.Settings.Github != nil {","\t\tcommentStrategy = v.repo.Spec.Settings.Github.CommentStrategy","\t}","","\tswitch commentStrategy {","\tcase \"disable_all\":","\t\tv.Logger.Warn(\"github: comments related to PipelineRuns status have been disabled for Github pull requests\")","\t\treturn nil","\tdefault:","\t\tif (status.Status == \"completed\" || (status.Status == \"queued\" \u0026\u0026 status.Title == pendingApproval)) \u0026\u0026","\t\t\tstatus.Text != \"\" \u0026\u0026 eventType == triggertype.PullRequest {","\t\t\t_, _, err = wrapAPI(v, \"create_issue_comment\", func() (*github.IssueComment, *github.Response, error) {","\t\t\t\treturn v.Client().Issues.CreateComment(ctx, runevent.Organization, runevent.Repository,","\t\t\t\t\trunevent.PullRequestNumber,","\t\t\t\t\t\u0026github.IssueComment{","\t\t\t\t\t\tBody: github.Ptr(fmt.Sprintf(\"%s\u003cbr\u003e%s\", status.Summary, status.Text)),","\t\t\t\t\t},","\t\t\t\t)","\t\t\t})","\t\t\tif err != nil {","\t\t\t\treturn err","\t\t\t}","\t\t}","\t}","","\treturn nil","}","","func (v *Provider) CreateStatus(ctx context.Context, runevent *info.Event, statusOpts provider.StatusOpts) error {","\tif v.ghClient == nil {","\t\treturn fmt.Errorf(\"cannot set status on github no token or url set\")","\t}","","\t// If the request comes from a bot user, skip setting the status and just log the event silently","\tif statusOpts.AccessDenied \u0026\u0026 v.userType == botType {","\t\treturn nil","\t}","","\tswitch statusOpts.Conclusion {","\tcase \"success\":","\t\tstatusOpts.Title = \"Success\"","\t\tstatusOpts.Summary = \"has \u003cb\u003esuccessfully\u003c/b\u003e validated your commit.\"","\tcase \"failure\":","\t\tif statusOpts.Title == \"\" {","\t\t\tstatusOpts.Title = \"Failed\"","\t\t}","\t\tstatusOpts.Summary = \"has \u003cb\u003efailed\u003c/b\u003e.\"","\tcase \"pending\":","\t\t// for concurrency set title as pending","\t\tif statusOpts.Title == \"\" {","\t\t\tstatusOpts.Title = \"Pending\"","\t\t\tstatusOpts.Summary = \"is skipping this commit.\"","\t\t} else {","\t\t\t// for unauthorized user set title as Pending approval","\t\t\tstatusOpts.Summary = \"is waiting for approval.\"","\t\t}","\tcase \"cancelled\":","\t\tstatusOpts.Title = \"Cancelled\"","\t\tstatusOpts.Summary = \"has been \u003cb\u003ecancelled\u003c/b\u003e.\"","\tcase \"neutral\":","\t\tif statusOpts.Title == \"\" {","\t\t\tstatusOpts.Title = \"Unknown\"","\t\t}","\t\tstatusOpts.Summary = \"\u003cb\u003eCompleted\u003c/b\u003e\"","\t}","","\tif statusOpts.Status == \"in_progress\" {","\t\tstatusOpts.Title = \"CI has Started\"","\t\tstatusOpts.Summary = \"is running.\"","\t}","","\tonPr := \"\"","\tif statusOpts.OriginalPipelineRunName != \"\" {","\t\tonPr = \"/\" + statusOpts.OriginalPipelineRunName","\t}","\tstatusOpts.Summary = fmt.Sprintf(\"%s%s %s\", v.pacInfo.ApplicationName, onPr, statusOpts.Summary)","\t// If we have an installationID which mean we have a github apps and we can use the checkRun API","\tif runevent.InstallationID \u003e 0 {","\t\treturn v.getOrUpdateCheckRunStatus(ctx, runevent, statusOpts)","\t}","","\t// Otherwise use the update status commit API","\treturn v.createStatusCommit(ctx, runevent, statusOpts)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,0,2,2,2,0,2,2,0,2,0,0,2,0,0,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,1,1,2,0,0,2,2,2,2,2,2,2,2,1,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,1,1,2,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,0,0,0,1,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,0,0,1,1,1,1,1,0,1,1,1,1,1,1,1,0,0,1,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,1,1,2,0,0,2,2,2,2,1,1,0,0,0,0,0,2,2,1,1,0,0,0,2,2,2,2,2,2,2,2,1,1,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,1,1,0,2,2,2,2,0,0,2,2,2,2,2,1,1,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,2,2,1,1,0,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,2,1,1,0,2,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,0,0,2,0,0,2,2,1,1,0,0,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,2,2,2,2,2,0,0,2,2,2,2,0,2,2,1,1,2,2,2,2,2,0,0,2,0]},{"id":123,"path":"pkg/provider/gitlab/acl.go","lines":["package gitlab","","import (","\t\"context\"","\t\"fmt\"","\t\"net/http\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/acl\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\tgitlab \"gitlab.com/gitlab-org/api/client-go\"",")","","// IsAllowedOwnersFile get the owner files (OWNERS, OWNERS_ALIASES) from main branch","// and check if we have explicitly allowed the user in there.","func (v *Provider) IsAllowedOwnersFile(_ context.Context, event *info.Event) (bool, error) {","\townerContent, _, _ := v.getObject(\"OWNERS\", event.DefaultBranch, v.targetProjectID)","\tif string(ownerContent) == \"\" {","\t\treturn false, nil","\t}","\t// OWNERS_ALIASES file existence is not required, if we get \"not found\" continue","\townerAliasesContent, resp, err := v.getObject(\"OWNERS_ALIASES\", event.DefaultBranch, v.targetProjectID)","\tif resp.StatusCode != http.StatusOK \u0026\u0026 resp.StatusCode != http.StatusNotFound {","\t\treturn false, err","\t}","\tallowed, _ := acl.UserInOwnerFile(string(ownerContent), string(ownerAliasesContent), event.Sender)","\treturn allowed, nil","}","","func (v *Provider) checkMembership(ctx context.Context, event *info.Event, userid int64) bool {","\t// Initialize cache lazily","\tif v.memberCache == nil {","\t\tv.memberCache = map[int64]bool{}","\t}","","\tif allowed, ok := v.memberCache[userid]; ok {","\t\treturn allowed","\t}","","\tmember, _, err := v.Client().ProjectMembers.GetInheritedProjectMember(v.targetProjectID, userid)","\tif err != nil {","\t\t// If the API call fails, fall back without caching the result so a","\t\t// transient failure can be retried on the next invocation.","\t\tisAllowed, _ := v.IsAllowedOwnersFile(ctx, event)","\t\treturn isAllowed","\t}","","\tif member.ID != 0 \u0026\u0026 member.ID == userid {","\t\tv.memberCache[userid] = true","\t\treturn true","\t}","","\tisAllowed, _ := v.IsAllowedOwnersFile(ctx, event)","\tv.memberCache[userid] = isAllowed","\treturn isAllowed","}","","func (v *Provider) checkOkToTestCommentFromApprovedMember(ctx context.Context, event *info.Event, page int64) (bool, error) {","\tswitch gitEvent := event.Event.(type) {","\tcase *gitlab.MergeEvent:","\t\tif !v.pacInfo.RememberOKToTest {","\t\t\tv.Logger.Debug(\"RememberOKToTest is disabled, skipping MergeRequest notes check as it is not needed\")","\t\t\treturn false, nil","\t\t}","\tcase *gitlab.MergeCommentEvent:","\t\tif !v.pacInfo.RememberOKToTest {","\t\t\tv.Logger.Debug(\"Event is a MergeCommentEvent and RememberOKToTest is disabled, checking current comment only\")","\t\t\treturn v.aclAllowedOkToTestCurrentComment(ctx, event, gitEvent.ObjectAttributes.ID)","\t\t}","\tdefault:","\t\tv.Logger.Debug(\"Event is not a MergeEvent or MergeCommentEvent, skipping merge request notes check\")","\t\treturn false, nil","\t}","","\tvar nextPage int64","\topt := \u0026gitlab.ListMergeRequestDiscussionsOptions{","\t\tListOptions: gitlab.ListOptions{","\t\t\tPage:    page,","\t\t\tPerPage: defaultGitlabListOptions.PerPage,","\t\t},","\t}","\tdiscussions, resp, err := v.Client().Discussions.ListMergeRequestDiscussions(v.targetProjectID, int64(event.PullRequestNumber), opt)","\tif err != nil || len(discussions) == 0 {","\t\treturn false, err","\t}","\tif resp.NextPage != 0 {","\t\tnextPage = resp.NextPage","\t}","","\tfor _, discussion := range discussions {","\t\t// Iterate through every note in the discussion thread and evaluate them.","\t\t// If a note contains an OK-to-test command, verify the commenter's permission","\t\t// (either project membership or presence in OWNERS/OWNERS_ALIASES).","\t\tfor _, note := range discussion.Notes {","\t\t\tif acl.MatchRegexp(acl.OKToTestCommentRegexp, note.Body) {","\t\t\t\tcommenterEvent := info.NewEvent()","\t\t\t\tcommenterEvent.Event = event.Event","\t\t\t\tcommenterEvent.Sender = note.Author.Username","\t\t\t\tcommenterEvent.BaseBranch = event.BaseBranch","\t\t\t\tcommenterEvent.HeadBranch = event.HeadBranch","\t\t\t\tcommenterEvent.DefaultBranch = event.DefaultBranch","\t\t\t\t// We could add caching for membership checks in the future.","\t\t\t\tif v.checkMembership(ctx, commenterEvent, note.Author.ID) {","\t\t\t\t\treturn true, nil","\t\t\t\t}","\t\t\t}","\t\t}","\t}","","\tif nextPage != 0 {","\t\treturn v.checkOkToTestCommentFromApprovedMember(ctx, event, nextPage)","\t}","","\treturn false, nil","}","","func (v *Provider) aclAllowedOkToTestCurrentComment(ctx context.Context, event *info.Event, commentID int64) (bool, error) {","\tcomment, _, err := v.Client().Notes.GetMergeRequestNote(v.targetProjectID, int64(event.PullRequestNumber), commentID)","\tif err != nil {","\t\treturn false, err","\t}","","\tif acl.MatchRegexp(acl.OKToTestCommentRegexp, comment.Body) {","\t\tcommenterEvent := info.NewEvent()","\t\tcommenterEvent.Event = event.Event","\t\tcommenterEvent.Sender = comment.Author.Username","\t\tcommenterEvent.BaseBranch = event.BaseBranch","\t\tcommenterEvent.HeadBranch = event.HeadBranch","\t\tcommenterEvent.DefaultBranch = event.DefaultBranch","\t\tif v.checkMembership(ctx, commenterEvent, comment.Author.ID) {","\t\t\treturn true, nil","\t\t}","\t}","\treturn false, nil","}","","func (v *Provider) IsAllowed(ctx context.Context, event *info.Event) (bool, error) {","\tif v.gitlabClient == nil {","\t\treturn false, fmt.Errorf(\"no github client has been initialized, \" +","\t\t\t\"exiting... (hint: did you forget setting a secret on your repo?)\")","\t}","\tif v.checkMembership(ctx, event, v.userID) {","\t\treturn true, nil","\t}","","\treturn v.checkOkToTestCommentFromApprovedMember(ctx, event, 1)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,2,2,2,2,2,2,0,0,2,2,2,2,2,0,2,2,2,0,2,2,2,2,2,2,2,0,2,2,2,2,0,2,2,2,0,0,2,2,1,1,1,1,1,1,1,1,1,1,2,2,2,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,0,1,0,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,1,0,0,2,2,2,2,2,2,2,2,0,2,0]},{"id":124,"path":"pkg/provider/gitlab/detect.go","lines":["package gitlab","","import (","\t\"encoding/json\"","\t\"fmt\"","\t\"net/http\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\tgitlab \"gitlab.com/gitlab-org/api/client-go\"","\t\"go.uber.org/zap\"",")","","// Detect detects events and validates if it is a valid gitlab event Pipelines as Code supports and","// decides whether to process or reject it.","// returns a boolean value whether to process or reject, logger with event metadata, and error if any occurred.","func (v *Provider) Detect(req *http.Request, payload string, logger *zap.SugaredLogger) (bool, bool, *zap.SugaredLogger, string, error) {","\tisGL := false","\tevent := req.Header.Get(\"X-Gitlab-Event\")","\tif event == \"\" {","\t\treturn false, false, logger, \"no gitlab event\", nil","\t}","","\t// it is a GitLab event","\tisGL = true","","\tsetLoggerAndProceed := func(processEvent bool, reason string, err error) (bool, bool, *zap.SugaredLogger,","\t\tstring, error,","\t) {","\t\tlogger = logger.With(\"provider\", \"gitlab\", \"event-id\", req.Header.Get(\"X-Request-Id\"))","\t\treturn isGL, processEvent, logger, reason, err","\t}","","\teventInt, err := gitlab.ParseWebhook(gitlab.EventType(event), []byte(payload))","\tif err != nil {","\t\treturn setLoggerAndProceed(false, \"\", err)","\t}","\t_ = json.Unmarshal([]byte(payload), \u0026eventInt)","","\tswitch gitEvent := eventInt.(type) {","\tcase *gitlab.MergeEvent:","\t\t// on a MR update, react only if OldRev is empty (no new commits pushed).","\t\t// If OldRev is empty, it's a metadata-only update (e.g., label changes).","\t\tif gitEvent.ObjectAttributes.Action == \"update\" \u0026\u0026 gitEvent.ObjectAttributes.OldRev == \"\" {","\t\t\tif !hasOnlyLabelsChanged(gitEvent) {","\t\t\t\treturn setLoggerAndProceed(false, \"this 'Merge Request' update event changes are not supported; cannot proceed\", nil)","\t\t\t}","\t\t}","","\t\tif provider.Valid(gitEvent.ObjectAttributes.Action, []string{\"open\", \"reopen\", \"update\"}) {","\t\t\treturn setLoggerAndProceed(true, \"\", nil)","\t\t}","","\t\t// on a MR Update only react when there is Oldrev set, since this means","\t\t// there is a Push of commit in there","\t\tif gitEvent.ObjectAttributes.Action == \"update\" \u0026\u0026 gitEvent.ObjectAttributes.OldRev != \"\" {","\t\t\treturn setLoggerAndProceed(true, \"\", nil)","\t\t}","\t\tif provider.Valid(gitEvent.ObjectAttributes.Action, []string{\"open\", \"reopen\", \"close\"}) {","\t\t\treturn setLoggerAndProceed(true, \"\", nil)","\t\t}","","\t\treturn setLoggerAndProceed(false, fmt.Sprintf(\"not a merge event we care about: \\\"%s\\\"\", gitEvent.ObjectAttributes.Action), nil)","\tcase *gitlab.PushEvent, *gitlab.TagEvent:","\t\treturn setLoggerAndProceed(true, \"\", nil)","\tcase *gitlab.MergeCommentEvent:","\t\tif gitEvent.MergeRequest.State == \"opened\" {","\t\t\treturn setLoggerAndProceed(true, \"\", nil)","\t\t}","\t\treturn setLoggerAndProceed(false, \"comments on closed merge requests is not supported\", nil)","\tcase *gitlab.CommitCommentEvent:","\t\tcomment := gitEvent.ObjectAttributes.Note","\t\tif gitEvent.ObjectAttributes.Action == gitlab.CommentEventActionCreate {","\t\t\tif provider.IsTestRetestComment(comment) || provider.IsCancelComment(comment) {","\t\t\t\treturn setLoggerAndProceed(true, \"\", nil)","\t\t\t}","\t\t\t// truncate comment to make logs readable","\t\t\tif len(comment) \u003e 50 {","\t\t\t\tcomment = comment[:50] + \"...\"","\t\t\t}","\t\t\treturn setLoggerAndProceed(false, fmt.Sprintf(\"gitlab: commit_comment: unsupported GitOps comment \\\"%s\\\" on pushed commits\", comment), nil)","\t\t}","\t\treturn setLoggerAndProceed(false, fmt.Sprintf(\"gitlab: commit_comment: unsupported action \\\"%s\\\" with comment \\\"%s\\\"\", gitEvent.ObjectAttributes.Action, comment), nil)","\tdefault:","\t\treturn setLoggerAndProceed(false, \"\", fmt.Errorf(\"gitlab: event \\\"%s\\\" is not supported\", event))","\t}","}","","// hasOnlyLabelsChanged checks if the only change in the merge request is to its labels.","// This function ensures that other fields remain unchanged.","func hasOnlyLabelsChanged(gitEvent *gitlab.MergeEvent) bool {","\tchanges := gitEvent.Changes","","\tlabelsChanged := len(changes.Labels.Previous) \u003e 0 || len(changes.Labels.Current) \u003e 0","","\t// Only Labels can change â€” everything else must be zero or nil","\tonlyUpdatedAtOrLabels := labelsChanged \u0026\u0026","\t\tchanges.Assignees.Previous == nil \u0026\u0026 changes.Assignees.Current == nil \u0026\u0026","\t\tchanges.Reviewers.Previous == nil \u0026\u0026 changes.Reviewers.Current == nil \u0026\u0026","\t\tchanges.Description.Previous == \"\" \u0026\u0026 changes.Description.Current == \"\" \u0026\u0026","\t\tchanges.MergeStatus.Previous == \"\" \u0026\u0026 changes.MergeStatus.Current == \"\" \u0026\u0026","\t\tchanges.MilestoneID.Previous == 0 \u0026\u0026 changes.MilestoneID.Current == 0 \u0026\u0026","\t\tchanges.SourceBranch.Previous == \"\" \u0026\u0026 changes.SourceBranch.Current == \"\" \u0026\u0026","\t\tchanges.SourceProjectID.Previous == 0 \u0026\u0026 changes.SourceProjectID.Current == 0 \u0026\u0026","\t\tchanges.StateID.Previous == 0 \u0026\u0026 changes.StateID.Current == 0 \u0026\u0026","\t\tchanges.TargetBranch.Previous == \"\" \u0026\u0026 changes.TargetBranch.Current == \"\" \u0026\u0026","\t\tchanges.TargetProjectID.Previous == 0 \u0026\u0026 changes.TargetProjectID.Current == 0 \u0026\u0026","\t\tchanges.Title.Previous == \"\" \u0026\u0026 changes.Title.Current == \"\"","","\treturn onlyUpdatedAtOrLabels","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,0,0,0,2,1,1,2,1,1,0,2,2,2,2,2,2,2,1,2,2,2,2,2,2,0,2,2,2,2,0,2,1,1,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2]},{"id":125,"path":"pkg/provider/gitlab/gitlab.go","lines":["package gitlab","","import (","\t\"context\"","\t\"crypto/subtle\"","\t\"fmt\"","\t\"net/http\"","\t\"net/url\"","\t\"path\"","\t\"path/filepath\"","\t\"regexp\"","\t\"strings\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/changedfiles\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/events\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/opscomments\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\tproviderMetrics \"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/providermetrics\"","\tgitlab \"gitlab.com/gitlab-org/api/client-go\"","\t\"go.uber.org/zap\"",")","","const (","\tapiPublicURL       = \"https://gitlab.com\"","\ttaskStatusTemplate = `","\u003ctable\u003e","  \u003ctr\u003e\u003cth\u003eStatus\u003c/th\u003e\u003cth\u003eDuration\u003c/th\u003e\u003cth\u003eName\u003c/th\u003e\u003c/tr\u003e","","{{- range $taskrun := .TaskRunList }}","\u003ctr\u003e","\u003ctd\u003e{{ formatCondition $taskrun.PipelineRunTaskRunStatus.Status.Conditions }}\u003c/td\u003e","\u003ctd\u003e{{ formatDuration $taskrun.PipelineRunTaskRunStatus.Status.StartTime $taskrun.PipelineRunTaskRunStatus.Status.CompletionTime }}\u003c/td\u003e\u003ctd\u003e","","{{ $taskrun.ConsoleLogURL }}","","\u003c/td\u003e\u003c/tr\u003e","{{- end }}","\u003c/table\u003e`","\tnoClientErrStr = `no gitlab client has been initialized, exiting... (hint: did you forget setting a secret on your repo?)`",")","","var anyMergeRequestEventType = []string{\"Merge Request\", \"MergeRequest\"}","","var _ provider.Interface = (*Provider)(nil)","","type Provider struct {","\tgitlabClient      *gitlab.Client","\tLogger            *zap.SugaredLogger","\trun               *params.Run","\tpacInfo           *info.PacOpts","\tToken             *string","\ttargetProjectID   int64","\tsourceProjectID   int64","\tuserID            int64","\tpathWithNamespace string","\trepoURL           string","\tapiURL            string","\teventEmitter      *events.EventEmitter","\trepo              *v1alpha1.Repository","\ttriggerEvent      string","\t// memberCache caches membership/permission checks by user ID within the","\t// current provider instance lifecycle to avoid repeated API calls.","\tmemberCache        map[int64]bool","\tcachedChangedFiles *changedfiles.ChangedFiles","}","","var defaultGitlabListOptions = gitlab.ListOptions{","\tPerPage: 100,","}","","func (v *Provider) Client() *gitlab.Client {","\tproviderMetrics.RecordAPIUsage(","\t\tv.Logger,","\t\t// URL used instead of \"gitlab\" to differentiate in the case of a CI cluster which","\t\t// serves multiple GitLab instances","\t\tv.apiURL,","\t\tv.triggerEvent,","\t\tv.repo,","\t)","\treturn v.gitlabClient","}","","func (v *Provider) SetGitLabClient(client *gitlab.Client) {","\tv.gitlabClient = client","}","","func (v *Provider) SetPacInfo(pacInfo *info.PacOpts) {","\tv.pacInfo = pacInfo","}","","func (v *Provider) CreateComment(_ context.Context, event *info.Event, commit, updateMarker string) error {","\tif v.gitlabClient == nil {","\t\treturn fmt.Errorf(\"no gitlab client has been initialized\")","\t}","","\tif event.PullRequestNumber == 0 {","\t\treturn fmt.Errorf(\"create comment only works on merge requests\")","\t}","","\t// List comments of the merge request","\tif updateMarker != \"\" {","\t\tcommentRe := regexp.MustCompile(updateMarker)","\t\toptions := []gitlab.RequestOptionFunc{}","","\t\tfor {","\t\t\tcomments, resp, err := v.Client().Notes.ListMergeRequestNotes(event.TargetProjectID, int64(event.PullRequestNumber), \u0026gitlab.ListMergeRequestNotesOptions{ListOptions: defaultGitlabListOptions}, options...)","\t\t\tif err != nil {","\t\t\t\treturn err","\t\t\t}","","\t\t\tfor _, comment := range comments {","\t\t\t\tif commentRe.MatchString(comment.Body) {","\t\t\t\t\t_, _, err := v.Client().Notes.UpdateMergeRequestNote(event.TargetProjectID, int64(event.PullRequestNumber), comment.ID, \u0026gitlab.UpdateMergeRequestNoteOptions{","\t\t\t\t\t\tBody: \u0026commit,","\t\t\t\t\t})","\t\t\t\t\tif err != nil {","\t\t\t\t\t\treturn fmt.Errorf(\"unable to update merge request note: %w\", err)","\t\t\t\t\t}","\t\t\t\t\treturn nil","\t\t\t\t}","\t\t\t}","","\t\t\t// Exit the loop when we've seen all pages.","\t\t\tif resp.NextLink == \"\" {","\t\t\t\tbreak","\t\t\t}","","\t\t\t// Otherwise, set param to query the next page","\t\t\toptions = []gitlab.RequestOptionFunc{","\t\t\t\tgitlab.WithKeysetPaginationParameters(resp.NextLink),","\t\t\t}","\t\t}","\t}","","\t_, _, err := v.Client().Notes.CreateMergeRequestNote(event.TargetProjectID, int64(event.PullRequestNumber), \u0026gitlab.CreateMergeRequestNoteOptions{","\t\tBody: \u0026commit,","\t})","\tif err != nil {","\t\treturn fmt.Errorf(\"unable to create merge request note: %w\", err)","\t}","\treturn nil","}","","// CheckPolicyAllowing TODO: Implement ME.","func (v *Provider) CheckPolicyAllowing(_ context.Context, _ *info.Event, _ []string) (bool, string) {","\treturn false, \"\"","}","","func (v *Provider) SetLogger(logger *zap.SugaredLogger) {","\tv.Logger = logger","}","","func (v *Provider) Validate(_ context.Context, _ *params.Run, event *info.Event) error {","\ttoken := event.Request.Header.Get(\"X-Gitlab-Token\")","\tif token == \"\" {","\t\treturn fmt.Errorf(\"no X-Gitlab-Token header detected: webhook validation requires a token for security\")","\t}","","\tif event.Provider.WebhookSecret == \"\" {","\t\treturn fmt.Errorf(\"no webhook secret configured: set webhook secret in repository CR or secret\")","\t}","","\tif subtle.ConstantTimeCompare([]byte(event.Provider.WebhookSecret), []byte(token)) == 0 {","\t\treturn fmt.Errorf(\"gitlab webhook validation failed: token does not match configured secret\")","\t}","\treturn nil","}","","// If I understood properly, you can have \"personal\" projects and groups","// attached projects. But this doesn't seem to show in the API, so we","// are just doing it the path_with_namespace to get the \"org\".","//","// Note that \"orgs/groups\" may have subgroups, so we get the first parts","// as Orgs and the last element as Repo It's just a detail to show for","// UI, we actually don't use this field for access or other logical","// stuff.","func getOrgRepo(pathWithNamespace string) (string, string) {","\torg := filepath.Dir(pathWithNamespace)","\treturn org, filepath.Base(pathWithNamespace)","}","","func (v *Provider) GetConfig() *info.ProviderConfig {","\treturn \u0026info.ProviderConfig{","\t\tTaskStatusTMPL: taskStatusTemplate,","\t\tAPIURL:         apiPublicURL,","\t\tName:           \"gitlab\",","\t}","}","","func (v *Provider) SetClient(_ context.Context, run *params.Run, runevent *info.Event, repo *v1alpha1.Repository, eventsEmitter *events.EventEmitter) error {","\tvar err error","\tif runevent.Provider.Token == \"\" {","\t\treturn fmt.Errorf(\"no git_provider.secret has been set in the repo crd\")","\t}","","\tv.run = run","\tv.eventEmitter = eventsEmitter","\tv.repo = repo","\tv.triggerEvent = runevent.EventType","","\t// Try to detect automatically the API url if url is not coming from public","\t// gitlab. Unless user has set a spec.provider.url in its repo crd","\tapiURL := \"\"","\tswitch {","\tcase runevent.Provider.URL != \"\":","\t\tapiURL = runevent.Provider.URL","\tcase v.repoURL != \"\" \u0026\u0026 !strings.HasPrefix(v.repoURL, apiPublicURL):","\t\tapiURL = strings.ReplaceAll(v.repoURL, v.pathWithNamespace, \"\")","\tcase runevent.URL != \"\":","\t\tburl, err := url.Parse(runevent.URL)","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\tapiURL = fmt.Sprintf(\"%s://%s\", burl.Scheme, burl.Host)","\tdefault:","\t\t// this really should not happen but let's just hope this is it","\t\tapiURL = apiPublicURL","\t}","\t_, err = url.Parse(apiURL)","\tif err != nil {","\t\treturn fmt.Errorf(\"failed to parse api url %s: %w\", apiURL, err)","\t}","\tv.apiURL = apiURL","","\tif v.gitlabClient == nil {","\t\tv.gitlabClient, err = gitlab.NewClient(runevent.Provider.Token, gitlab.WithBaseURL(apiURL))","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t}","\tv.Token = \u0026runevent.Provider.Token","","\trun.Clients.Log.Infof(\"gitlab: initialized for client with token for apiURL=%s, org=%s, repo=%s\", apiURL, runevent.Organization, runevent.Repository)","\t// In a scenario where the source repository is forked and a merge request (MR) is created on the upstream","\t// repository, runevent.SourceProjectID will not be 0 when SetClient is called from the pac-watcher code.","\t// This is because, in the controller, SourceProjectID is set in the annotation of the pull request,","\t// and runevent.SourceProjectID is set before SetClient is called. Therefore, we need to take","\t// the ID from runevent.SourceProjectID when v.sourceProject is 0 (nil).","\tif v.sourceProjectID == 0 \u0026\u0026 runevent.SourceProjectID \u003e 0 {","\t\tv.sourceProjectID = runevent.SourceProjectID","\t}","","\t// check that we have access to the source project if it's a private repo, this should only occur on Merge Requests","\tif runevent.TriggerTarget == triggertype.PullRequest {","\t\t_, resp, err := v.Client().Projects.GetProject(runevent.SourceProjectID, \u0026gitlab.GetProjectOptions{})","\t\terrmsg := fmt.Sprintf(\"failed to access GitLab source repository ID %d: please ensure token has 'read_repository' scope on that repository\",","\t\t\trunevent.SourceProjectID)","\t\tif resp != nil \u0026\u0026 resp.StatusCode == http.StatusNotFound {","\t\t\treturn fmt.Errorf(\"%s\", errmsg)","\t\t}","\t\tif err != nil {","\t\t\treturn fmt.Errorf(\"%s: %w\", errmsg, err)","\t\t}","\t}","","\t// if we don't have sourceProjectID (ie: incoming-webhook) then try to set","\t// it ASAP if we can.","\tif v.sourceProjectID == 0 \u0026\u0026 runevent.Organization != \"\" \u0026\u0026 runevent.Repository != \"\" {","\t\tprojectSlug := path.Join(runevent.Organization, runevent.Repository)","\t\tprojectinfo, _, err := v.Client().Projects.GetProject(projectSlug, \u0026gitlab.GetProjectOptions{})","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\t// TODO: we really need to move out the runevent.*ProjecTID to v.*ProjectID,","\t\t// I just spent half an hour debugging because i didn't realise it was there instead in v.*","\t\tv.sourceProjectID = projectinfo.ID","\t\trunevent.SourceProjectID = projectinfo.ID","\t\trunevent.TargetProjectID = projectinfo.ID","\t\trunevent.DefaultBranch = projectinfo.DefaultBranch","\t}","","\treturn nil","}","","//nolint:misspell","func (v *Provider) CreateStatus(_ context.Context, event *info.Event, statusOpts provider.StatusOpts,",") error {","\tvar detailsURL string","\tif v.gitlabClient == nil {","\t\treturn fmt.Errorf(\"no gitlab client has been initialized, \" +","\t\t\t\"exiting... (hint: did you forget setting a secret on your repo?)\")","\t}","\tswitch statusOpts.Conclusion {","\tcase \"skipped\":","\t\tstatusOpts.Conclusion = \"canceled\"","\t\tstatusOpts.Title = \"skipped validating this commit\"","\tcase \"neutral\":","\t\tstatusOpts.Conclusion = \"canceled\"","\t\tstatusOpts.Title = \"stopped\"","\tcase \"cancelled\":","\t\tstatusOpts.Conclusion = \"canceled\"","\t\tstatusOpts.Title = \"cancelled validating this commit\"","\tcase \"failure\":","\t\tstatusOpts.Conclusion = \"failed\"","\t\tstatusOpts.Title = \"failed\"","\tcase \"success\":","\t\tstatusOpts.Conclusion = \"success\"","\t\tstatusOpts.Title = \"successfully validated your commit\"","\tcase \"completed\":","\t\tstatusOpts.Conclusion = \"success\"","\t\tstatusOpts.Title = \"completed\"","\tcase \"pending\":","\t\tstatusOpts.Conclusion = \"pending\"","\t}","\t// When the pipeline is actually running (in_progress), show it as running","\t// not pending. Pending is only for waiting states like /ok-to-test approval.","\tif statusOpts.Status == \"in_progress\" {","\t\tstatusOpts.Conclusion = \"running\"","\t}","\tif statusOpts.DetailsURL != \"\" {","\t\tdetailsURL = statusOpts.DetailsURL","\t}","","\tonPr := \"\"","\tif statusOpts.OriginalPipelineRunName != \"\" {","\t\tonPr = \"/\" + statusOpts.OriginalPipelineRunName","\t}","\tbody := fmt.Sprintf(\"**%s%s** has %s\\n\\n%s\\n\\n\u003csmall\u003eFull log available [here](%s)\u003c/small\u003e\",","\t\tv.pacInfo.ApplicationName, onPr, statusOpts.Title, statusOpts.Text, detailsURL)","","\tcontextName := provider.GetCheckName(statusOpts, v.pacInfo)","\topt := \u0026gitlab.SetCommitStatusOptions{","\t\tState:       gitlab.BuildStateValue(statusOpts.Conclusion),","\t\tName:        gitlab.Ptr(contextName),","\t\tTargetURL:   gitlab.Ptr(detailsURL),","\t\tDescription: gitlab.Ptr(statusOpts.Title),","\t\tContext:     gitlab.Ptr(contextName),","\t}","","\t// In case we have access, set the status. Typically, on a Merge Request (MR)","\t// from a fork in an upstream repository, the token needs to have write access","\t// to the fork repository in order to create a status. However, the token set on the","\t// Repository CR usually doesn't have such broad access, preventing from creating","\t// a status comment on it.","\t// This would work on a push or an MR from a branch within the same repo.","\t// Ignoring errors because of the write access issues,","\t_, _, err := v.Client().Commits.SetCommitStatus(event.SourceProjectID, event.SHA, opt)","\tif err != nil {","\t\tv.Logger.Debugf(\"cannot set status with the GitLab token on the source project: %v\", err)","\t} else {","\t\t// we managed to set the status on the source repo, all good we are done","\t\tv.Logger.Debugf(\"created commit status on source project ID %d\", event.TargetProjectID)","\t\treturn nil","\t}","\tif _, _, err = v.Client().Commits.SetCommitStatus(event.TargetProjectID, event.SHA, opt); err == nil {","\t\tv.Logger.Debugf(\"created commit status on target project ID %d\", event.TargetProjectID)","\t\t// we managed to set the status on the target repo, all good we are done","\t\treturn nil","\t}","\tv.Logger.Debugf(\"cannot set status with the GitLab token on the target project: %v\", err)","","\t// Skip creating MR comments if the error is a state transition error","\t// (e.g., \"Cannot transition status via :run from :running\").","\t// This means the status is already set, so we should not create a comment.","\tif strings.Contains(err.Error(), \"Cannot transition status\") {","\t\tv.Logger.Debugf(\"skipping MR comment as error is not permission related: %v\", err)","\t\treturn nil","\t}","","\t// we only show the first error as it's likely something the user has more control to fix","\t// the second err is cryptic as it needs a dummy gitlab pipeline to start","\t// with and will only give more confusion in the event namespace","\tv.eventEmitter.EmitMessage(v.repo, zap.InfoLevel, \"FailedToSetCommitStatus\",","\t\tfmt.Sprintf(\"failed to create commit status: source project ID %d, target project ID %d. \"+","\t\t\t\"If you want Gitlab Pipeline Status update, ensure your GitLab token giving it access \"+","\t\t\t\"to the source repository. %v\",","\t\t\tevent.SourceProjectID, event.TargetProjectID, err))","","\teventType := triggertype.IsPullRequestType(event.EventType)","\t// When a GitOps command is sent on a pushed commit, it mistakenly treats it as a pull_request","\t// and attempts to create a note, but notes are not intended for pushed commits.","\tif event.TriggerTarget == triggertype.PullRequest \u0026\u0026 opscomments.IsAnyOpsEventType(event.EventType) {","\t\teventType = triggertype.PullRequest","\t}","","\tvar commentStrategy string","","\tif v.repo != nil \u0026\u0026 v.repo.Spec.Settings != nil \u0026\u0026 v.repo.Spec.Settings.Gitlab != nil {","\t\tcommentStrategy = v.repo.Spec.Settings.Gitlab.CommentStrategy","\t}","\tswitch commentStrategy {","\tcase \"disable_all\":","\t\tv.Logger.Warn(\"Comments related to PipelineRuns status have been disabled for GitLab merge requests\")","\t\treturn nil","\tdefault:","\t\tif eventType == triggertype.PullRequest || provider.Valid(event.EventType, anyMergeRequestEventType) {","\t\t\tmopt := \u0026gitlab.CreateMergeRequestNoteOptions{Body: gitlab.Ptr(body)}","\t\t\t_, _, err := v.Client().Notes.CreateMergeRequestNote(event.TargetProjectID, int64(event.PullRequestNumber), mopt)","\t\t\treturn err","\t\t}","\t}","","\treturn nil","}","","func (v *Provider) GetTektonDir(_ context.Context, event *info.Event, path, provenance string) (string, error) {","\tif v.gitlabClient == nil {","\t\treturn \"\", fmt.Errorf(\"no gitlab client has been initialized, \" +","\t\t\t\"exiting... (hint: did you forget setting a secret on your repo?)\")","\t}","\t// default set provenance from head","\trevision := event.HeadBranch","\tif provenance == \"default_branch\" {","\t\trevision = event.DefaultBranch","\t\tv.Logger.Infof(\"Using PipelineRun definition from default_branch: %s\", event.DefaultBranch)","\t} else {","\t\ttrigger := event.TriggerTarget.String()","\t\tif event.TriggerTarget == triggertype.PullRequest {","\t\t\ttrigger = \"merge request\"","\t\t}","\t\tv.Logger.Infof(\"Using PipelineRun definition from source %s on commit SHA: %s\", trigger, event.SHA)","\t}","","\topt := \u0026gitlab.ListTreeOptions{","\t\tPath:      gitlab.Ptr(path),","\t\tRef:       gitlab.Ptr(revision),","\t\tRecursive: gitlab.Ptr(true),","\t\tListOptions: gitlab.ListOptions{","\t\t\tOrderBy:    \"id\",","\t\t\tPagination: \"keyset\",","\t\t\tPerPage:    defaultGitlabListOptions.PerPage,","\t\t\tSort:       \"asc\",","\t\t},","\t}","","\toptions := []gitlab.RequestOptionFunc{}","\tnodes := []*gitlab.TreeNode{}","","\tfor {","\t\tobjects, resp, err := v.Client().Repositories.ListTree(v.sourceProjectID, opt, options...)","\t\tif err != nil {","\t\t\treturn \"\", fmt.Errorf(\"failed to list %s dir: %w\", path, err)","\t\t}","\t\tif resp != nil \u0026\u0026 resp.StatusCode == http.StatusNotFound {","\t\t\treturn \"\", nil","\t\t}","","\t\tnodes = append(nodes, objects...)","","\t\t// Exit the loop when we've seen all pages.","\t\tif resp.NextLink == \"\" {","\t\t\tbreak","\t\t}","","\t\t// Otherwise, set param to query the next page","\t\toptions = []gitlab.RequestOptionFunc{","\t\t\tgitlab.WithKeysetPaginationParameters(resp.NextLink),","\t\t}","\t}","","\treturn v.concatAllYamlFiles(nodes, revision)","}","","// concatAllYamlFiles concat all yaml files from a directory as one big multi document yaml string.","func (v *Provider) concatAllYamlFiles(objects []*gitlab.TreeNode, revision string) (string, error) {","\tvar allTemplates string","\tfor _, value := range objects {","\t\tif strings.HasSuffix(value.Name, \".yaml\") ||","\t\t\tstrings.HasSuffix(value.Name, \".yml\") {","\t\t\tdata, _, err := v.getObject(value.Path, revision, v.sourceProjectID)","\t\t\tif err != nil {","\t\t\t\treturn \"\", err","\t\t\t}","\t\t\tif err := provider.ValidateYaml(data, value.Path); err != nil {","\t\t\t\treturn \"\", err","\t\t\t}","\t\t\tif allTemplates != \"\" \u0026\u0026 !strings.HasPrefix(string(data), \"---\") {","\t\t\t\tallTemplates += \"---\"","\t\t\t}","\t\t\tallTemplates += \"\\n\" + string(data) + \"\\n\"","\t\t}","\t}","","\treturn allTemplates, nil","}","","func (v *Provider) getObject(fname, branch string, pid int64) ([]byte, *gitlab.Response, error) {","\topt := \u0026gitlab.GetRawFileOptions{","\t\tRef: gitlab.Ptr(branch),","\t}","\tfile, resp, err := v.Client().RepositoryFiles.GetRawFile(pid, fname, opt)","\tif err != nil {","\t\treturn []byte{}, resp, fmt.Errorf(\"failed to get filename from api %s dir: %w\", fname, err)","\t}","\tif resp != nil \u0026\u0026 resp.StatusCode == http.StatusNotFound {","\t\treturn []byte{}, resp, nil","\t}","\treturn file, resp, nil","}","","func (v *Provider) GetFileInsideRepo(_ context.Context, runevent *info.Event, path, _ string) (string, error) {","\tgetobj, _, err := v.getObject(path, runevent.HeadBranch, v.sourceProjectID)","\tif err != nil {","\t\treturn \"\", err","\t}","\treturn string(getobj), nil","}","","func (v *Provider) GetCommitInfo(_ context.Context, runevent *info.Event) error {","\tif v.gitlabClient == nil {","\t\treturn fmt.Errorf(\"%s\", noClientErrStr)","\t}","","\t// if we don't have a SHA (ie: incoming-webhook) then get it from the branch","\t// and populate in the runevent.","\tif runevent.SHA == \"\" \u0026\u0026 runevent.HeadBranch != \"\" {","\t\tbranchinfo, _, err := v.Client().Commits.GetCommit(v.sourceProjectID, runevent.HeadBranch, \u0026gitlab.GetCommitOptions{})","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\trunevent.SHA = branchinfo.ID","\t\trunevent.SHATitle = branchinfo.Title","\t\trunevent.SHAURL = branchinfo.WebURL","","\t\t// Populate full commit information for LLM context","\t\trunevent.SHAMessage = branchinfo.Message","\t\trunevent.SHAAuthorName = branchinfo.AuthorName","\t\trunevent.SHAAuthorEmail = branchinfo.AuthorEmail","\t\tif branchinfo.AuthoredDate != nil {","\t\t\trunevent.SHAAuthorDate = *branchinfo.AuthoredDate","\t\t}","\t\trunevent.SHACommitterName = branchinfo.CommitterName","\t\trunevent.SHACommitterEmail = branchinfo.CommitterEmail","\t\tif branchinfo.CommittedDate != nil {","\t\t\trunevent.SHACommitterDate = *branchinfo.CommittedDate","\t\t}","\t}","\trunevent.HasSkipCommand = provider.SkipCI(runevent.SHAMessage)","","\treturn nil","}","","// GetFiles gets and caches the list of files changed by a given event.","func (v *Provider) GetFiles(ctx context.Context, runevent *info.Event) (changedfiles.ChangedFiles, error) {","\tif v.cachedChangedFiles == nil {","\t\tchanges, err := v.fetchChangedFiles(ctx, runevent)","\t\tif err != nil {","\t\t\treturn changedfiles.ChangedFiles{}, err","\t\t}","\t\tv.cachedChangedFiles = \u0026changes","\t}","\treturn *v.cachedChangedFiles, nil","}","","func (v *Provider) fetchChangedFiles(_ context.Context, runevent *info.Event) (changedfiles.ChangedFiles, error) {","\tif v.gitlabClient == nil {","\t\treturn changedfiles.ChangedFiles{}, fmt.Errorf(\"no gitlab client has been initialized, \" +","\t\t\t\"exiting... (hint: did you forget setting a secret on your repo?)\")","\t}","","\tchangedFiles := changedfiles.ChangedFiles{}","","\tswitch runevent.TriggerTarget {","\tcase triggertype.PullRequest:","\t\topt := \u0026gitlab.ListMergeRequestDiffsOptions{","\t\t\tListOptions: gitlab.ListOptions{","\t\t\t\tOrderBy:    \"id\",","\t\t\t\tPagination: \"keyset\",","\t\t\t\tPerPage:    defaultGitlabListOptions.PerPage,","\t\t\t\tSort:       \"asc\",","\t\t\t},","\t\t}","\t\toptions := []gitlab.RequestOptionFunc{}","","\t\tfor {","\t\t\tmrchanges, resp, err := v.Client().MergeRequests.ListMergeRequestDiffs(v.targetProjectID, int64(runevent.PullRequestNumber), opt, options...)","\t\t\tif err != nil {","\t\t\t\t// TODO: Should this return the files found so far?","\t\t\t\treturn changedfiles.ChangedFiles{}, err","\t\t\t}","","\t\t\tfor _, change := range mrchanges {","\t\t\t\tchangedFiles.All = append(changedFiles.All, change.NewPath)","\t\t\t\tif change.NewFile {","\t\t\t\t\tchangedFiles.Added = append(changedFiles.Added, change.NewPath)","\t\t\t\t}","\t\t\t\tif change.DeletedFile {","\t\t\t\t\tchangedFiles.Deleted = append(changedFiles.Deleted, change.NewPath)","\t\t\t\t}","\t\t\t\tif !change.RenamedFile \u0026\u0026 !change.DeletedFile \u0026\u0026 !change.NewFile {","\t\t\t\t\tchangedFiles.Modified = append(changedFiles.Modified, change.NewPath)","\t\t\t\t}","\t\t\t\tif change.RenamedFile {","\t\t\t\t\tchangedFiles.Renamed = append(changedFiles.Renamed, change.NewPath)","\t\t\t\t}","\t\t\t}","","\t\t\t// Exit the loop when we've seen all pages.","\t\t\tif resp.NextLink == \"\" {","\t\t\t\tbreak","\t\t\t}","","\t\t\t// Otherwise, set param to query the next page","\t\t\toptions = []gitlab.RequestOptionFunc{","\t\t\t\tgitlab.WithKeysetPaginationParameters(resp.NextLink),","\t\t\t}","\t\t}","\tcase triggertype.Push:","\t\toptions := gitlab.GetCommitDiffOptions{ListOptions: defaultGitlabListOptions}","\t\tpageOpts := []gitlab.RequestOptionFunc{}","","\t\tfor {","\t\t\tpushChanges, resp, err := v.Client().Commits.GetCommitDiff(v.sourceProjectID, runevent.SHA, \u0026options, pageOpts...)","\t\t\tif err != nil {","\t\t\t\treturn changedfiles.ChangedFiles{}, err","\t\t\t}","","\t\t\tfor _, change := range pushChanges {","\t\t\t\tchangedFiles.All = append(changedFiles.All, change.NewPath)","\t\t\t\tif change.NewFile {","\t\t\t\t\tchangedFiles.Added = append(changedFiles.Added, change.NewPath)","\t\t\t\t}","\t\t\t\tif change.DeletedFile {","\t\t\t\t\tchangedFiles.Deleted = append(changedFiles.Deleted, change.NewPath)","\t\t\t\t}","\t\t\t\tif !change.RenamedFile \u0026\u0026 !change.DeletedFile \u0026\u0026 !change.NewFile {","\t\t\t\t\tchangedFiles.Modified = append(changedFiles.Modified, change.NewPath)","\t\t\t\t}","\t\t\t\tif change.RenamedFile {","\t\t\t\t\tchangedFiles.Renamed = append(changedFiles.Renamed, change.NewPath)","\t\t\t\t}","\t\t\t}","","\t\t\tif resp.NextLink == \"\" {","\t\t\t\t// Exit the loop when we've seen all pages.","\t\t\t\tbreak","\t\t\t}","\t\t\t// Otherwise, set param to query the next page","\t\t\tpageOpts = []gitlab.RequestOptionFunc{","\t\t\t\tgitlab.WithKeysetPaginationParameters(resp.NextLink),","\t\t\t}","\t\t}","\tdefault:","\t\t// No action necessary","\t}","\treturn changedFiles, nil","}","","func (v *Provider) CreateToken(_ context.Context, _ []string, _ *info.Event) (string, error) {","\treturn \"\", nil","}","","// isHeadCommitOfBranch validates that branch exists and the SHA is HEAD commit of the branch.","func (v *Provider) isHeadCommitOfBranch(runevent *info.Event, branchName string) error {","\tif v.gitlabClient == nil {","\t\treturn fmt.Errorf(\"no gitlab client has been initialized, \" +","\t\t\t\"exiting... (hint: did you forget setting a secret on your repo?)\")","\t}","\tbranch, _, err := v.Client().Branches.GetBranch(v.sourceProjectID, branchName)","\tif err != nil {","\t\treturn err","\t}","","\tif branch.Commit.ID == runevent.SHA {","\t\treturn nil","\t}","","\treturn fmt.Errorf(\"provided SHA %s is not the HEAD commit of the branch %s\", runevent.SHA, branchName)","}","","func (v *Provider) GetTemplate(commentType provider.CommentType) string {","\treturn provider.GetHTMLTemplate(commentType)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,0,1,1,1,0,2,2,2,2,0,2,2,2,0,0,2,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,1,1,2,0,0,0,0,2,2,0,0,0,2,2,2,0,0,0,2,2,2,2,1,1,2,0,0,0,1,1,1,0,1,1,1,0,2,2,2,2,2,0,2,2,2,0,2,2,2,2,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,2,2,2,2,2,2,2,0,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,1,1,1,1,0,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,1,1,0,0,0,0,2,1,1,1,1,1,0,0,1,1,1,1,0,0,2,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,0,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,2,2,2,2,2,2,2,2,2,2,1,1,0,2,2,2,1,1,2,1,1,1,2,2,1,1,1,1,0,0,2,0,0,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,2,2,2,2,0,0,0,2,2,2,0,0,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,0,0,2,2,2,2,2,2,2,2,2,1,1,2,0,0,2,2,2,2,2,2,0,0,2,2,2,2,0,0,0,2,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,0,0,0,2,2,2,2,2,2,2,0,2,0,0,2,2,1,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,0,0,0,2,2,2,0,2,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,0,0,2,2,2,0,0,0,0,2,0,0,1,1,1,0,0,2,2,2,2,2,2,2,2,2,0,2,2,2,0,2,0,0,1,1,1]},{"id":126,"path":"pkg/provider/gitlab/parse_payload.go","lines":["package gitlab","","import (","\t\"context\"","\t\"encoding/json\"","\t\"fmt\"","\t\"net/http\"","\t\"strings\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/matcher\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/opscomments\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/pipelineascode\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","","\tgitlab \"gitlab.com/gitlab-org/api/client-go\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"",")","","func (v *Provider) ParsePayload(ctx context.Context, run *params.Run, request *http.Request,","\tpayload string,",") (*info.Event, error) {","\tevent := request.Header.Get(\"X-Gitlab-Event\")","\tif event == \"\" {","\t\treturn nil, fmt.Errorf(\"failed to find event type in request header\")","\t}","","\tpayloadB := []byte(payload)","\teventInt, err := gitlab.ParseWebhook(gitlab.EventType(event), payloadB)","\tif err != nil {","\t\treturn nil, err","\t}","\t_ = json.Unmarshal(payloadB, \u0026eventInt)","","\t// Remove the \" Hook\" suffix so looks better in status, and since we don't","\t// really use it anymore we good to do whatever we want with it for","\t// cosmetics.","\tprocessedEvent := info.NewEvent()","\tprocessedEvent.EventType = strings.ReplaceAll(event, \" Hook\", \"\")","\tprocessedEvent.Event = eventInt","\tswitch gitEvent := eventInt.(type) {","\tcase *gitlab.MergeEvent:","\t\t// Organization:  event.GetRepo().GetOwner().GetLogin(),","\t\tprocessedEvent.Sender = gitEvent.User.Username","\t\tprocessedEvent.DefaultBranch = gitEvent.Project.DefaultBranch","\t\tprocessedEvent.URL = gitEvent.Project.WebURL","\t\tprocessedEvent.SHA = gitEvent.ObjectAttributes.LastCommit.ID","\t\tprocessedEvent.SHAURL = gitEvent.ObjectAttributes.LastCommit.URL","\t\tprocessedEvent.SHATitle = gitEvent.ObjectAttributes.LastCommit.Title","\t\tprocessedEvent.SHAMessage = gitEvent.ObjectAttributes.LastCommit.Message","\t\tprocessedEvent.HeadBranch = gitEvent.ObjectAttributes.SourceBranch","\t\tprocessedEvent.BaseBranch = gitEvent.ObjectAttributes.TargetBranch","\t\tprocessedEvent.HeadURL = gitEvent.ObjectAttributes.Source.WebURL","\t\tprocessedEvent.BaseURL = gitEvent.ObjectAttributes.Target.WebURL","\t\tprocessedEvent.PullRequestNumber = int(gitEvent.ObjectAttributes.IID)","\t\tprocessedEvent.PullRequestTitle = gitEvent.ObjectAttributes.Title","\t\tv.targetProjectID = gitEvent.Project.ID","\t\tv.sourceProjectID = gitEvent.ObjectAttributes.SourceProjectID","\t\tv.userID = gitEvent.User.ID","","\t\tv.pathWithNamespace = gitEvent.ObjectAttributes.Target.PathWithNamespace","\t\tprocessedEvent.Organization, processedEvent.Repository = getOrgRepo(v.pathWithNamespace)","\t\tprocessedEvent.SourceProjectID = gitEvent.ObjectAttributes.SourceProjectID","\t\tprocessedEvent.TargetProjectID = gitEvent.Project.ID","","\t\tprocessedEvent.TriggerTarget = triggertype.PullRequest","\t\tprocessedEvent.EventType = strings.ReplaceAll(event, \" Hook\", \"\")","","\t\t// This is a label update, like adding or removing a label from a MR.","\t\tif gitEvent.Changes.Labels.Current != nil {","\t\t\tprocessedEvent.EventType = triggertype.PullRequestLabeled.String()","\t\t}","\t\tfor _, label := range gitEvent.Labels {","\t\t\tprocessedEvent.PullRequestLabel = append(processedEvent.PullRequestLabel, label.Title)","\t\t}","\t\tif gitEvent.ObjectAttributes.Action == \"close\" {","\t\t\tprocessedEvent.TriggerTarget = triggertype.PullRequestClosed","\t\t}","\tcase *gitlab.TagEvent:","\t\t// GitLab sends same event for both Tag creation and deletion i.e. \"Tag Push Hook\".","\t\t// if gitEvent.After is containing all zeros and gitEvent.CheckoutSHA is empty","\t\t// it is Delete \"Tag Push Hook\".","\t\tif isZeroSHA(gitEvent.After) \u0026\u0026 gitEvent.CheckoutSHA == \"\" {","\t\t\treturn nil, fmt.Errorf(\"event Delete %s is not supported\", event)","\t\t}","","\t\t// sometime in gitlab tag push event contains no commit","\t\t// in this case we're not supposed to process the event.","\t\tif len(gitEvent.Commits) == 0 {","\t\t\treturn nil, fmt.Errorf(\"no commits attached to this %s event\", event)","\t\t}","","\t\tlastCommitIdx := len(gitEvent.Commits) - 1","\t\tprocessedEvent.Sender = gitEvent.UserUsername","\t\tprocessedEvent.DefaultBranch = gitEvent.Project.DefaultBranch","\t\tprocessedEvent.URL = gitEvent.Project.WebURL","\t\tprocessedEvent.SHA = gitEvent.Commits[lastCommitIdx].ID","\t\tprocessedEvent.SHAURL = gitEvent.Commits[lastCommitIdx].URL","\t\tprocessedEvent.SHATitle = gitEvent.Commits[lastCommitIdx].Title","\t\tprocessedEvent.HeadBranch = gitEvent.Ref","\t\tprocessedEvent.BaseBranch = gitEvent.Ref","\t\tprocessedEvent.HeadURL = gitEvent.Project.WebURL","\t\tprocessedEvent.BaseURL = processedEvent.HeadURL","\t\tprocessedEvent.TriggerTarget = \"push\"","\t\tv.pathWithNamespace = gitEvent.Project.PathWithNamespace","\t\tprocessedEvent.Organization, processedEvent.Repository = getOrgRepo(v.pathWithNamespace)","\t\tv.targetProjectID = gitEvent.ProjectID","\t\tv.sourceProjectID = gitEvent.ProjectID","\t\tv.userID = gitEvent.UserID","\t\tprocessedEvent.SourceProjectID = gitEvent.ProjectID","\t\tprocessedEvent.TargetProjectID = gitEvent.ProjectID","\t\tprocessedEvent.EventType = strings.ReplaceAll(event, \" Hook\", \"\")","\tcase *gitlab.PushEvent:","\t\tif len(gitEvent.Commits) == 0 {","\t\t\treturn nil, fmt.Errorf(\"no commits attached to this push event\")","\t\t}","\t\tlastCommitIdx := len(gitEvent.Commits) - 1","\t\tprocessedEvent.Sender = gitEvent.UserUsername","\t\tprocessedEvent.DefaultBranch = gitEvent.Project.DefaultBranch","\t\tprocessedEvent.URL = gitEvent.Project.WebURL","\t\tprocessedEvent.SHA = gitEvent.Commits[lastCommitIdx].ID","\t\tprocessedEvent.SHAURL = gitEvent.Commits[lastCommitIdx].URL","\t\tprocessedEvent.SHATitle = gitEvent.Commits[lastCommitIdx].Title","\t\tprocessedEvent.HeadBranch = gitEvent.Ref","\t\tprocessedEvent.BaseBranch = gitEvent.Ref","\t\tprocessedEvent.HeadURL = gitEvent.Project.WebURL","\t\tprocessedEvent.BaseURL = processedEvent.HeadURL","\t\tprocessedEvent.TriggerTarget = \"push\"","\t\tv.pathWithNamespace = gitEvent.Project.PathWithNamespace","\t\tprocessedEvent.Organization, processedEvent.Repository = getOrgRepo(v.pathWithNamespace)","\t\tv.targetProjectID = gitEvent.ProjectID","\t\tv.sourceProjectID = gitEvent.ProjectID","\t\tv.userID = gitEvent.UserID","\t\tprocessedEvent.SourceProjectID = gitEvent.ProjectID","\t\tprocessedEvent.TargetProjectID = gitEvent.ProjectID","\t\tprocessedEvent.EventType = strings.ToLower(strings.ReplaceAll(event, \" Hook\", \"\"))","\tcase *gitlab.MergeCommentEvent:","\t\tprocessedEvent.Sender = gitEvent.User.Username","\t\tprocessedEvent.DefaultBranch = gitEvent.Project.DefaultBranch","\t\tprocessedEvent.URL = gitEvent.Project.WebURL","\t\tprocessedEvent.SHA = gitEvent.MergeRequest.LastCommit.ID","\t\tprocessedEvent.SHAURL = gitEvent.MergeRequest.LastCommit.URL","\t\tprocessedEvent.SHATitle = gitEvent.MergeRequest.LastCommit.Title","\t\tprocessedEvent.BaseBranch = gitEvent.MergeRequest.TargetBranch","\t\tprocessedEvent.HeadBranch = gitEvent.MergeRequest.SourceBranch","\t\tprocessedEvent.BaseURL = gitEvent.MergeRequest.Target.WebURL","\t\tprocessedEvent.HeadURL = gitEvent.MergeRequest.Source.WebURL","","\t\topscomments.SetEventTypeAndTargetPR(processedEvent, gitEvent.ObjectAttributes.Note)","\t\tv.pathWithNamespace = gitEvent.Project.PathWithNamespace","\t\tprocessedEvent.Organization, processedEvent.Repository = getOrgRepo(v.pathWithNamespace)","\t\tprocessedEvent.TriggerTarget = triggertype.PullRequest","","\t\tprocessedEvent.PullRequestNumber = int(gitEvent.MergeRequest.IID)","\t\tv.targetProjectID = gitEvent.MergeRequest.TargetProjectID","\t\tv.sourceProjectID = gitEvent.MergeRequest.SourceProjectID","\t\tv.userID = gitEvent.User.ID","\t\tprocessedEvent.SourceProjectID = gitEvent.MergeRequest.SourceProjectID","\t\tprocessedEvent.TargetProjectID = gitEvent.MergeRequest.TargetProjectID","\tcase *gitlab.CommitCommentEvent:","\t\t// need run in fetching repository","\t\tv.run = run","\t\treturn v.handleCommitCommentEvent(ctx, gitEvent)","\tdefault:","\t\treturn nil, fmt.Errorf(\"event %s is not supported\", event)","\t}","","\tv.repoURL = processedEvent.URL","\treturn processedEvent, nil","}","","func (v *Provider) initGitLabClient(ctx context.Context, event *info.Event) (*info.Event, error) {","\t// This is to ensure the base URL of the client is not reinitialized during tests.","\tif v.gitlabClient != nil {","\t\treturn event, nil","\t}","","\t// need repo here to get secret info and create gitlab api client","\trepo, err := matcher.MatchEventURLRepo(ctx, v.run, event, \"\")","\tif err != nil {","\t\treturn event, err","\t}","","\tif repo == nil {","\t\treturn event, fmt.Errorf(\"cannot find a repository match for %s\", event.URL)","\t}","","\t// should check global repository for secrets","\tsecretNS := repo.GetNamespace()","\tglobalRepo, err := v.run.Clients.PipelineAsCode.PipelinesascodeV1alpha1().Repositories(v.run.Info.Kube.Namespace).Get(","\t\tctx, v.run.Info.Controller.GlobalRepository, metav1.GetOptions{},","\t)","\tif err == nil \u0026\u0026 globalRepo != nil {","\t\tif repo.Spec.GitProvider != nil \u0026\u0026 repo.Spec.GitProvider.Secret == nil \u0026\u0026 globalRepo.Spec.GitProvider != nil \u0026\u0026 globalRepo.Spec.GitProvider.Secret != nil {","\t\t\tsecretNS = globalRepo.GetNamespace()","\t\t}","\t\trepo.Spec.Merge(globalRepo.Spec)","\t}","","\tkubeInterface, err := kubeinteraction.NewKubernetesInteraction(v.run)","\tif err != nil {","\t\treturn event, err","\t}","","\tscm := pipelineascode.SecretFromRepository{","\t\tK8int:       kubeInterface,","\t\tConfig:      v.GetConfig(),","\t\tEvent:       event,","\t\tRepo:        repo,","\t\tWebhookType: v.pacInfo.WebhookType,","\t\tLogger:      v.Logger,","\t\tNamespace:   secretNS,","\t}","\tif err := scm.Get(ctx); err != nil {","\t\treturn event, fmt.Errorf(\"cannot get secret from repository: %w\", err)","\t}","","\terr = v.SetClient(ctx, v.run, event, repo, v.eventEmitter)","\tif err != nil {","\t\treturn event, err","\t}","\treturn event, nil","}","","func (v *Provider) handleCommitCommentEvent(ctx context.Context, event *gitlab.CommitCommentEvent) (*info.Event, error) {","\taction := \"trigger\"","\tprocessedEvent := info.NewEvent()","\tif event.Repository == nil {","\t\treturn nil, fmt.Errorf(\"error parse_payload: the repository in event payload must not be nil\")","\t}","\t// since comment is made on pushed commit, SourceProjectID and TargetProjectID will be equal.","\tv.sourceProjectID = event.ProjectID","\tv.targetProjectID = event.ProjectID","\tprocessedEvent.SourceProjectID = v.sourceProjectID","\tprocessedEvent.TargetProjectID = v.targetProjectID","\tv.userID = event.User.ID","\tv.pathWithNamespace = event.Project.PathWithNamespace","\tprocessedEvent.Organization, processedEvent.Repository = getOrgRepo(v.pathWithNamespace)","\tprocessedEvent.Sender = event.User.Username","\tprocessedEvent.Provider.User = processedEvent.Sender","\tprocessedEvent.URL = event.Project.WebURL","\tprocessedEvent.SHA = event.ObjectAttributes.CommitID","\tprocessedEvent.SHATitle = event.Commit.Title","\tprocessedEvent.HeadURL = processedEvent.URL","\tprocessedEvent.BaseURL = processedEvent.URL","\tprocessedEvent.TriggerTarget = triggertype.Push","\topscomments.SetEventTypeAndTargetPR(processedEvent, event.ObjectAttributes.Note)","\t// Set Head and Base branch to default_branch of the repo as this comment is made on","\t// a pushed commit.","\tdefaultBranch := event.Project.DefaultBranch","\tprocessedEvent.HeadBranch, processedEvent.BaseBranch = defaultBranch, defaultBranch","\tprocessedEvent.DefaultBranch = defaultBranch","","\tvar (","\t\tbranchName string","\t\tprName     string","\t\ttagName    string","\t\terr        error","\t)","","\t// since we're going to make an API call to ensure that the commit is HEAD of the branch","\t// therefore we need to initialize GitLab client here","\tprocessedEvent, err = v.initGitLabClient(ctx, processedEvent)","\tif err != nil {","\t\treturn processedEvent, err","\t}","","\t// get PipelineRun name from comment if it does contain e.g. `/test pr7`","\tif provider.IsTestRetestComment(event.ObjectAttributes.Note) {","\t\tprName, branchName, tagName, err = provider.GetPipelineRunAndBranchOrTagNameFromTestComment(event.ObjectAttributes.Note)","\t\tif err != nil {","\t\t\treturn processedEvent, err","\t\t}","\t\tprocessedEvent.TargetTestPipelineRun = prName","\t}","","\tif provider.IsCancelComment(event.ObjectAttributes.Note) {","\t\taction = \"cancellation\"","\t\tprName, branchName, tagName, err = provider.GetPipelineRunAndBranchOrTagNameFromCancelComment(event.ObjectAttributes.Note)","\t\tif err != nil {","\t\t\treturn processedEvent, err","\t\t}","\t\tprocessedEvent.CancelPipelineRuns = true","\t\tprocessedEvent.TargetCancelPipelineRun = prName","\t}","","\tif tagName != \"\" {","\t\ttagPath := fmt.Sprintf(\"refs/tags/%s\", tagName)","\t\ttag, _, err := v.gitlabClient.Tags.GetTag(v.sourceProjectID, tagName)","\t\tif err != nil {","\t\t\treturn processedEvent, fmt.Errorf(\"error getting tag %s: %w\", tagName, err)","\t\t}","","\t\tif tag.Commit.ID != processedEvent.SHA {","\t\t\treturn processedEvent, fmt.Errorf(\"provided SHA %s is not the tagged commit for the tag %s\", processedEvent.SHA, tagName)","\t\t}","","\t\tprocessedEvent.HeadBranch = tagPath","\t\tprocessedEvent.BaseBranch = tagPath","\t\treturn processedEvent, nil","\t}","","\tif branchName == \"\" {","\t\tbranchName = processedEvent.HeadBranch","\t}","","\t// check if the commit on which comment is made, is HEAD commit of the branch","\tif err := v.isHeadCommitOfBranch(processedEvent, branchName); err != nil {","\t\tif provider.IsCancelComment(event.ObjectAttributes.Note) {","\t\t\tprocessedEvent.CancelPipelineRuns = false","\t\t}","\t\treturn processedEvent, err","\t}","","\tprocessedEvent.HeadBranch = branchName","\tprocessedEvent.BaseBranch = branchName","\tv.Logger.Infof(\"gitlab commit_comment: pipelinerun %s has been requested on %s/%s#%s\", action, processedEvent.Organization, processedEvent.Repository, processedEvent.SHA)","\treturn processedEvent, nil","}","","func isZeroSHA(sha string) bool {","\treturn sha == \"0000000000000000000000000000000000000000\"","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,2,1,1,2,2,2,2,2,2,2,2,1,1,0,0,0,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,0,2,2,0,0,2,2,2,2,2,0,0,1,1,1,1,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,0,0,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,0,2,2,2,2,2,2,0,0,2,2,2,2,1,1,2,2,0,0,2,2,2,2,2,2,0,2,2,2,0,2,2,2,0,0,2,2,2,0,0,2,1,1,1,1,0,0,2,2,2,2,0,0,2,2,2]},{"id":127,"path":"pkg/provider/gitlab/task.go","lines":["package gitlab","","import (","\t\"context\"","\t\"errors\"","\t\"fmt\"","\t\"net/url\"","\t\"regexp\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\tgl \"gitlab.com/gitlab-org/api/client-go\"",")","","type gitLabInfo struct {","\tScheme      string","\tHost        string","\tGroupOrUser string","\tRepository  string","\tRevision    string","\tFilePath    string","}","","// extractGitLabInfo generated with chatGPT https://chatgpt.com/share/e3c06a7e-3f16-4891-85c7-832b3e7f25c5","func extractGitLabInfo(gitlabURL string) (*gitLabInfo, error) {","\tparsedURL, err := url.Parse(gitlabURL)","\tif err != nil {","\t\treturn nil, err","\t}","","\t// Regular expression to match the specific GitLab URL pattern","\tre := regexp.MustCompile(`^/([^/]+(?:/[^/]+)*)/([^/]+)/-/blob/([^/]+)(/.*)?|^/([^/]+(?:/[^/]+)*)/([^/]+)/-/raw/([^/]+)(/.*)?`)","\tmatches := re.FindStringSubmatch(parsedURL.Path)","","\tif len(matches) == 0 {","\t\treturn nil, fmt.Errorf(\"URL does not match the expected GitLab pattern\")","\t}","","\tgroupOrUser := \"\"","\trepoName := \"\"","\trevision := \"\"","\tfilePath := \"\"","","\tif matches[1] != \"\" { // For /blob/ URLs","\t\tgroupOrUser = matches[1]","\t\trepoName = matches[2]","\t\trevision = matches[3]","\t\tif len(matches) \u003e= 5 \u0026\u0026 matches[4] != \"\" {","\t\t\tfilePath = matches[4][1:] // Remove initial slash","\t\t}","\t} else if matches[5] != \"\" { // For /raw/ URLs","\t\tgroupOrUser = matches[5]","\t\trepoName = matches[6]","\t\trevision = matches[7]","\t\tif len(matches) \u003e= 9 \u0026\u0026 matches[8] != \"\" {","\t\t\tfilePath = matches[8][1:] // Remove initial slash","\t\t}","\t}","","\treturn \u0026gitLabInfo{","\t\tScheme:      parsedURL.Scheme,","\t\tHost:        parsedURL.Host,","\t\tGroupOrUser: groupOrUser,","\t\tRepository:  repoName,","\t\tRevision:    revision,","\t\tFilePath:    filePath,","\t}, nil","}","","// GetTaskURI if we are getting a URL from the same URL where the provider is,","// it means we can try to get the file with the provider token.","func (v *Provider) GetTaskURI(_ context.Context, event *info.Event, uri string) (bool, string, error) {","\tif ret := provider.CompareHostOfURLS(uri, event.URL); !ret {","\t\treturn false, \"\", nil","\t}","\textracted, err := extractGitLabInfo(uri)","\tif err != nil {","\t\treturn false, \"\", err","\t}","","\t// Use the existing client if available, otherwise create a temporary one.","\t// We avoid storing it to prevent side effects on the provider's state.","\tclient := v.gitlabClient","\tif client == nil {","\t\tbaseURL := fmt.Sprintf(\"%s://%s\", extracted.Scheme, extracted.Host)","\t\tvar clientErr error","\t\tclient, clientErr = gl.NewClient(event.Provider.Token, gl.WithBaseURL(baseURL))","\t\tif clientErr != nil {","\t\t\treturn false, \"\", fmt.Errorf(\"failed to create gitlab client: %w\", clientErr)","\t\t}","\t}","","\t// Construct the project slug for the remote repository","\tprojectSlug := extracted.GroupOrUser + \"/\" + extracted.Repository","","\t// Get the project ID for the remote repository","\tproject, _, err := client.Projects.GetProject(projectSlug, \u0026gl.GetProjectOptions{})","\tif err != nil {","\t\tif errors.Is(err, gl.ErrNotFound) {","\t\t\treturn false, \"\", nil","\t\t}","\t\treturn false, \"\", fmt.Errorf(\"failed to get project ID for %s: %w\", projectSlug, err)","\t}","","\t// Fetch the file from the remote repository","\topt := \u0026gl.GetRawFileOptions{","\t\tRef: gl.Ptr(extracted.Revision),","\t}","\tfile, _, err := client.RepositoryFiles.GetRawFile(project.ID, extracted.FilePath, opt)","\tif err != nil {","\t\tif errors.Is(err, gl.ErrNotFound) {","\t\t\treturn false, \"\", nil","\t\t}","\t\treturn false, \"\", fmt.Errorf(\"failed to get file %s from remote repository: %w\", extracted.FilePath, err)","\t}","\treturn true, string(file), nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,1,1,0,0,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,0,0,0,0,2,2,2,2,2,2,2,2,0,0,0,2,2,1,1,1,1,1,1,0,0,0,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,2,2,2,2,2,2,0,2,0]},{"id":128,"path":"pkg/provider/provider.go","lines":["package provider","","import (","\t\"fmt\"","\t\"net/url\"","\t\"regexp\"","\t\"strings\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/formatting\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"gopkg.in/yaml.v2\"",")","","const ValidationErrorTemplate = `\u003e [!CAUTION]","\u003e There are some errors in your PipelineRun template.","","| PipelineRun | Error |","|------|-------|`","","var (","\ttestRetestAllRegex    = regexp.MustCompile(`(?m)^(/retest|/test)\\s*$`)","\ttestRetestSingleRegex = regexp.MustCompile(`(?m)^(/test|/retest)[ \\t]+\\S+`)","\toktotestRegex         = regexp.MustCompile(`(?m)^/ok-to-test\\s*$`)","\tcancelAllRegex        = regexp.MustCompile(`(?m)^(/cancel)\\s*$`)","\tcancelSingleRegex     = regexp.MustCompile(`(?m)^(/cancel)[ \\t]+\\S+`)",")","","const (","\ttestComment   = \"/test\"","\tretestComment = \"/retest\"","\tcancelComment = \"/cancel\"",")","","const (","\tGitHubApp = \"GitHubApp\"",")","","type CommentType int","","const (","\tStartingPipelineType CommentType = iota","\tPipelineRunStatusType","\tQueueingPipelineType",")","","func GetHTMLTemplate(commentType CommentType) string {","\tswitch commentType {","\tcase StartingPipelineType:","\t\treturn formatting.StartingPipelineRunHTML","\tcase PipelineRunStatusType:","\t\treturn formatting.PipelineRunStatusHTML","\tcase QueueingPipelineType:","\t\treturn formatting.QueuingPipelineRunHTML","\t}","\treturn \"\"","}","","func GetMarkdownTemplate(commentType CommentType) string {","\tswitch commentType {","\tcase StartingPipelineType:","\t\treturn formatting.StartingPipelineRunMarkdown","\tcase PipelineRunStatusType:","\t\treturn formatting.PipelineRunStatusMarkDown","\tcase QueueingPipelineType:","\t\treturn formatting.QueuingPipelineRunMarkdown","\t}","\treturn \"\"","}","","func Valid(value string, validValues []string) bool {","\tfor _, v := range validValues {","\t\tif v == value {","\t\t\treturn true","\t\t}","\t}","\treturn false","}","","func IsTestRetestComment(comment string) bool {","\treturn testRetestSingleRegex.MatchString(comment) || testRetestAllRegex.MatchString(comment)","}","","func IsOkToTestComment(comment string) bool {","\treturn oktotestRegex.MatchString(comment)","}","","func IsCancelComment(comment string) bool {","\treturn cancelAllRegex.MatchString(comment) || cancelSingleRegex.MatchString(comment)","}","","func GetPipelineRunFromTestComment(comment string) string {","\tif strings.Contains(comment, testComment) {","\t\treturn getNameFromComment(testComment, comment)","\t}","\treturn getNameFromComment(retestComment, comment)","}","","func GetPipelineRunFromCancelComment(comment string) string {","\treturn getNameFromComment(cancelComment, comment)","}","","func getNameFromComment(typeOfComment, comment string) string {","\tsplitTest := strings.Split(comment, typeOfComment)","\t// now get the first line","\tgetFirstLine := strings.Split(splitTest[1], \"\\n\")","\t// trim spaces","\treturn strings.TrimSpace(getFirstLine[0])","}","","func GetPipelineRunAndBranchOrTagNameFromTestComment(comment string) (string, string, string, error) {","\tif strings.Contains(comment, testComment) {","\t\treturn getPipelineRunAndBranchOrTagNameFromComment(testComment, comment)","\t}","\treturn getPipelineRunAndBranchOrTagNameFromComment(retestComment, comment)","}","","func GetPipelineRunAndBranchOrTagNameFromCancelComment(comment string) (string, string, string, error) {","\treturn getPipelineRunAndBranchOrTagNameFromComment(cancelComment, comment)","}","","// getPipelineRunAndBranchOrTagNameFromComment function will take GitOps comment and split the comment","// by /test, /retest or /cancel to return branch name and pipelinerun name.","func getPipelineRunAndBranchOrTagNameFromComment(typeOfComment, comment string) (string, string, string, error) {","\tvar prName, branchName, tagName string","\t// avoid parsing error due to branch name contains /test, /retest or /cancel,","\t// here only split the first keyword and not split the later keywords.","\tsplitText := strings.SplitN(comment, typeOfComment, 2)","","\t// after the split get the second part of the typeOfComment (/test, /retest or /cancel)","\t// as second part can be branch name or pipelinerun name and branch name","\t// ex: /test branch:nightly, /test prname branch:nightly, /test prname branch:nightly key=value","\tif splitText[1] != \"\" \u0026\u0026 strings.Contains(splitText[1], \":\") {","\t\tbranchData := strings.Split(splitText[1], \":\")","","\t\t// make sure no other word is supported other than branch word","\t\tif !strings.Contains(splitText[1], \"branch:\") \u0026\u0026 !strings.Contains(splitText[1], \"tag:\") {","\t\t\treturn prName, branchName, tagName, fmt.Errorf(\"the GitOps comment `%s` does not contain a branch or tag word\", comment)","\t\t}","","\t\tif strings.Contains(splitText[1], \"tag\") {","\t\t\ttagName = getBranchOrTagNameFromComment(splitText[1], \"tag\")","\t\t} else {","\t\t\tbranchName = getBranchOrTagNameFromComment(splitText[1], \"branch\")","\t\t}","","\t\t// if data after the split contains prname then fetch that","\t\tprData := strings.Split(strings.TrimSpace(branchData[0]), \" \")","\t\tif len(prData) \u003e 1 {","\t\t\tprName = strings.TrimSpace(prData[0])","\t\t}","\t} else {","\t\t// get the second part of the typeOfComment (/test, /retest or /cancel)","\t\t// as second part contains pipelinerun name","\t\t// ex: /test prname","\t\tgetFirstLine := strings.Split(splitText[1], \"\\n\")","\t\t// trim spaces","\t\t// adapt for the comment contains the key=value pair","\t\tprName = strings.Split(strings.TrimSpace(getFirstLine[0]), \" \")[0]","\t}","\treturn prName, branchName, tagName, nil","}","","// getBranchOrTagNameFromComment extracts the tag or branch name that follows the \"tag:\" or \"branch:\" marker in","// the provided comment. It allows optional whitespace after the colon and","// returns the first contiguous non-whitespace token (e.g., \"v1.2.3\"). If no","// such token is found, it returns an empty string.","func getBranchOrTagNameFromComment(comment, prefix string) string {","\tre := regexp.MustCompile(fmt.Sprintf(`%s:\\s*(\\S+)`, prefix))","\tmatches := re.FindStringSubmatch(comment)","\tif len(matches) \u003e 1 {","\t\treturn matches[1]","\t}","\treturn \"\"","}","","// CompareHostOfURLS compares the host of two parsed URLs and returns true if","// they are.","func CompareHostOfURLS(uri1, uri2 string) bool {","\tu1, err := url.Parse(uri1)","\tif err != nil || u1.Host == \"\" {","\t\treturn false","\t}","\tu2, err := url.Parse(uri2)","\tif err != nil || u2.Host == \"\" {","\t\treturn false","\t}","\treturn u1.Host == u2.Host","}","","func ValidateYaml(content []byte, filename string) error {","\tvar validYaml any","\tif err := yaml.Unmarshal(content, \u0026validYaml); err != nil {","\t\treturn fmt.Errorf(\"error unmarshalling yaml file %s: %w\", filename, err)","\t}","\treturn nil","}","","// GetCheckName returns the name of the check to be created based on the status","// and the pacopts.","// If the pacopts.ApplicationName is set, it will be used as the check name.","// Otherwise, the OriginalPipelineRunName will be used.","// If the OriginalPipelineRunName is not set, an empty string will be returned.","// The check name will be in the format \"ApplicationName / OriginalPipelineRunName\".","func GetCheckName(status StatusOpts, pacopts *info.PacOpts) string {","\tif pacopts.ApplicationName != \"\" {","\t\tif status.OriginalPipelineRunName == \"\" {","\t\t\treturn pacopts.ApplicationName","\t\t}","\t\treturn fmt.Sprintf(\"%s / %s\", pacopts.ApplicationName, status.OriginalPipelineRunName)","\t}","\treturn status.OriginalPipelineRunName","}","","func IsZeroSHA(sha string) bool {","\treturn sha == \"0000000000000000000000000000000000000000\"","}","","// skipCIRegex is a compiled regular expression for detecting skip CI commands.","// It matches [skip ci], [ci skip], [skip tkn], or [tkn skip].","// Skip commands can be overridden by GitOps commands like /test, /retest.","var skipCIRegex = regexp.MustCompile(`\\[(skip ci|ci skip|skip tkn|tkn skip)\\]`)","","// SkipCI returns true if the given commit message contains any skip command.","func SkipCI(commitMessage string) bool {","\treturn skipCIRegex.MatchString(commitMessage)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,1,0,0,1,1,1,1,1,1,1,1,0,1,0,0,1,1,1,1,1,0,1,0,0,2,2,2,0,2,2,2,0,2,2,2,0,2,2,2,2,2,0,0,2,2,2,0,2,2,2,2,2,2,2,0,2,2,2,2,2,0,0,2,2,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,1,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,0,2,2,2,2,2,2,1,0,0,0,0,2,2,2,2,2,2,2,1,1,2,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,2,0,0,1,1,1,0,0,0,0,0,0,0,2,2,2]},{"id":129,"path":"pkg/provider/providermetrics/metrics.go","lines":["package providermetrics","","import (","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\tprmetrics \"github.com/openshift-pipelines/pipelines-as-code/pkg/pipelinerunmetrics\"","\t\"go.uber.org/zap\"",")","","func RecordAPIUsage(logger *zap.SugaredLogger, provider, eventType string, repo *v1alpha1.Repository) {","\trecorder, err := prmetrics.NewRecorder()","\tif err != nil {","\t\tlogger.Errorf(\"Error initializing metrics recorder: %v\", err)","\t}","\trepoName := \"\"","\tnamespace := \"\"","\tif repo != nil {","\t\trepoName = repo.Name","\t\tnamespace = repo.Namespace","\t}","","\tif err := recorder.ReportGitProviderAPIUsage(provider, eventType, namespace, repoName); err != nil {","\t\tlogger.Errorf(\"Error reporting git API usage metrics for %q repository %q in %q namespace: %v\", provider, namespace, repoName, err)","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,2,2,2,1,1,2,2,2,2,2,2,0,2,1,1,0]},{"id":130,"path":"pkg/queue/priority_queue.go","lines":["package queue","","import (","\t\"container/heap\"",")","","type (","\tkey = string",")","","type item struct {","\tkey      string","\tpriority int64","\tindex    int","}","","type priorityQueue struct {","\titems     []*item","\titemByKey map[string]*item","}","","func (pq *priorityQueue) isPending(key key) bool {","\tif _, ok := pq.itemByKey[key]; ok {","\t\treturn true","\t}","\treturn false","}","","func (pq *priorityQueue) add(key key, priority int64) {","\tif _, ok := pq.itemByKey[key]; ok {","\t\treturn","\t}","\theap.Push(pq, \u0026item{key: key, priority: priority})","}","","func (pq *priorityQueue) remove(key key) {","\tif item, ok := pq.itemByKey[key]; ok {","\t\theap.Remove(pq, item.index)","\t\tdelete(pq.itemByKey, key)","\t}","}","","func (pq *priorityQueue) pop() *item {","\titem, _ := heap.Pop(pq).(*item)","\treturn item","}","","func (pq *priorityQueue) peek() *item {","\treturn pq.items[0]","}","","func (pq priorityQueue) Len() int { return len(pq.items) }","","func (pq priorityQueue) Less(i, j int) bool {","\treturn pq.items[i].priority \u003c pq.items[j].priority","}","","func (pq priorityQueue) Swap(i, j int) {","\tpq.items[i], pq.items[j] = pq.items[j], pq.items[i]","\tpq.items[i].index = i","\tpq.items[j].index = j","}","","func (pq *priorityQueue) Push(x any) {","\tn := len(pq.items)","\titem, _ := x.(*item)","\titem.index = n","\tpq.items = append(pq.items, item)","\tpq.itemByKey[item.key] = item","}","","func (pq *priorityQueue) Pop() any {","\told := pq.items","\tn := len(old)","\titem := old[n-1]","\titem.index = -1","\tpq.items = old[0 : n-1]","\tdelete(pq.itemByKey, item.key)","\treturn item","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,0,2,2,2,2,2,0,0,2,2,2,2,2,0,0,2,2,2,2,0,2,2,2,0,2,0,2,2,2,0,2,2,2,2,2,0,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2]},{"id":131,"path":"pkg/queue/queue_manager.go","lines":["package queue","","import (","\t\"context\"","\t\"fmt\"","\t\"strings\"","\t\"sync\"","\t\"time\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/generated/clientset/versioned\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/sort\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\tversioned2 \"github.com/tektoncd/pipeline/pkg/client/clientset/versioned\"","\t\"go.uber.org/zap\"","\tv1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/apimachinery/pkg/runtime\"",")","","const (","\tcreationTimestamp = \"{.metadata.creationTimestamp}\"",")","","type Manager struct {","\tqueueMap map[string]Semaphore","\tlock     *sync.Mutex","\tlogger   *zap.SugaredLogger","}","","func NewManager(logger *zap.SugaredLogger) *Manager {","\treturn \u0026Manager{","\t\tqueueMap: make(map[string]Semaphore),","\t\tlock:     \u0026sync.Mutex{},","\t\tlogger:   logger,","\t}","}","","// getSemaphore returns existing semaphore created for repository or create","// a new one with limit provided in repository","// Semaphore: nothing but a waiting and a running queue for a repository","// with limit deciding how many should be running at a time.","func (qm *Manager) getSemaphore(repo *v1alpha1.Repository) (Semaphore, error) {","\trepoKey := RepoKey(repo)","","\tif sema, found := qm.queueMap[repoKey]; found {","\t\tif err := qm.checkAndUpdateSemaphoreSize(repo, sema); err != nil {","\t\t\treturn nil, err","\t\t}","\t\treturn sema, nil","\t}","","\t// create a new semaphore; can't assume callers have checked that ConcurrencyLimit is set","\tlimit := 0","\tif repo.Spec.ConcurrencyLimit != nil {","\t\tlimit = *repo.Spec.ConcurrencyLimit","\t}","\tqm.queueMap[repoKey] = newSemaphore(repoKey, limit)","","\treturn qm.queueMap[repoKey], nil","}","","func (qm *Manager) checkAndUpdateSemaphoreSize(repo *v1alpha1.Repository, semaphore Semaphore) error {","\tlimit := *repo.Spec.ConcurrencyLimit","\tif limit != semaphore.getLimit() {","\t\tif semaphore.resize(limit) {","\t\t\treturn nil","\t\t}","\t\treturn fmt.Errorf(\"failed to resize semaphore\")","\t}","\treturn nil","}","","// AddListToRunningQueue adds the pipelineRun to the waiting queue of the repository","// and if it is at the top and ready to run which means currently running pipelineRun \u003c limit","// then move it to running queue","// This adds the pipelineRuns in the same order as in the list.","func (qm *Manager) AddListToRunningQueue(repo *v1alpha1.Repository, list []string) ([]string, error) {","\tqm.lock.Lock()","\tdefer qm.lock.Unlock()","","\tsema, err := qm.getSemaphore(repo)","\tif err != nil {","\t\treturn []string{}, err","\t}","","\tfor _, pr := range list {","\t\tif sema.addToQueue(pr, time.Now()) {","\t\t\tqm.logger.Infof(\"added pipelineRun (%s) to running queue for repository (%s)\", pr, RepoKey(repo))","\t\t}","\t}","","\t// it is possible something besides PAC set the PipelineRun to Pending; if concurrency limit has not","\t// been set, return all the pending PipelineRuns; also, if the limit is zero, that also means do not throttle,","\t// so we return all the PipelinesRuns, the for loop below skips that case as well","\tif repo.Spec.ConcurrencyLimit == nil || *repo.Spec.ConcurrencyLimit == 0 {","\t\treturn sema.getCurrentPending(), nil","\t}","","\tacquiredList := []string{}","\tfor i := 0; i \u003c *repo.Spec.ConcurrencyLimit; i++ {","\t\tacquired := sema.acquireLatest()","\t\tif acquired != \"\" {","\t\t\tqm.logger.Infof(\"moved (%s) to running for repository (%s)\", acquired, RepoKey(repo))","\t\t\tacquiredList = append(acquiredList, acquired)","\t\t}","\t}","","\treturn acquiredList, nil","}","","func (qm *Manager) AddToPendingQueue(repo *v1alpha1.Repository, list []string) error {","\tqm.lock.Lock()","\tdefer qm.lock.Unlock()","","\tsema, err := qm.getSemaphore(repo)","\tif err != nil {","\t\treturn err","\t}","","\tfor _, pr := range list {","\t\tif sema.addToPendingQueue(pr, time.Now()) {","\t\t\tqm.logger.Infof(\"added pipelineRun (%s) to pending queue for repository (%s)\", pr, RepoKey(repo))","\t\t}","\t}","\treturn nil","}","","func (qm *Manager) RemoveFromQueue(repoKey, prKey string) bool {","\tqm.lock.Lock()","\tdefer qm.lock.Unlock()","","\tsema, found := qm.queueMap[repoKey]","\tif !found {","\t\treturn false","\t}","","\tsema.release(prKey)","\tsema.removeFromQueue(prKey)","\tqm.logger.Infof(\"removed (%s) for repository (%s)\", prKey, repoKey)","\treturn true","}","","func (qm *Manager) RemoveAndTakeItemFromQueue(repo *v1alpha1.Repository, run *tektonv1.PipelineRun) string {","\trepoKey := RepoKey(repo)","\tprKey := PrKey(run)","\tif !qm.RemoveFromQueue(repoKey, prKey) {","\t\treturn \"\"","\t}","\tsema, found := qm.queueMap[repoKey]","\tif !found {","\t\treturn \"\"","\t}","","\tif next := sema.acquireLatest(); next != \"\" {","\t\tqm.logger.Infof(\"moved (%s) to running for repository (%s)\", next, repoKey)","\t\treturn next","\t}","\treturn \"\"","}","","// FilterPipelineRunByInProgress filters the given list of PipelineRun names to only include those","// that are in a \"queued\" state and have a pending status. It retrieves the PipelineRun objects","// from the Tekton API and checks their annotations and status to determine if they should be included.","//","// Returns A list of PipelineRun names that are in a \"queued\" state and have a pending status.","func FilterPipelineRunByState(ctx context.Context, tekton versioned2.Interface, orderList []string, wantedStatus, wantedState string) []string {","\torderedList := []string{}","\tfor _, prName := range orderList {","\t\tprKey := strings.Split(prName, \"/\")","\t\tpr, err := tekton.TektonV1().PipelineRuns(prKey[0]).Get(ctx, prKey[1], v1.GetOptions{})","\t\tif err != nil {","\t\t\tcontinue","\t\t}","","\t\tstate, exist := pr.GetAnnotations()[keys.State]","\t\tif !exist {","\t\t\tcontinue","\t\t}","","\t\tif state == wantedState {","\t\t\tif wantedStatus != \"\" \u0026\u0026 pr.Spec.Status != tektonv1.PipelineRunSpecStatus(wantedStatus) {","\t\t\t\tcontinue","\t\t\t}","\t\t\torderedList = append(orderedList, prName)","\t\t}","\t}","\treturn orderedList","}","","// InitQueues rebuild all the queues for all repository if concurrency is defined before","// reconciler started reconciling them.","func (qm *Manager) InitQueues(ctx context.Context, tekton versioned2.Interface, pac versioned.Interface) error {","\t// fetch all repos","\trepos, err := pac.PipelinesascodeV1alpha1().Repositories(\"\").List(ctx, v1.ListOptions{})","\tif err != nil {","\t\treturn err","\t}","","\t// pipelineRuns from the namespace where repository is present","\t// those are required for creating queues","\tfor _, repo := range repos.Items {","\t\tif repo.Spec.ConcurrencyLimit == nil || *repo.Spec.ConcurrencyLimit == 0 {","\t\t\tcontinue","\t\t}","","\t\t// add all pipelineRuns in started state to pending queue","\t\tprs, err := tekton.TektonV1().PipelineRuns(repo.Namespace).","\t\t\tList(ctx, v1.ListOptions{","\t\t\t\tLabelSelector: fmt.Sprintf(\"%s=%s\", keys.State, kubeinteraction.StateStarted),","\t\t\t})","\t\tif err != nil {","\t\t\treturn err","\t\t}","","\t\t// sort the pipelinerun by creation time before adding to queue","\t\tsortedPRs := sortPipelineRunsByCreationTimestamp(prs.Items)","","\t\tfor _, pr := range sortedPRs {","\t\t\torder, exist := pr.GetAnnotations()[keys.ExecutionOrder]","\t\t\tif !exist {","\t\t\t\t// if the pipelineRun doesn't have order label then wait","\t\t\t\treturn nil","\t\t\t}","\t\t\torderedList := FilterPipelineRunByState(ctx, tekton, strings.Split(order, \",\"), \"\", kubeinteraction.StateStarted)","","\t\t\t_, err = qm.AddListToRunningQueue(\u0026repo, orderedList)","\t\t\tif err != nil {","\t\t\t\tqm.logger.Error(\"failed to init queue for repo: \", repo.GetName())","\t\t\t}","\t\t}","","\t\t// now fetch all queued pipelineRun","\t\tprs, err = tekton.TektonV1().PipelineRuns(repo.Namespace).","\t\t\tList(ctx, v1.ListOptions{","\t\t\t\tLabelSelector: fmt.Sprintf(\"%s=%s\", keys.State, kubeinteraction.StateQueued),","\t\t\t})","\t\tif err != nil {","\t\t\treturn err","\t\t}","","\t\t// sort the pipelinerun by creation time before adding to queue","\t\tsortedPRs = sortPipelineRunsByCreationTimestamp(prs.Items)","","\t\tfor _, pr := range sortedPRs {","\t\t\torder, exist := pr.GetAnnotations()[keys.ExecutionOrder]","\t\t\tif !exist {","\t\t\t\t// if the pipelineRun doesn't have order label then wait","\t\t\t\treturn nil","\t\t\t}","\t\t\torderedList := FilterPipelineRunByState(ctx, tekton, strings.Split(order, \",\"), tektonv1.PipelineRunSpecStatusPending, kubeinteraction.StateQueued)","\t\t\tif err := qm.AddToPendingQueue(\u0026repo, orderedList); err != nil {","\t\t\t\tqm.logger.Error(\"failed to init queue for repo: \", repo.GetName())","\t\t\t}","\t\t}","\t}","","\treturn nil","}","","func (qm *Manager) RemoveRepository(repo *v1alpha1.Repository) {","\tqm.lock.Lock()","\tdefer qm.lock.Unlock()","","\trepoKey := RepoKey(repo)","\tdelete(qm.queueMap, repoKey)","}","","func (qm *Manager) QueuedPipelineRuns(repo *v1alpha1.Repository) []string {","\tqm.lock.Lock()","\tdefer qm.lock.Unlock()","","\trepoKey := RepoKey(repo)","\tif sema, ok := qm.queueMap[repoKey]; ok {","\t\treturn sema.getCurrentPending()","\t}","\treturn []string{}","}","","func (qm *Manager) RunningPipelineRuns(repo *v1alpha1.Repository) []string {","\tqm.lock.Lock()","\tdefer qm.lock.Unlock()","","\trepoKey := RepoKey(repo)","\tif sema, ok := qm.queueMap[repoKey]; ok {","\t\treturn sema.getCurrentRunning()","\t}","\treturn []string{}","}","","func sortPipelineRunsByCreationTimestamp(prs []tektonv1.PipelineRun) []*tektonv1.PipelineRun {","\trunTimeObj := make([]runtime.Object, len(prs))","\tfor i := range prs {","\t\trunTimeObj[i] = \u0026prs[i]","\t}","\tsort.ByField(creationTimestamp, runTimeObj)","\tsortedPRs := make([]*tektonv1.PipelineRun, len(runTimeObj))","\tfor i, run := range runTimeObj {","\t\tpr, _ := run.(*tektonv1.PipelineRun)","\t\tsortedPRs[i] = pr","\t}","\treturn sortedPRs","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,0,0,0,0,0,2,2,2,2,2,1,1,2,0,0,0,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,1,0,2,0,0,0,0,0,0,2,2,2,2,2,2,1,1,0,2,2,2,2,0,0,0,0,0,2,2,2,0,2,2,2,2,2,2,2,0,0,2,0,0,2,2,2,2,2,2,1,1,0,2,2,2,2,0,2,0,0,2,2,2,2,2,2,1,1,0,2,2,2,2,0,0,2,2,2,2,1,1,2,2,1,1,0,2,2,2,2,2,0,0,0,0,0,0,0,2,2,2,2,2,2,1,0,0,2,2,1,0,0,2,2,2,0,2,0,0,2,0,0,0,0,2,2,2,2,1,1,0,0,0,2,2,1,0,0,0,2,2,2,2,2,1,1,0,0,2,2,2,2,2,1,1,1,2,2,2,2,1,1,0,0,0,2,2,2,2,2,1,1,0,0,2,2,2,2,2,1,1,1,2,2,1,1,0,0,0,2,0,0,1,1,1,1,1,1,1,0,2,2,2,2,2,2,2,2,1,0,0,2,2,2,2,2,2,2,2,1,0,0,2,2,2,2,2,2,2,2,2,2,2,2,0]},{"id":132,"path":"pkg/queue/queue_manager_interface.go","lines":["package queue","","import (","\t\"context\"","\t\"fmt\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/generated/clientset/versioned\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\ttektonVersionedClient \"github.com/tektoncd/pipeline/pkg/client/clientset/versioned\"",")","","type ManagerInterface interface {","\tInitQueues(ctx context.Context, tekton tektonVersionedClient.Interface, pac versioned.Interface) error","\tRemoveRepository(repo *v1alpha1.Repository)","\tQueuedPipelineRuns(repo *v1alpha1.Repository) []string","\tRunningPipelineRuns(repo *v1alpha1.Repository) []string","\tAddListToRunningQueue(repo *v1alpha1.Repository, list []string) ([]string, error)","\tAddToPendingQueue(repo *v1alpha1.Repository, list []string) error","\tRemoveFromQueue(repoKey, prKey string) bool","\tRemoveAndTakeItemFromQueue(repo *v1alpha1.Repository, run *tektonv1.PipelineRun) string","}","","func RepoKey(repo *v1alpha1.Repository) string {","\treturn fmt.Sprintf(\"%s/%s\", repo.Namespace, repo.Name)","}","","func PrKey(run *tektonv1.PipelineRun) string {","\treturn fmt.Sprintf(\"%s/%s\", run.Namespace, run.Name)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,2,2,2]},{"id":133,"path":"pkg/queue/semaphore.go","lines":["package queue","","import (","\t\"fmt\"","\t\"sync\"","\t\"time\"","","\tsema \"golang.org/x/sync/semaphore\"",")","","type prioritySemaphore struct {","\tname      string","\tlimit     int","\tpending   *priorityQueue","\trunning   map[string]bool","\tsemaphore *sema.Weighted","\tlock      *sync.Mutex","}","","var _ Semaphore = \u0026prioritySemaphore{}","","func newSemaphore(name string, limit int) *prioritySemaphore {","\treturn \u0026prioritySemaphore{","\t\tname:      name,","\t\tlimit:     limit,","\t\tpending:   \u0026priorityQueue{itemByKey: make(map[string]*item)},","\t\tsemaphore: sema.NewWeighted(int64(limit)),","\t\trunning:   make(map[string]bool),","\t\tlock:      \u0026sync.Mutex{},","\t}","}","","func (s *prioritySemaphore) getName() string {","\treturn s.name","}","","func (s *prioritySemaphore) getLimit() int {","\treturn s.limit","}","","func (s *prioritySemaphore) getCurrentPending() []string {","\tkeys := make([]string, 0, len(s.pending.items))","\tfor _, item := range s.pending.items {","\t\tkeys = append(keys, item.key)","\t}","\treturn keys","}","","func (s *prioritySemaphore) getCurrentRunning() []string {","\tkeys := make([]string, 0, len(s.running))","\tfor k := range s.running {","\t\tkeys = append(keys, k)","\t}","\treturn keys","}","","func (s *prioritySemaphore) resize(n int) bool {","\ts.lock.Lock()","\tdefer s.lock.Unlock()","","\tcur := len(s.running)","\t// downward case, acquired n locks","\tif cur \u003e n {","\t\tcur = n","\t}","","\tsemaphore := sema.NewWeighted(int64(n))","\tstatus := semaphore.TryAcquire(int64(cur))","\tif status {","\t\ts.semaphore = semaphore","\t\ts.limit = n","\t}","\treturn status","}","","func (s *prioritySemaphore) removeFromQueue(key string) {","\ts.lock.Lock()","\tdefer s.lock.Unlock()","","\ts.pending.remove(key)","}","","func (s *prioritySemaphore) addToPendingQueue(key string, creationTime time.Time) bool {","\ts.lock.Lock()","\tdefer s.lock.Unlock()","","\tif _, ok := s.running[key]; ok {","\t\treturn false","\t}","\tif s.pending.isPending(key) {","\t\treturn false","\t}","\ts.pending.add(key, creationTime.UnixNano())","\treturn true","}","","func (s *prioritySemaphore) acquireLatest() string {","\ts.lock.Lock()","\tdefer s.lock.Unlock()","","\tif s.pending.Len() == 0 {","\t\treturn \"\"","\t}","","\tready := s.pending.peek()","","\tif s.semaphore.TryAcquire(1) {","\t\t_ = s.pending.pop()","\t\ts.running[ready.key] = true","\t\treturn ready.key","\t}","\treturn \"\"","}","","func (s *prioritySemaphore) release(key string) bool {","\ts.lock.Lock()","\tdefer s.lock.Unlock()","","\tif _, ok := s.running[key]; ok {","\t\tdelete(s.running, key)","","\t\t// When semaphore resized downward","\t\t// Remove the excess holders from map once the done.","\t\tif len(s.running) \u003e= s.limit {","\t\t\treturn true","\t\t}","","\t\ts.semaphore.Release(1)","\t}","\treturn true","}","","func (s *prioritySemaphore) addToQueue(key string, creationTime time.Time) bool {","\ts.lock.Lock()","\tdefer s.lock.Unlock()","","\tif _, ok := s.running[key]; ok {","\t\treturn false","\t}","\tif s.pending.isPending(key) {","\t\treturn false","\t}","\ts.pending.add(key, creationTime.UnixNano())","\treturn true","}","","func (s *prioritySemaphore) tryAcquire(key string) (bool, string) {","\ts.lock.Lock()","\tdefer s.lock.Unlock()","","\tif _, ok := s.running[key]; ok {","\t\treturn true, \"\"","\t}","","\twaitingMsg := fmt.Sprintf(\"Waiting for %s lock. Available queue status: %d/%d\", s.name, s.limit-len(s.running), s.limit)","","\t// Check whether requested key is in front of priority queue.","\t// If it is in front position, it will allow to acquire lock.","\t// If it is not a front key, it needs to wait for its turn.","\tvar nextKey string","\tif s.pending.Len() \u003e 0 {","\t\titem := s.pending.peek()","\t\tnextKey = fmt.Sprintf(\"%v\", item.key)","\t\tif key != nextKey {","\t\t\treturn false, waitingMsg","\t\t}","\t}","","\tif s.semaphore.TryAcquire(1) {","\t\ts.running[key] = true","\t\ts.pending.pop()","\t\treturn true, \"\"","\t}","","\treturn false, waitingMsg","}","","func (s *prioritySemaphore) acquire(key string) bool {","\ts.lock.Lock()","\tdefer s.lock.Unlock()","","\tif s.semaphore.TryAcquire(1) {","\t\ts.running[key] = true","\t\treturn true","\t}","\treturn false","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,0,2,2,2,0,2,2,2,0,2,2,2,2,2,2,0,0,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,0,2,2,2,2,2,1,1,2,2,2,2,2,0,0,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,0,2,0,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,0,2,0,0,1,1,1,1,1,1,1,1,1,0]},{"id":134,"path":"pkg/random/random.go","lines":["package random","","import (","\t\"crypto/rand\"",")","","const (","\tletterBytes   = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\" // 52 possibilities","\tletterIdxBits = 6                                                      // 6 bits to represent 64 possibilities / indexes","\tletterIdxMask = 1\u003c\u003cletterIdxBits - 1                                   // All 1-bits, as many as letterIdxBits",")","","// AlphaString returns a random alphanumeric string of the requested length","// https://stackoverflow.com/a/35615565/145125","func AlphaString(length int) string {","\tresult := make([]byte, length)","\tbufferSize := int(float64(length) * 1.3)","\tfor i, j, randomBytes := 0, 0, []byte{}; i \u003c length; j++ {","\t\tif j%bufferSize == 0 {","\t\t\trandomBytes = secureRandomBytes(bufferSize)","\t\t}","\t\tif idx := int(randomBytes[j%length] \u0026 letterIdxMask); idx \u003c len(letterBytes) {","\t\t\tresult[i] = letterBytes[idx]","\t\t\ti++","\t\t}","\t}","","\treturn string(result)","}","","// secureRandomBytes returns the requested number of bytes using crypto/rand.","func secureRandomBytes(length int) []byte {","\trandomBytes := make([]byte, length)","\t_, _ = rand.Read(randomBytes)","\treturn randomBytes","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,0,0,2,0,0,0,2,2,2,2,2]},{"id":135,"path":"pkg/reconciler/cleanup.go","lines":["package reconciler","","import (","\t\"context\"","\t\"strconv\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"go.uber.org/zap\"",")","","func (r *Reconciler) cleanupPipelineRuns(ctx context.Context, logger *zap.SugaredLogger, pacInfo *info.PacOpts, repo *v1alpha1.Repository, pr *tektonv1.PipelineRun) error {","\tkeepMaxPipeline, ok := pr.Annotations[keys.MaxKeepRuns]","\tif ok {","\t\tmaxVal, err := strconv.Atoi(keepMaxPipeline)","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\t// if annotation value is more than max limit defined in config then use from config","\t\tif pacInfo.MaxKeepRunsUpperLimit \u003e 0 \u0026\u0026 maxVal \u003e pacInfo.MaxKeepRunsUpperLimit {","\t\t\tlogger.Infof(\"max-keep-run value in annotation (%v) is more than max-keep-run-upper-limit (%v), so using upper-limit\", maxVal, pacInfo.MaxKeepRunsUpperLimit)","\t\t\tmaxVal = pacInfo.MaxKeepRunsUpperLimit","\t\t}","\t\terr = r.kinteract.CleanupPipelines(ctx, logger, repo, pr, maxVal)","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t\treturn nil","\t}","","\t// if annotation is not defined but default max-keep-run value is defined then use that","\tif pacInfo.DefaultMaxKeepRuns \u003e 0 {","\t\tmaxVal := pacInfo.DefaultMaxKeepRuns","","\t\terr := r.kinteract.CleanupPipelines(ctx, logger, repo, pr, maxVal)","\t\tif err != nil {","\t\t\treturn err","\t\t}","\t}","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,1,1,0,2,2,2,2,2,2,1,1,2,0,0,0,2,2,2,2,2,1,1,0,2,0]},{"id":136,"path":"pkg/reconciler/controller.go","lines":["package reconciler","","import (","\t\"context\"","\t\"path\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/events\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/generated/injection/informers/pipelinesascode/v1alpha1/repository\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\tprmetrics \"github.com/openshift-pipelines/pipelines-as-code/pkg/pipelinerunmetrics\"","\tqueuepkg \"github.com/openshift-pipelines/pipelines-as-code/pkg/queue\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\ttektonPipelineRunInformerv1 \"github.com/tektoncd/pipeline/pkg/client/injection/informers/pipeline/v1/pipelinerun\"","\ttektonPipelineRunReconcilerv1 \"github.com/tektoncd/pipeline/pkg/client/injection/reconciler/pipeline/v1/pipelinerun\"","\t\"k8s.io/apimachinery/pkg/types\"","\t\"knative.dev/pkg/configmap\"","\t\"knative.dev/pkg/controller\"","\t\"knative.dev/pkg/kmeta\"","\t\"knative.dev/pkg/logging\"","\t\"knative.dev/pkg/system\"",")","","func NewController() func(context.Context, configmap.Watcher) *controller.Impl {","\treturn func(ctx context.Context, _ configmap.Watcher) *controller.Impl {","\t\tctx = info.StoreNS(ctx, system.Namespace())","\t\tlog := logging.FromContext(ctx)","","\t\trun := params.New()","\t\terr := run.Clients.NewClients(ctx, \u0026run.Info)","\t\tif err != nil {","\t\t\tlog.Fatal(\"failed to init clients : \", err)","\t\t}","","\t\tkinteract, err := kubeinteraction.NewKubernetesInteraction(run)","\t\tif err != nil {","\t\t\tlog.Fatal(\"failed to init kinit client : \", err)","\t\t}","","\t\t// Start pac config syncer","\t\tgo params.StartConfigSync(ctx, run)","","\t\tpipelineRunInformer := tektonPipelineRunInformerv1.Get(ctx)","\t\tmetrics, err := prmetrics.NewRecorder()","\t\tif err != nil {","\t\t\tlog.Fatalf(\"Failed to create pipeline as code metrics recorder %v\", err)","\t\t}","","\t\tr := \u0026Reconciler{","\t\t\trun:               run,","\t\t\tkinteract:         kinteract,","\t\t\tpipelineRunLister: pipelineRunInformer.Lister(),","\t\t\trepoLister:        repository.Get(ctx).Lister(),","\t\t\tqm:                queuepkg.NewManager(run.Clients.Log),","\t\t\tmetrics:           metrics,","\t\t\teventEmitter:      events.NewEventEmitter(run.Clients.Kube, run.Clients.Log),","\t\t}","\t\timpl := tektonPipelineRunReconcilerv1.NewImpl(ctx, r, ctrlOpts())","","\t\tif err := r.qm.InitQueues(ctx, run.Clients.Tekton, run.Clients.PipelineAsCode); err != nil {","\t\t\tlog.Fatal(\"failed to init queues\", err)","\t\t}","","\t\tif _, err := pipelineRunInformer.Informer().AddEventHandler(controller.HandleAll(checkStateAndEnqueue(impl))); err != nil {","\t\t\tlogging.FromContext(ctx).Panicf(\"Couldn't register PipelineRun informer event handler: %w\", err)","\t\t}","","\t\treturn impl","\t}","}","","// enqueue only the pipelineruns which are in `started` state","// pipelinerun will have a label `pipelinesascode.tekton.dev/state` to describe the state.","func checkStateAndEnqueue(impl *controller.Impl) func(obj any) {","\treturn func(obj any) {","\t\tobject, err := kmeta.DeletionHandlingAccessor(obj)","\t\tif err == nil {","\t\t\t_, exist := object.GetAnnotations()[keys.State]","\t\t\tif exist {","\t\t\t\timpl.EnqueueKey(types.NamespacedName{Namespace: object.GetNamespace(), Name: object.GetName()})","\t\t\t}","\t\t}","\t}","}","","func ctrlOpts() func(impl *controller.Impl) controller.Options {","\treturn func(_ *controller.Impl) controller.Options {","\t\treturn controller.Options{","\t\t\tFinalizerName: path.Join(pipelinesascode.GroupName, pipelinesascode.FinalizerName),","\t\t\tPromoteFilterFunc: func(obj any) bool {","\t\t\t\t_, exist := obj.(*tektonv1.PipelineRun).GetAnnotations()[keys.State]","\t\t\t\treturn exist","\t\t\t},","\t\t}","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,0,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,0,1,0,0,0,0,0,2,2,2,2,2,2,2,2,0,0,0,0,2,2,2,2,2,2,2,2,0,0,0]},{"id":137,"path":"pkg/reconciler/emit_metrics.go","lines":["package reconciler","","import (","\t\"fmt\"","\t\"time\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\tcorev1 \"k8s.io/api/core/v1\"","\t\"knative.dev/pkg/apis\"",")","","func (r *Reconciler) emitMetrics(pr *tektonv1.PipelineRun) error {","\tif err := r.countPipelineRun(pr); err != nil {","\t\treturn err","\t}","","\tif err := r.calculatePRDuration(pr); err != nil {","\t\treturn err","\t}","","\treturn nil","}","","func (r *Reconciler) countPipelineRun(pr *tektonv1.PipelineRun) error {","\tgitProvider := pr.GetAnnotations()[keys.GitProvider]","\teventType := pr.GetAnnotations()[keys.EventType]","\trepository := pr.GetAnnotations()[keys.Repository]","","\tswitch gitProvider {","\tcase \"github\", \"github-enterprise\":","\t\tif _, ok := pr.GetAnnotations()[keys.InstallationID]; ok {","\t\t\tgitProvider += \"-app\"","\t\t} else {","\t\t\tgitProvider += \"-webhook\"","\t\t}","\tcase \"gitlab\", \"gitea\", \"forgejo\", \"bitbucket-cloud\", \"bitbucket-datacenter\":","\t\tgitProvider += \"-webhook\"","\tdefault:","\t\treturn fmt.Errorf(\"no supported Git provider\")","\t}","","\treturn r.metrics.Count(gitProvider, eventType, pr.GetNamespace(), repository)","}","","func (r *Reconciler) calculatePRDuration(pr *tektonv1.PipelineRun) error {","\trepository := pr.GetAnnotations()[keys.Repository]","\tduration := time.Duration(0)","\tif pr.Status.StartTime != nil {","\t\tduration = time.Since(pr.Status.StartTime.Time)","\t\tif pr.Status.CompletionTime != nil {","\t\t\tduration = pr.Status.CompletionTime.Sub(pr.Status.StartTime.Time)","\t\t}","\t}","","\tcond := pr.Status.GetCondition(apis.ConditionSucceeded)","\tstatus := \"success\"","\tif cond.Status == corev1.ConditionFalse {","\t\tstatus = \"failed\"","\t\tif cond.Reason == tektonv1.PipelineRunReasonCancelled.String() {","\t\t\tstatus = \"cancelled\"","\t\t}","\t}","\treason := cond.Reason","","\treturn r.metrics.CountPRDuration(pr.GetNamespace(), repository, status, reason, duration)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,1,1,1,0,1,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,0,0,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,0,2,2,2,0]},{"id":138,"path":"pkg/reconciler/event.go","lines":["package reconciler","","import (","\t\"context\"","\t\"fmt\"","\t\"strconv\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/triggertype\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/bitbucketcloud\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/bitbucketdatacenter\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/gitea\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/github\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider/gitlab\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"go.uber.org/zap\"",")","","// detectProvider detects the git provider for the given PipelineRun and","// initializes the corresponding provider interface. It returns the provider","// interface, event information, and an error if any occurs during detection or","// initialization.","//","// Supported providers: github, gitlab, bitbucket-cloud, bitbucket-datacenter, forgejo (gitea)","// any new provider should be added to the switch case below.","func (r *Reconciler) detectProvider(ctx context.Context, logger *zap.SugaredLogger, pr *tektonv1.PipelineRun) (provider.Interface, *info.Event, error) {","\tgitProvider, ok := pr.GetAnnotations()[keys.GitProvider]","\tif !ok {","\t\treturn nil, nil, fmt.Errorf(\"failed to detect git provider for pipleinerun %s : git-provider label not found\", pr.GetName())","\t}","","\tevent := buildEventFromPipelineRun(pr)","","\tvar provider provider.Interface","\tswitch gitProvider {","\tcase \"github\", \"github-enterprise\":","\t\tgh := github.New()","\t\tgh.Logger = logger","\t\tgh.Run = r.run","\t\tif event.InstallationID != 0 {","\t\t\tif err := gh.InitAppClient(ctx, r.run.Clients.Kube, event); err != nil {","\t\t\t\treturn nil, nil, err","\t\t\t}","\t\t}","\t\tprovider = gh","\tcase \"gitlab\":","\t\tprovider = \u0026gitlab.Provider{}","\tcase \"bitbucket-cloud\":","\t\tprovider = \u0026bitbucketcloud.Provider{}","\tcase \"bitbucket-datacenter\":","\t\tprovider = \u0026bitbucketdatacenter.Provider{}","\tcase \"gitea\", \"forgejo\":","\t\tprovider = \u0026gitea.Provider{}","\tdefault:","\t\treturn nil, nil, fmt.Errorf(\"failed to detect provider for pipelinerun: %s : unknown provider\", pr.GetName())","\t}","\tprovider.SetLogger(logger)","\treturn provider, event, nil","}","","func buildEventFromPipelineRun(pr *tektonv1.PipelineRun) *info.Event {","\tevent := info.NewEvent()","","\tprAnno := pr.GetAnnotations()","","\tevent.URL = prAnno[keys.RepoURL]","\tevent.Organization = prAnno[keys.URLOrg]","\tevent.Repository = prAnno[keys.URLRepository]","\tevent.EventType = prAnno[keys.EventType]","\tevent.TriggerTarget = triggertype.StringToType(prAnno[keys.EventType])","\tevent.BaseBranch = prAnno[keys.Branch]","\tevent.SHA = prAnno[keys.SHA]","","\tevent.SHATitle = prAnno[keys.ShaTitle]","\tevent.SHAURL = prAnno[keys.ShaURL]","","\tprNumber := prAnno[keys.PullRequest]","\tif prNumber != \"\" {","\t\tevent.PullRequestNumber, _ = strconv.Atoi(prNumber)","\t\tevent.TriggerTarget = triggertype.PullRequest","\t}","","\t// GitHub","\tif installationID, ok := prAnno[keys.InstallationID]; ok {","\t\tid, _ := strconv.Atoi(installationID)","\t\tevent.InstallationID = int64(id)","\t}","\tif gheURL, ok := prAnno[keys.GHEURL]; ok {","\t\tevent.GHEURL = gheURL","\t}","","\t// GitLab","\tif projectID, ok := prAnno[keys.SourceProjectID]; ok {","\t\tid, _ := strconv.ParseInt(projectID, 10, 64)","\t\tevent.SourceProjectID = id","\t}","\tif projectID, ok := prAnno[keys.TargetProjectID]; ok {","\t\tid, _ := strconv.ParseInt(projectID, 10, 64)","\t\tevent.TargetProjectID = id","\t}","\treturn event","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,1,1,1,0,2,2,2,1,1,1,1,2,2,2,2,0,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,0]},{"id":139,"path":"pkg/reconciler/finalizer.go","lines":["package reconciler","","import (","\t\"context\"","\t\"fmt\"","\t\"strings\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"k8s.io/apimachinery/pkg/api/errors\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"knative.dev/pkg/logging\"","\tpkgreconciler \"knative.dev/pkg/reconciler\"",")","","func (r *Reconciler) FinalizeKind(ctx context.Context, pr *tektonv1.PipelineRun) pkgreconciler.Event {","\tlogger := logging.FromContext(ctx)","\tstate, exist := pr.GetAnnotations()[keys.State]","\tif !exist || state == kubeinteraction.StateCompleted {","\t\treturn nil","\t}","","\tif state == kubeinteraction.StateQueued || state == kubeinteraction.StateStarted {","\t\trepoName, ok := pr.GetAnnotations()[keys.Repository]","\t\tif !ok {","\t\t\treturn nil","\t\t}","\t\trepo, err := r.repoLister.Repositories(pr.Namespace).Get(repoName)","\t\t// if repository is not found then remove the queue for that repository if exist","\t\tif errors.IsNotFound(err) {","\t\t\tr.qm.RemoveRepository(\u0026v1alpha1.Repository{","\t\t\t\tObjectMeta: metav1.ObjectMeta{Name: repoName, Namespace: pr.Namespace},","\t\t\t})","\t\t\treturn nil","\t\t}","\t\tif err != nil {","\t\t\treturn err","\t\t}","","\t\t// report the PipelineRun as cancelled as it was queued or started but it is deleted","\t\t// but its status is still in progress or queued on git provider","\t\tif err := r.reportPipelineRunAsCancelled(ctx, repo, pr); err != nil {","\t\t\tlogger.Errorf(\"failed to report deleted pipeline run as cancelled: %w\", err)","\t\t}","","\t\tr.secretNS = repo.GetNamespace()","\t\tif r.globalRepo, err = r.repoLister.Repositories(r.run.Info.Kube.Namespace).Get(r.run.Info.Controller.GlobalRepository); err == nil \u0026\u0026 r.globalRepo != nil {","\t\t\tif repo.Spec.GitProvider != nil \u0026\u0026 repo.Spec.GitProvider.Secret == nil \u0026\u0026 r.globalRepo.Spec.GitProvider != nil \u0026\u0026 r.globalRepo.Spec.GitProvider.Secret != nil {","\t\t\t\tr.secretNS = r.globalRepo.GetNamespace()","\t\t\t}","\t\t\trepo.Spec.Merge(r.globalRepo.Spec)","\t\t}","\t\tlogger = logger.With(\"namespace\", repo.Namespace)","\t\tnext := r.qm.RemoveAndTakeItemFromQueue(repo, pr)","\t\tif next != \"\" {","\t\t\tkey := strings.Split(next, \"/\")","\t\t\tpr, err := r.run.Clients.Tekton.TektonV1().PipelineRuns(key[0]).Get(ctx, key[1], metav1.GetOptions{})","\t\t\tif err != nil {","\t\t\t\treturn err","\t\t\t}","\t\t\tif err := r.","\t\t\t\tupdatePipelineRunToInProgress(ctx, logger, repo, pr); err != nil {","\t\t\t\tlogger.Errorf(\"failed to update status: %w\", err)","\t\t\t\treturn err","\t\t\t}","\t\t\treturn nil","\t\t}","\t}","\treturn nil","}","","func (r *Reconciler) reportPipelineRunAsCancelled(ctx context.Context, repo *v1alpha1.Repository, pr *tektonv1.PipelineRun) error {","\tlogger := logging.FromContext(ctx)","\tdetectedProvider, event, err := r.initGitProviderClient(ctx, logger, repo, pr)","\tif err != nil {","\t\treturn err","\t}","","\tconsoleURL := r.run.Clients.ConsoleUI().DetailURL(pr)","\tstatus := provider.StatusOpts{","\t\tConclusion:              \"cancelled\",","\t\tText:                    fmt.Sprintf(\"PipelineRun %s was deleted\", pr.GetName()),","\t\tDetailsURL:              consoleURL,","\t\tPipelineRunName:         pr.GetName(),","\t\tPipelineRun:             pr,","\t\tOriginalPipelineRunName: pr.GetAnnotations()[keys.OriginalPRName],","\t}","","\tif err := createStatusWithRetry(ctx, logger, detectedProvider, event, status); err != nil {","\t\treturn fmt.Errorf(\"failed to report cancelled status to provider: %w\", err)","\t}","","\tlogger.Infof(\"updated cancelled status on provider platform for pipelineRun %s\", pr.GetName())","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,2,2,2,1,1,2,2,2,2,2,2,2,2,2,1,1,0,0,0,2,1,1,0,2,2,1,1,1,1,0,2,2,2,1,1,1,1,1,1,1,1,1,1,1,0,0,2,0,0,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,2,0]},{"id":140,"path":"pkg/reconciler/queue_pipelineruns.go","lines":["package reconciler","","import (","\t\"context\"","\t\"fmt\"","\t\"strings\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\tpacAPIv1alpha1 \"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction\"","\tqueuepkg \"github.com/openshift-pipelines/pipelines-as-code/pkg/queue\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"go.uber.org/zap\"","\t\"k8s.io/apimachinery/pkg/api/errors\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"",")","","func (r *Reconciler) queuePipelineRun(ctx context.Context, logger *zap.SugaredLogger, pr *tektonv1.PipelineRun) error {","\torder, exist := pr.GetAnnotations()[keys.ExecutionOrder]","\tif !exist {","\t\t// if the pipelineRun doesn't have order label then wait","\t\treturn nil","\t}","","\t// check if annotation exist","\trepoName, exist := pr.GetAnnotations()[keys.Repository]","\tif !exist {","\t\treturn fmt.Errorf(\"no %s annotation found\", keys.Repository)","\t}","\tif repoName == \"\" {","\t\treturn fmt.Errorf(\"annotation %s is empty\", keys.Repository)","\t}","\trepo, err := r.repoLister.Repositories(pr.Namespace).Get(repoName)","\tif err != nil {","\t\t// if repository is not found, then skip processing the pipelineRun and return nil","\t\tif errors.IsNotFound(err) {","\t\t\tr.qm.RemoveRepository(\u0026pacAPIv1alpha1.Repository{","\t\t\t\tObjectMeta: metav1.ObjectMeta{","\t\t\t\t\tName:      repoName,","\t\t\t\t\tNamespace: pr.Namespace,","\t\t\t\t},","\t\t\t})","\t\t\treturn nil","\t\t}","\t\treturn fmt.Errorf(\"error getting PipelineRun: %w\", err)","\t}","","\t// merge local repo with global repo here in order to derive settings from global","\t// for further concurrency and other operations.","\tif r.globalRepo, err = r.repoLister.Repositories(r.run.Info.Kube.Namespace).Get(r.run.Info.Controller.GlobalRepository); err == nil \u0026\u0026 r.globalRepo != nil {","\t\tlogger.Info(\"Merging global repository settings with local repository settings\")","\t\trepo.Spec.Merge(r.globalRepo.Spec)","\t}","","\t// if concurrency was set and later removed or changed to zero","\t// then remove pipelineRun from Queue and update pending state to running","\tif repo.Spec.ConcurrencyLimit != nil \u0026\u0026 *repo.Spec.ConcurrencyLimit == 0 {","\t\t_ = r.qm.RemoveAndTakeItemFromQueue(repo, pr)","\t\tif err := r.updatePipelineRunToInProgress(ctx, logger, repo, pr); err != nil {","\t\t\treturn fmt.Errorf(\"failed to update PipelineRun to in_progress: %w\", err)","\t\t}","\t\treturn nil","\t}","","\tvar processed bool","\tvar itered int","\tmaxIterations := 5","","\torderedList := queuepkg.FilterPipelineRunByState(ctx, r.run.Clients.Tekton, strings.Split(order, \",\"), tektonv1.PipelineRunSpecStatusPending, kubeinteraction.StateQueued)","\tfor {","\t\tacquired, err := r.qm.AddListToRunningQueue(repo, orderedList)","\t\tif err != nil {","\t\t\treturn fmt.Errorf(\"failed to add to queue: %s: %w\", pr.GetName(), err)","\t\t}","\t\tif len(acquired) == 0 {","\t\t\tlogger.Infof(\"no new PipelineRun acquired for repo %s\", repo.GetName())","\t\t\tbreak","\t\t}","","\t\tfor _, prKeys := range acquired {","\t\t\tnsName := strings.Split(prKeys, \"/\")","\t\t\trepoKey := queuepkg.RepoKey(repo)","\t\t\tpr, err = r.run.Clients.Tekton.TektonV1().PipelineRuns(nsName[0]).Get(ctx, nsName[1], metav1.GetOptions{})","\t\t\tif err != nil {","\t\t\t\tlogger.Info(\"failed to get pr with namespace and name: \", nsName[0], nsName[1])","\t\t\t\t_ = r.qm.RemoveFromQueue(repoKey, prKeys)","\t\t\t} else {","\t\t\t\tif err := r.updatePipelineRunToInProgress(ctx, logger, repo, pr); err != nil {","\t\t\t\t\tlogger.Errorf(\"failed to update pipelineRun to in_progress: %w\", err)","\t\t\t\t\t_ = r.qm.RemoveFromQueue(repoKey, prKeys)","\t\t\t\t} else {","\t\t\t\t\tprocessed = true","\t\t\t\t}","\t\t\t}","\t\t}","\t\tif processed {","\t\t\tbreak","\t\t}","\t\tif itered \u003e= maxIterations {","\t\t\treturn fmt.Errorf(\"max iterations reached of %d times trying to get a pipelinerun started for %s\", maxIterations, repo.GetName())","\t\t}","\t\titered++","\t}","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,0,0,0,0,2,2,2,2,0,0,0,2,1,1,1,1,1,0,0,2,2,2,2,2,2,2,2,1,1,2,2,2,0,0,2,2,2,2,2,2,2,2,1,1,1,1,1,1,0,0,2,1,0,2,2,2,2,0,2,0]},{"id":141,"path":"pkg/reconciler/reconciler.go","lines":["package reconciler","","import (","\t\"context\"","\t\"encoding/json\"","\t\"fmt\"","\t\"strings\"","","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\tpipelinerunreconciler \"github.com/tektoncd/pipeline/pkg/client/injection/reconciler/pipeline/v1/pipelinerun\"","\ttektonv1lister \"github.com/tektoncd/pipeline/pkg/client/listers/pipeline/v1\"","\t\"go.uber.org/zap\"","\tcorev1 \"k8s.io/api/core/v1\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"knative.dev/pkg/logging\"","\tpkgreconciler \"knative.dev/pkg/reconciler\"","\t\"knative.dev/pkg/system\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/action\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/customparams\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/events\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/formatting\"","\tpacapi \"github.com/openshift-pipelines/pipelines-as-code/pkg/generated/listers/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/llm\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/settings\"","\tpac \"github.com/openshift-pipelines/pipelines-as-code/pkg/pipelineascode\"","\tprmetrics \"github.com/openshift-pipelines/pipelines-as-code/pkg/pipelinerunmetrics\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\tqueuepkg \"github.com/openshift-pipelines/pipelines-as-code/pkg/queue\"",")","","// Reconciler implements controller.Reconciler for PipelineRun resources.","type Reconciler struct {","\trun               *params.Run","\trepoLister        pacapi.RepositoryLister","\tpipelineRunLister tektonv1lister.PipelineRunLister","\tkinteract         kubeinteraction.Interface","\tqm                queuepkg.ManagerInterface","\tmetrics           *prmetrics.Recorder","\teventEmitter      *events.EventEmitter","\tglobalRepo        *v1alpha1.Repository","\tsecretNS          string","}","","var (","\t_ pipelinerunreconciler.Interface = (*Reconciler)(nil)","\t_ pipelinerunreconciler.Finalizer = (*Reconciler)(nil)",")","","// ReconcileKind is the main entry point for reconciling PipelineRun resources.","func (r *Reconciler) ReconcileKind(ctx context.Context, pr *tektonv1.PipelineRun) pkgreconciler.Event {","\tctx = info.StoreNS(ctx, system.Namespace())","\tlogger := logging.FromContext(ctx).With(\"namespace\", pr.GetNamespace())","","\tlogger.Debugf(\"reconciling pipelineRun %s/%s\", pr.GetNamespace(), pr.GetName())","","\t// make sure we have the latest pipelinerun to reconcile, since there is something updating at the same time","\tlpr, err := r.run.Clients.Tekton.TektonV1().PipelineRuns(pr.GetNamespace()).Get(ctx, pr.GetName(), metav1.GetOptions{})","\tif err != nil {","\t\treturn fmt.Errorf(\"cannot get pipelineRun: %w\", err)","\t}","","\tif lpr.GetResourceVersion() != pr.GetResourceVersion() {","\t\tlogger.Debugf(\"Skipping reconciliation, pipelineRun was updated (cached version %s vs fresh version %s)\", pr.GetResourceVersion(), lpr.GetResourceVersion())","\t\treturn nil","\t}","","\t// if pipelineRun is in completed or failed state then return","\tstate, exist := pr.GetAnnotations()[keys.State]","\tif exist \u0026\u0026 (state == kubeinteraction.StateCompleted || state == kubeinteraction.StateFailed) {","\t\treturn nil","\t}","","\treason := \"\"","\tif len(pr.Status.GetConditions()) \u003e 0 {","\t\treason = pr.Status.GetConditions()[0].GetReason()","\t}","\t// This condition handles cases where the PipelineRun has entered a \"Running\" state,","\t// but its status in the Git provider remains \"queued\" (e.g., due to updates made by","\t// another controller outside PaC). To maintain consistency between the PipelineRun","\t// status and the Git provider status, we update both the PipelineRun resource and","\t// the corresponding status on the Git provider here.","\tscmReportingPLRStarted, exist := pr.GetAnnotations()[keys.SCMReportingPLRStarted]","\tstartReported := exist \u0026\u0026 scmReportingPLRStarted == \"true\"","\tlogger.Debugf(\"pipelineRun %s/%s scmReportingPLRStarted=%v, exist=%v\", pr.GetNamespace(), pr.GetName(), startReported, exist)","","\tif reason == string(tektonv1.PipelineRunReasonRunning) \u0026\u0026 !startReported {","\t\tlogger.Infof(\"pipelineRun %s/%s is running but not yet reported to provider, updating status\", pr.GetNamespace(), pr.GetName())","\t\trepoName := pr.GetAnnotations()[keys.Repository]","\t\trepo, err := r.repoLister.Repositories(pr.Namespace).Get(repoName)","\t\tif err != nil {","\t\t\treturn fmt.Errorf(\"failed to get repository CR: %w\", err)","\t\t}","\t\treturn r.updatePipelineRunToInProgress(ctx, logger, repo, pr)","\t}","\tlogger.Debugf(\"pipelineRun %s/%s condition not met: reason='%s', startReported=%v\", pr.GetNamespace(), pr.GetName(), reason, startReported)","","\t// if its a GitHub App pipelineRun PR then process only if check run id is added otherwise wait","\tif _, ok := pr.Annotations[keys.InstallationID]; ok {","\t\tif _, ok := pr.Annotations[keys.CheckRunID]; !ok {","\t\t\treturn nil","\t\t}","\t}","","\t// queue pipelines which are in queued state and pending status","\t// if status is not pending, it could be cancelled so let it be reported, even if state is queued","\tif state == kubeinteraction.StateQueued \u0026\u0026 pr.Spec.Status == tektonv1.PipelineRunSpecStatusPending {","\t\treturn r.queuePipelineRun(ctx, logger, pr)","\t}","","\tif !pr.IsDone() \u0026\u0026 !pr.IsCancelled() {","\t\treturn nil","\t}","","\t// If we have a controllerInfo annotation, then we need to get the","\t// configmap configuration for it","\t//","\t// The annotation is a json string with a label, the pac controller","\t// configmap and the GitHub app secret .","\t//","\t// We always assume the controller is in the same namespace as the original","\t// controller but that may changes","\tif controllerInfo, ok := pr.GetAnnotations()[keys.ControllerInfo]; ok {","\t\tvar parsedControllerInfo *info.ControllerInfo","\t\tif err := json.Unmarshal([]byte(controllerInfo), \u0026parsedControllerInfo); err != nil {","\t\t\treturn fmt.Errorf(\"failed to parse controllerInfo: %w\", err)","\t\t}","\t\tr.run.Info.Controller = parsedControllerInfo","\t} else {","\t\tr.run.Info.Controller = info.GetControllerInfoFromEnvOrDefault()","\t}","","\tctx = info.StoreCurrentControllerName(ctx, r.run.Info.Controller.Name)","","\tlogFields := []interface{}{","\t\t\"pipeline-run\", pr.GetName(),","\t\t\"event-sha\", pr.GetAnnotations()[keys.SHA],","\t}","","\t// Add source repository URL if available","\tif repoURL := pr.GetAnnotations()[keys.RepoURL]; repoURL != \"\" {","\t\tlogFields = append(logFields, \"source-repo-url\", repoURL)","\t}","","\t// Add branch information if available","\tif targetBranch := pr.GetAnnotations()[keys.Branch]; targetBranch != \"\" {","\t\tlogFields = append(logFields, \"target-branch\", targetBranch)","\t\tif sourceBranch := pr.GetAnnotations()[keys.SourceBranch]; sourceBranch != \"\" \u0026\u0026 sourceBranch != targetBranch {","\t\t\tlogFields = append(logFields, \"source-branch\", sourceBranch)","\t\t}","\t}","","\t// Add event type information if available","\tif eventType := pr.GetAnnotations()[keys.EventType]; eventType != \"\" {","\t\tlogFields = append(logFields, \"event-type\", eventType)","\t}","","\tlogger = logger.With(logFields...)","\tlogger.Infof(\"pipelineRun %v/%v is done, reconciling to report status!  \", pr.GetNamespace(), pr.GetName())","\tr.eventEmitter.SetLogger(logger)","","\t// use same pac opts across the reconciliation","\tpacInfo := r.run.Info.GetPacOpts()","","\tdetectedProvider, event, err := r.detectProvider(ctx, logger, pr)","\tif err != nil {","\t\tmsg := fmt.Sprintf(\"detectProvider: %v\", err)","\t\tr.eventEmitter.EmitMessage(nil, zap.ErrorLevel, \"RepositoryDetectProvider\", msg)","\t\treturn nil","\t}","\tdetectedProvider.SetPacInfo(\u0026pacInfo)","","\tif repo, err := r.reportFinalStatus(ctx, logger, \u0026pacInfo, event, pr, detectedProvider); err != nil {","\t\tmsg := fmt.Sprintf(\"report status: %v\", err)","\t\tr.eventEmitter.EmitMessage(repo, zap.ErrorLevel, \"RepositoryReportFinalStatus\", msg)","\t\treturn err","\t}","\treturn nil","}","","func (r *Reconciler) reportFinalStatus(ctx context.Context, logger *zap.SugaredLogger, pacInfo *info.PacOpts, event *info.Event, pr *tektonv1.PipelineRun, provider provider.Interface) (*v1alpha1.Repository, error) {","\trepoName := pr.GetAnnotations()[keys.Repository]","\trepo, err := r.repoLister.Repositories(pr.Namespace).Get(repoName)","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"reportFinalStatus: %w\", err)","\t}","","\tr.secretNS = repo.GetNamespace()","\tif r.globalRepo, err = r.repoLister.Repositories(r.run.Info.Kube.Namespace).Get(r.run.Info.Controller.GlobalRepository); err == nil \u0026\u0026 r.globalRepo != nil {","\t\tif repo.Spec.GitProvider != nil \u0026\u0026 repo.Spec.GitProvider.Secret == nil \u0026\u0026 r.globalRepo.Spec.GitProvider != nil \u0026\u0026 r.globalRepo.Spec.GitProvider.Secret != nil {","\t\t\tr.secretNS = r.globalRepo.GetNamespace()","\t\t}","\t\trepo.Spec.Merge(r.globalRepo.Spec)","\t}","","\tcp := customparams.NewCustomParams(event, repo, r.run, r.kinteract, r.eventEmitter, nil)","\tmaptemplate, _, err := cp.GetParams(ctx)","\tif err != nil {","\t\tr.eventEmitter.EmitMessage(repo, zap.ErrorLevel, \"ParamsError\",","\t\t\tfmt.Sprintf(\"error processing repository CR custom params: %s\", err.Error()))","\t}","\tr.run.Clients.ConsoleUI().SetParams(maptemplate)","","\tif event.InstallationID \u003e 0 {","\t\tevent.Provider.WebhookSecret, _ = pac.GetCurrentNSWebhookSecret(ctx, r.kinteract, r.run)","\t} else {","\t\tsecretFromRepo := pac.SecretFromRepository{","\t\t\tK8int:       r.kinteract,","\t\t\tConfig:      provider.GetConfig(),","\t\t\tEvent:       event,","\t\t\tRepo:        repo,","\t\t\tWebhookType: pacInfo.WebhookType,","\t\t\tLogger:      logger,","\t\t\tNamespace:   r.secretNS,","\t\t}","\t\tif err := secretFromRepo.Get(ctx); err != nil {","\t\t\treturn repo, fmt.Errorf(\"cannot get secret from repository: %w\", err)","\t\t}","\t}","","\tif r.run.Clients.Log == nil {","\t\tr.run.Clients.Log = logger","\t}","\terr = provider.SetClient(ctx, r.run, event, repo, r.eventEmitter)","\tif err != nil {","\t\treturn repo, fmt.Errorf(\"cannot set client: %w\", err)","\t}","","\tfinalState := kubeinteraction.StateCompleted","\tnewPr, err := r.postFinalStatus(ctx, logger, pacInfo, provider, event, pr)","\tif err != nil {","\t\tlogger.Errorf(\"failed to post final status, moving on: %v\", err)","\t\tfinalState = kubeinteraction.StateFailed","\t}","","\t// Perform LLM analysis only for failed pipeline runs (best-effort, non-blocking)","\t// Users can use CEL expressions in role configurations for more fine-grained control","\tif len(newPr.Status.Conditions) \u003e 0 \u0026\u0026 newPr.Status.Conditions[0].Status == corev1.ConditionFalse {","\t\tif err := r.performLLMAnalysis(ctx, logger, repo, newPr, event, provider); err != nil {","\t\t\tlogger.Warnf(\"LLM analysis failed (non-blocking): %v\", err)","\t\t\tr.eventEmitter.EmitMessage(repo, zap.WarnLevel, \"LLMAnalysisFailed\",","\t\t\t\tfmt.Sprintf(\"AI/LLM analysis failed for repository %s/%s and pipeline run %s: %v\", repo.Namespace, repo.Name, newPr.Name, err))","\t\t}","\t}","","\tif err := r.updateRepoRunStatus(ctx, logger, newPr, repo, event); err != nil {","\t\treturn repo, fmt.Errorf(\"cannot update run status: %w\", err)","\t}","","\tif _, err := r.updatePipelineRunState(ctx, logger, pr, finalState); err != nil {","\t\treturn repo, fmt.Errorf(\"cannot update state: %w\", err)","\t}","","\tif err := r.emitMetrics(pr); err != nil {","\t\tlogger.Error(\"failed to emit metrics: \", err)","\t}","","\t// remove pipelineRun from Queue and start the next one","\tfor {","\t\tnext := r.qm.RemoveAndTakeItemFromQueue(repo, pr)","\t\tif next == \"\" {","\t\t\tbreak","\t\t}","\t\tkey := strings.Split(next, \"/\")","\t\tpr, err := r.run.Clients.Tekton.TektonV1().PipelineRuns(key[0]).Get(ctx, key[1], metav1.GetOptions{})","\t\tif err != nil {","\t\t\tlogger.Errorf(\"cannot get pipeline for next in queue: %w\", err)","\t\t\tcontinue","\t\t}","","\t\tif err := r.updatePipelineRunToInProgress(ctx, logger, repo, pr); err != nil {","\t\t\tlogger.Errorf(\"failed to update status: %w\", err)","\t\t\t_ = r.qm.RemoveFromQueue(queuepkg.RepoKey(repo), queuepkg.PrKey(pr))","\t\t\tcontinue","\t\t}","\t\tbreak","\t}","","\tif err := r.cleanupPipelineRuns(ctx, logger, pacInfo, repo, pr); err != nil {","\t\treturn repo, fmt.Errorf(\"error cleaning pipelineruns: %w\", err)","\t}","","\treturn repo, nil","}","","func (r *Reconciler) updatePipelineRunToInProgress(ctx context.Context, logger *zap.SugaredLogger, repo *v1alpha1.Repository, pr *tektonv1.PipelineRun) error {","\tpr, err := r.updatePipelineRunState(ctx, logger, pr, kubeinteraction.StateStarted)","\tif err != nil {","\t\treturn fmt.Errorf(\"cannot update state: %w\", err)","\t}","","\tdetectedProvider, event, err := r.initGitProviderClient(ctx, logger, repo, pr)","\tif err != nil {","\t\treturn fmt.Errorf(\"cannot initialize git provider client: %w\", err)","\t}","","\tconsoleURL := r.run.Clients.ConsoleUI().DetailURL(pr)","","\tmt := formatting.MessageTemplate{","\t\tPipelineRunName: pr.GetName(),","\t\tNamespace:       repo.GetNamespace(),","\t\tConsoleName:     r.run.Clients.ConsoleUI().GetName(),","\t\tConsoleURL:      consoleURL,","\t\tTknBinary:       settings.TknBinaryName,","\t\tTknBinaryURL:    settings.TknBinaryURL,","\t}","\tmsg, err := mt.MakeTemplate(detectedProvider.GetTemplate(provider.StartingPipelineType))","\tif err != nil {","\t\treturn fmt.Errorf(\"cannot create message template: %w\", err)","\t}","\tstatus := provider.StatusOpts{","\t\tStatus:                  \"in_progress\",","\t\tConclusion:              \"pending\",","\t\tText:                    msg,","\t\tDetailsURL:              consoleURL,","\t\tPipelineRunName:         pr.GetName(),","\t\tPipelineRun:             pr,","\t\tOriginalPipelineRunName: pr.GetAnnotations()[keys.OriginalPRName],","\t}","","\tif err := createStatusWithRetry(ctx, logger, detectedProvider, event, status); err != nil {","\t\t// if failed to report status for running state, let the pipelineRun continue,","\t\t// pipelineRun is already started so we will try again once it completes","\t\tlogger.Errorf(\"failed to report status to running on provider continuing! error: %v\", err)","\t\treturn nil","\t}","","\tlogger.Info(\"updated in_progress status on provider platform for pipelineRun \", pr.GetName())","\treturn nil","}","","func (r *Reconciler) initGitProviderClient(ctx context.Context, logger *zap.SugaredLogger, repo *v1alpha1.Repository, pr *tektonv1.PipelineRun) (provider.Interface, *info.Event, error) {","\tpacInfo := r.run.Info.GetPacOpts()","\tdetectedProvider, event, err := r.detectProvider(ctx, logger, pr)","\tif err != nil {","\t\treturn nil, nil, fmt.Errorf(\"cannot detect provider: %w\", err)","\t}","\tdetectedProvider.SetPacInfo(\u0026pacInfo)","","\t// installation ID indicates Github App installation","\tif event.InstallationID \u003e 0 {","\t\tevent.Provider.WebhookSecret, _ = pac.GetCurrentNSWebhookSecret(ctx, r.kinteract, r.run)","\t} else {","\t\t// secretNS is needed when git provider is other than Github App.","\t\tsecretNS := repo.GetNamespace()","\t\tif repo.Spec.GitProvider != nil \u0026\u0026 repo.Spec.GitProvider.Secret == nil \u0026\u0026 r.globalRepo != nil \u0026\u0026 r.globalRepo.Spec.GitProvider != nil \u0026\u0026 r.globalRepo.Spec.GitProvider.Secret != nil {","\t\t\tsecretNS = r.globalRepo.GetNamespace()","\t\t}","","\t\tsecretFromRepo := pac.SecretFromRepository{","\t\t\tK8int:       r.kinteract,","\t\t\tConfig:      detectedProvider.GetConfig(),","\t\t\tEvent:       event,","\t\t\tRepo:        repo,","\t\t\tWebhookType: pacInfo.WebhookType,","\t\t\tLogger:      logger,","\t\t\tNamespace:   secretNS,","\t\t}","\t\tif err := secretFromRepo.Get(ctx); err != nil {","\t\t\treturn nil, nil, fmt.Errorf(\"cannot get secret from repository: %w\", err)","\t\t}","\t}","","\terr = detectedProvider.SetClient(ctx, r.run, event, repo, r.eventEmitter)","\tif err != nil {","\t\treturn nil, nil, fmt.Errorf(\"cannot set client: %w\", err)","\t}","","\treturn detectedProvider, event, nil","}","","func (r *Reconciler) updatePipelineRunState(ctx context.Context, logger *zap.SugaredLogger, pr *tektonv1.PipelineRun, state string) (*tektonv1.PipelineRun, error) {","\tcurrentState := pr.GetAnnotations()[keys.State]","\tlogger.Infof(\"updating pipelineRun %v/%v state from %s to %s\", pr.GetNamespace(), pr.GetName(), currentState, state)","\tannotations := map[string]string{","\t\tkeys.State: state,","\t}","\tif state == kubeinteraction.StateStarted {","\t\tannotations[keys.SCMReportingPLRStarted] = \"true\"","\t}","","\tmergePatch := map[string]any{","\t\t\"metadata\": map[string]any{","\t\t\t\"labels\": map[string]string{","\t\t\t\tkeys.State: state,","\t\t\t},","\t\t\t\"annotations\": annotations,","\t\t},","\t}","","\t// if state is started then remove pipelineRun pending status","\tif state == kubeinteraction.StateStarted {","\t\tmergePatch[\"spec\"] = map[string]any{","\t\t\t\"status\": \"\",","\t\t}","\t}","\tactionLog := state + \" state\"","\tpatchedPR, err := action.PatchPipelineRun(ctx, logger, actionLog, r.run.Clients.Tekton, pr, mergePatch)","\tif err != nil {","\t\treturn pr, fmt.Errorf(\"error patching the pipelinerun: %w\", err)","\t}","\treturn patchedPR, nil","}","","// performLLMAnalysis executes LLM analysis on the completed pipeline if configured.","func (r *Reconciler) performLLMAnalysis(","\tctx context.Context,","\tlogger *zap.SugaredLogger,","\trepo *v1alpha1.Repository,","\tpr *tektonv1.PipelineRun,","\tevent *info.Event,","\tprovider provider.Interface,",") error {","\torchestrator := llm.NewOrchestrator(r.run, r.kinteract, logger)","\treturn orchestrator.ExecuteAnalysis(ctx, repo, pr, event, provider)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,1,1,0,2,1,1,1,0,0,2,2,1,1,0,2,2,2,2,0,0,0,0,0,2,2,2,2,2,2,2,2,2,1,1,2,0,2,2,2,2,1,1,1,0,0,0,0,2,2,2,0,2,2,2,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,0,0,0,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,2,2,2,2,1,1,0,2,2,1,1,1,1,0,0,2,2,2,1,1,1,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,0,0,2,2,2,2,2,1,1,0,2,2,2,1,1,1,0,0,0,2,2,1,1,1,1,0,0,2,1,1,0,2,1,1,0,2,2,2,0,0,2,2,2,2,0,1,1,1,1,1,0,0,1,1,1,1,0,1,0,0,2,1,1,0,2,0,0,2,2,2,1,1,0,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,0,2,2,0,0,2,2,2,2,1,1,2,2,2,2,1,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,1,1,0,0,2,2,1,1,0,2,0,0,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,2,0,0,0,0,0,0,0,0,0,0,2,2,2,2]},{"id":142,"path":"pkg/reconciler/status.go","lines":["package reconciler","","import (","\t\"context\"","\t\"fmt\"","\t\"strings\"","\t\"time\"","","\t\"github.com/google/go-github/v81/github\"","\tapipac \"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\tpacv1a1 \"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/formatting\"","\tkstatus \"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction/status\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/settings\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/pipelineascode\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/secrets\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/sort\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\t\"go.uber.org/zap\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"knative.dev/pkg/apis\"",")","","const (","\tmaxPipelineRunStatusRun = 5",")","","var backoffSchedule = []time.Duration{","\t1 * time.Second,","\t3 * time.Second,","\t5 * time.Second,","}","","func (r *Reconciler) updateRepoRunStatus(ctx context.Context, logger *zap.SugaredLogger, pr *tektonv1.PipelineRun, repo *pacv1a1.Repository, event *info.Event) error {","\trefsanitized := formatting.SanitizeBranch(event.BaseBranch)","\trepoStatus := pacv1a1.RepositoryRunStatus{","\t\tStatus:          pr.Status.Status,","\t\tPipelineRunName: pr.Name,","\t\tStartTime:       pr.Status.StartTime,","\t\tCompletionTime:  pr.Status.CompletionTime,","\t\tSHA:             \u0026event.SHA,","\t\tSHAURL:          \u0026event.SHAURL,","\t\tTitle:           \u0026event.SHATitle,","\t\tLogURL:          github.Ptr(r.run.Clients.ConsoleUI().DetailURL(pr)),","\t\tEventType:       \u0026event.EventType,","\t\tTargetBranch:    \u0026refsanitized,","\t}","","\t// Get repository again in case it was updated while we were running the CI","\t// we try multiple time until we get right in case of conflicts.","\t// that's what the error message tell us anyway, so i guess we listen.","\tmaxRun := 10","\tfor i := 0; i \u003c maxRun; i++ {","\t\tlastrepo, err := r.run.Clients.PipelineAsCode.PipelinesascodeV1alpha1().Repositories(","\t\t\tpr.GetNamespace()).Get(ctx, repo.Name, metav1.GetOptions{})","\t\tif err != nil {","\t\t\treturn err","\t\t}","","\t\t// Append PipelineRun status files to the repo status","\t\tif len(lastrepo.Status) \u003e= maxPipelineRunStatusRun {","\t\t\tcopy(lastrepo.Status, lastrepo.Status[len(lastrepo.Status)-maxPipelineRunStatusRun+1:])","\t\t\tlastrepo.Status = lastrepo.Status[:maxPipelineRunStatusRun-1]","\t\t}","","\t\tlastrepo.Status = append(lastrepo.Status, repoStatus)","\t\tnrepo, err := r.run.Clients.PipelineAsCode.PipelinesascodeV1alpha1().Repositories(lastrepo.Namespace).Update(","\t\t\tctx, lastrepo, metav1.UpdateOptions{})","\t\tif err != nil {","\t\t\tlogger.Infof(\"Could not update repo %s, retrying %d/%d: %s\", lastrepo.Namespace, i, maxRun, err.Error())","\t\t\tcontinue","\t\t}","\t\tlogger.Infof(\"Repository status of %s has been updated\", nrepo.Name)","\t\tlogger.Warn(\"The `pipelinerun_status` field in the Repository CR is scheduled for deprecation and will be removed in a future release. Please avoid relying on it.\")","\t\treturn nil","\t}","","\treturn fmt.Errorf(\"cannot update %s\", repo.Name)","}","","func (r *Reconciler) getFailureSnippet(ctx context.Context, pr *tektonv1.PipelineRun) string {","\tlines := int64(settings.DefaultSettings().ErrorLogSnippetNumberOfLines)","\tif r.run.Info.Pac != nil {","\t\tlines = int64(r.run.Info.Pac.ErrorLogSnippetNumberOfLines)","\t}","\ttaskinfos := kstatus.CollectFailedTasksLogSnippet(ctx, r.run, r.kinteract, pr, lines)","\tif len(taskinfos) == 0 {","\t\treturn \"\"","\t}","\tsortedTaskInfos := sort.TaskInfos(taskinfos)","\ttext := strings.TrimSpace(sortedTaskInfos[0].LogSnippet)","\tif text == \"\" {","\t\ttext = sortedTaskInfos[0].Message","\t}","\tname := sortedTaskInfos[0].Name","\tif sortedTaskInfos[0].DisplayName != \"\" {","\t\tname = strings.ToLower(sortedTaskInfos[0].DisplayName)","\t}","\treturn fmt.Sprintf(\"task \u003cb\u003e%s\u003c/b\u003e has the status \u003cb\u003e\\\"%s\\\"\u003c/b\u003e:\\n\u003cpre\u003e%s\u003c/pre\u003e\", name, sortedTaskInfos[0].Reason, text)","}","","func (r *Reconciler) postFinalStatus(ctx context.Context, logger *zap.SugaredLogger, pacInfo *info.PacOpts, vcx provider.Interface, event *info.Event, createdPR *tektonv1.PipelineRun) (*tektonv1.PipelineRun, error) {","\tpr, err := r.run.Clients.Tekton.TektonV1().PipelineRuns(createdPR.GetNamespace()).Get(","\t\tctx, createdPR.GetName(), metav1.GetOptions{},","\t)","\tif err != nil {","\t\treturn pr, err","\t}","","\ttrStatus := kstatus.GetStatusFromTaskStatusOrFromAsking(ctx, pr, r.run)","\tvar taskStatusText string","\tif len(trStatus) \u003e 0 {","\t\tvar err error","\t\ttaskStatusText, err = sort.TaskStatusTmpl(pr, trStatus, r.run, vcx.GetConfig())","\t\tif err != nil {","\t\t\treturn pr, err","\t\t}","\t} else {","\t\ttaskStatusText = pr.Status.GetCondition(apis.ConditionSucceeded).Message","\t}","","\tnamespaceURL := r.run.Clients.ConsoleUI().NamespaceURL(pr)","\tconsoleURL := r.run.Clients.ConsoleUI().DetailURL(pr)","\tmt := formatting.MessageTemplate{","\t\tPipelineRunName: pr.GetName(),","\t\tNamespace:       pr.GetNamespace(),","\t\tNamespaceURL:    namespaceURL,","\t\tConsoleName:     r.run.Clients.ConsoleUI().GetName(),","\t\tConsoleURL:      consoleURL,","\t\tTknBinary:       settings.TknBinaryName,","\t\tTknBinaryURL:    settings.TknBinaryURL,","\t\tTaskStatus:      taskStatusText,","\t}","\tif pacInfo.ErrorLogSnippet {","\t\tfailures := r.getFailureSnippet(ctx, pr)","\t\tif failures != \"\" {","\t\t\tsecretValues := secrets.GetSecretsAttachedToPipelineRun(ctx, r.kinteract, pr)","\t\t\tfailures = secrets.ReplaceSecretsInText(failures, secretValues)","\t\t\tmt.FailureSnippet = failures","\t\t}","\t}","\tvar tmplStatusText string","\tif tmplStatusText, err = mt.MakeTemplate(vcx.GetTemplate(provider.PipelineRunStatusType)); err != nil {","\t\treturn nil, fmt.Errorf(\"cannot create message template: %w\", err)","\t}","","\tstatus := provider.StatusOpts{","\t\tStatus:                  pipelineascode.CompletedStatus,","\t\tPipelineRun:             pr,","\t\tConclusion:              formatting.PipelineRunStatus(pr),","\t\tText:                    tmplStatusText,","\t\tPipelineRunName:         pr.Name,","\t\tDetailsURL:              r.run.Clients.ConsoleUI().DetailURL(pr),","\t\tOriginalPipelineRunName: pr.GetAnnotations()[apipac.OriginalPRName],","\t}","","\terr = createStatusWithRetry(ctx, logger, vcx, event, status)","\tlogger.Infof(\"pipelinerun %s has a status of '%s'\", pr.Name, status.Conclusion)","\treturn pr, err","}","","func createStatusWithRetry(ctx context.Context, logger *zap.SugaredLogger, vcx provider.Interface, event *info.Event, status provider.StatusOpts) error {","\tvar finalError error","\tfor _, backoff := range backoffSchedule {","\t\terr := vcx.CreateStatus(ctx, event, status)","\t\tif err == nil {","\t\t\treturn nil","\t\t}","\t\tlogger.Infof(\"failed to create status, error: %v, retrying in %v\", err, backoff)","\t\ttime.Sleep(backoff)","\t\tfinalError = err","\t}","\treturn fmt.Errorf(\"failed to report status: %w\", finalError)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,0,2,1,1,1,0,2,2,2,2,1,1,0,2,2,2,0,0,1,0,0,2,2,2,1,1,2,2,2,2,2,2,2,1,1,2,2,1,1,2,0,0,2,2,2,2,2,1,1,0,2,2,2,2,2,2,1,1,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,0,2,0]},{"id":143,"path":"pkg/resolve/logging.go","lines":["package resolve","","import \"go.uber.org/zap\"","","func debugf(log *zap.SugaredLogger, format string, args ...any) {","\tif log == nil {","\t\treturn","\t}","\tlog.Debugf(format, args...)","}","","func logInfo(log *zap.SugaredLogger, msg string) {","\tif log == nil {","\t\treturn","\t}","\tlog.Info(msg)","}"],"coverage":[0,0,0,0,2,2,2,2,2,0,0,2,2,1,1,2,0]},{"id":144,"path":"pkg/resolve/remote.go","lines":["package resolve","","import (","\t\"context\"","\t\"fmt\"","\t\"net/url\"","\t\"path\"","\t\"strings\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/matcher\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"",")","","type NamedItem interface {","\tGetName() string","}","","func alreadyFetchedResource[T NamedItem](resources map[string]T, resourceName string) bool {","\tif _, ok := resources[resourceName]; ok {","\t\treturn true","\t}","\treturn false","}","","// Tries to assemble task FQDNs based on the base URL","// of a remote pipeline.","//","// If there isn't a remote pipeline reference for the current","// run, tasks are returned as they are. Any task with an already","// valid URL is skipped.","func assembleTaskFQDNs(pipelineURL string, tasks []string) ([]string, error) {","\tif pipelineURL == \"\" {","\t\treturn tasks, nil // no pipeline URL, return tasks as is","\t}","","\t// Only HTTP(S) URLs can serve as base for relative task resolution.","\t// Hub catalog references (e.g., \"catalog://resource:version\") use a","\t// different scheme where relative paths are meaningless.","\tlowered := strings.ToLower(pipelineURL)","\tif !strings.HasPrefix(lowered, \"http://\") \u0026\u0026 !strings.HasPrefix(lowered, \"https://\") {","\t\treturn tasks, nil","\t}","","\tpURL, err := url.Parse(pipelineURL)","\tif err != nil {","\t\treturn tasks, err","\t}","\t// pop the pipeline file path from the URL","\tpURL.Path = path.Dir(pURL.Path)","","\ttaskURLS := make([]string, len(tasks))","\tfor i, t := range tasks {","\t\ttURL, err := url.Parse(t)","\t\tif err == nil \u0026\u0026 tURL.Scheme != \"\" \u0026\u0026 tURL.Host != \"\" {","\t\t\ttaskURLS[i] = t","\t\t\tcontinue // it's already an absolute URL","\t\t}","\t\ttURL = pURL","\t\ttURL = tURL.JoinPath(t)","\t\ttaskURLS[i] = tURL.String()","\t}","\treturn taskURLS, nil","}","","// resolveRemoteResources will get remote tasks or Pipelines from annotations.","//","// It already has some tasks or pipeline coming from the tekton directory stored in [types]","//","// The precedence logic for tasks is in this order:","//","// * Tasks from the PipelineRun annotations","// * Tasks from the Pipeline annotations","// * Tasks from the Tekton directory","//","// The precedence logic for Pipeline is first from PipelineRun annotations and","// then from Tekton directory.","func resolveRemoteResources(ctx context.Context, rt *matcher.RemoteTasks, types TektonTypes, ropt *Opts) ([]*tektonv1.PipelineRun, error) {","\t// contain Resources fetched for the event","\tfetchedResourcesForEvent := FetchedResources{","\t\tTasks:     map[string]*tektonv1.Task{},","\t\tPipelines: map[string]*tektonv1.Pipeline{},","\t}","\tpipelineRuns := []*tektonv1.PipelineRun{}","\trt.Logger.Debugf(\"resolveRemoteResources: pipelineruns=%d pipelines=%d tasks=%d remote_tasks=%t\", len(types.PipelineRuns), len(types.Pipelines), len(types.Tasks), ropt.RemoteTasks)","\tfor _, pipelinerun := range types.PipelineRuns {","\t\tprName := pipelinerun.GetName()","\t\tif prName == \"\" {","\t\t\tprName = pipelinerun.GetGenerateName()","\t\t}","\t\trt.Logger.Debugf(\"resolveRemoteResources: processing pipelinerun=%s\", prName)","\t\t// contain Resources specific to run","\t\tfetchedResourcesForPipelineRun := FetchedResourcesForRun{","\t\t\tTasks:       map[string]*tektonv1.Task{},","\t\t\tPipelineURL: \"\",","\t\t}","\t\tvar pipeline *tektonv1.Pipeline","\t\tvar err error","\t\tif ropt.RemoteTasks {","\t\t\t// no annotations on run, then skip","\t\t\tif pipelinerun.GetObjectMeta().GetAnnotations() == nil {","\t\t\t\trt.Logger.Debugf(\"resolveRemoteResources: pipelinerun=%s has no annotations, skipping remote resolution\", prName)","\t\t\t\tcontinue","\t\t\t}","","\t\t\tif len(pipelinerun.GetObjectMeta().GetAnnotations()) == 0 {","\t\t\t\trt.Logger.Debugf(\"resolveRemoteResources: pipelinerun=%s has empty annotations, skipping remote resolution\", prName)","\t\t\t\tcontinue","\t\t\t}","","\t\t\t// get first all the pipeline from the pipelinerun annotations","\t\t\tremotePipeline, err := matcher.GrabPipelineFromAnnotations(pipelinerun.GetObjectMeta().GetAnnotations())","\t\t\tif err != nil {","\t\t\t\treturn []*tektonv1.PipelineRun{}, fmt.Errorf(\"error getting remote pipeline from pipelinerun annotations: %w\", err)","\t\t\t}","","\t\t\t// if we got the pipeline name from annotation, we need to fetch the pipeline","\t\t\tif remotePipeline != \"\" {","\t\t\t\trt.Logger.Debugf(\"resolveRemoteResources: pipelinerun=%s remote pipeline=%s\", prName, remotePipeline)","\t\t\t\t// making sure that the pipeline with same annotation name is not fetched","\t\t\t\tif alreadyFetchedResource(fetchedResourcesForEvent.Pipelines, remotePipeline) {","\t\t\t\t\trt.Logger.Debugf(\"skipping already fetched pipeline %s in annotations on pipelinerun %s\", remotePipeline, pipelinerun.GetName())","\t\t\t\t\t// already fetched, then just get the pipeline to add to run specific Resources","\t\t\t\t\tpipeline = fetchedResourcesForEvent.Pipelines[remotePipeline]","\t\t\t\t} else {","\t\t\t\t\t// seems like a new pipeline, fetch it based on name in annotation","\t\t\t\t\tpipeline, err = rt.GetPipelineFromAnnotationName(ctx, remotePipeline)","\t\t\t\t\tif err != nil {","\t\t\t\t\t\treturn []*tektonv1.PipelineRun{}, fmt.Errorf(\"error getting remote pipeline from pipelinerun annotations: %w\", err)","\t\t\t\t\t}","\t\t\t\t\t// add the pipeline to the Resources fetched for the Event","\t\t\t\t\tfetchedResourcesForEvent.Pipelines[remotePipeline] = pipeline","\t\t\t\t\t// add the pipeline URL to the run specific Resources","\t\t\t\t\tfetchedResourcesForPipelineRun.PipelineURL = remotePipeline","\t\t\t\t}","\t\t\t}","\t\t}","\t\tpipelineTasks := []string{}","\t\t// if run is referring to the pipelineRef and pipeline fetched from annotation have name equal to the pipelineRef","\t\tif pipelinerun.Spec.PipelineRef != nil \u0026\u0026 pipelinerun.Spec.PipelineRef.Resolver == \"\" {","\t\t\tif pipeline == nil || pipeline.Name != pipelinerun.Spec.PipelineRef.Name {","\t\t\t\t// if pipeline fetched from annotation is not having same name as PipelineRef, then we need to get a local pipeline if exist by same name","\t\t\t\tpipeline, err = getPipelineByName(pipelinerun.Spec.PipelineRef.Name, types.Pipelines)","\t\t\t\tif err != nil {","\t\t\t\t\treturn []*tektonv1.PipelineRun{}, err","\t\t\t\t}","\t\t\t}","\t\t\t// add the pipeline to the run specific Resources","\t\t\tfetchedResourcesForPipelineRun.Pipeline = pipeline","\t\t\trt.Logger.Debugf(\"resolveRemoteResources: pipelinerun=%s using pipeline=%s\", prName, pipeline.GetName())","\t\t\t// grab the tasks, that we may need to fetch for this pipeline from its annotations","\t\t\tif pipeline.GetObjectMeta().GetAnnotations() != nil {","\t\t\t\t// get all the tasks from the pipeline annotations","\t\t\t\tpipelineTasks, err = matcher.GrabTasksFromAnnotations(pipeline.GetObjectMeta().GetAnnotations())","\t\t\t\tif err != nil {","\t\t\t\t\treturn []*tektonv1.PipelineRun{}, fmt.Errorf(\"error getting remote task from pipeline annotations: %w\", err)","\t\t\t\t}","\t\t\t\trt.Logger.Debugf(\"resolveRemoteResources: pipeline=%s annotation tasks=%d\", pipeline.GetName(), len(pipelineTasks))","\t\t\t\t// check for relative task references and assemble FQDNs","\t\t\t\tpipelineTasks, err = assembleTaskFQDNs(fetchedResourcesForPipelineRun.PipelineURL, pipelineTasks)","\t\t\t\tif err != nil {","\t\t\t\t\treturn []*tektonv1.PipelineRun{}, err","\t\t\t\t}","\t\t\t}","\t\t}","","\t\t// now start fetching the tasks","\t\tif ropt.RemoteTasks {","\t\t\t// first get all the tasks from the pipelinerun annotations","\t\t\tremoteTasks, err := matcher.GrabTasksFromAnnotations(pipelinerun.GetObjectMeta().GetAnnotations())","\t\t\tif err != nil {","\t\t\t\treturn []*tektonv1.PipelineRun{}, fmt.Errorf(\"error getting remote task from pipelinerun annotations: %w\", err)","\t\t\t}","\t\t\trt.Logger.Debugf(\"resolveRemoteResources: pipelinerun=%s annotation tasks=%d\", prName, len(remoteTasks))","","\t\t\t// now fetch all the tasks from pipelinerun and pipeline annotations, giving preference to pipelinerun annotation tasks","\t\t\tfor _, remoteTask := range append(remoteTasks, pipelineTasks...) {","\t\t\t\tvar task *tektonv1.Task","\t\t\t\t// if task is already fetched in the event, then just copy the task","\t\t\t\tif alreadyFetchedResource(fetchedResourcesForEvent.Tasks, remoteTask) {","\t\t\t\t\trt.Logger.Debugf(\"skipping already fetched task %s in annotations on pipelinerun %s\", remoteTask, pipelinerun.GetName())","\t\t\t\t\ttask = fetchedResourcesForEvent.Tasks[remoteTask]","\t\t\t\t} else {","\t\t\t\t\t// get the task from annotation name","\t\t\t\t\ttask, err = rt.GetTaskFromAnnotationName(ctx, remoteTask)","\t\t\t\t\tif err != nil {","\t\t\t\t\t\treturn []*tektonv1.PipelineRun{}, fmt.Errorf(\"error getting remote task from pipelinerun annotations: %w\", err)","\t\t\t\t\t}","\t\t\t\t\t// add the newly fetched tasks to fetchedResourcesForEvent with key annotation value","\t\t\t\t\tfetchedResourcesForEvent.Tasks[remoteTask] = task","\t\t\t\t}","\t\t\t\t// now checking if run specific resources already contain a task with same name, then don't add it","\t\t\t\t// this is to give preference to the pipelinerun annotation then pipeline annotation","\t\t\t\tif !alreadyFetchedResource(fetchedResourcesForPipelineRun.Tasks, task.GetName()) {","\t\t\t\t\trt.Logger.Infof(\"skipping remote task %s as already fetched task %s for pipelinerun %s\", remoteTask, task.GetName(), pipelinerun.GetName())","\t\t\t\t\tfetchedResourcesForPipelineRun.Tasks[task.GetName()] = task","\t\t\t\t}","\t\t\t}","\t\t}","","\t\t// now add all the tasks in .tekton directory to Tasks, as we add them by default if not found in annotation","\t\t// we will skip the ones which exist in run specific resources with same name","\t\tfor _, task := range types.Tasks {","\t\t\tif alreadyFetchedResource(fetchedResourcesForPipelineRun.Tasks, task.GetName()) {","\t\t\t\trt.Logger.Infof(\"overriding task %s coming from .tekton directory by an annotation task for pipelinerun %s\", task.GetName(), pipelinerun.GetName())","\t\t\t\tcontinue","\t\t\t}","\t\t\tfetchedResourcesForPipelineRun.Tasks[task.GetName()] = task","\t\t}","\t\trt.Logger.Debugf(\"resolveRemoteResources: pipelinerun=%s final task count=%d\", prName, len(fetchedResourcesForPipelineRun.Tasks))","","\t\t// if PipelineRef is used then, first resolve pipeline and replace all taskRef{Finally/Task} of Pipeline, then put inlinePipeline in PipelineRun","\t\tif pipelinerun.Spec.PipelineRef != nil \u0026\u0026 pipelinerun.Spec.PipelineRef.Resolver == \"\" {","\t\t\tpipelineResolved := fetchedResourcesForPipelineRun.Pipeline","\t\t\tturns, err := inlineTasks(pipelineResolved.Spec.Tasks, ropt, fetchedResourcesForPipelineRun)","\t\t\tif err != nil {","\t\t\t\treturn nil, err","\t\t\t}","\t\t\tpipelineResolved.Spec.Tasks = turns","","\t\t\tfruns, err := inlineTasks(pipelineResolved.Spec.Finally, ropt, fetchedResourcesForPipelineRun)","\t\t\tif err != nil {","\t\t\t\treturn nil, err","\t\t\t}","\t\t\tpipelineResolved.Spec.Finally = fruns","","\t\t\tpipelinerun.Spec.PipelineRef = nil","\t\t\tpipelinerun.Spec.PipelineSpec = \u0026pipelineResolved.Spec","\t\t\trt.Logger.Debugf(\"resolveRemoteResources: pipelinerun=%s inlined pipeline tasks=%d finally=%d\", prName, len(pipelineResolved.Spec.Tasks), len(pipelineResolved.Spec.Finally))","\t\t}","","\t\t// if PipelineSpec is used then, now resolve the PipelineRun by replacing all taskRef{Finally/Task}","\t\tif pipelinerun.Spec.PipelineSpec != nil {","\t\t\tturns, err := inlineTasks(pipelinerun.Spec.PipelineSpec.Tasks, ropt, fetchedResourcesForPipelineRun)","\t\t\tif err != nil {","\t\t\t\treturn nil, err","\t\t\t}","\t\t\tpipelinerun.Spec.PipelineSpec.Tasks = turns","","\t\t\tfruns, err := inlineTasks(pipelinerun.Spec.PipelineSpec.Finally, ropt, fetchedResourcesForPipelineRun)","\t\t\tif err != nil {","\t\t\t\treturn nil, err","\t\t\t}","\t\t\tpipelinerun.Spec.PipelineSpec.Finally = fruns","\t\t\trt.Logger.Debugf(\"resolveRemoteResources: pipelinerun=%s inlined pipelineSpec tasks=%d finally=%d\", prName, len(pipelinerun.Spec.PipelineSpec.Tasks), len(pipelinerun.Spec.PipelineSpec.Finally))","\t\t}","","\t\t// Add a GenerateName based on the pipeline name and a \"-\"","\t\t// if we already have a GenerateName then just keep it like this","\t\tif ropt.GenerateName \u0026\u0026 pipelinerun.GenerateName == \"\" {","\t\t\tpipelinerun.GenerateName = pipelinerun.Name + \"-\"","\t\t\tpipelinerun.Name = \"\"","\t\t\trt.Logger.Debugf(\"resolveRemoteResources: pipelinerun=%s set generateName=%s\", prName, pipelinerun.GenerateName)","\t\t}","\t\tpipelineRuns = append(pipelineRuns, pipelinerun)","\t}","\t// return all resolved PipelineRuns","\trt.Logger.Debugf(\"resolveRemoteResources: resolved pipelineruns=%d\", len(pipelineRuns))","\treturn pipelineRuns, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,0,0,0,0,0,0,0,2,2,2,2,0,0,0,0,2,2,2,2,0,2,2,1,1,0,2,2,2,2,2,2,2,2,0,2,2,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,0,2,2,2,0,0,0,2,2,1,1,0,0,2,2,2,2,1,1,1,2,2,2,2,2,2,0,2,2,2,0,0,0,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,1,1,2,2,2,2,1,1,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,0,0,0,2,2,2,2,0,0,0,0,0,2,2,2,2,0,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,2,2,0,0,0,2,2,2,1,1,2,2,2,2,1,1,2,2,0,0,0,0,2,2,2,2,2,2,0,0,2,2,0]},{"id":145,"path":"pkg/resolve/resolve.go","lines":["package resolve","","import (","\t\"context\"","\t\"fmt\"","\t\"regexp\"","\t\"slices\"","\t\"strings\"","","\tapipac \"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\tpacerrors \"github.com/openshift-pipelines/pipelines-as-code/pkg/errors\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/formatting\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/matcher\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"","\ttektonv1beta1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1beta1\"","\t\"go.uber.org/zap\"","\tk8scheme \"k8s.io/client-go/kubernetes/scheme\"","\tyaml \"sigs.k8s.io/yaml/goyaml.v2\"",")","","// Contains Resources Fetched from tektondir.","type TektonTypes struct {","\tPipelineRuns     []*tektonv1.PipelineRun","\tPipelines        []*tektonv1.Pipeline","\tTaskRuns         []*tektonv1.TaskRun","\tTasks            []*tektonv1.Task","\tValidationErrors []*pacerrors.PacYamlValidations","}","","// Contains Fetched Resources for Event, with key equals to annotation value.","type FetchedResources struct {","\tTasks     map[string]*tektonv1.Task","\tPipelines map[string]*tektonv1.Pipeline","}","","// Contains Fetched Resources for Run, with key equals to resource name from metadata.name field.","type FetchedResourcesForRun struct {","\tTasks       map[string]*tektonv1.Task","\tPipeline    *tektonv1.Pipeline","\tPipelineURL string","}","","func NewTektonTypes() TektonTypes {","\treturn TektonTypes{","\t\tValidationErrors: []*pacerrors.PacYamlValidations{},","\t}","}","","var yamlDocSeparatorRe = regexp.MustCompile(`(?m)^---\\s*$`)","","// detectAtleastNameOrGenerateNameAndSchemaFromPipelineRun detects the name or","// generateName of a yaml files even if there is an error decoding it as tekton types.","func detectAtleastNameOrGenerateNameAndSchemaFromPipelineRun(data string) (string, string) {","\tvar genericKubeObj struct {","\t\tAPIVersion string `yaml:\"apiVersion\"`","\t\tMetadata   struct {","\t\t\tName         string `yaml:\"name,omitempty\"`","\t\t\tGenerateName string `yaml:\"generateName,omitempty\"`","\t\t} `yaml:\"metadata\"`","\t}","\terr := yaml.Unmarshal([]byte(data), \u0026genericKubeObj)","\tif err != nil {","\t\treturn \"nokube\", \"\"","\t}","\tif genericKubeObj.Metadata.Name != \"\" {","\t\treturn genericKubeObj.Metadata.Name, genericKubeObj.APIVersion","\t}","","\tif genericKubeObj.Metadata.GenerateName != \"\" {","\t\treturn genericKubeObj.Metadata.GenerateName, genericKubeObj.APIVersion","\t}","\treturn \"unknown\", genericKubeObj.APIVersion","}","","// getPipelineByName returns the Pipeline with the given name the first one found","// will be matched. It does not handle conflicts so user has fetched multiple","// pipeline with the same name it will just pick up the first one.","// if the pipeline is not found it returns an error.","func getPipelineByName(name string, tasks []*tektonv1.Pipeline) (*tektonv1.Pipeline, error) {","\tfor _, value := range tasks {","\t\tif value.Name == name {","\t\t\treturn value, nil","\t\t}","\t}","\treturn \u0026tektonv1.Pipeline{}, fmt.Errorf(\"cannot find referenced pipeline %s. for a remote pipeline make sure to add it in the annotation\", name)","}","","func pipelineRunsWithSameName(prs []*tektonv1.PipelineRun) error {","\tprNames := map[string]bool{}","\tfor _, pr := range prs {","\t\tname := pr.GetName()","\t\tgenerateName := pr.GetGenerateName()","","\t\tif name != \"\" {","\t\t\tif _, exist := prNames[name]; exist {","\t\t\t\treturn fmt.Errorf(\"found multiple pipelinerun in .tekton with the same name: %v, please update\", name)","\t\t\t}","\t\t\tprNames[name] = true","\t\t}","","\t\tif generateName != \"\" {","\t\t\tif _, exist := prNames[generateName]; exist {","\t\t\t\treturn fmt.Errorf(\"found multiple pipelinerun in .tekton with the same generateName: %v, please update\", generateName)","\t\t\t}","\t\t\tprNames[generateName] = true","\t\t}","\t}","\treturn nil","}","","func isTektonAPIVersion(apiVersion string) bool {","\treturn strings.HasPrefix(apiVersion, \"tekton.dev/\") || apiVersion == \"\"","}","","func inlineTasks(tasks []tektonv1.PipelineTask, ropt *Opts, remoteResource FetchedResourcesForRun) ([]tektonv1.PipelineTask, error) {","\tpipelineTasks := []tektonv1.PipelineTask{}","\tfor _, task := range tasks {","\t\tif task.TaskRef != nil \u0026\u0026","\t\t\ttask.TaskRef.Resolver == \"\" \u0026\u0026","\t\t\tisTektonAPIVersion(task.TaskRef.APIVersion) \u0026\u0026","\t\t\tstring(task.TaskRef.Kind) != \"ClusterTask\" \u0026\u0026","\t\t\t!slices.Contains(ropt.SkipInlining, task.TaskRef.Name) {","\t\t\ttaskResolved, ok := remoteResource.Tasks[task.TaskRef.Name]","\t\t\tif !ok {","\t\t\t\treturn nil, fmt.Errorf(\"cannot find referenced task %s. if it's a remote task make sure to add it in the annotations\", task.TaskRef.Name)","\t\t\t}","\t\t\ttmd := tektonv1.PipelineTaskMetadata{","\t\t\t\tAnnotations: taskResolved.GetObjectMeta().GetAnnotations(),","\t\t\t\tLabels:      taskResolved.GetObjectMeta().GetLabels(),","\t\t\t}","\t\t\ttask.TaskRef = nil","\t\t\ttask.TaskSpec = \u0026tektonv1.EmbeddedTask{","\t\t\t\tTaskSpec: taskResolved.Spec,","\t\t\t\tMetadata: tmd,","\t\t\t}","\t\t}","\t\tpipelineTasks = append(pipelineTasks, task)","\t}","\treturn pipelineTasks, nil","}","","type Opts struct {","\tGenerateName  bool     // whether to GenerateName","\tRemoteTasks   bool     // whether to parse annotation to fetch tasks from remote","\tSkipInlining  []string // task to skip inlining","\tProviderToken string","}","","func ReadTektonTypes(ctx context.Context, log *zap.SugaredLogger, data string) (TektonTypes, error) {","\ttypes := NewTektonTypes()","\tdecoder := k8scheme.Codecs.UniversalDeserializer()","\tdocs := yamlDocSeparatorRe.Split(data, -1)","\tdebugf(log, \"ReadTektonTypes: data length=%d docs=%d\", len(data), len(docs))","","\tfor _, doc := range docs {","\t\tif strings.TrimSpace(doc) == \"\" {","\t\t\tcontinue","\t\t}","","\t\tobj, _, err := decoder.Decode([]byte(doc), nil, nil)","\t\tif err != nil {","\t\t\tdt, dv := detectAtleastNameOrGenerateNameAndSchemaFromPipelineRun(doc)","\t\t\tdebugf(log, \"ReadTektonTypes: decode error for doc name=%s schema=%s err=%v\", dt, dv, err)","\t\t\ttypes.ValidationErrors = append(types.ValidationErrors, \u0026pacerrors.PacYamlValidations{","\t\t\t\tName:   dt,","\t\t\t\tErr:    fmt.Errorf(\"error decoding yaml document: %w\", err),","\t\t\t\tSchema: dv,","\t\t\t})","\t\t\tcontinue","\t\t}","\t\tswitch o := obj.(type) {","\t\tcase *tektonv1beta1.Pipeline: //nolint: staticcheck // we need to support v1beta1","\t\t\tc := \u0026tektonv1.Pipeline{}","\t\t\tif err := o.ConvertTo(ctx, c); err != nil {","\t\t\t\treturn types, fmt.Errorf(\"pipeline v1beta1 %s cannot be converted as v1: err: %w\", o.GetName(), err)","\t\t\t}","\t\t\ttypes.Pipelines = append(types.Pipelines, c)","\t\t\tdebugf(log, \"ReadTektonTypes: loaded pipeline v1beta1 name=%s\", c.GetName())","\t\tcase *tektonv1beta1.PipelineRun: //nolint: staticcheck // we need to support v1beta1","\t\t\tc := \u0026tektonv1.PipelineRun{}","\t\t\tif err := o.ConvertTo(ctx, c); err != nil {","\t\t\t\treturn types, fmt.Errorf(\"pipelinerun v1beta1 %s cannot be converted as v1: err: %w\", o.GetName(), err)","\t\t\t}","\t\t\ttypes.PipelineRuns = append(types.PipelineRuns, c)","\t\t\tdebugf(log, \"ReadTektonTypes: loaded pipelinerun v1beta1 name=%s\", c.GetName())","\t\tcase *tektonv1beta1.Task: //nolint: staticcheck // we need to support v1beta1","\t\t\tc := \u0026tektonv1.Task{}","\t\t\tif err := o.ConvertTo(ctx, c); err != nil {","\t\t\t\treturn types, fmt.Errorf(\"task v1beta1 %s cannot be converted as v1: err: %w\", o.GetName(), err)","\t\t\t}","\t\t\ttypes.Tasks = append(types.Tasks, c)","\t\t\tdebugf(log, \"ReadTektonTypes: loaded task v1beta1 name=%s\", c.GetName())","\t\tcase *tektonv1.PipelineRun:","\t\t\ttypes.PipelineRuns = append(types.PipelineRuns, o)","\t\t\tdebugf(log, \"ReadTektonTypes: loaded pipelinerun v1 name=%s\", o.GetName())","\t\tcase *tektonv1.Pipeline:","\t\t\ttypes.Pipelines = append(types.Pipelines, o)","\t\t\tdebugf(log, \"ReadTektonTypes: loaded pipeline v1 name=%s\", o.GetName())","\t\tcase *tektonv1.Task:","\t\t\ttypes.Tasks = append(types.Tasks, o)","\t\t\tdebugf(log, \"ReadTektonTypes: loaded task v1 name=%s\", o.GetName())","\t\tdefault:","\t\t\tlogInfo(log, \"skipping yaml document not looking like a tekton resource we can Resolve.\")","\t\t}","\t}","","\tdebugf(log, \"ReadTektonTypes: result pipelineruns=%d pipelines=%d tasks=%d validation_errors=%d\", len(types.PipelineRuns), len(types.Pipelines), len(types.Tasks), len(types.ValidationErrors))","\treturn types, nil","}","","// Resolve gets a large string which is a yaml multi documents containing","// Pipeline/PipelineRuns/Tasks and resolve them inline as a single PipelineRun","// generateName can be set as True to set the name as a generateName + \"-\" for","// unique pipelinerun.","func Resolve(ctx context.Context, cs *params.Run, logger *zap.SugaredLogger, providerintf provider.Interface, types TektonTypes, event *info.Event, ropt *Opts) ([]*tektonv1.PipelineRun, error) {","\tif len(types.PipelineRuns) == 0 {","\t\treturn []*tektonv1.PipelineRun{}, fmt.Errorf(\"could not find any PipelineRun in your .tekton/ directory\")","\t}","\tdebugf(logger, \"Resolve: pipelineruns=%d pipelines=%d tasks=%d remote_tasks=%t generate_name=%t\", len(types.PipelineRuns), len(types.Pipelines), len(types.Tasks), ropt.RemoteTasks, ropt.GenerateName)","","\tif _, err := MetadataResolve(types.PipelineRuns); err != nil {","\t\treturn []*tektonv1.PipelineRun{}, err","\t}","","\trt := \u0026matcher.RemoteTasks{","\t\tRun:               cs,","\t\tEvent:             event,","\t\tProviderInterface: providerintf,","\t\tLogger:            logger,","\t}","","\tfetchedResources, err := resolveRemoteResources(ctx, rt, types, ropt)","\tif err != nil {","\t\treturn []*tektonv1.PipelineRun{}, err","\t}","\tdebugf(logger, \"Resolve: resolved pipelineruns=%d\", len(fetchedResources))","\treturn fetchedResources, nil","}","","func MetadataResolve(prs []*tektonv1.PipelineRun) ([]*tektonv1.PipelineRun, error) {","\tif err := pipelineRunsWithSameName(prs); err != nil {","\t\treturn []*tektonv1.PipelineRun{}, err","\t}","","\tfor _, prun := range prs {","\t\toriginPipelineRunName := prun.GetName()","\t\tif originPipelineRunName == \"\" \u0026\u0026 prun.GenerateName != \"\" {","\t\t\toriginPipelineRunName = prun.GetGenerateName()","\t\t}","","\t\t// keep the originalPipelineRun in a label","\t\t// because we would need it later on when grouping by cleanups and we","\t\t// can attach that pr file from .tekton directory.","","\t\t// Don't overwrite the labels if there is some who already exist set by the user in repo","\t\tif prun.GetLabels() == nil {","\t\t\tprun.Labels = map[string]string{}","\t\t}","\t\t// Don't overwrite the annotation if there is some who already exist set by the user in repo","\t\tif prun.GetAnnotations() == nil {","\t\t\tprun.Annotations = map[string]string{}","\t\t}","\t\tprun.GetLabels()[apipac.OriginalPRName] = formatting.CleanValueKubernetes(originPipelineRunName)","\t\tprun.GetAnnotations()[apipac.OriginalPRName] = originPipelineRunName","\t}","\treturn prs, nil","}","","//nolint:gochecknoinits","func init() {","\t_ = tektonv1.AddToScheme(k8scheme.Scheme)","\t_ = tektonv1beta1.AddToScheme(k8scheme.Scheme)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,0,0,0,0,0,0,2,2,2,2,2,0,2,0,0,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,0,0,2,0,0,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,2,0,2,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,1,1,2,2,2,2,2,1,1,2,2,2,2,2,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,0,0,0,0,0,0,2,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,1,1,0,2,2,2,2,2,0,0,0,0,0,0,2,2,2,0,2,2,2,2,2,0,2,0,0,0,2,2,2,2]},{"id":146,"path":"pkg/secrets/basic_auth.go","lines":["package secrets","","import (","\t\"fmt\"","\t\"net/url\"","\t\"strings\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/formatting\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/provider\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/random\"","\tcorev1 \"k8s.io/api/core/v1\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"",")","","const (","\tbasicAuthGitConfigData = `","\t[credential \"%s\"]","\thelper=store","\t`","\t//nolint:gosec","\tbasicAuthSecretName = `pac-gitauth-%s`","\tranStringSeedLen    = 6",")","","// MakeBasicAuthSecret Make a secret for git-clone basic-auth workspace.","func MakeBasicAuthSecret(runevent *info.Event, secretName string) (*corev1.Secret, error) {","\t// Bitbucket Data Center have a different Clone URL than it's Repository URL, so we","\t// have to separate them","\tcloneURL := runevent.URL","\tif runevent.CloneURL != \"\" {","\t\tcloneURL = runevent.CloneURL","\t}","","\trepoURL, err := url.Parse(cloneURL)","\tif err != nil {","\t\treturn nil, fmt.Errorf(\"cannot parse url %s: %w\", cloneURL, err)","\t}","","\tgitUser := provider.DefaultProviderAPIUser","\tif runevent.Provider.User != \"\" {","\t\tgitUser = runevent.Provider.User","\t}","","\t// Bitbucket Data Center token have / into it, so unless we quote the URL them it's","\t// impossible to use itðŸ¤¡","\t//","\t// It supposed not working on GitHub according to","\t// https://stackoverflow.com/a/24719496 but arguably GitHub have a better","\t// product and would not do such things.","\t//","\t// maybe we could patch the git-clone task too but that probably be a pain","\t// in the *** to do it in shell.","\ttoken := url.QueryEscape(runevent.Provider.Token)","","\tbaseCloneURL := fmt.Sprintf(\"%s://%s\", repoURL.Scheme, repoURL.Host)","\turlWithToken := fmt.Sprintf(\"%s://%s:%s@%s%s\", repoURL.Scheme, gitUser, token, repoURL.Host, repoURL.Path)","\tsecretData := map[string]string{","\t\t\".gitconfig\":       fmt.Sprintf(basicAuthGitConfigData, baseCloneURL),","\t\t\".git-credentials\": urlWithToken,","\t\t// With the GitHub APP method the token is available for 8h if you have","\t\t// the user to server token expiration.  the token is scoped to the","\t\t// installation ID","\t\t\"git-provider-token\": token,","\t}","\tannotations := map[string]string{","\t\t\"pipelinesascode.tekton.dev/url\": cloneURL,","\t\tkeys.SHA:                         runevent.SHA,","\t\tkeys.URLOrg:                      runevent.Organization,","\t\tkeys.URLRepository:               runevent.Repository,","\t}","","\tlabels := map[string]string{","\t\t\"app.kubernetes.io/managed-by\": pipelinesascode.GroupName,","\t\tkeys.URLOrg:                    formatting.CleanValueKubernetes(runevent.Organization),","\t\tkeys.URLRepository:             formatting.CleanValueKubernetes(runevent.Repository),","\t}","","\treturn \u0026corev1.Secret{","\t\tTypeMeta: metav1.TypeMeta{","\t\t\tAPIVersion: corev1.SchemeGroupVersion.String(),","\t\t\tKind:       \"Secret\",","\t\t},","\t\tObjectMeta: metav1.ObjectMeta{","\t\t\tName:        secretName,","\t\t\tLabels:      labels,","\t\t\tAnnotations: annotations,","\t\t},","\t\tStringData: secretData,","\t}, nil","}","","func GenerateBasicAuthSecretName() string {","\treturn strings.ToLower(","\t\tfmt.Sprintf(basicAuthSecretName, random.AlphaString(ranStringSeedLen)))","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,0,2,2,1,1,0,2,2,2,2,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2]},{"id":147,"path":"pkg/secrets/secrets.go","lines":["package secrets","","import (","\t\"context\"","\t\"fmt\"","\t\"strings\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/kubeinteraction\"","\tktypes \"github.com/openshift-pipelines/pipelines-as-code/pkg/secrets/types\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"",")","","const leakedReplacement = \"*****\"","","// GetSecretsAttachedToPipelineRun get all secrets attached to a PipelineRun and","// grab their values attached to it.","func GetSecretsAttachedToPipelineRun(ctx context.Context, k kubeinteraction.Interface, pr *tektonv1.PipelineRun) []ktypes.SecretValue {","\tret := []ktypes.SecretValue{}","\t// check if pipelineRef is defined or exist","\tif pr.Spec.PipelineSpec == nil {","\t\treturn ret","\t}","","\tfor _, pt := range append(pr.Spec.PipelineSpec.Finally, pr.Spec.PipelineSpec.Tasks...) {","\t\tif pt.TaskSpec == nil || pt.TaskSpec.Steps == nil {","\t\t\tcontinue","\t\t}","\t\tfor _, step := range pt.TaskSpec.Steps {","\t\t\tfor _, ev := range step.Env {","\t\t\t\tif ev.ValueFrom == nil {","\t\t\t\t\tcontinue","\t\t\t\t}","\t\t\t\tif ev.ValueFrom.SecretKeyRef == nil {","\t\t\t\t\tcontinue","\t\t\t\t}","\t\t\t\tsecretValue, err := k.GetSecret(ctx, ktypes.GetSecretOpt{","\t\t\t\t\tName:      ev.ValueFrom.SecretKeyRef.Name,","\t\t\t\t\tKey:       ev.ValueFrom.SecretKeyRef.Key,","\t\t\t\t\tNamespace: pr.GetNamespace(),","\t\t\t\t})","\t\t\t\t// that really should not happen but let's go on and continue if that's the case","\t\t\t\tif err != nil {","\t\t\t\t\tcontinue","\t\t\t\t}","\t\t\t\tkeyv := fmt.Sprintf(\"%s-%s\", ev.ValueFrom.SecretKeyRef.Name, ev.ValueFrom.SecretKeyRef.Key)","\t\t\t\tthere := false","\t\t\t\tfor _, value := range ret {","\t\t\t\t\tif value.Name == keyv {","\t\t\t\t\t\tthere = true","\t\t\t\t\t}","\t\t\t\t}","\t\t\t\tif !there {","\t\t\t\t\tret = append(ret, ktypes.SecretValue{","\t\t\t\t\t\tName:  keyv,","\t\t\t\t\t\tValue: secretValue,","\t\t\t\t\t})","\t\t\t\t}","\t\t\t}","\t\t}","\t}","","\treturn ret","}","","// sortSecretsByLongests sort all secrets by length, the longest first","// if we don't sort by longest then if there two passwords with the same prefix","// the shortest one will replace and would leak the end of the passwords of the longest after.","func sortSecretsByLongests(values []ktypes.SecretValue) []ktypes.SecretValue {","\tret := make([]ktypes.SecretValue, len(values))","\tcopy(ret, values)","\tfor i := 0; i \u003c len(ret); i++ {","\t\tfor j := i + 1; j \u003c len(ret); j++ {","\t\t\tif len(ret[i].Value) \u003c len(ret[j].Value) {","\t\t\t\tret[i], ret[j] = ret[j], ret[i]","\t\t\t}","\t\t}","\t}","\treturn ret","}","","// ReplaceSecretsInText this will take a text snippet and hide the leaked secret.","func ReplaceSecretsInText(text string, values []ktypes.SecretValue) string {","\tfor _, sv := range sortSecretsByLongests(values) {","\t\ttext = strings.ReplaceAll(text, sv.Value, leakedReplacement)","\t}","\treturn text","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,1,1,0,2,2,2,0,2,2,2,1,0,2,2,0,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,0,2,2,2,2,2,2,0,0,0,0,2,0,0,0,0,0,2,2,2,2,2,2,2,2,0,0,2,0,0,0,2,2,2,2,2,0]},{"id":148,"path":"pkg/sort/pipelinerun.go","lines":["package sort","","import (","\t\"sort\"","","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"",")","","// From tekton cli prsort package.","type prSortByCompletionTime []tektonv1.PipelineRun","","func (prs prSortByCompletionTime) Len() int      { return len(prs) }","func (prs prSortByCompletionTime) Swap(i, j int) { prs[i], prs[j] = prs[j], prs[i] }","func (prs prSortByCompletionTime) Less(i, j int) bool {","\tif prs[j].Status.CompletionTime == nil {","\t\treturn false","\t}","\tif prs[i].Status.CompletionTime == nil {","\t\treturn true","\t}","\treturn prs[j].Status.CompletionTime.Before(prs[i].Status.CompletionTime)","}","","func PipelineRunSortByCompletionTime(items []tektonv1.PipelineRun) []tektonv1.PipelineRun {","\tsort.Sort(prSortByCompletionTime(items))","\treturn items","}","","func PipelineRunSortByStartTime(prs []tektonv1.PipelineRun) {","\tsort.Sort(byStartTime(prs))","}","","type byStartTime []tektonv1.PipelineRun","","func (prs byStartTime) Len() int      { return len(prs) }","func (prs byStartTime) Swap(i, j int) { prs[i], prs[j] = prs[j], prs[i] }","func (prs byStartTime) Less(i, j int) bool {","\tif prs[j].Status.StartTime == nil {","\t\treturn false","\t}","\tif prs[i].Status.StartTime == nil {","\t\treturn true","\t}","\t// If times are equal, sort by name descending","\tif prs[j].Status.StartTime.Time.Equal(prs[i].Status.StartTime.Time) {","\t\treturn prs[i].GetName() \u003e prs[j].GetName()","\t}","\treturn prs[j].Status.StartTime.Before(prs[i].Status.StartTime)","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,0,2,2,2,0,0,0,2,2,2,2,2,2,2,2,2,0,2,2,2,2,0]},{"id":149,"path":"pkg/sort/repository.go","lines":["package sort","","import (","\t\"sort\"","","\tpacv1alpha1 \"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"",")","","// From tekton cli prsort package.","type repositorySortByCompletionTime []pacv1alpha1.Repository","","func (repositorys repositorySortByCompletionTime) Len() int { return len(repositorys) }","func (repositorys repositorySortByCompletionTime) Swap(i, j int) {","\trepositorys[j], repositorys[i] = repositorys[i], repositorys[j]","}","","func (repositorys repositorySortByCompletionTime) Less(i, j int) bool {","\treturn repositorys[j].CreationTimestamp.After(repositorys[i].CreationTimestamp.Time)","}","","func RepositorySortByCreationOldestTime(repositorys []pacv1alpha1.Repository) {","\tsort.Sort(repositorySortByCompletionTime(repositorys))","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,2,2,2,0,2,2,2]},{"id":150,"path":"pkg/sort/repository_status.go","lines":["package sort","","import (","\t\"sort\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"",")","","type repoSortRunStatus []v1alpha1.RepositoryRunStatus","","func (rs repoSortRunStatus) Len() int {","\treturn len(rs)","}","","func (rs repoSortRunStatus) Swap(i, j int) {","\trs[i], rs[j] = rs[j], rs[i]","}","","func (rs repoSortRunStatus) Less(i, j int) bool {","\tif rs[j].StartTime == nil {","\t\treturn false","\t}","","\tif rs[i].StartTime == nil {","\t\treturn true","\t}","","\treturn rs[j].StartTime.Before(rs[i].StartTime)","}","","func RepositorySortRunStatus(repoStatus []v1alpha1.RepositoryRunStatus) []v1alpha1.RepositoryRunStatus {","\trrstatus := make(repoSortRunStatus, len(repoStatus))","\tcopy(rrstatus, repoStatus)","\tsort.Sort(rrstatus)","\treturn rrstatus","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,2,2,2,0,2,2,2,0,2,2,1,1,0,2,1,1,0,2,0,0,2,2,2,2,2,2]},{"id":151,"path":"pkg/sort/runtime_sort.go","lines":["package sort","","import (","\t\"fmt\"","\t\"reflect\"","\t\"sort\"","","\t\"github.com/fvbommel/sortorder\"","\t\"k8s.io/apimachinery/pkg/api/resource\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured\"","\t\"k8s.io/apimachinery/pkg/runtime\"","\t\"k8s.io/client-go/util/jsonpath\"","\t\"k8s.io/klog/v2\"","\t\"k8s.io/utils/integer\"",")","","// The following code has been take from kubectl get command","// instead of importing all the dependencies, copying only the required part","// https://github.com/kubernetes/kubernetes/blob/20d0ab7ae808aaddb1556c3c38ca0607663c50ac/staging/src/k8s.io/kubectl/pkg/cmd/get/sorter.go#L150","","// RuntimeSort is an implementation of the golang sort interface that knows how to sort","// lists of runtime.Object.","type RuntimeSort struct {","\tfield        string","\tobjs         []runtime.Object","\torigPosition []int","}","","// NewRuntimeSort creates a new RuntimeSort struct that implements golang sort interface.","func NewRuntimeSort(field string, objs []runtime.Object) *RuntimeSort {","\tsorter := \u0026RuntimeSort{field: field, objs: objs, origPosition: make([]int, len(objs))}","\tfor ix := range objs {","\t\tsorter.origPosition[ix] = ix","\t}","\treturn sorter","}","","func (r *RuntimeSort) Len() int {","\treturn len(r.objs)","}","","func (r *RuntimeSort) Swap(i, j int) {","\tr.objs[i], r.objs[j] = r.objs[j], r.objs[i]","\tr.origPosition[i], r.origPosition[j] = r.origPosition[j], r.origPosition[i]","}","","func isLess(i, j reflect.Value) (bool, error) {","\t//nolint","\tswitch i.Kind() {","\tcase reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64:","\t\treturn i.Int() \u003c j.Int(), nil","\tcase reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64:","\t\treturn i.Uint() \u003c j.Uint(), nil","\tcase reflect.Float32, reflect.Float64:","\t\treturn i.Float() \u003c j.Float(), nil","\tcase reflect.String:","\t\treturn sortorder.NaturalLess(i.String(), j.String()), nil","\tcase reflect.Pointer:","\t\treturn isLess(i.Elem(), j.Elem())","\tcase reflect.Struct:","\t\t// sort metav1.Time","\t\tin := i.Interface()","\t\tif t, ok := in.(metav1.Time); ok {","\t\t\ttime := j.Interface().(metav1.Time)","\t\t\treturn t.Before(\u0026time), nil","\t\t}","\t\t// sort resource.Quantity","\t\tif iQuantity, ok := in.(resource.Quantity); ok {","\t\t\tjQuantity := j.Interface().(resource.Quantity)","\t\t\treturn iQuantity.Cmp(jQuantity) \u003c 0, nil","\t\t}","\t\t// fallback to the fields comparison","\t\tfor idx := 0; idx \u003c i.NumField(); idx++ {","\t\t\tless, err := isLess(i.Field(idx), j.Field(idx))","\t\t\tif err != nil || !less {","\t\t\t\treturn less, err","\t\t\t}","\t\t}","\t\treturn true, nil","\tcase reflect.Array, reflect.Slice:","\t\t// note: the length of i and j may be different","\t\tfor idx := 0; idx \u003c integer.IntMin(i.Len(), j.Len()); idx++ {","\t\t\tless, err := isLess(i.Index(idx), j.Index(idx))","\t\t\tif err != nil || !less {","\t\t\t\treturn less, err","\t\t\t}","\t\t}","\t\treturn true, nil","\tcase reflect.Interface:","\t\t//nolint","\t\tif i.IsNil() \u0026\u0026 j.IsNil() {","\t\t\treturn false, nil","\t\t} else if i.IsNil() {","\t\t\treturn true, nil","\t\t} else if j.IsNil() {","\t\t\treturn false, nil","\t\t}","\t\tswitch itype := i.Interface().(type) {","\t\tcase uint8:","\t\t\tif jtype, ok := j.Interface().(uint8); ok {","\t\t\t\treturn itype \u003c jtype, nil","\t\t\t}","\t\tcase uint16:","\t\t\tif jtype, ok := j.Interface().(uint16); ok {","\t\t\t\treturn itype \u003c jtype, nil","\t\t\t}","\t\tcase uint32:","\t\t\tif jtype, ok := j.Interface().(uint32); ok {","\t\t\t\treturn itype \u003c jtype, nil","\t\t\t}","\t\tcase uint64:","\t\t\tif jtype, ok := j.Interface().(uint64); ok {","\t\t\t\treturn itype \u003c jtype, nil","\t\t\t}","\t\tcase int8:","\t\t\tif jtype, ok := j.Interface().(int8); ok {","\t\t\t\treturn itype \u003c jtype, nil","\t\t\t}","\t\tcase int16:","\t\t\tif jtype, ok := j.Interface().(int16); ok {","\t\t\t\treturn itype \u003c jtype, nil","\t\t\t}","\t\tcase int32:","\t\t\tif jtype, ok := j.Interface().(int32); ok {","\t\t\t\treturn itype \u003c jtype, nil","\t\t\t}","\t\tcase int64:","\t\t\tif jtype, ok := j.Interface().(int64); ok {","\t\t\t\treturn itype \u003c jtype, nil","\t\t\t}","\t\tcase uint:","\t\t\tif jtype, ok := j.Interface().(uint); ok {","\t\t\t\treturn itype \u003c jtype, nil","\t\t\t}","\t\tcase int:","\t\t\tif jtype, ok := j.Interface().(int); ok {","\t\t\t\treturn itype \u003c jtype, nil","\t\t\t}","\t\tcase float32:","\t\t\tif jtype, ok := j.Interface().(float32); ok {","\t\t\t\treturn itype \u003c jtype, nil","\t\t\t}","\t\tcase float64:","\t\t\tif jtype, ok := j.Interface().(float64); ok {","\t\t\t\treturn itype \u003c jtype, nil","\t\t\t}","\t\tcase string:","\t\t\tif jtype, ok := j.Interface().(string); ok {","\t\t\t\t// check if it's a Quantity","\t\t\t\titypeQuantity, err := resource.ParseQuantity(itype)","\t\t\t\tif err != nil {","\t\t\t\t\treturn sortorder.NaturalLess(itype, jtype), nil","\t\t\t\t}","\t\t\t\tjtypeQuantity, err := resource.ParseQuantity(jtype)","\t\t\t\tif err != nil {","\t\t\t\t\treturn sortorder.NaturalLess(itype, jtype), nil","\t\t\t\t}","\t\t\t\t// Both strings are quantity","\t\t\t\treturn itypeQuantity.Cmp(jtypeQuantity) \u003c 0, nil","\t\t\t}","\t\tdefault:","\t\t\treturn false, fmt.Errorf(\"unsortable type: %T\", itype)","\t\t}","\t\treturn false, fmt.Errorf(\"unsortable interface: %v\", i.Kind())","","\tdefault:","\t\treturn false, fmt.Errorf(\"unsortable type: %v\", i.Kind())","\t}","}","","func (r *RuntimeSort) Less(i, j int) bool {","\tiObj := r.objs[i]","\tjObj := r.objs[j]","","\tvar iValues [][]reflect.Value","\tvar jValues [][]reflect.Value","\tvar err error","","\tparser := jsonpath.New(\"sorting\").AllowMissingKeys(true)","\terr = parser.Parse(r.field)","\tif err != nil {","\t\tpanic(err)","\t}","","\tiValues, err = findJSONPathResults(parser, iObj)","\tif err != nil {","\t\tklog.Fatalf(\"Failed to get i values for %#v using %s (%#v)\", iObj, r.field, err)","\t}","","\tjValues, err = findJSONPathResults(parser, jObj)","\tif err != nil {","\t\tklog.Fatalf(\"Failed to get j values for %#v using %s (%v)\", jObj, r.field, err)","\t}","","\tif len(iValues) == 0 || len(iValues[0]) == 0 {","\t\treturn true","\t}","\tif len(jValues) == 0 || len(jValues[0]) == 0 {","\t\treturn false","\t}","\tiField := iValues[0][0]","\tjField := jValues[0][0]","","\tless, err := isLess(iField, jField)","\tif err != nil {","\t\tklog.Exitf(\"Field %s in %T is an unsortable type: %s, err: %v\", r.field, iObj, iField.Kind().String(), err)","\t}","\treturn less","}","","// OriginalPosition returns the starting (original) position of a particular index.","// e.g. If OriginalPosition(0) returns 5 than the","// item currently at position 0 was at position 5 in the original unsorted array.","func (r *RuntimeSort) OriginalPosition(ix int) int {","\tif ix \u003c 0 || ix \u003e len(r.origPosition) {","\t\treturn -1","\t}","\treturn r.origPosition[ix]","}","","func findJSONPathResults(parser *jsonpath.JSONPath, from runtime.Object) ([][]reflect.Value, error) {","\tif unstructuredObj, ok := from.(*unstructured.Unstructured); ok {","\t\treturn parser.FindResults(unstructuredObj.Object)","\t}","\treturn parser.FindResults(reflect.ValueOf(from).Elem().Interface())","}","","// ByField sorts the runtime objects passed by the field.","func ByField(field string, runTimeObj []runtime.Object) {","\tsorter := NewRuntimeSort(field, runTimeObj)","\tif len(runTimeObj) \u003e 0 {","\t\tsort.Sort(sorter)","\t}","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,0,1,1,1,0,1,1,1,1,0,2,2,2,1,1,1,1,1,1,2,2,1,1,2,2,2,2,1,1,1,0,2,2,2,2,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,1,1,0,1,0,1,1,0,0,0,2,2,2,2,2,2,2,2,2,2,2,1,0,0,2,2,1,1,0,2,2,1,1,0,2,1,1,2,1,1,2,2,2,2,2,1,1,2,0,0,0,0,0,1,1,1,1,1,0,0,2,2,1,1,2,0,0,0,1,1,1,1,1,0]},{"id":152,"path":"pkg/sort/task_log_snippets.go","lines":["package sort","","import (","\t\"sort\"","","\tpacv1alpha1 \"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"",")","","type taskInfoSorter []pacv1alpha1.TaskInfos","","func (s taskInfoSorter) Len() int { return len(s) }","","func (s taskInfoSorter) Swap(i, j int) {","\ts[i], s[j] = s[j], s[i]","}","","func (s taskInfoSorter) Less(i, j int) bool {","\tif s[i].CompletionTime.Equal(s[j].CompletionTime) {","\t\treturn s[i].Name \u003c s[j].Name","\t}","\treturn s[i].CompletionTime.Before(s[j].CompletionTime)","}","","func TaskInfos(taskinfos map[string]pacv1alpha1.TaskInfos) []pacv1alpha1.TaskInfos {","\ttis := make(taskInfoSorter, 0, len(taskinfos))","\tfor _, ti := range taskinfos {","\t\ttis = append(tis, ti)","\t}","\tsort.Sort(tis)","\treturn tis","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,2,0,2,2,2,0,2,2,2,2,2,0,0,2,2,2,2,2,2,2,0]},{"id":153,"path":"pkg/sort/task_status.go","lines":["package sort","","import (","\t\"bytes\"","\t\"fmt\"","\t\"sort\"","\t\"text/template\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/formatting\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/params/info\"","\ttektonv1 \"github.com/tektoncd/pipeline/pkg/apis/pipeline/v1\"",")","","type tkr struct {","\ttaskLogURL string","\t*tektonv1.PipelineRunTaskRunStatus","}","","func (t tkr) ConsoleLogURL() string {","\tname := t.PipelineTaskName","\tif t.Status != nil \u0026\u0026 t.Status.TaskSpec != nil \u0026\u0026 t.Status.TaskSpec.DisplayName != \"\" {","\t\tname = t.Status.TaskSpec.DisplayName","\t}","\treturn fmt.Sprintf(\"[%s](%s)\", name, t.taskLogURL)","}","","type taskrunList []tkr","","func (trs taskrunList) Len() int      { return len(trs) }","func (trs taskrunList) Swap(i, j int) { trs[i], trs[j] = trs[j], trs[i] }","func (trs taskrunList) Less(i, j int) bool {","\tif trs[j].Status == nil || trs[j].Status.StartTime == nil {","\t\treturn false","\t}","","\tif trs[i].Status == nil || trs[i].Status.StartTime == nil {","\t\treturn true","\t}","","\tif trs[i].Status.StartTime.Equal(trs[j].Status.StartTime) {","\t\treturn trs[i].PipelineTaskName \u003e trs[j].PipelineTaskName","\t}","\treturn trs[j].Status.StartTime.Before(trs[i].Status.StartTime)","}","","// TaskStatusTmpl generate a template of all status of a TaskRuns sorted to a statusTemplate as defined by the git provider.","func TaskStatusTmpl(pr *tektonv1.PipelineRun, trStatus map[string]*tektonv1.PipelineRunTaskRunStatus, runs *params.Run, config *info.ProviderConfig) (string, error) {","\ttrl := taskrunList{}","\toutputBuffer := bytes.Buffer{}","","\tif len(trStatus) == 0 {","\t\treturn \"PipelineRun has no taskruns\", nil","\t}","","\tfor _, taskrunStatus := range trStatus {","\t\ttrl = append(trl, tkr{","\t\t\ttaskLogURL:               runs.Clients.ConsoleUI().TaskLogURL(pr, taskrunStatus),","\t\t\tPipelineRunTaskRunStatus: taskrunStatus,","\t\t})","\t}","\tsort.Sort(sort.Reverse(trl))","","\tfuncMap := template.FuncMap{","\t\t\"formatDuration\":  formatting.Duration,","\t\t\"formatCondition\": formatting.ConditionEmoji,","\t}","","\tif config.SkipEmoji {","\t\tfuncMap[\"formatCondition\"] = formatting.ConditionSad","\t}","","\tdata := struct{ TaskRunList taskrunList }{TaskRunList: trl}","\tt := template.Must(template.New(\"Task Status\").Funcs(funcMap).Parse(config.TaskStatusTMPL))","\tif err := t.Execute(\u0026outputBuffer, data); err != nil {","\t\t_, _ = fmt.Fprintf(\u0026outputBuffer, \"failed to execute template: \")","\t\treturn \"\", err","\t}","","\treturn outputBuffer.String(), nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,0,0,0,2,2,2,2,1,1,0,2,1,1,0,2,2,2,2,0,0,0,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,0,2,2,2,2,2,2,0,2,0]},{"id":154,"path":"pkg/templates/templating.go","lines":["package templates","","import (","\t\"encoding/json\"","\t\"net/http\"","\t\"reflect\"","\t\"strings\"","","\t\"github.com/google/cel-go/common/types\"","\t\"github.com/google/cel-go/common/types/traits\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/keys\"","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/cel\"","\t\"google.golang.org/protobuf/encoding/protojson\"","\t\"google.golang.org/protobuf/proto\"","\t\"google.golang.org/protobuf/types/known/structpb\"",")","","var (","\tstructType = reflect.TypeOf(\u0026structpb.Value{})","\tlistType   = reflect.TypeOf(\u0026structpb.ListValue{})","\tmapType    = reflect.TypeOf(\u0026structpb.Struct{})",")","","// ReplacePlaceHoldersVariables is a function that replaces placeholders in a","// given string template with their corresponding values. The placeholders are","// expected to be in the format `{{key}}`, where `key` is the identifier for a","// value.","//","// The function first checks if the key in the placeholder has a prefix of","// \"body\", \"headers\", \"files\", or \"cel:\". If it does and both `rawEvent` and `headers`","// are not nil, it attempts to retrieve the value for the key using the","// `cel.Value` function and returns the corresponding string","// representation. The \"cel:\" prefix allows evaluating arbitrary CEL expressions","// with access to body, headers, files, and pac (standard PAC parameters) namespaces.","// If the key does not have any of the mentioned prefixes, the","// function checks if the key exists in the `dico` map. If it does, the","// function replaces the placeholder with the corresponding value from the","// `dico` map.","//","// Parameters:","//   - template (string): The input string that may contain placeholders in the","//     format `{{key}}`.","//   - dico (map[string]string): A dictionary mapping keys to their corresponding","//     string values. If a placeholder's key is found in this dictionary, it will","//     be replaced with the corresponding value.","//   - rawEvent (any): The raw event data that may be used to retrieve values for","//     placeholders with keys that have a prefix of \"body\", \"headers\", or \"files\".","//   - headers (http.Header): The HTTP headers that may be used to retrieve","//     values for placeholders with keys that have a prefix of \"headers\".","//   - changedFiles (map[string]any): A map of changed files that may be","//     used to retrieve values for placeholders with keys that have a prefix of","//     \"files\".","func ReplacePlaceHoldersVariables(template string, dico map[string]string, rawEvent any, headers http.Header, changedFiles map[string]any) string {","\treturn keys.ParamsRe.ReplaceAllStringFunc(template, func(s string) string {","\t\tparts := keys.ParamsRe.FindStringSubmatch(s)","\t\tkey := strings.TrimSpace(parts[1])","","\t\t// Check for cel: prefix first - it allows arbitrary CEL expressions","\t\tisCelExpr := strings.HasPrefix(key, \"cel:\")","","\t\tif strings.HasPrefix(key, \"body\") || strings.HasPrefix(key, \"headers\") || strings.HasPrefix(key, \"files\") || isCelExpr {","\t\t\t// Check specific requirements for each prefix","\t\t\tcanEvaluate := false","\t\t\tcelExpr := key","\t\t\tswitch {","\t\t\tcase isCelExpr:","\t\t\t\tcanEvaluate = true","\t\t\t\tcelExpr = strings.TrimSpace(strings.TrimPrefix(key, \"cel:\"))","\t\t\tcase strings.HasPrefix(key, \"body\") \u0026\u0026 rawEvent != nil:","\t\t\t\tcanEvaluate = true","\t\t\tcase strings.HasPrefix(key, \"headers\") \u0026\u0026 headers != nil:","\t\t\t\tcanEvaluate = true","\t\t\tcase strings.HasPrefix(key, \"files\"):","\t\t\t\tcanEvaluate = true // files evaluation doesn't depend on rawEvent or headers","\t\t\t}","","\t\t\tif canEvaluate {","\t\t\t\t// convert headers to map[string]string","\t\t\t\theaderMap := make(map[string]string)","\t\t\t\tfor k, v := range headers {","\t\t\t\t\theaderMap[k] = v[0]","\t\t\t\t}","\t\t\t\t// For cel: prefix, pass dico as pacParams so pac.* variables are available","\t\t\t\tpacParams := map[string]string{}","\t\t\t\tif isCelExpr {","\t\t\t\t\tpacParams = dico","\t\t\t\t}","\t\t\t\tval, err := cel.Value(celExpr, rawEvent, headerMap, pacParams, changedFiles)","\t\t\t\tif err != nil {","\t\t\t\t\t// For cel: prefix, return empty string on error","\t\t\t\t\tif isCelExpr {","\t\t\t\t\t\treturn \"\"","\t\t\t\t\t}","\t\t\t\t\treturn s","\t\t\t\t}","\t\t\t\tvar raw any","\t\t\t\tvar b []byte","","\t\t\t\tswitch val.(type) {","\t\t\t\tcase types.String:","\t\t\t\t\tif v, ok := val.Value().(string); ok {","\t\t\t\t\t\tb = []byte(v)","\t\t\t\t\t}","","\t\t\t\tcase types.Bytes, types.Double, types.Int:","\t\t\t\t\traw, err = val.ConvertToNative(structType)","\t\t\t\t\tif err == nil {","\t\t\t\t\t\tif structVal, ok := raw.(*structpb.Value); ok {","\t\t\t\t\t\t\tb, err = structVal.MarshalJSON()","\t\t\t\t\t\t\tif err != nil {","\t\t\t\t\t\t\t\tb = []byte{}","\t\t\t\t\t\t\t}","\t\t\t\t\t\t}","\t\t\t\t\t}","","\t\t\t\tcase traits.Lister:","\t\t\t\t\traw, err = val.ConvertToNative(listType)","\t\t\t\t\tif err == nil {","\t\t\t\t\t\tif msg, ok := raw.(proto.Message); ok {","\t\t\t\t\t\t\tb, err = protojson.Marshal(msg)","\t\t\t\t\t\t\tif err != nil {","\t\t\t\t\t\t\t\tb = []byte{}","\t\t\t\t\t\t\t}","\t\t\t\t\t\t}","\t\t\t\t\t}","","\t\t\t\tcase traits.Mapper:","\t\t\t\t\traw, err = val.ConvertToNative(mapType)","\t\t\t\t\tif err == nil {","\t\t\t\t\t\tif msg, ok := raw.(proto.Message); ok {","\t\t\t\t\t\t\tb, err = protojson.Marshal(msg)","\t\t\t\t\t\t\tif err != nil {","\t\t\t\t\t\t\t\tb = []byte{}","\t\t\t\t\t\t\t}","\t\t\t\t\t\t}","\t\t\t\t\t}","","\t\t\t\tcase types.Bool:","\t\t\t\t\traw, err = val.ConvertToNative(structType)","\t\t\t\t\tif err == nil {","\t\t\t\t\t\tif structVal, ok := raw.(*structpb.Value); ok {","\t\t\t\t\t\t\tb, err = json.Marshal(structVal.GetBoolValue())","\t\t\t\t\t\t\tif err != nil {","\t\t\t\t\t\t\t\tb = []byte{}","\t\t\t\t\t\t\t}","\t\t\t\t\t\t}","\t\t\t\t\t}","","\t\t\t\tdefault:","\t\t\t\t\traw, err = val.ConvertToNative(reflect.TypeOf([]byte{}))","\t\t\t\t\tif err == nil {","\t\t\t\t\t\tif v, ok := raw.([]byte); ok {","\t\t\t\t\t\t\tb = v","\t\t\t\t\t\t}","\t\t\t\t\t}","\t\t\t\t}","\t\t\t\treturn string(b)","\t\t\t}","\t\t\treturn s","\t\t}","\t\tif _, ok := dico[key]; !ok {","\t\t\treturn s","\t\t}","\t\treturn dico[key]","\t})","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,1,1,0,0,0,2,2,2,2,2,2,1,1,0,0,0,2,2,2,2,2,2,1,1,0,0,0,2,2,2,2,2,2,1,1,0,0,0,2,2,2,1,1,1,0,0,2,0,2,0,2,2,2,2,0,0]},{"id":155,"path":"pkg/webhook/controller.go","lines":["package webhook","","import (","\t\"context\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/generated/injection/informers/pipelinesascode/v1alpha1/repository\"","\t\"k8s.io/apimachinery/pkg/types\"","\t\"k8s.io/client-go/tools/cache\"","\tkubeclient \"knative.dev/pkg/client/injection/kube/client\"","\tvwhinformer \"knative.dev/pkg/client/injection/kube/informers/admissionregistration/v1/validatingwebhookconfiguration\"","\t\"knative.dev/pkg/controller\"","\tsecretinformer \"knative.dev/pkg/injection/clients/namespacedkube/informers/core/v1/secret\"","\t\"knative.dev/pkg/logging\"","\tpkgreconciler \"knative.dev/pkg/reconciler\"","\t\"knative.dev/pkg/system\"","\t\"knative.dev/pkg/webhook\"",")","","// NewAdmissionController constructs a reconciler.","func NewAdmissionController(","\tctx context.Context,","\tname, path string,","\twc func(context.Context) context.Context,","\tdisallowUnknownFields bool,",") *controller.Impl {","\tclient := kubeclient.Get(ctx)","\tvwhInformer := vwhinformer.Get(ctx)","\tsecretInformer := secretinformer.Get(ctx)","\toptions := webhook.GetOptions(ctx)","\trepositoryInformer := repository.Get(ctx)","","\tkey := types.NamespacedName{Name: name}","","\twh := \u0026reconciler{","\t\tLeaderAwareFuncs: pkgreconciler.LeaderAwareFuncs{","\t\t\t// Have this reconciler enqueue our singleton whenever it becomes leader.","\t\t\tPromoteFunc: func(bkt pkgreconciler.Bucket, enq func(pkgreconciler.Bucket, types.NamespacedName)) error {","\t\t\t\tenq(bkt, key)","\t\t\t\treturn nil","\t\t\t},","\t\t},","","\t\tkey:  key,","\t\tpath: path,","","\t\twithContext:           wc,","\t\tdisallowUnknownFields: disallowUnknownFields,","\t\tsecretName:            options.SecretName,","","\t\tclient:       client,","\t\tvwhlister:    vwhInformer.Lister(),","\t\tsecretlister: secretInformer.Lister(),","","\t\tpacLister: repositoryInformer.Lister(),","\t}","","\tlogger := logging.FromContext(ctx)","\tc := controller.NewContext(ctx, wh, controller.ControllerOptions{WorkQueueName: \"ValidationWebhook\", Logger: logger.Named(\"ValidationWebhook\")})","","\t// Reconcile when the named ValidatingWebhookConfiguration changes.","\tif _, err := vwhInformer.Informer().AddEventHandler(cache.FilteringResourceEventHandler{","\t\tFilterFunc: controller.FilterWithName(name),","\t\t// It doesn't matter what we enqueue because we will always Reconcile","\t\t// the named VWH resource.","\t\tHandler: controller.HandleAll(c.Enqueue),","\t}); err != nil {","\t\tlogging.FromContext(ctx).Panicf(\"Couldn't register ValidatingWebhookConfiguration informer event handler: %w\", err)","\t}","","\t// Reconcile when the cert bundle changes.","\tif _, err := secretInformer.Informer().AddEventHandler(cache.FilteringResourceEventHandler{","\t\tFilterFunc: controller.FilterWithNameAndNamespace(system.Namespace(), wh.secretName),","\t\t// It doesn't matter what we enqueue because we will always Reconcile","\t\t// the named MWH resource.","\t\tHandler: controller.HandleAll(c.Enqueue),","\t}); err != nil {","\t\tlogging.FromContext(ctx).Panicf(\"Couldn't register Secret informer event handler: %w\", err)","\t}","","\treturn c","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,0,1,0]},{"id":156,"path":"pkg/webhook/reconciler.go","lines":["package webhook","","import (","\t\"context\"","\t\"fmt\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode\"","\tpac \"github.com/openshift-pipelines/pipelines-as-code/pkg/generated/listers/pipelinesascode/v1alpha1\"","\t\"go.uber.org/zap\"","\tadmissionregistrationv1 \"k8s.io/api/admissionregistration/v1\"","\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"","\t\"k8s.io/apimachinery/pkg/types\"","\t\"k8s.io/client-go/kubernetes\"","\tadmissionlisters \"k8s.io/client-go/listers/admissionregistration/v1\"","\tcorelisters \"k8s.io/client-go/listers/core/v1\"","\t\"knative.dev/pkg/controller\"","\t\"knative.dev/pkg/kmp\"","\t\"knative.dev/pkg/logging\"","\t\"knative.dev/pkg/ptr\"","\tpkgreconciler \"knative.dev/pkg/reconciler\"","\t\"knative.dev/pkg/system\"","\t\"knative.dev/pkg/webhook\"","\tcertresources \"knative.dev/pkg/webhook/certificates/resources\"",")","","type reconciler struct {","\twebhook.StatelessAdmissionImpl","\tpkgreconciler.LeaderAwareFuncs","","\tkey  types.NamespacedName","\tpath string","","\twithContext func(context.Context) context.Context","","\tclient       kubernetes.Interface","\tvwhlister    admissionlisters.ValidatingWebhookConfigurationLister","\tsecretlister corelisters.SecretLister","","\tdisallowUnknownFields bool","\tsecretName            string","","\tpacLister pac.RepositoryLister","}","","var (","\t_ controller.Reconciler                = (*reconciler)(nil)","\t_ pkgreconciler.LeaderAware            = (*reconciler)(nil)","\t_ webhook.AdmissionController          = (*reconciler)(nil)","\t_ webhook.StatelessAdmissionController = (*reconciler)(nil)",")","","// Reconcile implements controller.Reconciler.","func (ac *reconciler) Reconcile(ctx context.Context, _ string) error {","\tlogger := logging.FromContext(ctx)","","\tif !ac.IsLeaderFor(ac.key) {","\t\tlogger.Debugf(\"Skipping key %q, not the leader.\", ac.key)","\t\treturn nil","\t}","","\t// Look up the webhook secret, and fetch the CA cert bundle.","\tsecret, err := ac.secretlister.Secrets(system.Namespace()).Get(ac.secretName)","\tif err != nil {","\t\tlogger.Errorw(\"Error fetching secret\", zap.Error(err))","\t\treturn err","\t}","\tcaCert, ok := secret.Data[certresources.CACert]","\tif !ok {","\t\treturn fmt.Errorf(\"secret %q is missing %q key\", ac.secretName, certresources.CACert)","\t}","","\t// Reconcile the webhook configuration.","\treturn ac.reconcileValidatingWebhook(ctx, caCert)","}","","func (ac *reconciler) reconcileValidatingWebhook(ctx context.Context, caCert []byte) error {","\tlogger := logging.FromContext(ctx)","","\trules := []admissionregistrationv1.RuleWithOperations{","\t\t{","\t\t\tOperations: []admissionregistrationv1.OperationType{","\t\t\t\tadmissionregistrationv1.Create,","\t\t\t\tadmissionregistrationv1.Update,","\t\t\t},","\t\t\tRule: admissionregistrationv1.Rule{","\t\t\t\tAPIGroups:   []string{pipelinesascode.GroupName},","\t\t\t\tAPIVersions: []string{\"v1alpha1\"},","\t\t\t\tResources:   []string{\"repositories\", \"repositories\" + \"/status\"},","\t\t\t},","\t\t},","\t}","","\tconfiguredWebhook, err := ac.vwhlister.Get(ac.key.Name)","\tif err != nil {","\t\treturn fmt.Errorf(\"error retrieving webhook: %w\", err)","\t}","","\twebhook := configuredWebhook.DeepCopy()","","\t// Clear out any previous (bad) OwnerReferences.","\t// See: https://github.com/knative/serving/issues/5845","\twebhook.OwnerReferences = nil","","\tfor i, wh := range webhook.Webhooks {","\t\tif wh.Name != webhook.Name {","\t\t\tcontinue","\t\t}","\t\twebhook.Webhooks[i].Rules = rules","\t\twebhook.Webhooks[i].ClientConfig.CABundle = caCert","\t\tif webhook.Webhooks[i].ClientConfig.Service == nil {","\t\t\treturn fmt.Errorf(\"missing service reference for webhook: %s\", wh.Name)","\t\t}","\t\twebhook.Webhooks[i].ClientConfig.Service.Path = ptr.String(ac.Path())","\t}","","\tok, err := kmp.SafeEqual(configuredWebhook, webhook)","\tif err != nil {","\t\treturn fmt.Errorf(\"error diffing webhooks: %w\", err)","\t}","\tif !ok {","\t\tlogger.Info(\"Updating webhook\")","\t\tmwhclient := ac.client.AdmissionregistrationV1().ValidatingWebhookConfigurations()","\t\tif _, err := mwhclient.Update(ctx, webhook, metav1.UpdateOptions{}); err != nil {","\t\t\treturn fmt.Errorf(\"failed to update webhook: %w\", err)","\t\t}","\t} else {","\t\tlogger.Info(\"Webhook is valid\")","\t}","\treturn nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,2,0,0,1,1,1,1,1,1,1,1,1,0,0,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0]},{"id":157,"path":"pkg/webhook/validation.go","lines":["package webhook","","import (","\t\"context\"","\t\"net/url\"","\t\"os\"","","\t\"github.com/openshift-pipelines/pipelines-as-code/pkg/apis/pipelinesascode/v1alpha1\"","\tpac \"github.com/openshift-pipelines/pipelines-as-code/pkg/generated/listers/pipelinesascode/v1alpha1\"","\tv1 \"k8s.io/api/admission/v1\"","\t\"k8s.io/apimachinery/pkg/labels\"","\t\"k8s.io/apimachinery/pkg/runtime\"","\t\"k8s.io/apimachinery/pkg/runtime/serializer\"","\t\"k8s.io/apimachinery/pkg/util/sets\"","\t\"knative.dev/pkg/webhook\"",")","","var universalDeserializer = serializer.NewCodecFactory(runtime.NewScheme()).UniversalDeserializer()","","var allowedGitlabDisableCommentStrategyOnMr = sets.NewString(\"\", \"disable_all\")","","// Path implements AdmissionController.","func (ac *reconciler) Path() string {","\treturn ac.path","}","","// Admit implements AdmissionController.","func (ac *reconciler) Admit(_ context.Context, request *v1.AdmissionRequest) *v1.AdmissionResponse {","\traw := request.Object.Raw","\trepo := v1alpha1.Repository{}","\tif _, _, err := universalDeserializer.Decode(raw, nil, \u0026repo); err != nil {","\t\treturn webhook.MakeErrorStatus(\"validation failed: %v\", err)","\t}","","\t// Check that if we have a URL set only for non global repository which can be set as empty.","\tif repo.GetNamespace() != os.Getenv(\"SYSTEM_NAMESPACE\") {","\t\tif repo.Spec.URL == \"\" {","\t\t\treturn webhook.MakeErrorStatus(\"URL must be set\")","\t\t}","","\t\tparsed, err := url.Parse(repo.Spec.URL)","\t\tif err != nil {","\t\t\treturn webhook.MakeErrorStatus(\"invalid URL format: %v\", err)","\t\t}","","\t\tif parsed.Scheme != \"http\" \u0026\u0026 parsed.Scheme != \"https\" {","\t\t\treturn webhook.MakeErrorStatus(\"URL scheme must be http or https\")","\t\t}","\t}","","\texist, err := checkIfRepoExist(ac.pacLister, \u0026repo, \"\")","\tif err != nil {","\t\treturn webhook.MakeErrorStatus(\"validation failed: %v\", err)","\t}","","\tif exist {","\t\treturn webhook.MakeErrorStatus(\"repository already exists with URL: %s\", repo.Spec.URL)","\t}","","\tif repo.Spec.ConcurrencyLimit != nil \u0026\u0026 *repo.Spec.ConcurrencyLimit == 0 {","\t\treturn webhook.MakeErrorStatus(\"concurrency limit must be greater than 0\")","\t}","","\tif repo.Spec.Settings != nil \u0026\u0026 repo.Spec.Settings.Gitlab != nil {","\t\tif !allowedGitlabDisableCommentStrategyOnMr.Has(repo.Spec.Settings.Gitlab.CommentStrategy) {","\t\t\treturn webhook.MakeErrorStatus(\"comment strategy '%s' is not supported for Gitlab MRs\", repo.Spec.Settings.Gitlab.CommentStrategy)","\t\t}","\t}","","\treturn \u0026v1.AdmissionResponse{Allowed: true}","}","","func checkIfRepoExist(pac pac.RepositoryLister, repo *v1alpha1.Repository, ns string) (bool, error) {","\trepositories, err := pac.Repositories(ns).List(labels.NewSelector())","\tif err != nil {","\t\treturn false, err","\t}","\tfor i := len(repositories) - 1; i \u003e= 0; i-- {","\t\trepoFromCluster := repositories[i]","\t\tif repoFromCluster.Spec.URL == repo.Spec.URL \u0026\u0026","\t\t\t(repoFromCluster.Name != repo.Name || repoFromCluster.Namespace != repo.Namespace) {","\t\t\treturn true, nil","\t\t}","\t}","\treturn false, nil","}"],"coverage":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,2,2,2,2,1,1,0,0,2,2,1,1,0,2,2,2,2,0,2,2,2,0,0,2,2,1,1,0,2,2,2,0,2,1,1,0,2,1,1,1,0,0,2,0,0,2,2,2,1,1,2,2,2,2,2,2,0,2,0]}],"tree":{"name":".","type":"dir","children":[{"name":"pkg","type":"dir","children":[{"name":"acl","type":"dir","children":[{"name":"owners.go","type":"file","fileId":0},{"name":"regexp.go","type":"file","fileId":1}]},{"name":"action","type":"dir","children":[{"name":"patch.go","type":"file","fileId":2}]},{"name":"adapter","type":"dir","children":[{"name":"adapter.go","type":"file","fileId":3},{"name":"incoming.go","type":"file","fileId":4},{"name":"sinker.go","type":"file","fileId":5},{"name":"tls.go","type":"file","fileId":6}]},{"name":"apis","type":"dir","children":[{"name":"incoming","type":"dir","children":[{"name":"incoming.go","type":"file","fileId":7}]},{"name":"pipelinesascode","type":"dir","children":[{"name":"v1alpha1","type":"dir","children":[{"name":"register.go","type":"file","fileId":8},{"name":"types.go","type":"file","fileId":9},{"name":"types_llm.go","type":"file","fileId":10}]}]}]},{"name":"cel","type":"dir","children":[{"name":"cel.go","type":"file","fileId":11}]},{"name":"changedfiles","type":"dir","children":[{"name":"changedfiles.go","type":"file","fileId":12}]},{"name":"cli","type":"dir","children":[{"name":"browser","type":"dir","children":[{"name":"browser.go","type":"file","fileId":13}]},{"name":"info","type":"dir","children":[{"name":"configmap.go","type":"file","fileId":16}]},{"name":"prompt","type":"dir","children":[{"name":"select_repo.go","type":"file","fileId":18},{"name":"stubber.go","type":"file","fileId":19}]},{"name":"status","type":"dir","children":[{"name":"status.go","type":"file","fileId":20}]},{"name":"webhook","type":"dir","children":[{"name":"bitbucket_cloud.go","type":"file","fileId":21},{"name":"github.go","type":"file","fileId":22},{"name":"gitlab.go","type":"file","fileId":23},{"name":"secret.go","type":"file","fileId":24},{"name":"webhook.go","type":"file","fileId":25}]},{"name":"cli.go","type":"file","fileId":14},{"name":"color.go","type":"file","fileId":15},{"name":"iostreams.go","type":"file","fileId":17}]},{"name":"configutil","type":"dir","children":[{"name":"config.go","type":"file","fileId":26}]},{"name":"consoleui","type":"dir","children":[{"name":"custom.go","type":"file","fileId":27},{"name":"interface.go","type":"file","fileId":28},{"name":"openshift.go","type":"file","fileId":29},{"name":"tektondashboard.go","type":"file","fileId":30}]},{"name":"customparams","type":"dir","children":[{"name":"customparams.go","type":"file","fileId":31},{"name":"standard.go","type":"file","fileId":32}]},{"name":"events","type":"dir","children":[{"name":"emit.go","type":"file","fileId":33}]},{"name":"formatting","type":"dir","children":[{"name":"age.go","type":"file","fileId":34},{"name":"array.go","type":"file","fileId":35},{"name":"emoji.go","type":"file","fileId":36},{"name":"k8labels.go","type":"file","fileId":37},{"name":"k8names.go","type":"file","fileId":38},{"name":"pipelinerun.go","type":"file","fileId":39},{"name":"repository.go","type":"file","fileId":40},{"name":"starting.go","type":"file","fileId":41},{"name":"vcs.go","type":"file","fileId":42}]},{"name":"git","type":"dir","children":[{"name":"git.go","type":"file","fileId":43}]},{"name":"hub","type":"dir","children":[{"name":"artifacthub.go","type":"file","fileId":44},{"name":"get.go","type":"file","fileId":45},{"name":"hub.go","type":"file","fileId":46},{"name":"tektonhub.go","type":"file","fileId":47}]},{"name":"kubeinteraction","type":"dir","children":[{"name":"status","type":"dir","children":[{"name":"task_status.go","type":"file","fileId":55}]},{"name":"cleanups.go","type":"file","fileId":48},{"name":"events.go","type":"file","fileId":49},{"name":"kubeinteraction.go","type":"file","fileId":50},{"name":"labels.go","type":"file","fileId":51},{"name":"pod_logs.go","type":"file","fileId":52},{"name":"resultsannotation.go","type":"file","fileId":53},{"name":"secrets.go","type":"file","fileId":54},{"name":"wait.go","type":"file","fileId":56}]},{"name":"llm","type":"dir","children":[{"name":"context","type":"dir","children":[{"name":"assembler.go","type":"file","fileId":58}]},{"name":"ltypes","type":"dir","children":[{"name":"types.go","type":"file","fileId":60}]},{"name":"providers","type":"dir","children":[{"name":"gemini","type":"dir","children":[{"name":"client.go","type":"file","fileId":64}]},{"name":"openai","type":"dir","children":[{"name":"client.go","type":"file","fileId":65}]},{"name":"common.go","type":"file","fileId":63}]},{"name":"analyzer.go","type":"file","fileId":57},{"name":"factory.go","type":"file","fileId":59},{"name":"orchestrator.go","type":"file","fileId":61},{"name":"output_handler.go","type":"file","fileId":62}]},{"name":"matcher","type":"dir","children":[{"name":"annotation_matcher.go","type":"file","fileId":66},{"name":"annotation_tasks_install.go","type":"file","fileId":67},{"name":"cel.go","type":"file","fileId":68},{"name":"cel_ast.go","type":"file","fileId":69},{"name":"errors.go","type":"file","fileId":70},{"name":"repo_runinfo_matcher.go","type":"file","fileId":71}]},{"name":"opscomments","type":"dir","children":[{"name":"args.go","type":"file","fileId":72},{"name":"comments.go","type":"file","fileId":73}]},{"name":"params","type":"dir","children":[{"name":"clients","type":"dir","children":[{"name":"clients.go","type":"file","fileId":75}]},{"name":"info","type":"dir","children":[{"name":"controller_info.go","type":"file","fileId":77},{"name":"events.go","type":"file","fileId":78},{"name":"info.go","type":"file","fileId":79},{"name":"kube.go","type":"file","fileId":80},{"name":"ns.go","type":"file","fileId":81},{"name":"pac.go","type":"file","fileId":82}]},{"name":"settings","type":"dir","children":[{"name":"config.go","type":"file","fileId":85},{"name":"default.go","type":"file","fileId":86}]},{"name":"triggertype","type":"dir","children":[{"name":"types.go","type":"file","fileId":87}]},{"name":"cli.go","type":"file","fileId":74},{"name":"config_sync.go","type":"file","fileId":76},{"name":"install.go","type":"file","fileId":83},{"name":"run.go","type":"file","fileId":84}]},{"name":"pipelineascode","type":"dir","children":[{"name":"cancel_pipelineruns.go","type":"file","fileId":88},{"name":"client_setup.go","type":"file","fileId":89},{"name":"concurrency.go","type":"file","fileId":90},{"name":"errors.go","type":"file","fileId":91},{"name":"logging.go","type":"file","fileId":92},{"name":"match.go","type":"file","fileId":93},{"name":"pipelineascode.go","type":"file","fileId":94},{"name":"secret.go","type":"file","fileId":95},{"name":"template.go","type":"file","fileId":96}]},{"name":"pipelinerunmetrics","type":"dir","children":[{"name":"injection.go","type":"file","fileId":97},{"name":"metrics.go","type":"file","fileId":98}]},{"name":"policy","type":"dir","children":[{"name":"policy.go","type":"file","fileId":99}]},{"name":"provider","type":"dir","children":[{"name":"bitbucketcloud","type":"dir","children":[{"name":"acl.go","type":"file","fileId":100},{"name":"bitbucket.go","type":"file","fileId":101},{"name":"detect.go","type":"file","fileId":102},{"name":"parse_payload.go","type":"file","fileId":103}]},{"name":"bitbucketdatacenter","type":"dir","children":[{"name":"acl.go","type":"file","fileId":104},{"name":"bitbucketdatacenter.go","type":"file","fileId":105},{"name":"detect.go","type":"file","fileId":106},{"name":"parse_payload.go","type":"file","fileId":107}]},{"name":"gitea","type":"dir","children":[{"name":"forgejostructs","type":"dir","children":[{"name":"types.go","type":"file","fileId":110}]},{"name":"acl.go","type":"file","fileId":108},{"name":"detect.go","type":"file","fileId":109},{"name":"gitea.go","type":"file","fileId":111},{"name":"parse_payload.go","type":"file","fileId":112},{"name":"webhook.go","type":"file","fileId":113}]},{"name":"github","type":"dir","children":[{"name":"app","type":"dir","children":[{"name":"token.go","type":"file","fileId":115}]},{"name":"acl.go","type":"file","fileId":114},{"name":"detect.go","type":"file","fileId":116},{"name":"github.go","type":"file","fileId":117},{"name":"parse_payload.go","type":"file","fileId":118},{"name":"profiler.go","type":"file","fileId":119},{"name":"repository.go","type":"file","fileId":120},{"name":"scope.go","type":"file","fileId":121},{"name":"status.go","type":"file","fileId":122}]},{"name":"gitlab","type":"dir","children":[{"name":"acl.go","type":"file","fileId":123},{"name":"detect.go","type":"file","fileId":124},{"name":"gitlab.go","type":"file","fileId":125},{"name":"parse_payload.go","type":"file","fileId":126},{"name":"task.go","type":"file","fileId":127}]},{"name":"providermetrics","type":"dir","children":[{"name":"metrics.go","type":"file","fileId":129}]},{"name":"provider.go","type":"file","fileId":128}]},{"name":"queue","type":"dir","children":[{"name":"priority_queue.go","type":"file","fileId":130},{"name":"queue_manager.go","type":"file","fileId":131},{"name":"queue_manager_interface.go","type":"file","fileId":132},{"name":"semaphore.go","type":"file","fileId":133}]},{"name":"random","type":"dir","children":[{"name":"random.go","type":"file","fileId":134}]},{"name":"reconciler","type":"dir","children":[{"name":"cleanup.go","type":"file","fileId":135},{"name":"controller.go","type":"file","fileId":136},{"name":"emit_metrics.go","type":"file","fileId":137},{"name":"event.go","type":"file","fileId":138},{"name":"finalizer.go","type":"file","fileId":139},{"name":"queue_pipelineruns.go","type":"file","fileId":140},{"name":"reconciler.go","type":"file","fileId":141},{"name":"status.go","type":"file","fileId":142}]},{"name":"resolve","type":"dir","children":[{"name":"logging.go","type":"file","fileId":143},{"name":"remote.go","type":"file","fileId":144},{"name":"resolve.go","type":"file","fileId":145}]},{"name":"secrets","type":"dir","children":[{"name":"basic_auth.go","type":"file","fileId":146},{"name":"secrets.go","type":"file","fileId":147}]},{"name":"sort","type":"dir","children":[{"name":"pipelinerun.go","type":"file","fileId":148},{"name":"repository.go","type":"file","fileId":149},{"name":"repository_status.go","type":"file","fileId":150},{"name":"runtime_sort.go","type":"file","fileId":151},{"name":"task_log_snippets.go","type":"file","fileId":152},{"name":"task_status.go","type":"file","fileId":153}]},{"name":"templates","type":"dir","children":[{"name":"templating.go","type":"file","fileId":154}]},{"name":"webhook","type":"dir","children":[{"name":"controller.go","type":"file","fileId":155},{"name":"reconciler.go","type":"file","fileId":156},{"name":"validation.go","type":"file","fileId":157}]}]}]},"summary":{"totalLines":14575,"coveredLines":10701,"percent":73.42024013722127}};
    </script>
    <script>
      window.COVERAGE_CONFIG = {"syntaxEnabled":true};
    </script>
    <script>
      (function() {
  'use strict';

  const data = window.COVERAGE_DATA;
  const config = window.COVERAGE_CONFIG || { syntaxEnabled: true };

  // State
  let currentFileId = null;
  let searchQuery = '';
  let contentSearchQuery = '';
  let matches = [];
  let currentMatchIndex = -1;
  let expandedDirs = new Set();
  let syntaxHighlightEnabled = config.syntaxEnabled;
  let sortMode = 'name'; // 'name' or 'coverage'
  let anchorLine = null;        // First line clicked (anchor for shift-select)
  let selectedRange = null;     // { start: N, end: M } or null

  // DOM elements
  const fileTree = document.getElementById('file-tree');
  const viewport = document.getElementById('viewport');
  const filePath = document.getElementById('file-path');
  const summary = document.getElementById('summary');
  const searchInput = document.getElementById('search-input');
  const contentSearch = document.getElementById('content-search');
  const matchInfo = document.getElementById('match-info');
  const prevMatch = document.getElementById('prev-match');
  const nextMatch = document.getElementById('next-match');
  const themeToggle = document.getElementById('theme-toggle');
  const syntaxToggle = document.getElementById('syntax-toggle');
  const helpModal = document.getElementById('help-modal');
  const closeHelp = document.getElementById('close-help');
  const helpToggle = document.getElementById('help-toggle');

  // Coverage cache: fileId -> percentage
  let coverageCache = new Map();

  function initCoverageCache() {
    data.files.forEach((file, idx) => {
      coverageCache.set(idx, calculateFileCoverage(idx));
    });
  }

  function calculateFileCoverage(fileId) {
    const file = data.files[fileId];
    let totalStatements = 0;
    let coveredStatements = 0;

    file.coverage.forEach(cov => {
      if (cov > 0) totalStatements++;
      if (cov === 2) coveredStatements++;
    });

    return totalStatements === 0 ? 0 : (coveredStatements / totalStatements) * 100;
  }

  function calculateDirectoryCoverage(node) {
    if (node.type === 'file') {
      return coverageCache.get(node.fileId) || 0;
    }

    let totalCoverage = 0;
    let fileCount = 0;

    node.children?.forEach(child => {
      const childCov = calculateDirectoryCoverage(child);
      totalCoverage += childCov;
      fileCount++;
    });

    return fileCount === 0 ? 0 : totalCoverage / fileCount;
  }

  function sortTreeNodes(node, mode) {
    if (!node.children || node.children.length === 0) return node;

    // Deep copy to avoid mutating original
    const sorted = { ...node };
    sorted.children = [...node.children].map(child => sortTreeNodes(child, mode));

    // Sort children
    sorted.children.sort((a, b) => {
      // Directories always first
      if (a.type !== b.type) return a.type === 'dir' ? -1 : 1;

      if (mode === 'coverage') {
        const aCov = calculateDirectoryCoverage(a);
        const bCov = calculateDirectoryCoverage(b);
        console.log('Sorting:', a.name, '('+aCov.toFixed(1)+'%) vs', b.name, '('+bCov.toFixed(1)+'%)', '=', bCov - aCov);
        // Descending: high coverage first
        return aCov !== bCov ? bCov - aCov : a.name.localeCompare(b.name);
      }

      return a.name.localeCompare(b.name);
    });

    return sorted;
  }

  // Initialize
  function init() {
    initCoverageCache();
    loadSortPreference();
    renderSummary();
    renderTree();
    setupEventListeners();
    loadTheme();
    loadSyntaxPreference();

    // Check for deep link hash first, otherwise select first file
    if (!navigateToHash() && data.files.length > 0) {
      selectFile(0);
    }

    // Listen for hash changes (browser back/forward)
    window.addEventListener('hashchange', navigateToHash);
  }

  // Deep linking: parse URL hash
  function parseHash() {
    const hash = window.location.hash.slice(1);
    if (!hash) return null;

    const match = hash.match(/^file-(\d+)(?::line-(\d+)(?:-(\d+))?)?$/);
    if (!match) return null;

    return {
      fileId: parseInt(match[1], 10),
      lineStart: match[2] ? parseInt(match[2], 10) : null,
      lineEnd: match[3] ? parseInt(match[3], 10) : null
    };
  }

  // Deep linking: navigate to hash location
  function navigateToHash() {
    const target = parseHash();
    if (!target) return false;

    if (target.fileId < 0 || target.fileId >= data.files.length) return false;

    selectFile(target.fileId);

    if (target.lineStart) {
      requestAnimationFrame(() => {
        const lineEnd = target.lineEnd || target.lineStart;
        anchorLine = target.lineStart;
        selectedRange = { start: target.lineStart, end: lineEnd };
        selectLineRange(target.lineStart, lineEnd);
        scrollToLine(target.lineStart);
      });
    }

    return true;
  }

  // Deep linking: scroll to and highlight a line
  function scrollToLine(lineNum) {
    const lineEl = document.querySelector('.code-line[data-line="' + lineNum + '"]');
    if (!lineEl) return;

    lineEl.scrollIntoView({ behavior: 'smooth', block: 'center' });
  }

  // Clear all selected lines
  function clearLineSelection() {
    document.querySelectorAll('.code-line.selected-line').forEach(el => {
      el.classList.remove('selected-line');
    });
  }

  // Select a range of lines (inclusive)
  function selectLineRange(start, end) {
    clearLineSelection();
    const minLine = Math.min(start, end);
    const maxLine = Math.max(start, end);
    for (let i = minLine; i <= maxLine; i++) {
      const lineEl = document.querySelector('.code-line[data-line="' + i + '"]');
      if (lineEl) {
        lineEl.classList.add('selected-line');
      }
    }
  }

  // Deep linking: update URL hash
  function updateHash(fileId, lineStart, lineEnd) {
    let hash = 'file-' + fileId;
    if (lineStart) {
      hash += ':line-' + lineStart;
      if (lineEnd && lineEnd !== lineStart) {
        // Normalise so start < end
        const minLine = Math.min(lineStart, lineEnd);
        const maxLine = Math.max(lineStart, lineEnd);
        hash = 'file-' + fileId + ':line-' + minLine + '-' + maxLine;
      }
    }
    history.replaceState(null, '', '#' + hash);
  }

  function renderSummary() {
    // Build summary safely using DOM methods
    summary.textContent = '';
    const span = document.createElement('span');
    span.className = 'percent';
    span.textContent = data.summary.percent.toFixed(1) + '%';
    summary.appendChild(span);
    summary.appendChild(document.createTextNode(
      ' coverage (' + data.summary.coveredLines + '/' + data.summary.totalLines + ' lines)'
    ));
  }

  function renderTree() {
    fileTree.textContent = '';
    // Auto-expand all top-level directories
    if (data.tree.children && data.tree.children.length > 0) {
      data.tree.children.forEach(child => {
        if (child.type === 'dir') {
          expandedDirs.add(getNodePath(child, 0));
        }
      });
    }
    const sortedTree = sortTreeNodes(data.tree, sortMode);
    renderNode(sortedTree, fileTree, 0);
  }

  function renderNode(node, container, depth) {
    if (node.name === '.' && node.type === 'dir') {
      // Root node, render children directly
      node.children.forEach(child => renderNode(child, container, depth));
      return;
    }

    const nodeEl = document.createElement('div');
    nodeEl.className = 'tree-node';
    nodeEl.dataset.name = node.name.toLowerCase();

    const item = document.createElement('div');
    item.className = 'tree-item';
    item.style.setProperty('--depth', depth);

    const icon = document.createElement('span');
    icon.className = 'icon';

    const name = document.createElement('span');
    name.className = 'name';
    name.textContent = node.name;

    if (node.type === 'dir') {
      const dirPath = getNodePath(node, depth);
      icon.textContent = expandedDirs.has(dirPath) ? '\u25BC' : '\u25B6';
      if (expandedDirs.has(dirPath)) {
        nodeEl.classList.add('expanded');
      }

      item.addEventListener('click', (e) => {
        e.stopPropagation();
        toggleDir(nodeEl, dirPath, icon);
      });

      item.appendChild(icon);
      item.appendChild(name);

      // Add coverage badge to all directories
      const badge = document.createElement('span');
      badge.className = 'coverage-badge';
      badge.textContent = calculateDirectoryCoverage(node).toFixed(1) + '%';
      item.appendChild(badge);

      nodeEl.appendChild(item);

      if (node.children && node.children.length > 0) {
        const children = document.createElement('div');
        children.className = 'tree-children';
        node.children.forEach(child => renderNode(child, children, depth + 1));
        nodeEl.appendChild(children);
      }
    } else {
      icon.textContent = '\uD83D\uDCC4';
      nodeEl.dataset.fileId = node.fileId;

      item.addEventListener('click', (e) => {
        e.stopPropagation();
        selectFile(node.fileId);
      });

      item.appendChild(icon);
      item.appendChild(name);

      // Add coverage badge to files
      const badge = document.createElement('span');
      badge.className = 'coverage-badge';
      badge.textContent = calculateDirectoryCoverage(node).toFixed(1) + '%';
      item.appendChild(badge);

      nodeEl.appendChild(item);
    }

    container.appendChild(nodeEl);
  }

  function getNodePath(node, depth) {
    return node.name + '_' + depth;
  }

  function toggleDir(nodeEl, path, icon) {
    if (nodeEl.classList.contains('expanded')) {
      nodeEl.classList.remove('expanded');
      expandedDirs.delete(path);
      icon.textContent = '\u25B6';
    } else {
      nodeEl.classList.add('expanded');
      expandedDirs.add(path);
      icon.textContent = '\u25BC';
    }
  }

  function selectFile(fileId) {
    currentFileId = fileId;
    matches = [];
    currentMatchIndex = -1;
    matchInfo.textContent = '';
    contentSearch.value = '';
    contentSearchQuery = '';
    anchorLine = null;
    selectedRange = null;

    // Update selection in tree
    document.querySelectorAll('.tree-item.selected').forEach(el => {
      el.classList.remove('selected');
    });
    const selected = document.querySelector('[data-file-id="' + fileId + '"] .tree-item');
    if (selected) {
      selected.classList.add('selected');
    }

    const file = data.files[fileId];
    if (!file) return;

    filePath.textContent = file.path;
    renderCode(file);

    // Update URL hash for deep linking
    updateHash(fileId, null);
  }

  function renderCode(file) {
    viewport.textContent = '';

    if (!file.lines || file.lines.length === 0) {
      const empty = document.createElement('div');
      empty.className = 'empty-state';
      const iconDiv = document.createElement('div');
      iconDiv.className = 'icon';
      iconDiv.textContent = '\uD83D\uDCED';
      const textDiv = document.createElement('div');
      textDiv.textContent = 'No content';
      empty.appendChild(iconDiv);
      empty.appendChild(textDiv);
      viewport.appendChild(empty);
      return;
    }

    const container = document.createElement('div');
    container.className = 'code-container';

    file.lines.forEach((line, idx) => {
      const lineEl = document.createElement('div');
      lineEl.className = 'code-line';
      lineEl.dataset.line = idx + 1;

      const cov = file.coverage[idx];
      if (cov === 2) {
        lineEl.classList.add('covered');
      } else if (cov === 1) {
        lineEl.classList.add('uncovered');
      }

      const gutter = document.createElement('div');
      gutter.className = 'gutter';

      const lineNum = document.createElement('div');
      lineNum.className = 'line-number';
      lineNum.textContent = idx + 1;
      lineNum.title = 'Click to select line, Shift+Click for range';

      // Add click handler for line number deep linking
      const lineNumber = idx + 1;
      lineNum.addEventListener('click', (e) => {
        e.stopPropagation();

        if (e.shiftKey && anchorLine !== null) {
          // Shift-click: select range from anchor to clicked line
          const start = Math.min(anchorLine, lineNumber);
          const end = Math.max(anchorLine, lineNumber);
          selectedRange = { start: start, end: end };
          selectLineRange(start, end);
          updateHash(currentFileId, start, end);
        } else {
          // Regular click: set anchor and select single line
          anchorLine = lineNumber;
          selectedRange = { start: lineNumber, end: lineNumber };
          selectLineRange(lineNumber, lineNumber);
          updateHash(currentFileId, lineNumber, null);
        }
      });

      const content = document.createElement('div');
      content.className = 'line-content';
      content.textContent = line || ' ';

      lineEl.appendChild(gutter);
      lineEl.appendChild(lineNum);
      lineEl.appendChild(content);
      container.appendChild(lineEl);
    });

    viewport.appendChild(container);

    // Apply syntax highlighting after rendering if enabled
    if (syntaxHighlightEnabled) {
      applySyntaxHighlighting();
    }
  }

  function setupEventListeners() {
    // File search
    let searchTimeout;
    searchInput.addEventListener('input', (e) => {
      clearTimeout(searchTimeout);
      searchTimeout = setTimeout(() => {
        searchQuery = e.target.value.toLowerCase();
        filterTree();
      }, 300);
    });

    // Content search
    let contentTimeout;
    contentSearch.addEventListener('input', (e) => {
      clearTimeout(contentTimeout);
      contentTimeout = setTimeout(() => {
        contentSearchQuery = e.target.value;
        searchInFile();
      }, 300);
    });

    contentSearch.addEventListener('keydown', (e) => {
      if (e.key === 'Enter') {
        if (e.shiftKey) {
          goToPrevMatch();
        } else {
          goToNextMatch();
        }
      }
    });

    prevMatch.addEventListener('click', goToPrevMatch);
    nextMatch.addEventListener('click', goToNextMatch);

    // Theme toggle
    themeToggle.addEventListener('click', toggleTheme);

    // Syntax toggle
    syntaxToggle.addEventListener('click', toggleSyntax);

    // Sort controls
    const sortButtons = document.querySelectorAll('.sort-btn');
    console.log('Found', sortButtons.length, 'sort buttons');
    sortButtons.forEach(btn => {
      console.log('Attaching click handler to button:', btn.dataset.sort);
      btn.addEventListener('click', () => {
        console.log('Sort button clicked:', btn.dataset.sort);
        changeSortMode(btn.dataset.sort);
      });
    });

    // Keyboard shortcuts
    document.addEventListener('keydown', (e) => {
      if ((e.ctrlKey || e.metaKey) && e.key === 'f' && currentFileId !== null) {
        e.preventDefault();
        contentSearch.focus();
      }
      if ((e.ctrlKey || e.metaKey) && e.key === 'p') {
        e.preventDefault();
        searchInput.focus();
      }
      // Help modal
      if (e.key === '?' && !e.ctrlKey && !e.metaKey) {
        e.preventDefault();
        showHelp();
      }
      if (e.key === 'Escape') {
        // Exit search if focused
        if (document.activeElement === searchInput) {
          searchInput.value = '';
          searchQuery = '';
          filterTree();
          searchInput.blur();
          viewport.focus();
          return;
        }
        if (document.activeElement === contentSearch) {
          contentSearch.value = '';
          contentSearchQuery = '';
          matchInfo.textContent = '';
          matches = [];
          currentMatchIndex = -1;
          if (currentFileId !== null) {
            renderCode(data.files[currentFileId]);
          }
          contentSearch.blur();
          viewport.focus();
          return;
        }
        hideHelp();
      }
    });

    closeHelp.addEventListener('click', hideHelp);
    helpToggle.addEventListener('click', showHelp);
    helpModal.addEventListener('click', (e) => {
      if (e.target === helpModal) hideHelp();
    });
  }

  function filterTree() {
    const nodes = document.querySelectorAll('.tree-node');

    if (!searchQuery) {
      nodes.forEach(n => n.classList.remove('hidden'));
      return;
    }

    nodes.forEach(node => {
      const name = node.dataset.name || '';
      const fileId = node.dataset.fileId;

      if (fileId !== undefined) {
        const file = data.files[parseInt(fileId)];
        const matchesQuery = file && file.path.toLowerCase().includes(searchQuery);
        node.classList.toggle('hidden', !matchesQuery);
      } else {
        const hasVisibleChild = Array.from(node.querySelectorAll('[data-file-id]')).some(f => {
          const fid = parseInt(f.dataset.fileId);
          const file = data.files[fid];
          return file && file.path.toLowerCase().includes(searchQuery);
        });
        node.classList.toggle('hidden', !hasVisibleChild);
        if (hasVisibleChild && searchQuery) {
          node.classList.add('expanded');
          const icon = node.querySelector('.icon');
          if (icon && icon.textContent === '\u25B6') {
            icon.textContent = '\u25BC';
          }
        }
      }
    });
  }

  function searchInFile() {
    matches = [];
    currentMatchIndex = -1;

    // Re-render code to clear highlights
    if (currentFileId !== null) {
      const file = data.files[currentFileId];
      if (file) {
        renderCode(file);
      }
    }

    if (!contentSearchQuery || currentFileId === null) {
      matchInfo.textContent = '';
      return;
    }

    const file = data.files[currentFileId];
    if (!file) return;

    const query = contentSearchQuery.toLowerCase();

    file.lines.forEach((line, idx) => {
      const text = line || '';
      const lowerText = text.toLowerCase();
      let pos = 0;
      let matchIndex;

      while ((matchIndex = lowerText.indexOf(query, pos)) !== -1) {
        matches.push({ line: idx, start: matchIndex, length: query.length });
        pos = matchIndex + 1;
      }
    });

    if (matches.length > 0) {
      highlightMatches();
      currentMatchIndex = 0;
      scrollToMatch(0);
      updateMatchInfo();
    } else {
      matchInfo.textContent = 'No matches';
    }
  }

  function highlightMatches() {
    const file = data.files[currentFileId];
    if (!file) return;

    const lineEls = document.querySelectorAll('.code-line');

    // Group matches by line
    const matchesByLine = {};
    matches.forEach((m, idx) => {
      if (!matchesByLine[m.line]) matchesByLine[m.line] = [];
      matchesByLine[m.line].push({ ...m, idx });
    });

    Object.keys(matchesByLine).forEach(lineIdx => {
      const lineEl = lineEls[parseInt(lineIdx)];
      if (!lineEl) return;

      const content = lineEl.querySelector('.line-content');
      if (!content) return;

      const text = file.lines[parseInt(lineIdx)] || '';
      const lineMatches = matchesByLine[lineIdx].sort((a, b) => a.start - b.start);

      // Build content using DOM nodes for safety
      content.textContent = '';
      let lastEnd = 0;

      lineMatches.forEach(m => {
        // Text before match
        if (m.start > lastEnd) {
          content.appendChild(document.createTextNode(text.substring(lastEnd, m.start)));
        }
        // Match span
        const span = document.createElement('span');
        span.className = 'match-highlight';
        span.dataset.matchIdx = m.idx;
        span.textContent = text.substring(m.start, m.start + m.length);
        content.appendChild(span);
        lastEnd = m.start + m.length;
      });

      // Text after last match
      if (lastEnd < text.length) {
        content.appendChild(document.createTextNode(text.substring(lastEnd)));
      }

      // Handle empty line
      if (content.childNodes.length === 0) {
        content.textContent = ' ';
      }
    });
  }

  function scrollToMatch(idx) {
    document.querySelectorAll('.current-match').forEach(el => {
      el.classList.remove('current-match');
    });

    const matchEl = document.querySelector('[data-match-idx="' + idx + '"]');
    if (matchEl) {
      matchEl.classList.add('current-match');
      matchEl.scrollIntoView({ behavior: 'smooth', block: 'center' });
    }
  }

  function updateMatchInfo() {
    if (matches.length === 0) {
      matchInfo.textContent = 'No matches';
    } else {
      matchInfo.textContent = (currentMatchIndex + 1) + '/' + matches.length;
    }
  }

  function goToNextMatch() {
    if (matches.length === 0) return;
    currentMatchIndex = (currentMatchIndex + 1) % matches.length;
    scrollToMatch(currentMatchIndex);
    updateMatchInfo();
  }

  function goToPrevMatch() {
    if (matches.length === 0) return;
    currentMatchIndex = (currentMatchIndex - 1 + matches.length) % matches.length;
    scrollToMatch(currentMatchIndex);
    updateMatchInfo();
  }

  function toggleTheme() {
    const body = document.body;
    const current = body.dataset.theme;
    const next = current === 'dark' ? 'light' : 'dark';
    body.dataset.theme = next;
    localStorage.setItem('coverage-theme', next);
  }

  function loadTheme() {
    const saved = localStorage.getItem('coverage-theme');
    if (saved) {
      document.body.dataset.theme = saved;
    }
  }

  function applySyntaxHighlighting() {
    if (!syntaxHighlightEnabled || currentFileId === null) return;
    if (typeof hljs === 'undefined') return;

    const file = data.files[currentFileId];
    if (!file) return;

    const lineEls = document.querySelectorAll('.code-line');

    lineEls.forEach((lineEl, idx) => {
      const cov = file.coverage[idx];
      // Only highlight lines with no coverage info
      if (cov !== 0) return;

      const content = lineEl.querySelector('.line-content');
      if (!content || !content.textContent.trim()) return;

      const text = content.textContent;

      // Use hljs.highlight() which returns result object
      const result = hljs.highlight(text, { language: 'go' });

      // Parse the highlighted HTML safely using DOMParser
      const parser = new DOMParser();
      const doc = parser.parseFromString('<div>' + result.value + '</div>', 'text/html');
      const wrapper = doc.body.firstChild;

      // Clear and append parsed nodes
      content.textContent = '';
      while (wrapper.firstChild) {
        content.appendChild(wrapper.firstChild);
      }
    });
  }

  function toggleSyntax() {
    syntaxHighlightEnabled = !syntaxHighlightEnabled;
    syntaxToggle.classList.toggle('active', syntaxHighlightEnabled);
    localStorage.setItem('coverage-syntax', syntaxHighlightEnabled ? 'on' : 'off');

    // Re-render current file
    if (currentFileId !== null) {
      const file = data.files[currentFileId];
      if (file) {
        renderCode(file);
      }
    }
  }

  function loadSyntaxPreference() {
    const saved = localStorage.getItem('coverage-syntax');
    if (saved !== null) {
      // User preference overrides default
      syntaxHighlightEnabled = saved === 'on';
    }
    // Update button state
    syntaxToggle.classList.toggle('active', syntaxHighlightEnabled);
  }

  function changeSortMode(mode) {
    if (sortMode === mode) return;

    console.log('Changing sort mode from', sortMode, 'to', mode);
    sortMode = mode;
    localStorage.setItem('coverage-sort-mode', mode);

    // Update button states
    document.querySelectorAll('.sort-btn').forEach(btn => {
      btn.classList.toggle('active', btn.dataset.sort === mode);
    });

    // Re-render tree
    renderTree();
  }

  function loadSortPreference() {
    const saved = localStorage.getItem('coverage-sort-mode');
    if (saved && (saved === 'name' || saved === 'coverage')) {
      sortMode = saved;
    }

    // Update button states
    document.querySelectorAll('.sort-btn').forEach(btn => {
      btn.classList.toggle('active', btn.dataset.sort === sortMode);
    });
  }

  function showHelp() {
    helpModal.classList.remove('hidden');
  }

  function hideHelp() {
    helpModal.classList.add('hidden');
  }

  // Start the app
  init();
})();

    </script>
  </body>
</html>
